{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b7bd79d",
   "metadata": {},
   "source": [
    "# Tutorial 1: VrsDataProvider Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19aaad07",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "[Aria Gen2](https://www.projectaria.com/) glasses are Meta’s dedicated research tool in an always-on glasses form factor. The data recorded by Aria Gen2 glasses are stored in VRS files, where each VRS file captures time-synchronized data streams from various sensors, such cameras, IMUs, audio, and more.\n",
    "The **VrsDataProvider** interface provides a unified way to access multimodal sensor data from these VRS files. \n",
    "\n",
    "**What you'll learn:**\n",
    "\n",
    "- How to create a `VrsDataProvider` from a VRS file.  \n",
    "- Discover available sensor data streams, and check their configurations  \n",
    "- Retrieve data using either sequential (index-based) or temporal (timestamp-based) access APIs.   \n",
    "- Learn about timing domains and time query options\n",
    "\n",
    "**Prerequisites:** Basic Python knowledge and a general understanding of multimodal sensor data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012c10ab",
   "metadata": {},
   "source": [
    "## Basic File Loading\n",
    "\n",
    "The `create_vrs_data_provider($FILE_PATH)` factory function will create **a \\`vrs\\_data\\_provider\\` object**. This object is **your entry point** for accessing VRS data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1986265",
   "metadata": {},
   "outputs": [],
   "source": [
    "from projectaria_tools.core import data_provider\n",
    "\n",
    "# Load local VRS file\n",
    "vrs_file_path = \"path/to/your/recording.vrs\"\n",
    "vrs_data_provider = data_provider.create_vrs_data_provider(vrs_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f76322b",
   "metadata": {},
   "source": [
    "## Stream Discovery and Navigation\n",
    "\n",
    "### Understanding Stream IDs\n",
    "\n",
    "A VRS file contains multiple **streams**, each storing data from a specific sensor or on-device algorithm result. \n",
    "\n",
    "Each VRS stream is identified by a unique **`StreamId`** (e.g. `1201-1`), consisting `RecordableTypeId` (sensor type, e.g. `1201`, standing for “SLAM camera”), and an `instance_id` (for multiple sensors of the same type, e.g. `-1`, standing for “instance \\#1 of this sensor type”). Below are some common `RecordableTypeId` in Aria recordings. Full definitions of all Recordable Types are given in this wiki page (TODO: Add website page), or refer to [the \\`StreamId.h\\` file in the VRS repo](https://github.com/facebookresearch/vrs/blob/main/vrs/StreamId.h#L49). \n",
    "\n",
    "| RecordableTypeId | Description |\n",
    "| :---- | :---- |\n",
    "| 214 | RGB camera stream |\n",
    "| 1201 | SLAM camera stream |\n",
    "| 211 | EyeTracking camera stream |\n",
    "| 1202 | IMU sensor stream |\n",
    "| 231 | Audio sensor stream |\n",
    "| 373 | EyeGaze data stream from on-device EyeTracking algorithm. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7febeebe",
   "metadata": {},
   "source": [
    "### Query StreamId By Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8692f4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all available streams\n",
    "all_streams = vrs_data_provider.get_all_streams()\n",
    "print(f\"Found {len(all_streams)} streams in the VRS file:\")\n",
    "\n",
    "# Print out each stream id, and their corresponding sensor label\n",
    "for stream_id in all_streams:\n",
    "    label = vrs_data_provider.get_label_from_stream_id(stream_id)\n",
    "    print(f\" --- Data stream {stream_id}'s label is: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e639cf9b-2e93-4a6d-a399-8c37f2742bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find a specific stream's StreamId by sensor label\n",
    "print(\"Seeking RGB data stream...\")\n",
    "rgb_stream_id = vrs_data_provider.get_stream_id_from_label(\"camera-rgb\")\n",
    "if rgb_stream_id is not None:\n",
    "    print(f\"Found camera-rgb stream in VRS file: {rgb_stream_id}\")\n",
    "else:\n",
    "    print(\"Cannot find camera-rgb stream in VRS file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c81d455",
   "metadata": {},
   "source": [
    "## Sensor Data Query APIs\n",
    "\n",
    "1. ### Query by index\n",
    "The query-by-index API allows you to retrieve the k-th data sample from a specific stream using the following syntax:\n",
    "```python\n",
    "get_${SENSOR}_data_by_index(stream_id, index)\n",
    "```\n",
    "where `${SENSOR}` can be replaced by any sensor data type available in Aria VRS. See here for [a full list of supported ${SENSOR}](https://github.com/facebookresearch/projectaria_tools/blob/main/core/data_provider/VrsDataProvider.h#L377)\n",
    "\n",
    "This API is commonly used for sequential processing—such as iterating through all frames of a single stream—or when you know the exact frame number you want to query within a specific stream.\n",
    "\n",
    "**Important Note**:\n",
    "The indices in each stream are independent and not correlated across different sensor streams. For example, the i-th RGB image does not necessarily correspond to the i-th SLAM image. This is because different sensors may operate at different frequencies or have missing frames, so their data streams are not synchronized by index.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f77cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize with Rerun\n",
    "import rerun as rr\n",
    "rr.init(\"rerun_viz_query_by_index\")\n",
    "\n",
    "# Get number of samples in stream\n",
    "num_samples = vrs_data_provider.get_num_data(rgb_stream_id)\n",
    "print(f\"RGB stream has a total of {num_samples} frames\\n\")\n",
    "\n",
    "# Access frames sequentially, and plot the first few frames\n",
    "first_few = min(10, num_samples)\n",
    "print(f\"Printing the capture timestamps from the first {first_few} frames\")\n",
    "for i in range(first_few):  # First 10 frames\n",
    "    image_data, image_record = vrs_data_provider.get_image_data_by_index(\n",
    "        rgb_stream_id, i\n",
    "    )\n",
    "\n",
    "    # Access image properties\n",
    "    timestamp_ns = image_record.capture_timestamp_ns\n",
    "    print(f\"Frame {i}: timestamp = {timestamp_ns}\")\n",
    "\n",
    "    # Process image data\n",
    "    if image_data.is_valid():\n",
    "        rr.set_time(\"device_time\", duration = timestamp_ns * 1e-9)\n",
    "        rr.log(\"camera_rgb\", rr.Image(image_data.to_numpy_array()))\n",
    "\n",
    "rr.notebook_show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a4756b",
   "metadata": {},
   "source": [
    "2. ### Query by Timestamp: TimeDomain and TimeQueryOptions\n",
    "A key feature of Aria devices is the ability to capture time-synchronized, multi-modal sensor data. To help you access this data with precise temporal control, `projectaria_tools` provides a comprehensive suite of time-based APIs.\n",
    "\n",
    "The most commonly used is the **timestamp-based query**:\n",
    "\n",
    "```python\n",
    "get_${SENSOR}_by_time_ns(stream_id, time_ns, time_domain=None, time_query_options=None)\n",
    "```\n",
    "where `${SENSOR}` can be replaced by any sensor data type available in Aria VRS. See here for [a full list of supported ${SENSOR}](https://github.com/facebookresearch/projectaria_tools/blob/main/core/data_provider/VrsDataProvider.h#L377)\n",
    "\n",
    "This API is often used to synchronize data across multiple sensor streams, fetch sensor data at specific timestamps, or perform temporal analysis.\n",
    "\n",
    "#### TimeDomain and TimeQueryOptions\n",
    "\n",
    "When querying sensor data by timestamp, two important concepts are:\n",
    "\n",
    "- **TimeDomain**: Specifies the time reference for your query.\n",
    "- **TimeQueryOptions**: Controls how the API selects data relative to your requested timestamp.\n",
    "\n",
    "Below are all available options for each:\n",
    "\n",
    "#### TimeDomain Options\n",
    "\n",
    "| Name        | Description                                                                                      | Typical Use Case                        |\n",
    "|-------------|--------------------------------------------------------------------------------------------------|-----------------------------------------|\n",
    "| RECORD_TIME  | Timestamp stored directly in the VRS index. Fast access, but time domain may vary.               | Quick access, not recommended for sync. |\n",
    "| DEVICE_TIME  | Accurate device capture time. All sensors on the same Aria device share this domain.             | **Recommended for single-device data.** |\n",
    "| HOST_TIME    | Arrival time in the host computer’s domain. May not be accurate.                                 | Debugging, host-side analysis.          |\n",
    "| TIME_CODE    | [Aria-Gen1 only] TimeSync server’s domain using external time-code devices, accurate across devices in multi-device capture.                       | Multi-device synchronization.           |\n",
    "| TIC_SYNC     | [Aria-Gen1 only] TimeSync server’s domain using tic-sync, accurate for multi-device capture.      | Multi-device synchronization.           |\n",
    "| SubGhz      | [Aria-Gen2 only] TimeSync server’s domain using SubGhz signals, accurate for multi-device capture.           | Multi-device synchronization.           |\n",
    "| Utc         | UTC time domain, only seconds-level accuracy.                                                    | Coarse, global time reference.          |\n",
    "\n",
    "#### TimeQueryOptions\n",
    "\n",
    "| Name    | Description                                                                                                 |\n",
    "|---------|-------------------------------------------------------------------------------------------------------------|\n",
    "| Before  | Returns the last valid data with `timestamp <= t_query`.                                                    |\n",
    "| After   | Returns the first valid data with `timestamp >= t_query`.                                                   |\n",
    "| Closest | Returns the data sample closest to `t_query`. If two are equally close, returns the one **before** the query.   |\n",
    "\n",
    "> For detailed usage and best practices—especially for time-sync across multiple devices—see **Tutorial_6**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831df531",
   "metadata": {},
   "outputs": [],
   "source": [
    "from projectaria_tools.core.sensor_data import TimeDomain, TimeQueryOptions\n",
    "\n",
    "rr.init(\"rerun_viz_query_by_timestamp\")\n",
    "\n",
    "# Get time bounds for RGB images\n",
    "first_timestamp_ns = vrs_data_provider.get_first_time_ns(rgb_stream_id, TimeDomain.DEVICE_TIME)\n",
    "last_timestamp_ns = vrs_data_provider.get_last_time_ns(rgb_stream_id, TimeDomain.DEVICE_TIME)\n",
    "\n",
    "# Query specific timestamp\n",
    "target_time_ns = first_timestamp_ns + 1000000000  # 1 second later\n",
    "image_data, image_record = vrs_data_provider.get_image_data_by_time_ns(\n",
    "    rgb_stream_id,\n",
    "    target_time_ns,\n",
    "    TimeDomain.DEVICE_TIME,\n",
    "    TimeQueryOptions.CLOSEST\n",
    ")\n",
    "\n",
    "actual_time_ns = image_record.capture_timestamp_ns\n",
    "print(f\"Requested RGB data that is closest to: {target_time_ns} ns, Got closest sample at: {actual_time_ns} ns\")\n",
    "\n",
    "# Plot RGB and SLAM images at approx 1 hz\n",
    "camera_label_list = [\"camera-rgb\", \"slam-front-left\", \"slam-front-right\", \"slam-side-left\", \"slam-side-right\"]\n",
    "camera_stream_ids = [vrs_data_provider.get_stream_id_from_label(camera_label) for camera_label in camera_label_list]\n",
    "\n",
    "query_timestamp_ns = first_timestamp_ns\n",
    "for _ in range(10):\n",
    "    for label, stream_id in zip(camera_label_list, camera_stream_ids):\n",
    "        # Query each camera's data according to query timestamp\n",
    "        image_data, image_record = vrs_data_provider.get_image_data_by_time_ns(\n",
    "            stream_id,\n",
    "            query_timestamp_ns,\n",
    "            TimeDomain.DEVICE_TIME,\n",
    "            TimeQueryOptions.CLOSEST)\n",
    "        # note that the actual timestamp of the image data is stored within image_record. It can be different from query_time.\n",
    "        capture_time_ns = image_record.capture_timestamp_ns\n",
    "\n",
    "        # Plot to ReRun\n",
    "        if image_data.is_valid():\n",
    "            rr.set_time(\"device_time\", duration = capture_time_ns * 1e-9)\n",
    "            rr.log(label, rr.Image(image_data.to_numpy_array()))\n",
    "\n",
    "    query_timestamp_ns = query_timestamp_ns + int(1e9) # 1 second\n",
    "\n",
    "rr.notebook_show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ab571d-018f-4047-b857-7269463ec8fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
