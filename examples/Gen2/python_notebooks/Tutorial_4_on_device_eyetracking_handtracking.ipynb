{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5567afff",
   "metadata": {},
   "source": [
    "# Tutorial 4: On-Device Eye-tracking and Hand-tracking data streams\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In Aria-Gen2 glasses, one of the key upgrade from Aria-Gen1 is the capability to run Machine Perception (MP) algorithms on the device during streaming / recording. Currently supported on-device MP algorithms include Eye-tracking, Hand-tracking, and VIO. These algorithm results are stored as separate data streams in the VRS file. \n",
    "\n",
    "This tutorial focuses on demonstration of how to use the **Eye-tracking and Hand-tracking** results. \n",
    "\n",
    "**What you'll learn:**\n",
    "\n",
    "- How to access on-device EyeGaze and HandTracking data from VRS files\n",
    "- Understanding the concept of interpolated hand tracking and why interpolation\n",
    "  is needed\n",
    "- How to visualize EyeGaze and HandTracking data projected onto 2D camera images\n",
    "  using DeviceCalibration\n",
    "- How to match MP data with camera frames using timestamps\n",
    "\n",
    "**Prerequisites**\n",
    "- Complete Tutorial 1 (VrsDataProvider Basics) to understand basic data provider concepts\n",
    "- Complete Tutorial 2 (Device Calibration) to understand how to properly use calibration in Aria data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79330336",
   "metadata": {},
   "outputs": [],
   "source": [
    "from projectaria_tools.core import data_provider\n",
    "\n",
    "# Load local VRS file\n",
    "vrs_file_path = \"path/to/your/recording.vrs\"\n",
    "vrs_data_provider = data_provider.create_vrs_data_provider(vrs_file_path)\n",
    "\n",
    "\n",
    "# Query EyeGaze data streams\n",
    "eyegaze_label = \"eyegaze\"\n",
    "eyegaze_stream_id = vrs_data_provider.get_stream_id_from_label(eyegaze_label)\n",
    "if eyegaze_stream_id is None:\n",
    "    raise RuntimeError(\n",
    "        f\"{eyegaze_label} data stream does not exist! Please use a VRS that contains valid eyegaze data for this tutorial.\"\n",
    "    )\n",
    "\n",
    "# Query HandTracking data streams\n",
    "handtracking_label = \"handtracking\"\n",
    "handtracking_stream_id = vrs_data_provider.get_stream_id_from_label(handtracking_label)\n",
    "if handtracking_stream_id is None:\n",
    "    raise RuntimeError(\n",
    "        f\"{handtracking_label} data stream does not exist! Please use a VRS that contains valid handtracking data for this tutorial.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506003eb",
   "metadata": {},
   "source": [
    "## On-Device Eye-tracking results\n",
    "### EyeGaze Data Structure\n",
    "\n",
    "The EyeGaze data type represents on-device eye tracking results. \n",
    "**Importantly, it directly reuses [the EyeGaze data structure](https://github.com/facebookresearch/projectaria_tools/blob/main/core/mps/EyeGaze.h)\n",
    " from MPS (Machine Perception Services)**, providing guaranteed compatibility across VRS and MPS.\n",
    "\n",
    "**Key `EyeGaze` fields**\n",
    "\n",
    "| Field Name                    |  Description                                                             |\n",
    "| :---------------------------- | :---------------------------------------------------------------------- |\n",
    "| `session_uid`                 |  Unique ID for the eyetracking session                                   |\n",
    "| `tracking_timestamp`          |  Timestamp of the eye tracking camera frame in device time domain, in us.       |\n",
    "| `vergence.t[x,y,z]_[left,right]_eye`          |  Translation for each eye origin in CPF frame       |\n",
    "| `yaw`,`vergence.[left,right]_yaw`        | Eye gaze yaw angle (horizontal) in radians in CPF frame                 |\n",
    "| `pitch`,`vergence.[left,right]_pitch`(Gen2-only)                       | Eye gaze pitch angle (vertical) in radians in CPF frame. The left and right pitch are assumed to be the same in Aria-Gen1.    |\n",
    "| `depth`                       | Depth in meters of the 3D eye gaze point in CPF frame (0 = unavailable) |\n",
    "| `yaw_low`,`yaw_high`,`pitch_low`,`pitch_high`  | Confidence interval bounds for yaw and pitch angle                                |\n",
    "| **Aria-Gen2 specific fields**                                            | \n",
    "| `combined_gaze_origin_in_cpf` | Combined gaze origin in CPF frame (Gen2 only)                           |\n",
    "| `spatial_gaze_point_in_cpf`   | 3D spatial gaze point in CPF frame                                      |\n",
    "| `vergence.[left,right]_entrance_pupil_position_meter`   |  Entrance pupil positions for each eye                                      |\n",
    "| `vergence.[left,right]_pupil_diameter_meter`   |  Entrance pupil diameter for each eye                                      |\n",
    "| `vergence.[left,right]_blink`   | Blink detection for left and right eyes                                      |\n",
    "| `*_valid`    | Boolean flags to indicating if the corresponding data field in EyeGaze is valid                          |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### EyeGaze API Reference\n",
    "In `vrs_data_provider`, EyeGaze is treated the same way as any other sensor data, and share similar query APIs covered in `Tutorial_1_vrs_data_provider_basics`: \n",
    "- `vrs_data_provider.get_eye_gaze_data_by_index(stream_id, index)`: Query by index. \n",
    "- `vrs_data_provider.get_eye_gaze_data_by_time_ns(stream_id, timestamp, time_domain, query_options)`: Query by timestamp. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50413a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from projectaria_tools.core.mps import get_unit_vector_from_yaw_pitch\n",
    "from datetime import timedelta\n",
    "\n",
    "print(\"=== EyeGaze Data Sample ===\")\n",
    "num_eyegaze_samples = vrs_data_provider.get_num_data(eyegaze_stream_id)\n",
    "selected_index = min(5, num_eyegaze_samples)\n",
    "print(f\"Sample {selected_index}:\")\n",
    "\n",
    "eyegaze_data = vrs_data_provider.get_eye_gaze_data_by_index(eyegaze_stream_id, selected_index)\n",
    "\n",
    "# Eyegaze timestamp is in format of datetime.deltatime in microseconds, convert it to integer\n",
    "eyegaze_timestamp_ns = (eyegaze_data.tracking_timestamp // timedelta(microseconds=1)) * 1000\n",
    "print(f\"  Tracking timestamp: {eyegaze_timestamp_ns}\")\n",
    "\n",
    "# check if combined gaze is valid, if so, print out the gaze direction\n",
    "print(f\"  Combined gaze valid: {eyegaze_data.combined_gaze_valid}\")\n",
    "if eyegaze_data.combined_gaze_valid:\n",
    "    print(f\"  Yaw: {eyegaze_data.yaw:.3f} rad\")\n",
    "    print(f\"  Pitch: {eyegaze_data.pitch:.3f} rad\")\n",
    "    print(f\"  Depth: {eyegaze_data.depth:.3f} m\")\n",
    "    # Can also print gaze direction in unit vector\n",
    "    gaze_direction_in_unit_vec = get_unit_vector_from_yaw_pitch(eyegaze_data.yaw, eyegaze_data.pitch)\n",
    "    print(f\"  Gaze direction in unit vec [xyz]: {gaze_direction_in_unit_vec}\")\n",
    "\n",
    "# Check if spatial gaze point is valid, if so, print out the spatial gaze point\n",
    "print(\n",
    "    f\"  Spatial gaze point valid: {eyegaze_data.spatial_gaze_point_valid}\"\n",
    ")\n",
    "if eyegaze_data.spatial_gaze_point_valid:\n",
    "    print(\n",
    "        f\"  Spatial gaze point in CPF: {eyegaze_data.spatial_gaze_point_in_cpf}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fe8239",
   "metadata": {},
   "source": [
    "### EyeGaze visualization in camera images\n",
    "To visualize EyeGaze in camera images, you just need to project eye tracking results into the camera images using the camera's calibration. But please note the coordinate frame difference, entailed below. \n",
    "\n",
    "**EyeGaze Coordinate System - Central Pupil Frame (CPF)** \n",
    "\n",
    "All Eyetracking results in Aria are stored in a reference coordinates system called **Central Pupil Frame (`CPF`)**, which is approximately the center of user's two eye positions. Note that this **`CPF` frame is DIFFERENT from the `Device` frame in device calibration**, where the latter is essentially the `slam-front-left` (for Gen2) or `camera-slam-left` (for Gen1) camera. To transform between `CPF` and `Device`, we provide the following API to query their relative pose, and see the following code cell for usage: \n",
    "```\n",
    "device_calibration.get_transform_device_cpf()\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f27dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rerun as rr\n",
    "from projectaria_tools.core.sensor_data import SensorDataType, TimeDomain, TimeQueryOptions\n",
    "from projectaria_tools.utils.rerun_helpers import create_hand_skeleton_from_landmarks\n",
    "\n",
    "def plot_eyegaze_in_camera(eyegaze_data, camera_label, camera_calib, T_device_cpf):\n",
    "    \"\"\"\n",
    "    A helper function to plot eyegaze's spatial gaze point into a camera image\n",
    "    \"\"\"\n",
    "    # Skip if eyegaze data is invalid\n",
    "    if not (\n",
    "        eyegaze_data.spatial_gaze_point_valid and eyegaze_data.combined_gaze_valid\n",
    "    ):\n",
    "        return\n",
    "\n",
    "    # First, transform spatial gaze point from CPF -> Device -> Camera frame\n",
    "    spatial_gaze_point_in_cpf = eyegaze_data.spatial_gaze_point_in_cpf\n",
    "    spatial_gaze_point_in_device = T_device_cpf @ spatial_gaze_point_in_cpf\n",
    "    spatial_gaze_point_in_camera = (\n",
    "        camera_calib.get_transform_device_camera().inverse()\n",
    "        @ spatial_gaze_point_in_device\n",
    "    )\n",
    "\n",
    "    # Project into camera and plot 2D gaze location\n",
    "    maybe_pixel = camera_calib.project(spatial_gaze_point_in_camera)\n",
    "    if maybe_pixel is not None:\n",
    "        rr.log(\n",
    "            f\"{camera_label}/gaze_point\",\n",
    "            rr.Points2D(\n",
    "                positions=[maybe_pixel],\n",
    "                colors=[255, 64, 255],\n",
    "                radii = [30.0]\n",
    "            ),\n",
    "        )\n",
    "\n",
    "print(\"\\n=== Visualizing on-device eye tracking in camera images ===\")\n",
    "\n",
    "# First, query the RGB camera stream ids\n",
    "device_calib = vrs_data_provider.get_device_calibration()\n",
    "T_device_cpf = device_calib.get_transform_device_cpf()\n",
    "\n",
    "rgb_camera_label = \"camera-rgb\"\n",
    "rgb_stream_id = vrs_data_provider.get_stream_id_from_label(rgb_camera_label)\n",
    "rgb_camera_calib = device_calib.get_camera_calib(rgb_camera_label)\n",
    "\n",
    "rr.init(\"rerun_viz_et_in_cameras\")\n",
    "\n",
    "# Set up a sensor queue with only RGB image + EyeGaze\n",
    "deliver_options = vrs_data_provider.get_default_deliver_queued_options()\n",
    "deliver_options.deactivate_stream_all()\n",
    "deliver_options.activate_stream(rgb_stream_id)\n",
    "deliver_options.activate_stream(eyegaze_stream_id)\n",
    "\n",
    "# Play for only 3 seconds\n",
    "total_length_ns = vrs_data_provider.get_last_time_ns_all_streams(TimeDomain.DEVICE_TIME) - vrs_data_provider.get_first_time_ns_all_streams(TimeDomain.DEVICE_TIME)\n",
    "skip_begin_ns = int(15 * 1e9) # Skip 15 seconds\n",
    "duration_ns = int(3 * 1e9) # 3 seconds\n",
    "skip_end_ns = max(total_length_ns - skip_begin_ns - duration_ns, 0)\n",
    "deliver_options.set_truncate_first_device_time_ns(skip_begin_ns)\n",
    "deliver_options.set_truncate_last_device_time_ns(skip_end_ns)\n",
    "\n",
    "# Plot image data, and plot EyeGaze on top of RGB image data\n",
    "for sensor_data in vrs_data_provider.deliver_queued_sensor_data(deliver_options):\n",
    "    stream_id = sensor_data.stream_id()\n",
    "    data_type = sensor_data.sensor_data_type()\n",
    "\n",
    "    # ---------------\n",
    "    # Image data: plot RGB images. \n",
    "    # ---------------\n",
    "    if data_type == SensorDataType.IMAGE:\n",
    "        # Convert back to image data, and plot in ReRun\n",
    "        device_time_ns = sensor_data.get_time_ns(TimeDomain.DEVICE_TIME)\n",
    "        image_data_and_record = sensor_data.image_data_and_record()\n",
    "\n",
    "        # Visualize the images\n",
    "        rr.set_time(\"device_time\", duration = device_time_ns * 1e-9)\n",
    "        rr.log(rgb_camera_label, rr.Image(image_data_and_record[0].to_numpy_array()))\n",
    "\n",
    "    # ---------------\n",
    "    #  Eye gaze data: plot EyeGaze's projection into camera images\n",
    "    # ---------------\n",
    "    elif data_type == SensorDataType.EYE_GAZE:\n",
    "        device_time_ns = sensor_data.get_time_ns(TimeDomain.DEVICE_TIME)\n",
    "        eye_gaze = sensor_data.eye_gaze_data()\n",
    "\n",
    "        # Plot Eyegaze overlay on top of camera images\n",
    "        rr.set_time(\"device_time\", duration = device_time_ns * 1e-9)\n",
    "        plot_eyegaze_in_camera(eyegaze_data = eye_gaze, camera_label = rgb_camera_label, camera_calib = rgb_camera_calib, T_device_cpf = T_device_cpf)\n",
    "\n",
    "\n",
    "rr.notebook_show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71790f5c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## On-Device Hand-tracking results\n",
    "### Handtracking Data Structure\n",
    "HandTracking data contains comprehensive 3D hand pose information. \n",
    "**Importantly, it directly reuses the [HandTrackingResults data structure](https://github.com/facebookresearch/projectaria_tools/blob/main/core/mps/HandTracking.h) from MPS (Machine Perception\n",
    "Services)**, providing guaranteed compatibility across VRS and MPS.\n",
    "\n",
    "**Key `EyeGaze` fields**\n",
    "\n",
    "**Key Fields in `HandTrackingResults`**\n",
    "| Field Name           | Description                                                             |\n",
    "| -------------------- | ----------------------------------------------------------------------- |\n",
    "| `tracking_timestamp` | Timestamp of the hand-tracking estimate in the device time domain.      |\n",
    "| `left_hand`          | Left-hand pose, or `None` if no valid pose is found for the timestamp.  |\n",
    "| `right_hand`         | Right-hand pose, or `None` if no valid pose is found for the timestamp. |\n",
    "\n",
    "**Single Hand fields (left or right):**\n",
    "| Field Name                     | Description                                                                                                                                                                                                                          |\n",
    "| ------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n",
    "| `confidence`                   | Tracking confidence score for this hand.                                                                                                                                                                                             |\n",
    "| `landmark_positions_device`    | List of 21 hand-landmark positions in the device frame (3D points). <br>See the [wiki page](https://facebookresearch.github.io/projectaria_tools/docs/data_formats/mps/hand_tracking#hand_tracking_resultscsv) for landmark definitions. |\n",
    "| `transform_device_wrist`       | Full SE3 transform of the wrist in the `Device` frame.                                                                                                                                                                               |\n",
    "| `wrist_and_palm_normal_device` | Normal vectors for the wrist and palm joints in the `Device` frame.                                     \n",
    "\n",
    "### Handtracking Coordinate System\n",
    "All Handtracking results in Aria are stored in the `Device` coordinate frame, which is the same as device calibration. See `Tutorial_2_device_calibration` for definition of `Device` frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e38590a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def print_single_hand_information(single_hand):\n",
    "    \"\"\"\n",
    "    A helper function to print the hand tracking result of one hand\n",
    "    \"\"\"\n",
    "    print(f\"  Confidence: {single_hand.confidence:.3f}\")\n",
    "    print(\n",
    "        f\"    Landmarks shape: {np.array(single_hand.landmark_positions_device).shape}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"    Wrist location: {single_hand.get_wrist_position_device()}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"    Palm location: {single_hand.get_wrist_position_device()}\"\n",
    "    )\n",
    "\n",
    "print(\"=== HandTracking Data Sample ===\")\n",
    "num_handtracking_samples = vrs_data_provider.get_num_data(handtracking_stream_id)\n",
    "selected_index = min(5, num_handtracking_samples)\n",
    "hand_data = vrs_data_provider.get_hand_pose_data_by_index(\n",
    "    handtracking_stream_id, selected_index\n",
    ")\n",
    "\n",
    "print(f\"Sample {selected_index}:\")\n",
    "print(f\"  Tracking timestamp: {hand_data.tracking_timestamp}\")\n",
    "\n",
    "# Print the content of left and right hand if valid\n",
    "if hand_data.left_hand is not None:\n",
    "    print(\" Left hand detected\")\n",
    "    print_single_hand_information(hand_data.left_hand)\n",
    "else:\n",
    "    print(\"  Left hand: Not detected\")\n",
    "\n",
    "if hand_data.right_hand is not None:\n",
    "    print(\" Right hand detected\")\n",
    "    print_single_hand_information(hand_data.right_hand)\n",
    "else:\n",
    "    print(\"  Right hand: Not detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43616cd2",
   "metadata": {},
   "source": [
    "### Interpolated Hand-tracking Results\n",
    "**Context:**\n",
    "\n",
    "In Aria-Gen2 glasses, **the on-device hand-tracking data are calculated from the SLAM cameras, not RGB cameras**. \n",
    "In the mean time, the SLAM cameras and RGB camera often runs at different sampling frequency, and their triggering are not aligned either. \n",
    "This causes that the handtracking result's timestamp often do NOT line up with that of RGB camera, causing additional challenges in accurately visualize handtracking results in RGB images. \n",
    "\n",
    "**API to query interpolated handtracking results**\n",
    "\n",
    "To resolve this, `vrs_data_provider` enables a special query API for handtracking results: \n",
    "```\n",
    "vrs_data_provider.get_interpolated_hand_pose_data(stream_id, timestamp_ns)\n",
    "```\n",
    "which will return an interpolated handtracking results, given any timestamp within valid timestamps of the VRS file. \n",
    "\n",
    "**Handtracking Interpolation Implementation**\n",
    "\n",
    "1. Find the 2 nearest hand-tracking results before and after the target timestamp.  \n",
    "2. If the 2 hand-tracking results time delta is larger than 100 ms, interpolation is considered unreliable → return `None`.  \n",
    "3. Otherwise, interpolate each hand separately:  \n",
    "   a. For the left or right hand, perform interpolation **only if both the \"before\" and \"after\" samples contain a valid result for that hand**.  \n",
    "   b. If either sample is missing, the interpolated result for that hand will be `None`.  Example:  \n",
    "      ```text\n",
    "      interpolate(\n",
    "          before = [left = valid, right = None],\n",
    "          after  = [left = valid, right = valid]\n",
    "      )\n",
    "      → result = [left = interpolated, right = None]\n",
    "      ```\n",
    "4. Single-hand interpolation is calculated as:  \n",
    "   a. Apply linear interpolation on the 3D hand landmark positions.  \n",
    "   b. Apply SE3 interpolation on `T_Device_Wrist` 3D pose.  \n",
    "   c. Re-calculate the wrist and palm normal vectors.  \n",
    "   d. Take the `min` of confidence values.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee4d486",
   "metadata": {},
   "outputs": [],
   "source": [
    "from projectaria_tools.core.sensor_data import SensorDataType, TimeDomain, TimeQueryOptions\n",
    "from datetime import timedelta\n",
    "\n",
    "print(\"\\n=== Demonstrating query interpolated hand tracking results ===\")\n",
    "\n",
    "# Demonstrate how to query interpolated handtracking results\n",
    "slam_stream_id = vrs_data_provider.get_stream_id_from_label(\"slam-front-left\")\n",
    "rgb_stream_id = vrs_data_provider.get_stream_id_from_label(\"camera-rgb\")\n",
    "\n",
    "# Retrieve a SLAM frame, use its timestamp as query\n",
    "slam_sample_index = min(10, vrs_data_provider.get_num_data(slam_stream_id) - 1)\n",
    "slam_data_and_record = vrs_data_provider.get_image_data_by_index(slam_stream_id, slam_sample_index)\n",
    "slam_timestamp_ns = slam_data_and_record[1].capture_timestamp_ns\n",
    "\n",
    "# Retrieve the closest RGB frame\n",
    "rgb_data_and_record = vrs_data_provider.get_image_data_by_time_ns(\n",
    "    rgb_stream_id, slam_timestamp_ns, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST\n",
    ")\n",
    "rgb_timestamp_ns = rgb_data_and_record[1].capture_timestamp_ns\n",
    "\n",
    "# Retrieve the closest hand tracking data sample\n",
    "raw_ht_data = vrs_data_provider.get_hand_pose_data_by_time_ns(\n",
    "    handtracking_stream_id, slam_timestamp_ns, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST\n",
    ")\n",
    "raw_ht_timestamp_ns = (raw_ht_data.tracking_timestamp // timedelta(microseconds=1)) * 1000\n",
    "\n",
    "# Check if hand tracking aligns with RGB or SLAM data\n",
    "print(f\"SLAM timestamp: {slam_timestamp_ns}\")\n",
    "print(f\"RGB timestamp:  {rgb_timestamp_ns}\")\n",
    "print(f\"hand tracking timestamp:   {raw_ht_timestamp_ns}\")\n",
    "print(f\"hand tracking-SLAM time diff: {abs(raw_ht_timestamp_ns - slam_timestamp_ns) / 1e6:.2f} ms\")\n",
    "print(f\"hand tracking- RGB time diff: {abs(raw_ht_timestamp_ns - rgb_timestamp_ns) / 1e6:.2f} ms\")\n",
    "\n",
    "# Now, query interpolated hand tracking data sample using RGB timestamp.\n",
    "interpolated_ht_data = vrs_data_provider.get_interpolated_hand_pose_data(\n",
    "    handtracking_stream_id, rgb_timestamp_ns\n",
    ")\n",
    "\n",
    "# Check that interpolated hand tracking now aligns with RGB data\n",
    "if interpolated_ht_data is not None:\n",
    "    interpolated_ht_timestamp_ns = (interpolated_ht_data.tracking_timestamp// timedelta(microseconds=1)) * 1000\n",
    "    print(f\"Interpolated hand tracking timestamp: {interpolated_ht_timestamp_ns}\")\n",
    "    print(f\"Interpolated hand tracking-RGB time diff: {abs(interpolated_ht_timestamp_ns - rgb_timestamp_ns) / 1e6:.2f} ms\")\n",
    "else:\n",
    "    print(\"Interpolated hand tracking data is None - interpolation failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ff7d13",
   "metadata": {},
   "source": [
    "### Visualize Hand-tracking Results in Cameras\n",
    "In this section, we show some example code on how to visualize the hand-tracking results in SLAM and RGB camera images. \n",
    "Basically, you need to project the hand tracking results (landmarks, skeleton lines) into the camera images using the camera's calibration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768f3816",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rerun as rr\n",
    "from projectaria_tools.core.sensor_data import SensorDataType, TimeDomain, TimeQueryOptions\n",
    "from projectaria_tools.utils.rerun_helpers import create_hand_skeleton_from_landmarks\n",
    "\n",
    "def plot_single_hand_in_camera(hand_joints_in_device, camera_label, camera_calib, hand_label):\n",
    "    \"\"\"\n",
    "    A helper function to plot a single hand data in 2D camera view\n",
    "    \"\"\"\n",
    "    # Setting different marker plot sizes for RGB and SLAM since they have different resolutions\n",
    "    plot_ratio = 3.0 if camera_label == \"camera-rgb\" else 1.0\n",
    "    marker_color = [255,64,0] if hand_label == \"left\" else [255, 255, 0]\n",
    "\n",
    "    # project into camera frame, and also create line segments\n",
    "    hand_joints_in_camera = []\n",
    "    for pt_in_device in hand_joints_in_device:\n",
    "        pt_in_camera = (\n",
    "            camera_calib.get_transform_device_camera().inverse() @ pt_in_device\n",
    "        )\n",
    "        pixel = camera_calib.project(pt_in_camera)\n",
    "        hand_joints_in_camera.append(pixel)\n",
    "\n",
    "    # Create hand skeleton in 2D image space\n",
    "    hand_skeleton = create_hand_skeleton_from_landmarks(hand_joints_in_camera)\n",
    "\n",
    "    # Remove \"None\" markers from hand joints in camera. This is intentionally done AFTER the hand skeleton creation\n",
    "    hand_joints_in_camera = list(\n",
    "        filter(lambda x: x is not None, hand_joints_in_camera)\n",
    "    )\n",
    "\n",
    "    rr.log(\n",
    "        f\"{camera_label}/{hand_label}/landmarks\",\n",
    "        rr.Points2D(\n",
    "            positions=hand_joints_in_camera,\n",
    "            colors= marker_color,\n",
    "            radii= [3.0 * plot_ratio]\n",
    "        ),\n",
    "    )\n",
    "    rr.log(\n",
    "        f\"{camera_label}/{hand_label}/skeleton\",\n",
    "        rr.LineStrips2D(\n",
    "            hand_skeleton,\n",
    "            colors=[0, 255, 0],\n",
    "            radii= [0.5 * plot_ratio],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "def plot_handpose_in_camera(hand_pose, camera_label, camera_calib):\n",
    "    \"\"\"\n",
    "    A helper function to plot hand tracking results into a camera image\n",
    "    \"\"\"\n",
    "    # Clear the canvas first\n",
    "    #rr.log(\n",
    "    #    f\"{camera_label}/handtracking\",\n",
    "    #    rr.Clear.recursive(),\n",
    "    #)\n",
    "\n",
    "    # Plot both hands\n",
    "    if hand_pose.left_hand is not None:\n",
    "        plot_single_hand_in_camera(\n",
    "            hand_joints_in_device=hand_pose.left_hand.landmark_positions_device,\n",
    "            camera_label=camera_label,\n",
    "            camera_calib = camera_calib,\n",
    "            hand_label=\"left\")\n",
    "    if hand_pose.right_hand is not None:\n",
    "        plot_single_hand_in_camera(\n",
    "            hand_joints_in_device=hand_pose.right_hand.landmark_positions_device,\n",
    "            camera_label=camera_label,\n",
    "            camera_calib = camera_calib,\n",
    "            hand_label=\"right\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45463b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Visualizing on-device hand tracking in camera images ===\")\n",
    "\n",
    "# First, query the RGB camera stream id\n",
    "device_calib = vrs_data_provider.get_device_calibration()\n",
    "rgb_camera_label = \"camera-rgb\"\n",
    "slam_camera_labels = [\"slam-front-left\", \"slam-front-right\", \"slam-side-left\", \"slam-side-right\"]\n",
    "rgb_stream_id = vrs_data_provider.get_stream_id_from_label(rgb_camera_label)\n",
    "slam_stream_ids = [vrs_data_provider.get_stream_id_from_label(label) for label in slam_camera_labels]\n",
    "\n",
    "rr.init(\"rerun_viz_ht_in_cameras\")\n",
    "\n",
    "# Set up a sensor queue with only RGB images.\n",
    "# Handtracking data will be queried with interpolated API.\n",
    "deliver_options = vrs_data_provider.get_default_deliver_queued_options()\n",
    "deliver_options.deactivate_stream_all()\n",
    "for stream_id in slam_stream_ids + [rgb_stream_id]:\n",
    "    deliver_options.activate_stream(stream_id)\n",
    "\n",
    "# Play for only 3 seconds\n",
    "total_length_ns = vrs_data_provider.get_last_time_ns_all_streams(TimeDomain.DEVICE_TIME) - vrs_data_provider.get_first_time_ns_all_streams(TimeDomain.DEVICE_TIME)\n",
    "skip_begin_ns = int(15 * 1e9) # Skip 15 seconds\n",
    "duration_ns = int(3 * 1e9) # 3 seconds\n",
    "skip_end_ns = max(total_length_ns - skip_begin_ns - duration_ns, 0)\n",
    "deliver_options.set_truncate_first_device_time_ns(skip_begin_ns)\n",
    "deliver_options.set_truncate_last_device_time_ns(skip_end_ns)\n",
    "\n",
    "# Plot image data, and overlay hand tracking data\n",
    "for sensor_data in vrs_data_provider.deliver_queued_sensor_data(deliver_options):\n",
    "    # ---------------\n",
    "    # Only image data will be obtained.\n",
    "    # ---------------\n",
    "    device_time_ns = sensor_data.get_time_ns(TimeDomain.DEVICE_TIME)\n",
    "    image_data_and_record = sensor_data.image_data_and_record()\n",
    "    stream_id = sensor_data.stream_id()\n",
    "    camera_label = vrs_data_provider.get_label_from_stream_id(stream_id)\n",
    "    camera_calib = device_calib.get_camera_calib(camera_label)\n",
    "    \n",
    "\n",
    "    # Visualize the RGB images.\n",
    "    rr.set_time(\"device_time\", duration = device_time_ns * 1e-9)\n",
    "    rr.log(f\"{camera_label}\", rr.Image(image_data_and_record[0].to_numpy_array()))\n",
    "    \n",
    "    # Query and plot interpolated hand tracking result\n",
    "    interpolated_hand_pose = vrs_data_provider.get_interpolated_hand_pose_data(handtracking_stream_id, device_time_ns, TimeDomain.DEVICE_TIME)\n",
    "    if interpolated_hand_pose is not None:\n",
    "        plot_handpose_in_camera(hand_pose = interpolated_hand_pose, camera_label = camera_label, camera_calib = camera_calib)\n",
    "\n",
    "# Wait for rerun to buffer 1 second of data\n",
    "import time\n",
    "time.sleep(1)\n",
    "\n",
    "rr.notebook_show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c220f6d3-0638-4ae1-a95b-dcfe8a1356c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
