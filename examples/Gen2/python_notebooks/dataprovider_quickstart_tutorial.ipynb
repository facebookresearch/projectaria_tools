{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "fb140897",
            "metadata": {},
            "source": [
                "# Interactive Examples on Project Aria Tools\n",
                "\n",
                "### Notebook stuck?\n",
                "Note that because of Jupyter issues, sometimes the code may stuck at visualization. We recommend **restart the kernels** and try again to see if the issue is resolved."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "10e0572c",
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "\n",
                "# Specifics for Google Colab\n",
                "google_colab_env = 'google.colab' in str(get_ipython())\n",
                "if google_colab_env:\n",
                "    print(\"Running from Google Colab, installing projectaria_tools and getting sample data\")\n",
                "    !pip install projectaria-tools\n",
                "    # TODO: Update the data path here.\n",
                "    !curl -O -J -L  \"https://github.com/facebookresearch/projectaria_tools/raw/main/data/gen1/mps_sample/sample.vrs\"\n",
                "    vrsfile = \"sample.vrs\"\n",
                "else:\n",
                "    print(\"Using a pre-existing projectaria_tool github repository\")\n",
                "    # Define the paths to check\n",
                "    possible_path_1 = \"../../../data/mps_sample/sample.vrs\"\n",
                "    possible_path_2 = \"../../../data/gen1/mps_sample/sample.vrs\"\n",
                "    # Check which path contains the actual data file\n",
                "    if os.path.exists(possible_path_1):\n",
                "        vrsfile = possible_path_1\n",
                "        print(f\"Using data from: {vrsfile}\")\n",
                "    elif os.path.exists(possible_path_2):\n",
                "        vrsfile = possible_path_2\n",
                "        print(f\"Using data from: {vrsfile}\")\n",
                "    else:\n",
                "        # Exit with an error message if no data file is found\n",
                "        sys.exit(\"Error: No data file found in the specified paths.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8211dc17",
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "\n",
                "# Add the current repository path to sys.path\n",
                "repo_path = os.path.abspath(os.path.join(os.getcwd(), '../../'))\n",
                "sys.path.insert(0, repo_path)\n",
                "print(repo_path)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7e0909f2-ba66-4758-8a05-2e925574f43b",
            "metadata": {},
            "outputs": [],
            "source": [
                "from projectaria_tools.core import data_provider, calibration\n",
                "from projectaria_tools.core.image import InterpolationMethod\n",
                "from projectaria_tools.core.sensor_data import TimeDomain, TimeQueryOptions\n",
                "from projectaria_tools.core.stream_id import RecordableTypeId, StreamId\n",
                "import numpy as np\n",
                "from matplotlib import pyplot as plt\n",
                "from PIL import Image"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8196ad05",
            "metadata": {},
            "source": [
                "## Create data provider"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fb04b53b",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"Creating data provider from {vrsfile}\")\n",
                "provider = data_provider.create_vrs_data_provider(vrsfile)\n",
                "if not provider:\n",
                "    print(\"Invalid vrs data provider\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8884a2ff-a26b-40d1-a033-268306283788",
            "metadata": {},
            "source": [
                "## Check device version\n",
                "Create device-version specific variables. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "42053ae3-f48e-4c42-a14a-70618883e9d3",
            "metadata": {},
            "outputs": [],
            "source": [
                "from typing import Optional\n",
                "from projectaria_tools.core.calibration import DeviceVersion\n",
                "# Print out the device version of the recording\n",
                "device_version = provider.get_device_version()\n",
                "print(f\"Device version is {calibration.get_name(device_version)}\")\n",
                "\n",
                "# Example variables used in this notebook\n",
                "rgb_stream_id = StreamId('214-1')\n",
                "\n",
                "# Some example variables are different for Gen1 and Gen2,\n",
                "# because they have different HW configs, sensor label names, etc.\n",
                "if device_version == DeviceVersion.Gen1:\n",
                "    example_stream_mappings = {\n",
                "    \"camera-slam-left\": StreamId(\"1201-1\"),\n",
                "    \"camera-slam-right\":StreamId(\"1201-2\"),\n",
                "    \"camera-rgb\":StreamId(\"214-1\"),\n",
                "    \"camera-eyetracking\":StreamId(\"211-1\"),\n",
                "    }\n",
                "    example_slam_stream_label = \"camera-slam-left\"\n",
                "\n",
                "    # Gen1 images are rotated 90 degrees for better visualization\n",
                "    ROTATE_90_FLAG = True\n",
                "\n",
                "    # A linear camera model used in undistortion example: [width, height, focal]\n",
                "    example_linear_rgb_camera_model_params = [512, 512, 150]\n",
                "elif device_version == DeviceVersion.Gen2:\n",
                "    example_stream_mappings = {\n",
                "    \"slam-front-left\": StreamId(\"1201-1\"),\n",
                "    \"slam-front-right\":StreamId(\"1201-2\"),\n",
                "    \"slam-side-left\": StreamId(\"1201-3\"),\n",
                "    \"slam-side-right\": StreamId(\"1201-4\"),\n",
                "    \"camera-rgb\":StreamId(\"214-1\"),\n",
                "    \"camera-et-left\":StreamId(\"211-1\"),\n",
                "    \"camera-et-right\":StreamId(\"211-2\"),\n",
                "    }\n",
                "    example_slam_stream_label = \"slam-front-left\"\n",
                "    # Gen2 images are already in up-right orientation\n",
                "    ROTATE_90_FLAG = False\n",
                "\n",
                "    # A linear camera model used in undistortion example: [width, height, focal]\n",
                "    example_linear_rgb_camera_model_params = [4032, 3024, 1600]\n",
                "\n",
                "example_slam_stream_id = provider.get_stream_id_from_label(example_slam_stream_label)\n",
                "\n",
                "# A helper function to auto rotate Aria image, if necessary\n",
                "def auto_image_rotation(img: np.array, stream_label: Optional[str] = None):\n",
                "    if stream_label != \"camera-eyetracking\" and ROTATE_90_FLAG:\n",
                "        return np.rot90(img, -1)\n",
                "    else:\n",
                "        return img"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "87be2866",
            "metadata": {},
            "source": [
                "# Retrieving image data\n",
                "\n",
                "Goals:\n",
                "- Learn how to retrieve Image data for a given Image stream\n",
                "\n",
                "Key learnings:\n",
                "- VRS contains data streams are identified with a Unique Identifier: stream_id\n",
                "- Learn what are the Stream Ids used by Aria data (Slam, Rgb, EyeTracking)\n",
                "- Learn that image data can be retrieved by using a record Index or a timestamp\n",
                "- For each stream_id, index ranges from [0, get_num_data(stream_id)], and the same index for different streams could have different timestamps\n",
                "- Query data from different sensors of the same timestamp can be done through `get_image_data_by_time_ns`, `get_imu_data_by_time_ns`, etc"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "023ae1a6",
            "metadata": {},
            "outputs": [],
            "source": [
                "axes = []\n",
                "fig, axes = plt.subplots(1, len(example_stream_mappings), figsize=(12, 4))\n",
                "fig.suptitle('Retrieving image data using Record Index')\n",
                "\n",
                "# Query data with index\n",
                "frame_index = 1\n",
                "for idx, [stream_name, stream_id] in enumerate(list(example_stream_mappings.items())):\n",
                "    image = provider.get_image_data_by_index(stream_id, frame_index)\n",
                "    image_to_show = auto_image_rotation(image[0].to_numpy_array(), stream_name)\n",
                "    axes[idx].imshow(image_to_show, cmap=\"gray\", vmin=0, vmax=255)\n",
                "    axes[idx].title.set_text(stream_name)\n",
                "    axes[idx].tick_params(left=False, right=False, labelleft=False, labelbottom=False, bottom=False)\n",
                "plt.show()\n",
                "\n",
                "# Same example using Time\n",
                "plt.figure()\n",
                "fig, axes = plt.subplots(1, len(example_stream_mappings), figsize=(12, 4))\n",
                "fig.suptitle('Retrieving image data using Time')\n",
                "\n",
                "time_domain = TimeDomain.DEVICE_TIME  # query data based on host time\n",
                "option = TimeQueryOptions.CLOSEST # get data whose time [in TimeDomain] is CLOSEST to query time\n",
                "start_time = provider.get_first_time_ns(rgb_stream_id, time_domain)\n",
                "\n",
                "for idx, [stream_name, stream_id] in enumerate(list(example_stream_mappings.items())):\n",
                "    image = provider.get_image_data_by_time_ns(stream_id, start_time, time_domain, option)\n",
                "    image_to_show = auto_image_rotation(image[0].to_numpy_array(), stream_name)\n",
                "    axes[idx].imshow(image_to_show, cmap=\"gray\", vmin=0, vmax=255)\n",
                "    axes[idx].title.set_text(stream_name)\n",
                "    axes[idx].tick_params(left=False, right=False, labelleft=False, labelbottom=False, bottom=False)\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c5033225",
            "metadata": {},
            "source": [
                "# Summarize a VRS using thumbnails\n",
                "\n",
                "Goals:\n",
                "- Summarize a VRS using 10 image side by side\n",
                "\n",
                "Key learnings:\n",
                "- Image streams are identified with a Unique Identifier: stream_id\n",
                "- PIL images can be created from Numpy array"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "933725b6",
            "metadata": {},
            "outputs": [],
            "source": [
                "from PIL import Image, ImageOps\n",
                "from tqdm import tqdm\n",
                "\n",
                "# Retrieve Start and End time for the given Sensor Stream Id\n",
                "start_time = provider.get_first_time_ns(rgb_stream_id, time_domain)\n",
                "end_time = provider.get_last_time_ns(rgb_stream_id, time_domain)\n",
                "\n",
                "# Retrieve image size for the RGB stream\n",
                "time_domain = TimeDomain.DEVICE_TIME  # query data based on host time\n",
                "option = TimeQueryOptions.CLOSEST # get data whose time [in TimeDomain] is CLOSEST to query time\n",
                "\n",
                "image_config = provider.get_image_configuration(rgb_stream_id)\n",
                "width = image_config.image_width\n",
                "height = image_config.image_height\n",
                "\n",
                "sample_count = 10\n",
                "resize_ratio = 10\n",
                "thumbnail = newImage = Image.new(\n",
                "    \"RGB\", (int(width * sample_count / resize_ratio), int(height / resize_ratio))\n",
                ")\n",
                "current_width = 0\n",
                "\n",
                "\n",
                "# Samples 10 timestamps\n",
                "sample_timestamps = np.linspace(start_time, end_time, sample_count)\n",
                "for sample in tqdm(sample_timestamps):\n",
                "    image_tuple = provider.get_image_data_by_time_ns(rgb_stream_id, int(sample), time_domain, option)\n",
                "    image_array = auto_image_rotation(image_tuple[0].to_numpy_array())\n",
                "    image = Image.fromarray(image_array)\n",
                "    new_size = (\n",
                "        int(image.size[0] / resize_ratio),\n",
                "        int(image.size[1] / resize_ratio),\n",
                "    )\n",
                "    image = image.resize(new_size)\n",
                "    thumbnail.paste(image, (current_width, 0))\n",
                "    current_width = int(current_width + width / resize_ratio)\n",
                "\n",
                "from IPython.display import Image\n",
                "display(thumbnail)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0850064e",
            "metadata": {},
            "source": [
                "# Obtain mapping between stream_id and sensor label\n",
                "Goals:\n",
                "- In a vrs file, each sensor data is identified through stream_id\n",
                "- Learn mapping between stream_id and label for each sensor\n",
                "\n",
                "Key learnings:\n",
                "- VRS is using Unique Identifier for each stream called stream_id. \n",
                "- For each sensor data, it is attached with a stream_id, which contains two parts [RecordableTypeId, InstanceId]. \n",
                "- To get the actual readable name of each sensor,\n",
                "we can use `get_label_from_stream_id` vise versa `get_stream_id_from_label`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "43d83a1e",
            "metadata": {},
            "outputs": [],
            "source": [
                "streams = provider.get_all_streams()\n",
                "for stream_id in streams:\n",
                "    label = provider.get_label_from_stream_id(stream_id)\n",
                "    print(\n",
                "        f\"stream_id: [{stream_id}] convert to label: [{label}] and back: [{provider.get_stream_id_from_label(label)}]\"\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3def75bb",
            "metadata": {},
            "source": [
                "# Get sensor data in a sequence based on data capture time\n",
                "Goal:\n",
                "- Obtain sensor data sequentially based on timestamp\n",
                "\n",
                "Key learnings\n",
                "- Default option activates all sensors and playback the entire dataset from vrs\n",
                "- Setup option to only activate certain streams, truncate start/end time, and sample rate\n",
                "- Obtain data from different sensor types\n",
                "- `TimeDomain` are separated into four categories: `RECORD_TIME`, `DEVICE_TIME`, `HOST_TIME`, `TIME_CODE`"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "89aba5a2",
            "metadata": {},
            "source": [
                "### Step 1: obtain default options that provides the whole dataset from VRS\n",
                "* activates all sensor streams\n",
                "* No truncation for first/last timestamp\n",
                "* Subsample rate = 1 (do not skip any data per sensor)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f0124864",
            "metadata": {},
            "outputs": [],
            "source": [
                "options = (\n",
                "    provider.get_default_deliver_queued_options()\n",
                ")  # default options activates all streams"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "fb6dca83",
            "metadata": {},
            "source": [
                "### Step 2: set preferred deliver options\n",
                "* truncate first/last time: `set_truncate_first_device_time_ns/set_truncate_last_device_time_ns()`\n",
                "* subselect sensor streams to play: `activate_stream(stream_id)`\n",
                "* skip sensor data : `set_subsample_rate(stream_id, rate)`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a046582c",
            "metadata": {},
            "outputs": [],
            "source": [
                "options.set_truncate_first_device_time_ns(int(1e8))  # 0.1 secs after vrs first timestamp\n",
                "options.set_truncate_last_device_time_ns(int(2e8))  # 0.2 sec before vrs last timestamp\n",
                "\n",
                "# deactivate all sensors\n",
                "options.deactivate_stream_all()\n",
                "\n",
                "# activate only a subset of sensors\n",
                "slam_stream_ids = options.get_stream_ids(RecordableTypeId.SLAM_CAMERA_DATA)\n",
                "imu_stream_ids = options.get_stream_ids(RecordableTypeId.SLAM_IMU_DATA)\n",
                "\n",
                "for stream_id in slam_stream_ids:\n",
                "    options.activate_stream(stream_id)  # activate slam cameras\n",
                "    options.set_subsample_rate(stream_id, 1)  # sample every data for each slam camera\n",
                "\n",
                "for stream_id in imu_stream_ids:\n",
                "    options.activate_stream(stream_id)  # activate imus\n",
                "    options.set_subsample_rate(stream_id, 10)  # sample every 10th data for each imu"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "fdff6dd3",
            "metadata": {},
            "source": [
                "### Step 3: create iterator to deliver data\n",
                "`TimeDomain` contains the following\n",
                "* `RECORD_TIME`: timestamp stored in vrs index, fast to access, but not guaranteed which time domain\n",
                "* `DEVICE_TIME`: capture time in device's timedomain, accurate\n",
                "* `HOST_TIME`: arrival time in host computer's timedomain, may not be accurate\n",
                "* `TIME_CODE`: capture in TimeSync server's timedomain\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "efa5aad8",
            "metadata": {},
            "outputs": [],
            "source": [
                "iterator = provider.deliver_queued_sensor_data(options)\n",
                "for sensor_data in iterator:\n",
                "    label = provider.get_label_from_stream_id(sensor_data.stream_id())\n",
                "    sensor_type = sensor_data.sensor_data_type()\n",
                "    device_timestamp = sensor_data.get_time_ns(TimeDomain.DEVICE_TIME)\n",
                "    host_timestamp = sensor_data.get_time_ns(TimeDomain.HOST_TIME)\n",
                "    timecode_timestamp = sensor_data.get_time_ns(TimeDomain.TIME_CODE)\n",
                "    print(\n",
                "        f\"\"\"obtain data from {label} of type {sensor_type} with\n",
                "        DEVICE_TIME: {device_timestamp} nanoseconds\n",
                "        HOST_TIME: {host_timestamp} nanoseconds\n",
                "        \"\"\"\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a0796407",
            "metadata": {},
            "source": [
                "# Random access data\n",
                "Goal\n",
                "- Access data from a stream randomly using a data index or a timestamp\n",
                "\n",
                "Key learnings\n",
                "- Sensor data can be obtained through index within the range of [0, number of data for this stream_id)\n",
                "\n",
                "  - `get_sensor_data_by_index(stream_id, index)`\n",
                "  - `get_image_data_by_index(stream_id, index)`\n",
                "  - Access other sensor data by index interface is available in core/python/VrsDataProviderPyBind.h\n",
                "  \n",
                "- `TimeQueryOptions` has three options: `TimeQueryOptions.BEFORE`, `TimeQueryOptions.AFTER`, `TimeQueryOptions.CLOSEST`\n",
                "- Query through index will provide the exact data vs query through a timestamp that is not exact, data nearby will be omitted base on `TimeQueryOptions`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "63657c1f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# get all image data by index, skip every 20 frames\n",
                "num_data = provider.get_num_data(example_slam_stream_id)\n",
                "\n",
                "for index in range(0, num_data, 20):\n",
                "    image_data = provider.get_image_data_by_index(example_slam_stream_id, index)\n",
                "    print(\n",
                "        f\"Get image: {index} with timestamp {image_data[1].capture_timestamp_ns}\"\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "dca84bb5",
            "metadata": {},
            "source": [
                "### Sensor data can be obtained by timestamp (nanoseconds)\n",
                "* Get stream time range `get_first_time_ns` and `get_last_time_ns`\n",
                "* Specify timedomain: `TimeDomain.DEVICE_TIME` (default)\n",
                "* Query data by queryTime\n",
                "  * `TimeQueryOptions.BEFORE` (default): sensor_dataTime <= queryTime\n",
                "  * `TimeQueryOptions.AFTER` : sensor_dataTime >= queryTime\n",
                "  * `TimeQueryOptions.CLOSEST` : sensor_dataTime closest to queryTime"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7783e83f",
            "metadata": {},
            "outputs": [],
            "source": [
                "time_domain = TimeDomain.DEVICE_TIME  # query data based on DEVICE_TIME\n",
                "option = TimeQueryOptions.CLOSEST # get data whose time [in TimeDomain] is CLOSEST to query time\n",
                "\n",
                "start_time = provider.get_first_time_ns(example_slam_stream_id, time_domain)\n",
                "end_time = provider.get_last_time_ns(example_slam_stream_id, time_domain)\n",
                "\n",
                "# Fetch every 1 second (1e9 ns)\n",
                "for time in range(start_time, end_time, int(1e9)):\n",
                "    image_data = provider.get_image_data_by_time_ns(\n",
                "        example_slam_stream_id, time, time_domain, option\n",
                "    )\n",
                "    print(\n",
                "        f\"query time {time} and get capture image time {image_data[1].capture_timestamp_ns} within range {start_time} {end_time}\"\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a8be0b53",
            "metadata": {},
            "source": [
                "### Get sensor data configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6824e56a",
            "metadata": {},
            "outputs": [],
            "source": [
                "def image_config_example(config):\n",
                "    print(f\"device_type {config.device_type}\")\n",
                "    print(f\"device_version {config.device_version}\")\n",
                "    print(f\"device_serial {config.device_serial}\")\n",
                "    print(f\"sensor_serial {config.sensor_serial}\")\n",
                "    print(f\"nominal_rate_hz {config.nominal_rate_hz}\")\n",
                "    print(f\"image_width {config.image_width}\")\n",
                "    print(f\"image_height {config.image_height}\")\n",
                "    print(f\"pixel_format {config.pixel_format}\")\n",
                "    print(f\"gamma_factor {config.gamma_factor}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f3bf5afb",
            "metadata": {},
            "outputs": [],
            "source": [
                "config = provider.get_image_configuration(example_slam_stream_id)\n",
                "image_config_example(config)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ddf4af2e",
            "metadata": {},
            "source": [
                "# Calibration examples\n",
                "Goal:\n",
                "- Obtain camera extrinsics and intrinsics\n",
                "- Learn to project a 3D point to camera frame\n",
                "\n",
                "Key learnings\n",
                "- Get calibration for different sensors using sensor labels\n",
                "- Learn how to use extrinsics/intrinsics to project a 3D points to a given camera\n",
                "- Reference frame convention"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c47e8e18",
            "metadata": {},
            "outputs": [],
            "source": [
                "device_calib = provider.get_device_calibration()\n",
                "all_sensor_labels = device_calib.get_all_labels()\n",
                "print(f\"device calibration contains calibrations for the following sensors \\n {all_sensor_labels}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "872040fa",
            "metadata": {},
            "source": [
                "### Project a 3D point to camera frame\n",
                "\n",
                "In this section we will learn how to retrieve calibration data and how to use it.\n",
                "Aria calibration is defined by two objects: one defining the intrinsics (`rgb_calib.project` and `rgb_calib.unproject`) and one defining the extrinsics as a SE3 pose (`device_calib.get_transform_device_sensor(sensor_label`).\n",
                "\n",
                "Intrinsics can be used to project a 3d point to the image plane or un-project a 2d point as a bearing vector. Extrinsics are used to set the camera in world coordinates at a given rotation and position in space.\n",
                "\n",
                "### Reference frame convention\n",
                "\n",
                "> `transform_sensor1_sensor3` = `transform_sensor1_sensor2` * `transform_sensor2_sensor3` \\\n",
                "> `point_in_sensor`: 3D point measured from sensor's reference frame \\\n",
                "> `point_in_sensor` = `transform_sensor1_sensor` * `point_in_sensor`\n",
                "\n",
                "Device Frame: `device_calib.get_origin_label() = camera-slam-left`\\\n",
                "Sensor extrinsics: `device_calib.get_transform_device_sensor(sensor_label)`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f7e39f66",
            "metadata": {},
            "outputs": [],
            "source": [
                "camera_name = \"camera-rgb\"\n",
                "transform_device_camera = device_calib.get_transform_device_sensor(camera_name).to_matrix()\n",
                "transform_camera_device = np.linalg.inv(transform_device_camera)\n",
                "print(f\"Device calibration origin label {device_calib.get_origin_label()}\")\n",
                "print(f\"{camera_name} has extrinsics of \\n {transform_device_camera}\")\n",
                "\n",
                "rgb_calib = device_calib.get_camera_calib(\"camera-rgb\")\n",
                "if rgb_calib is not None:\n",
                "    # project a 3D point in device frame [camera-slam-left] to rgb camera\n",
                "    point_in_device = np.array([0, 0, 10])\n",
                "    point_in_camera = (\n",
                "        np.matmul(transform_camera_device[0:3,0:3], point_in_device.transpose())\n",
                "        + transform_camera_device[0:3,3]\n",
                "    )\n",
                "\n",
                "    maybe_pixel = rgb_calib.project(point_in_camera)\n",
                "    if maybe_pixel is not None:\n",
                "        print(\n",
                "            f\"Get pixel {maybe_pixel} within image of size {rgb_calib.get_image_size()}\"\n",
                "        )"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3ad7ddcb",
            "metadata": {},
            "source": [
                "### Get calibration data for other sensors\n",
                "Aria is a multimodal capture device, each sensors calibration can be retrieved using the same interface. \n",
                "\n",
                "For Aria Gen1, EyeTracking (`get_aria_et_camera_calib()`) and Audio calibration (`get_aria_microphone_calib()`) is a bit different since we have multiple sensors that share the same stream_id."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cc276139",
            "metadata": {},
            "outputs": [],
            "source": [
                "et_calib = device_calib.get_aria_et_camera_calib()\n",
                "if et_calib is not None:\n",
                "    print(f\"Camera {et_calib[0].get_label()} has image size {et_calib[0].get_image_size()}\")\n",
                "    print(f\"Camera {et_calib[1].get_label()} has image size {et_calib[1].get_image_size()}\"),\n",
                "\n",
                "imu_calib = device_calib.get_imu_calib(\"imu-left\")\n",
                "if imu_calib is not None:\n",
                "    print(f\"{imu_calib.get_label()} has extrinsics transform_Device_Imu:\\n {imu_calib.get_transform_device_imu().to_matrix3x4()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "700e8af5",
            "metadata": {},
            "source": [
                "### Undistort an image\n",
                "You can remove distortions in an image in three steps. \n",
                "\n",
                "First, use the provider to access the image and the camera calibration of the stream. Then create a \"linear\" spherical camera model with `get_spherical_camera_calibration`. The function allows you to specify the image size as well as focal length of the model, assuming principal point is at the image center. Finally, apply `distort_by_calibration` function to distort the image."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e20cd362",
            "metadata": {},
            "outputs": [],
            "source": [
                "# input: retrieve image as a numpy array\n",
                "sensor_name = \"camera-rgb\"\n",
                "sensor_stream_id = provider.get_stream_id_from_label(sensor_name)\n",
                "image_data = provider.get_image_data_by_index(sensor_stream_id, 0)\n",
                "image_array = image_data[0].to_numpy_array()\n",
                "# input: retrieve image distortion\n",
                "device_calib = provider.get_device_calibration()\n",
                "src_calib = device_calib.get_camera_calib(sensor_name)\n",
                "\n",
                "# create output calibration: a linear model of image example_linear_rgb_camera_model_params.\n",
                "# Invisible pixels are shown as black.\n",
                "dst_calib = calibration.get_linear_camera_calibration(example_linear_rgb_camera_model_params[0], example_linear_rgb_camera_model_params[1], example_linear_rgb_camera_model_params[2], camera_name)\n",
                "\n",
                "# distort image\n",
                "rectified_array = calibration.distort_by_calibration(image_array, dst_calib, src_calib, InterpolationMethod.BILINEAR)\n",
                "\n",
                "# visualize input and results\n",
                "plt.figure()\n",
                "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
                "fig.suptitle(f\"Image undistortion (focal length = {dst_calib.get_focal_lengths()})\")\n",
                "\n",
                "axes[0].imshow(image_array, cmap=\"gray\", vmin=0, vmax=255)\n",
                "axes[0].title.set_text(f\"sensor image ({sensor_name})\")\n",
                "axes[0].tick_params(left=False, right=False, labelleft=False, labelbottom=False, bottom=False)\n",
                "axes[1].imshow(rectified_array, cmap=\"gray\", vmin=0, vmax=255)\n",
                "axes[1].title.set_text(f\"undistorted image ({sensor_name})\")\n",
                "axes[1].tick_params(left=False, right=False, labelleft=False, labelbottom=False, bottom=False)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a098c273",
            "metadata": {},
            "source": [
                "Note the rectified image shows a circular area of visible pixels. If you want the entire rectified image to be covered by pixels, you can increase the magnification."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "37718fa1-3482-437c-8cf0-bb6bb9686468",
            "metadata": {},
            "source": [
                "# Retrieve on-device machine perception data: EyeGaze + HandTracking (Aria Gen2 only)\n",
                "\n",
                "Goals:\n",
                "- Learn how to retrieve on-device machine perception data from VRS\n",
                "\n",
                "Key learnings:\n",
                "- Learn what on-device MP data streams are available in Aria Gen2. \n",
                "- Learn how to query such data either by timestamp, or by index. \n",
                "- Learn how to match the on-device MP data with camera images."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "76f91a77-1f54-4ccc-9927-f713942faa29",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Helper functions for on device MP plotting\n",
                "from typing import List\n",
                "from projectaria_tools.core.mps import hand_tracking\n",
                "from matplotlib.collections import LineCollection\n",
                "\n",
                "def create_hand_skeleton_segments_from_landmarks(\n",
                "    all_landmark_locations, segment_landmark_names):\n",
                "    skeleton_segments = []\n",
                "\n",
                "    # insert pairs into outline segments\n",
                "    for i in range(len(segment_landmark_names) - 1):\n",
                "        start_index = segment_landmark_names[i]\n",
                "        end_index = segment_landmark_names[i + 1]\n",
                "        skeleton_segments.append(\n",
                "            [all_landmark_locations[start_index], all_landmark_locations[end_index]]\n",
                "        )\n",
                "    return skeleton_segments\n",
                "\n",
                "\n",
                "def create_hand_skeleton_from_landmarks(landmark_locations):\n",
                "    HandLandmark = hand_tracking.HandLandmark\n",
                "    hand_skeleton = []\n",
                "    # Palm shape\n",
                "    hand_skeleton.extend(\n",
                "        create_hand_skeleton_segments_from_landmarks(\n",
                "            landmark_locations,\n",
                "            [\n",
                "                HandLandmark.WRIST,\n",
                "                HandLandmark.THUMB_INTERMEDIATE,\n",
                "                HandLandmark.INDEX_PROXIMAL,\n",
                "                HandLandmark.MIDDLE_PROXIMAL,\n",
                "                HandLandmark.RING_PROXIMAL,\n",
                "                HandLandmark.PINKY_PROXIMAL,\n",
                "                HandLandmark.WRIST,\n",
                "                HandLandmark.PALM_CENTER,\n",
                "            ],\n",
                "        )\n",
                "    )\n",
                "\n",
                "    # Thumb line\n",
                "    hand_skeleton.extend(\n",
                "        create_hand_skeleton_segments_from_landmarks(\n",
                "            landmark_locations,\n",
                "            [\n",
                "                HandLandmark.WRIST,\n",
                "                HandLandmark.THUMB_INTERMEDIATE,\n",
                "                HandLandmark.THUMB_DISTAL,\n",
                "                HandLandmark.THUMB_FINGERTIP,\n",
                "            ],\n",
                "        )\n",
                "    )\n",
                "\n",
                "    # Index line\n",
                "    hand_skeleton.extend(\n",
                "        create_hand_skeleton_segments_from_landmarks(\n",
                "            landmark_locations,\n",
                "            [\n",
                "                HandLandmark.WRIST,\n",
                "                HandLandmark.INDEX_PROXIMAL,\n",
                "                HandLandmark.INDEX_INTERMEDIATE,\n",
                "                HandLandmark.INDEX_DISTAL,\n",
                "                HandLandmark.INDEX_FINGERTIP,\n",
                "            ],\n",
                "        )\n",
                "    )\n",
                "\n",
                "    # Middle line\n",
                "    hand_skeleton.extend(\n",
                "        create_hand_skeleton_segments_from_landmarks(\n",
                "            landmark_locations,\n",
                "            [\n",
                "                HandLandmark.WRIST,\n",
                "                HandLandmark.MIDDLE_PROXIMAL,\n",
                "                HandLandmark.MIDDLE_INTERMEDIATE,\n",
                "                HandLandmark.MIDDLE_DISTAL,\n",
                "                HandLandmark.MIDDLE_FINGERTIP,\n",
                "            ],\n",
                "        )\n",
                "    )\n",
                "\n",
                "    # Ring line\n",
                "    hand_skeleton.extend(\n",
                "        create_hand_skeleton_segments_from_landmarks(\n",
                "            landmark_locations,\n",
                "            [\n",
                "                HandLandmark.WRIST,\n",
                "                HandLandmark.RING_PROXIMAL,\n",
                "                HandLandmark.RING_INTERMEDIATE,\n",
                "                HandLandmark.RING_DISTAL,\n",
                "                HandLandmark.RING_FINGERTIP,\n",
                "            ],\n",
                "        )\n",
                "    )\n",
                "\n",
                "    # Pinky line\n",
                "    hand_skeleton.extend(\n",
                "        create_hand_skeleton_segments_from_landmarks(\n",
                "            landmark_locations,\n",
                "            [\n",
                "                HandLandmark.WRIST,\n",
                "                HandLandmark.PINKY_PROXIMAL,\n",
                "                HandLandmark.PINKY_INTERMEDIATE,\n",
                "                HandLandmark.PINKY_DISTAL,\n",
                "                HandLandmark.PINKY_FINGERTIP,\n",
                "            ],\n",
                "        )\n",
                "    )\n",
                "\n",
                "    # Remove segments that may contain empty pixels\n",
                "    hand_skeleton = list(\n",
                "        filter(lambda x: x[0] is not None and x[1] is not None, hand_skeleton)\n",
                "    )\n",
                "\n",
                "    return hand_skeleton\n",
                "\n",
                "\n",
                "def plot_single_hand(axes, hand_markers_in_device, rgb_calib, hand_label):\n",
                "    hand_markers_in_rgb = []\n",
                "    # Project markers into RGB camera frame\n",
                "    for marker_in_device in hand_markers_in_device:\n",
                "        marker_in_rgb = rgb_calib.project(rgb_calib.get_transform_device_camera().inverse() @ marker_in_device)\n",
                "        hand_markers_in_rgb.append(marker_in_rgb)\n",
                "\n",
                "    # Create hand skeleton\n",
                "    hand_skeleton = create_hand_skeleton_from_landmarks(hand_markers_in_rgb)\n",
                "    hand_skeleton_line_collection = LineCollection(hand_skeleton, linewidths=2, colors='g')\n",
                "\n",
                "    # Remove \"None\" markers from hand joints in camera. This is intentionally done AFTER the hand skeleton creation\n",
                "    hand_markers_in_rgb = list(\n",
                "        filter(lambda x: x is not None, hand_markers_in_rgb)\n",
                "    )\n",
                "    if len(hand_markers_in_rgb) == 0:\n",
                "        return\n",
                "\n",
                "    hand_markers_x = [x[0] for x in hand_markers_in_rgb]\n",
                "    hand_markers_y = [x[1] for x in hand_markers_in_rgb]\n",
                "\n",
                "    # Plot hand markers\n",
                "    if hand_label == \"left\":\n",
                "        color = \"orangered\"\n",
                "    else:\n",
                "        color = \"yellow\"\n",
                "    axes.plot(hand_markers_x, hand_markers_y, 'o', markersize=5, color=color)  # 'o' is for circle markers\n",
                "\n",
                "    axes.add_collection(hand_skeleton_line_collection)\n",
                "\n",
                "def plot_hand_pose_data(axes, provider, timestamp, time_domain, time_tolerance, rgb_calib):\n",
                "    hand_stream_id = provider.get_stream_id_from_label(\"handtracking\")\n",
                "    if hand_stream_id is None:\n",
                "        print(\"Hand tracking stream not found in current VRS, skipping.\")\n",
                "    else:\n",
                "        # Query hand pose data\n",
                "        hand_pose_data = provider.get_hand_pose_data_by_time_ns(hand_stream_id, timestamp, time_domain)\n",
                "\n",
                "        if abs(hand_pose_data.tracking_timestamp.total_seconds() * 1e9 - timestamp) <= time_tolerance and (hand_pose_data.left_hand is not None or hand_pose_data.right_hand is not None):\n",
                "            print(\"Hand data valid at this timestamp\")\n",
                "\n",
                "            if hand_pose_data.left_hand is not None:\n",
                "                plot_single_hand(axes, hand_pose_data.left_hand.landmark_positions_device, rgb_calib, \"left\")\n",
                "            if hand_pose_data.right_hand is not None:\n",
                "                plot_single_hand(axes, hand_pose_data.right_hand.landmark_positions_device, rgb_calib, \"right\")\n",
                "            plt.show()\n",
                "\n",
                "        else:\n",
                "            print(\"Hand data invalid at this timestamp\")\n",
                "\n",
                "\n",
                "def plot_eye_gaze_data(axes, provider, timestamp, time_domain, time_tolerance, rgb_calib, T_device_cpf):\n",
                "    eyegaze_stream_id = provider.get_stream_id_from_label(\"eyegaze\")\n",
                "    if eyegaze_stream_id is None:\n",
                "        print(\"eyegaze stream not found in current VRS, skipping.\")\n",
                "    else:\n",
                "        # Query eyegaze data\n",
                "        eyegaze_data = provider.get_eye_gaze_data_by_time_ns(eyegaze_stream_id, timestamp, time_domain)\n",
                "\n",
                "        if eyegaze_data.spatial_gaze_point_valid and abs(eyegaze_data.tracking_timestamp.total_seconds() * 1e9 - timestamp) <= time_tolerance:\n",
                "            print(\"spatial gaze point is valid at this timestamp\")\n",
                "            spatial_gaze_point_in_cpf = eyegaze_data.spatial_gaze_point_in_cpf\n",
                "            spatial_gaze_point_in_device = T_device_cpf @ spatial_gaze_point_in_cpf\n",
                "\n",
                "            # Project spatial gaze point into RGB frame\n",
                "            point = rgb_calib.get_transform_device_camera().inverse() @ spatial_gaze_point_in_device\n",
                "            projected_gaze_point = rgb_calib.project(point)\n",
                "            if projected_gaze_point is not None:\n",
                "                # Plot a red cross as gaze point\n",
                "                axes.plot(projected_gaze_point[0], projected_gaze_point[1], 'ro', linewidth = 3, markersize=8)\n",
                "                axes.text(projected_gaze_point[0]+50, projected_gaze_point[1], 'EyeGazePoint', color='red', fontsize=10,\n",
                "                    bbox=dict(facecolor='red', alpha=0, boxstyle='round,pad=0.5'))\n",
                "            else:\n",
                "                print(\"eyegaze point projection out of camera frame\")\n",
                "        else:\n",
                "            print(\"spatial gaze point is not valid at this timestamp\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "95ff54a8-1bed-4d06-988d-29b7136da3e0",
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "if device_version == DeviceVersion.Gen2:\n",
                "    # Use a slider to get a certain RGB frame, and try to plot the corresponding EyeGaze and HandPose data in RGB image.\n",
                "    rgb_stream_id = StreamId(\"214-1\")\n",
                "    time_domain = TimeDomain.DEVICE_TIME\n",
                "    num_rgb_frames = provider.get_num_data(rgb_stream_id)\n",
                "\n",
                "    # Get RGB calibration\n",
                "    device_calib = provider.get_device_calibration()\n",
                "    rgb_calib = device_calib.get_camera_calib(\"camera-rgb\")\n",
                "    T_device_cpf = device_calib.get_transform_device_cpf()\n",
                "\n",
                "    # Create a widget with slider to choose an RGB frame to plot\n",
                "    import ipywidgets as widgets\n",
                "    from IPython.display import display\n",
                "    from functools import partial\n",
                "\n",
                "    # Get the very first frame (frame=0) so we can initialize the image.\n",
                "    initial_rgb_record = provider.get_image_data_by_index(rgb_stream_id, 0)\n",
                "    initial_rgb_array  = initial_rgb_record[0].to_numpy_array()\n",
                "    # Normalize [0,255] → [0,1]\n",
                "    initial_norm = (initial_rgb_array - 0) / 255.0\n",
                "    initial_norm = np.clip(initial_norm, 0, 1)\n",
                "\n",
                "    # Create figure & axes just once:\n",
                "    fig, axes = plt.subplots(figsize=(6, 6))\n",
                "    img_handle = axes.imshow(initial_norm, cmap=\"gray\", vmin=0, vmax=1)\n",
                "    axes.axis(\"off\")  # hide ticks\n",
                "    plt.close(fig)\n",
                "\n",
                "    output = widgets.Output()\n",
                "    with output:\n",
                "        display(fig)\n",
                "\n",
                "    slider = widgets.IntSlider(value=0, min=0, max=num_rgb_frames-1, continuous_update = False)\n",
                "\n",
                "    def on_slider_change(change, output, provider, time_domain, rgb_stream_id,  rgb_calib, T_device_cpf):\n",
                "        with output: # you need this for Bento Next\n",
                "            output.clear_output(wait=True)\n",
                "            rgb_frame_index = change['new']\n",
                "            print(f\"Selecting RGB frame {rgb_frame_index}\")\n",
                "\n",
                "            # Plot RGB image\n",
                "            rgb_image_and_record = provider.get_image_data_by_index(\n",
                "                rgb_stream_id, rgb_frame_index)\n",
                "            rgb_image_array = rgb_image_and_record[0].to_numpy_array()\n",
                "            rgb_timestamp = rgb_image_and_record[1].capture_timestamp_ns\n",
                "            min_val, max_val = 0, 255  # Set your desired min and max values\n",
                "            normalized_rgb_image = (rgb_image_array - min_val) / (max_val - min_val)\n",
                "            normalized_rgb_image = np.clip(normalized_rgb_image, 0, 1)  # Ensure values are within [0, 1]\n",
                "\n",
                "            img_handle.set_data(normalized_rgb_image)\n",
                "\n",
                "            # Remove any old overlays (eye gaze / hand pose) from previous call\n",
                "            for artist in axes.artists + axes.lines + axes.collections:\n",
                "                artist.remove()\n",
                "            for txt in axes.texts:\n",
                "                txt.remove()\n",
                "\n",
                "            # tolerance time to ensure the MP data is close to the query time.\n",
                "            time_tolerance = 500e6\n",
                "\n",
                "            # Plot Eye gaze data\n",
                "            plot_eye_gaze_data(axes, provider, rgb_timestamp, time_domain, time_tolerance, rgb_calib, T_device_cpf)\n",
                "\n",
                "            # Plot hand pose data\n",
                "            plot_hand_pose_data(axes, provider, rgb_timestamp, time_domain, time_tolerance, rgb_calib)\n",
                "\n",
                "            display(fig)\n",
                "\n",
                "\n",
                "    # Attach the function to the slider\n",
                "    print(\"Please select a RGB Frame ID, note that plotting may be slow in Bento notebook\")\n",
                "    wrapped_function = partial(on_slider_change, output = output, provider = provider, time_domain=time_domain, rgb_stream_id=rgb_stream_id, rgb_calib=rgb_calib, T_device_cpf=T_device_cpf)\n",
                "    slider.observe(wrapped_function, names='value')\n",
                "\n",
                "    display(slider, output)\n",
                "else:\n",
                "    print(\"On-device machine perception data is only available in Aria Gen2. \")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b9f67149-e5eb-44c4-b3d1-1ac00dcb6166",
            "metadata": {},
            "source": [
                "## Retrieve on-device machine perception data (VIO high frequency and VIO)\n",
                "\n",
                "Goals:\n",
                "- Learn how to retrieve on-device machine perception data (VIO, VIO high frequency) from VRS\n",
                "\n",
                "Key learnings:\n",
                "- Learn how to query VIO pose information from VRS."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7c7b150c-801c-4c99-b149-0e1dfaed94ca",
            "metadata": {},
            "outputs": [],
            "source": [
                "import plotly.graph_objs as go\n",
                "from matplotlib import pyplot as plt\n",
                "from projectaria_tools.core.sophus import SE3f\n",
                "from projectaria_tools.core.sensor_data import TrackingQuality\n",
                "\n",
                "# Tune this parameter to control the plotted camera frustum size\n",
                "CAMERA_FRUSTUM_SIZE = 0.1\n",
                "\n",
                "# Helper function to build the frustum\n",
                "def build_camera_frustum(T_world_camera):\n",
                "    points = (\n",
                "        np.array(\n",
                "            [[0, 0, 0], [0.5, 0.5, 1], [-0.5, 0.5, 1], [-0.5, -0.5, 1], [0.5, -0.5, 1]]\n",
                "        )\n",
                "        * CAMERA_FRUSTUM_SIZE\n",
                "    )\n",
                "    points_transformed = T_world_camera @ points.transpose()\n",
                "    return go.Mesh3d(\n",
                "        x=points_transformed[0, :],\n",
                "        y=points_transformed[1, :],\n",
                "        z=points_transformed[2, :],\n",
                "        i=[0, 0, 0, 0, 1, 1],\n",
                "        j=[1, 2, 3, 4, 2, 3],\n",
                "        k=[2, 3, 4, 1, 3, 4],\n",
                "        showscale=False,\n",
                "        visible=False,\n",
                "        colorscale=\"jet\",\n",
                "        intensity=points[:, 2],\n",
                "        opacity=1.0,\n",
                "        hoverinfo=\"none\",\n",
                "    )\n",
                "\n",
                "# helper function to cast from double to float\n",
                "def cast_SE3_to_SE3f(se3_double):\n",
                "    # Ensure size=1\n",
                "    if len(se3_double) != 1:\n",
                "        raise ValueError(\"Expected SE3 of size 1 for this cast helper\")\n",
                "    mat = se3_double.to_matrix()           # shape (4,4)\n",
                "    mat_f = mat.astype(np.float32)\n",
                "    return SE3f.from_matrix(mat_f)         # returns SE3f of size 1\n",
                "\n",
                "vio_high_freq_stream_id = provider.get_stream_id_from_label(\"vio_high_frequency\")\n",
                "vio_stream_id = provider.get_stream_id_from_label(\"vio\")\n",
                "if vio_high_freq_stream_id is not None and vio_stream_id is not None:\n",
                "    T_device_rgb = device_calib.get_transform_device_sensor(\"camera-rgb\")\n",
                "\n",
                "    vio_high_freq_data_num = provider.get_num_data(vio_high_freq_stream_id)\n",
                "\n",
                "    # Record RGB locations in the vio-high-freq trajectory (subsample by 20)\n",
                "    vio_high_freq_subsample_rate = 20\n",
                "    vio_high_freq_trajectory = np.empty([vio_high_freq_data_num // vio_high_freq_subsample_rate + 1, 3])\n",
                "    print(f\"--- vio high freq: num of data is {vio_high_freq_data_num}, size of traj is {vio_high_freq_trajectory.shape}\")\n",
                "    all_high_freq_poses = []\n",
                "    j = 0\n",
                "    for i in range(0, vio_high_freq_data_num, vio_high_freq_subsample_rate):\n",
                "        vio_high_freq_pose = provider.get_vio_high_freq_data_by_index(vio_high_freq_stream_id, i)\n",
                "        T_odometry_rgb = vio_high_freq_pose.transform_odometry_device @ T_device_rgb\n",
                "        vio_high_freq_trajectory[j, :] = T_odometry_rgb.translation()\n",
                "        all_high_freq_poses.append(vio_high_freq_pose)\n",
                "        j = j+1\n",
                "\n",
                "    # Plot camera frustum trace along high freq trajectory\n",
                "    cam_frustums = [None]*len(vio_high_freq_trajectory)\n",
                "    steps = [None] * len(vio_high_freq_trajectory)\n",
                "    for i in range(len(vio_high_freq_trajectory)):\n",
                "        pose = all_high_freq_poses[i]\n",
                "        cam_frustums[i] = build_camera_frustum(pose.transform_odometry_device @ T_device_rgb)\n",
                "        timestamp = pose.tracking_timestamp.total_seconds()\n",
                "        step = dict(method=\"update\", args=[{\"visible\": [False] * len(cam_frustums) + [True] * 2}, {\"title\": \"Trajectory and Point Cloud\"},], label=timestamp,)\n",
                "        step[\"args\"][0][\"visible\"][i] = True  # Toggle i'th trace to \"visible\"\n",
                "        steps[i] = step\n",
                "    cam_frustums[0].visible = True\n",
                "\n",
                "    # Record RGB poses in the vio trajectory, check validity\n",
                "    valid_vio_poses = []\n",
                "    vio_data_num = provider.get_num_data(vio_stream_id)\n",
                "    for i in range(vio_data_num):\n",
                "        vio_data = provider.get_vio_data_by_index(vio_stream_id, i)\n",
                "        # Check if the pose quality is GOOD\n",
                "        if vio_data.pose_quality == TrackingQuality.GOOD:\n",
                "            T_odometry_rgb = (vio_data.transform_odometry_bodyimu @\n",
                "                              vio_data.transform_bodyimu_device @\n",
                "                              cast_SE3_to_SE3f(T_device_rgb))\n",
                "            valid_vio_poses.append(T_odometry_rgb.translation().transpose())\n",
                "    \n",
                "    # Convert the list of good poses to a NumPy array\n",
                "    vio_trajectory = np.array(valid_vio_poses).squeeze()\n",
                "\n",
                "    # Create slider to allow scrubbing and set the layout\n",
                "    sliders = [dict(currentvalue={\"suffix\": \" s\", \"prefix\": \"Time :\"}, pad={\"t\": 5}, steps=steps,)]\n",
                "    layout = go.Layout(\n",
                "        sliders=sliders,\n",
                "        scene=dict(\n",
                "            bgcolor='lightgray',\n",
                "            dragmode='orbit',\n",
                "            aspectmode='data',\n",
                "            xaxis_visible=False,\n",
                "            yaxis_visible=False,\n",
                "            zaxis_visible=False,\n",
                "            camera=dict(\n",
                "            eye=dict(x=0.5, y=0.5, z=0.5),\n",
                "            center=dict(x=0, y=0, z=0),\n",
                "            up=dict(x=0, y=0, z=1)\n",
                "        )),\n",
                "        width=1100,\n",
                "        height=1000,\n",
                "    )\n",
                "\n",
                "    # Plot trajectory\n",
                "    plotter_vio_high_freq_trajectory = go.Scatter3d(x=vio_high_freq_trajectory[:, 0], y=vio_high_freq_trajectory[:, 1], z=vio_high_freq_trajectory[:, 2],\n",
                "                                                    mode=\"markers\", marker={\"size\": 2, \"opacity\": 0.8, \"color\": \"red\"},\n",
                "                                                    name=\"Vio High Freq Trajectory\")\n",
                "    plotter_vio_trajectory = go.Scatter3d(x=vio_trajectory[:, 0], y=vio_trajectory[:, 1], z=vio_trajectory[:, 2],\n",
                "                                          mode=\"markers\", marker={\"size\": 4, \"opacity\": 0.8, \"color\": \"green\"},\n",
                "                                          name=\"Vio Trajectory\")\n",
                "\n",
                "    # draw\n",
                "    plot_figure = go.Figure(data=cam_frustums + [plotter_vio_high_freq_trajectory, plotter_vio_trajectory], layout=layout)\n",
                "    plot_figure.show()\n",
                "\n",
                "else:\n",
                "    print(\"Vio high-freq stream does not exist in the current VRS file. \")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "bfc89c4b-361f-4188-9d5e-50eb2e38994c",
            "metadata": {},
            "source": [
                "# Image color correction and devignetting examples (Aria Gen1 only)\n",
                "## Correcting Color Distortion in Older Aria Captures\n",
                "Videos and images captured with earlier versions of the Aria OS may exhibit color distortion due to inconsistent gamma curves and unconventional color temperatures. This can result in colors appearing inconsistent across images and overly blue.\n",
                "This issue has been resolved in the new OS update V1.13. For images and videos captured before this update, we offer a Color Correction API to address the distortion. The images will be corrected to a reference color temperature of 5000K. \n",
                "\n",
                "Below, we demonstrate how to apply color correction: \n",
                "1. set `set_color_correction` with True, default value is False\n",
                "2. The output from `provider.get_image_data_by_index` would be color corrected. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ee1c8caf-6b6f-42ed-adaf-35c3c593b0b8",
            "metadata": {},
            "outputs": [],
            "source": [
                "if device_version == DeviceVersion.Gen1:\n",
                "    # save source image for comparison\n",
                "    stream_id = provider.get_stream_id_from_label(\"camera-rgb\")\n",
                "    provider.set_color_correction(False)\n",
                "    provider.set_devignetting(False)\n",
                "    src_image_array = provider.get_image_data_by_index(stream_id, 0)[0].to_numpy_array()\n",
                "\n",
                "    provider.set_color_correction(True)\n",
                "    provider.set_devignetting(False)\n",
                "    color_corrected_image_array = provider.get_image_data_by_index(stream_id, 0)[0].to_numpy_array()\n",
                "\n",
                "    # visualize input and results\n",
                "    plt.figure()\n",
                "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
                "    fig.suptitle(f\"Color Correction\")\n",
                "\n",
                "    axes[0].imshow(src_image_array, vmin=0, vmax=255)\n",
                "    axes[0].title.set_text(f\"before color correction\")\n",
                "    axes[1].imshow(color_corrected_image_array, vmin=0, vmax=255)\n",
                "    axes[1].title.set_text(f\"after color correction\")\n",
                "\n",
                "    plt.show()\n",
                "else:\n",
                "    print(\"Color correction feature is Gen1 only\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4d29fa4b",
            "metadata": {},
            "source": [
                "#### Devignetting\n",
                "\n",
                "Devignetting corrects uneven lighting, enhancing image uniformity and clarity. We provide devignetting for camera-rgb full size image [2880, 2880], camera-rgb half size image[1408, 1408] and slam image [640, 480].\n",
                "1. Aria devignetting masks can be downloaded from [Link](https://www.projectaria.com/async/sample/download/?bucket=core&filename=devignetting_masks_bin.zip). It contains the following files:\n",
                "\n",
                "```\n",
                "devignetting_masks_bin\n",
                "|- new_isp\n",
                "   |- slam_devignetting_mask.bin\n",
                "   |- rgb_half_devignetting_mask.bin\n",
                "   |- rgb_full_devignetting_mask.bin\n",
                "|- old_isp\n",
                "   |- slam_devignetting_mask.bin\n",
                "   |- rgb_half_devignetting_mask.bin\n",
                "   |- rgb_full_devignetting_mask.bin\n",
                "```\n",
                "2. Turn on devignetting. Set devignetting mask folder path with the local aria camera devignetting masks folder path.\n",
                "   `set_devignetting(True)`\n",
                "   `mask_folder_path = \"devignetting_masks_bin\"`\n",
                "   `set_devignetting_mask_folder_path(mask_folder_path)`\n",
                "3. The image data from `get_image_data_by_index` will be devignetted. \n",
                "4. (Optional) If you don't want to devignetting feature, turn off by calling `set_devignetting(False)`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "747698e2-5cba-4b03-a90d-2d08b3e53456",
            "metadata": {},
            "outputs": [],
            "source": [
                "if device_version == DeviceVersion.Gen1:\n",
                "    # ==============================================================================\n",
                "    # Step 1: Download devignetting mask\n",
                "    # ==============================================================================\n",
                "    from urllib.request import urlretrieve\n",
                "    import zipfile\n",
                "    import ssl\n",
                "    ssl._create_default_https_context = ssl._create_unverified_context\n",
                "\n",
                "    # Download from url\n",
                "    devignetting_mask_folder_path = os.path.join(repo_path, \"devignetting_masks\")\n",
                "    downloaded_devignetting_mask_zip = os.path.join(devignetting_mask_folder_path, \"aria_camera_devignetting_masks.zip\")\n",
                "    if not os.path.exists(devignetting_mask_folder_path):\n",
                "        os.mkdir(devignetting_mask_folder_path)\n",
                "    urlretrieve(\"https://www.projectaria.com/async/sample/download/?bucket=core&filename=devignetting_masks_bin.zip\", downloaded_devignetting_mask_zip)\n",
                "\n",
                "    # unzip the mask files, with cross-platform compatibility\n",
                "    with zipfile.ZipFile(downloaded_devignetting_mask_zip, 'r') as zip_ref:\n",
                "        # Extract all files\n",
                "        zip_ref.extractall(devignetting_mask_folder_path)\n",
                "\n",
                "        # Print out the filenames\n",
                "        print(f\"Successfully downloaded and extracted the following files for devignetting:\")\n",
                "        for file_info in zip_ref.infolist():\n",
                "            print(file_info.filename)\n",
                "\n",
                "    # ==============================================================================\n",
                "    # Step 2: Turn on devignetting and set devignetting mask folder path\n",
                "    # ==============================================================================\n",
                "    index = 1\n",
                "    provider.set_devignetting(False)\n",
                "    provider.set_color_correction(False)\n",
                "    src_image_array = provider.get_image_data_by_index(stream_id, index)[0].to_numpy_array()\n",
                "    provider.set_devignetting(True)\n",
                "    provider.set_devignetting_mask_folder_path(devignetting_mask_folder_path)\n",
                "\n",
                "    # ==============================================================================\n",
                "    # Step 3: Retrieve Image from stream\n",
                "    # ==============================================================================\n",
                "    devignetted_image_array = provider.get_image_data_by_index(stream_id, index)[0].to_numpy_array()\n",
                "\n",
                "    # visualize input and results\n",
                "    plt.figure()\n",
                "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
                "    fig.suptitle(f\"Image devignetting (camera-rgb)\")\n",
                "\n",
                "    axes[0].imshow(src_image_array, vmin=0, vmax=255)\n",
                "    axes[0].title.set_text(f\"before devignetting\")\n",
                "    axes[1].imshow(devignetted_image_array, vmin=0, vmax=255)\n",
                "    axes[1].title.set_text(f\"after devignetting\")\n",
                "\n",
                "    plt.show()\n",
                "else:\n",
                "    print(\"Devignetting is only supported on Gen1.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f78c9dbb-6d22-4261-b314-aed8584f8346",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "custom": {
            "cells": [],
            "metadata": {
                "fileHeader": "",
                "kernelspec": {
                    "display_name": "Python 3 (ipykernel)",
                    "language": "python",
                    "name": "python3"
                },
                "language_info": {
                    "codemirror_mode": {
                        "name": "ipython",
                        "version": 3
                    },
                    "file_extension": ".py",
                    "mimetype": "text/x-python",
                    "name": "python",
                    "nbconvert_exporter": "python",
                    "pygments_lexer": "ipython3",
                    "version": "3.10.11"
                }
            },
            "nbformat": 4,
            "nbformat_minor": 5
        },
        "indentAmount": 2,
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
