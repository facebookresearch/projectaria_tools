{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Tutorial 7: Machine Perception Services (MPS) for Aria Gen2\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Machine Perception Services, or MPS, is a post-processing cloud service that we provide to Aria users.\n",
    "It runs a set of proprietary, Spatial AI machine perception algorithms, that are designed for Project Aria glasses.\n",
    "MPS is designed to provide superior accuracy and robustness compared to off-the-shelf open algorithms.\n",
    "\n",
    "We are excited to share that we have extended MPS to Aria Gen2 users.\n",
    "Currently, the supported MPS algorithms for Aria Gen2 include:\n",
    "- **SLAM Single-Sequence Trajectory**: generates device trajectories, semi-dense point cloud data, and online calibration.\n",
    "- **Hand Tracking**: generates 21 landmarks, wrist-to-device transforms, palm and wrist normals, and confidence scores.\n",
    "\n",
    "This tutorial focuses on demonstrating how to load and visualize the MPS results.\n",
    "\n",
    "**What you'll learn:**\n",
    "\n",
    "- How to load MPS output data, and definitions of the data types.\n",
    "- How to visualize the MPS data together with Aria VRS files.\n",
    "\n",
    "**Prerequisites**\n",
    "- Complete Tutorial 1 (VrsDataProvider Basics) to understand basic data provider concepts\n",
    "- Complete Tutorial 2 (Device Calibration) to understand how to properly use calibration in Aria data.\n",
    "- Download Aria Gen2 sample data: [VRS](https://www.projectaria.com/async/sample/download/?bucket=core&filename=aria_gen2_sample_data_1.vrs) and [MPS output zip file](https://www.projectaria.com/async/sample/download/?bucket=core&filename=aria_gen2_sample_data_1_mps_output_dec_2025.zip).\n",
    "\n",
    "### ⚠️ Important Notes\n",
    "- **Google Colab Users:**\n",
    "  If you encounter a `ModuleNotFoundError: No module named 'rerun'` error after installing `rerun-sdk`, Colab may not recognize the new package until the runtime is restarted.\n",
    "  **Fix:** Go to **Runtime → Restart session and run all**.\n",
    "\n",
    "- **Visualization Issue :**\n",
    "  If a Rerun visualization window does not appear, this may be due to a known caching issue. Simply re-run the visualization cell to resolve it.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment (Google Colab)\n",
    "\n",
    "If running on Google Colab, install projectaria-tools and download sample data (VRS file and MPS output).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "google_colab_env = 'google.colab' in str(get_ipython())\n",
    "\n",
    "if google_colab_env:\n",
    "    print(\"Running from Google Colab, installing projectaria_tools and downloading sample data\")\n",
    "\n",
    "    # Install projectaria-tools\n",
    "    !pip install projectaria-tools['all']==2.0.0\n",
    "\n",
    "    # Set up data path\n",
    "    vrs_sample_path = \"./vrs_sample_data\"\n",
    "\n",
    "    # Sample VRS file and MPS output URLs\n",
    "    vrs_url = \"https://www.projectaria.com/async/sample/download/?bucket=core&filename=aria_gen2_sample_data_1.vrs\"\n",
    "    mps_url = \"https://www.projectaria.com/async/sample/download/?bucket=core&filename=aria_gen2_sample_data_1_mps_output_dec_2025.zip\"\n",
    "\n",
    "    vrs_filename = \"aria_gen2_sample_data_1.vrs\"\n",
    "    mps_zip_filename = \"aria_gen2_sample_data_1_mps_output_dec_2025.zip\"\n",
    "\n",
    "    vrs_file_path = os.path.join(vrs_sample_path, vrs_filename)\n",
    "    mps_zip_path = os.path.join(vrs_sample_path, mps_zip_filename)\n",
    "    mps_folder_path = os.path.join(vrs_sample_path, \"mps_output\")\n",
    "\n",
    "    # Download and unzip commands\n",
    "    command_list = [\n",
    "        f\"mkdir -p {vrs_sample_path}\",\n",
    "        f'curl -o {vrs_file_path} -C - -O -L \"{vrs_url}\"',\n",
    "        f'curl -o {mps_zip_path} -C - -O -L \"{mps_url}\"',\n",
    "        f\"unzip -o {mps_zip_path} -d {mps_folder_path}\"\n",
    "    ]\n",
    "\n",
    "    # Execute the commands for downloading dataset\n",
    "    print(f\"Downloading VRS and MPS sample data...\")\n",
    "    for command in command_list:\n",
    "        !$command\n",
    "\n",
    "    print(f\"Download complete!\")\n",
    "    print(f\"VRS file saved to: {vrs_file_path}\")\n",
    "    print(f\"MPS data extracted to: {mps_folder_path}\")\n",
    "\n",
    "    # Running this command to trigger early failure of importing ReRun.\n",
    "    # Should be resolved by restarting the Colab session.\n",
    "    import rerun as rr\n",
    "else:\n",
    "    # For local environment, user needs to specify their own paths\n",
    "    vrs_file_path = \"path/to/your/recording.vrs\"\n",
    "    mps_folder_path = \"path/to/your/mps/folder/\"\n",
    "    print(f\"Please update vrs_file_path and mps_folder_path to point to your data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from projectaria_tools.core import data_provider, mps\n",
    "import os\n",
    "\n",
    "# Load local VRS file\n",
    "vrs_data_provider = data_provider.create_vrs_data_provider(vrs_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MPS - SLAM\n",
    "\n",
    "### [MPS - SLAM] Output Files\n",
    "MPS output result files are categorized into sub-folders by algorithms. \n",
    "For SLAM algorithm output, it generates the following files: \n",
    "- `closed_loop_trajectory.csv`\n",
    "- `open_loop_trajectory.csv`\n",
    "- `semidense_observations.csv.gz`\n",
    "- `semidense_points.csv.gz`\n",
    "- `online_calibration.jsonl`\n",
    "- `summary.json`\n",
    "\n",
    "Please refer to the [MPS Wiki page](https://facebookresearch.github.io/projectaria_tools/docs/data_formats/mps/slam) for details of each file. \n",
    "\n",
    "### [MPS - SLAM] Closed vs Open Loop trajectory\n",
    "\n",
    "MPS SLAM algorithm outputs 2 trajectory files (see [wiki page](https://facebookresearch.github.io/projectaria_tools/docs/data_formats/mps/slam/mps_trajectory) for data type definitions): \n",
    "- **Open loop trajectory**: High-frequency (1kHz) odometry from visual-inertial odometry (VIO), accurate over short periods but drifts over time and distance.\n",
    "- **Closed loop trajectory**: High-frequency (1kHz) pose from mapping with loop closure corrections, reducing drift but possibly less accurate locally over short spans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from projectaria_tools.core.mps.utils import (\n",
    "    filter_points_from_confidence,\n",
    "    get_nearest_pose,\n",
    ")\n",
    "\n",
    "print(\"=== MPS - Closed loop trajectory ===\")\n",
    "\n",
    "# Load MPS closed-loop trajectory data\n",
    "closed_loop_trajectory_file = os.path.join(\n",
    "    mps_folder_path, \"slam\", \"closed_loop_trajectory.csv\"\n",
    ")\n",
    "closed_loop_trajectory = mps.read_closed_loop_trajectory(closed_loop_trajectory_file)\n",
    "\n",
    "# Print out the content of the first sample in closed_loop_trajectory\n",
    "if closed_loop_trajectory:\n",
    "    sample = closed_loop_trajectory[0]\n",
    "    print(\"ClosedLoopTrajectoryPose sample:\")\n",
    "    print(f\"  tracking_timestamp: {int(sample.tracking_timestamp.total_seconds() * 1e6)} us\")\n",
    "    print(f\"  utc_timestamp: {int(sample.utc_timestamp.total_seconds() * 1e6)} us\")\n",
    "    print(f\"  transform_world_device:\\n{sample.transform_world_device}\")\n",
    "    print(f\"  device_linear_velocity_device: {sample.device_linear_velocity_device}\")\n",
    "    print(f\"  angular_velocity_device: {sample.angular_velocity_device}\")\n",
    "    print(f\"  quality_score: {sample.quality_score}\")\n",
    "    print(f\"  gravity_world: {sample.gravity_world}\")\n",
    "    print(f\"  graph_uid: {sample.graph_uid}\")\n",
    "else:\n",
    "    print(\"closed_loop_trajectory is empty.\")\n",
    "\n",
    "\n",
    "print(\"=== MPS - Open loop trajectory ===\")\n",
    "\n",
    "# Load MPS open-loop trajectory data\n",
    "open_loop_trajectory_file = os.path.join(\n",
    "    mps_folder_path, \"slam\", \"open_loop_trajectory.csv\"\n",
    ")\n",
    "open_loop_trajectory = mps.read_open_loop_trajectory(open_loop_trajectory_file)\n",
    "\n",
    "# Print out the content of the first sample in open_loop_trajectory\n",
    "if open_loop_trajectory:\n",
    "    sample = open_loop_trajectory[0]\n",
    "    print(\"OpenLoopTrajectoryPose sample:\")\n",
    "    print(f\"  tracking_timestamp: {int(sample.tracking_timestamp.total_seconds() * 1e6)} us\")\n",
    "    print(f\"  utc_timestamp: {int(sample.utc_timestamp.total_seconds() * 1e6)} us\")\n",
    "    print(f\"  transform_odometry_device:\\n{sample.transform_odometry_device}\")\n",
    "    print(f\"  device_linear_velocity_odometry: {sample.device_linear_velocity_odometry}\")\n",
    "    print(f\"  angular_velocity_device: {sample.angular_velocity_device}\")\n",
    "    print(f\"  quality_score: {sample.quality_score}\")\n",
    "    print(f\"  gravity_odometry: {sample.gravity_odometry}\")\n",
    "    print(f\"  session_uid: {sample.session_uid}\")\n",
    "else:\n",
    "    print(\"open_loop_trajectory is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [MPS - SLAM] Semi-dense Point Cloud and Observations\n",
    "\n",
    "MPS SLAM algorithm outputs 2 files related to semi-dense point cloud (see [wiki page](https://facebookresearch.github.io/projectaria_tools/docs/data_formats/mps/slam/mps_pointcloud) for data type definitions): \n",
    "- `semidense_points.csv.gz`: Global points in the world coordinate frame. \n",
    "- `semidense_observations.csv.gz`: Point observations for each camera, at each timestamp.\n",
    "\n",
    "Note that semidense point files are normally large, therefore loading them may take some time. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== MPS - Semi-dense Point Cloud ===\")\n",
    "\n",
    "# Load MPS semi-dense point cloud data\n",
    "semidense_points_file = os.path.join(\n",
    "    mps_folder_path, \"slam\", \"semidense_points.csv.gz\"\n",
    ")\n",
    "semidense_points = mps.read_global_point_cloud(semidense_points_file)\n",
    "\n",
    "# Print out the content of the first sample in semidense_points\n",
    "if semidense_points:\n",
    "    sample = semidense_points[0]\n",
    "    print(\"GlobalPointPosition sample:\")\n",
    "    print(f\"  uid: {sample.uid}\")\n",
    "    print(f\"  graph_uid: {sample.graph_uid}\")\n",
    "    print(f\"  position_world: {sample.position_world}\")\n",
    "    print(f\"  inverse_distance_std: {sample.inverse_distance_std}\")\n",
    "    print(f\"  distance_std: {sample.distance_std}\")\n",
    "    print(f\"Total number of semi-dense points: {len(semidense_points)}\")\n",
    "else:\n",
    "    print(\"semidense_points is empty.\")\n",
    "\n",
    "# Filter semidense points by inv_dep or depth.\n",
    "# The filter will KEEP points with (inv_dep or depth < threshold)\n",
    "filtered_semidense_points = filter_points_from_confidence(raw_points = semidense_points, threshold_invdep = 1e-3, threshold_dep = 5e-2)\n",
    "print(f\"Filtering semidense points from a total of {len(semidense_points)} points down to {len(filtered_semidense_points)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== MPS - Semi-dense Point Observations ===\")\n",
    "\n",
    "# Load MPS semi-dense point observations data\n",
    "semidense_observations_file = os.path.join(\n",
    "    mps_folder_path, \"slam\", \"semidense_observations.csv.gz\"\n",
    ")\n",
    "semidense_observations = mps.read_point_observations(semidense_observations_file)\n",
    "\n",
    "# Print out the content of the first sample in semidense_observations\n",
    "if semidense_observations:\n",
    "    sample = semidense_observations[0]\n",
    "    print(\"PointObservation sample:\")\n",
    "    print(f\"  point_uid: {sample.point_uid}\")\n",
    "    print(f\"  frame_capture_timestamp: {int(sample.frame_capture_timestamp.total_seconds() * 1e6)} us\")\n",
    "    print(f\"  camera_serial: {sample.camera_serial}\")\n",
    "    print(f\"  uv: {sample.uv}\")\n",
    "    print(f\"Total number of point observations: {len(semidense_observations)}\")\n",
    "else:\n",
    "    print(\"semidense_observations is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [MPS - HandTracking]\n",
    "\n",
    "The MPS Hand Tracking algorithm augments each sequence with temporal hand pose estimates produced offline. Each sequence folder contains a `hand_tracking/` directory with `hand_tracking_results.csv` and metadata summaries that describe the quality of the run. The CSV stores one `HandTrackingResult` per device timestamp and includes left/right hand outputs when a hand is detected.\n",
    "\n",
    "#### Hand Tracking Outputs\n",
    "- **21 landmarks** per detected hand expressed in the device frame\n",
    "- **Wrist-to-device transform** capturing the full 6DoF pose of the wrist\n",
    "- **Palm and wrist normals** to reason about hand orientation\n",
    "- **Confidence scores** indicating tracking quality for each hand\n",
    "\n",
    "#### Query Utilities\n",
    "The `projectaria_tools.core.mps.hand_tracking` module exposes helpers for loading and working with results:\n",
    "- `read_hand_tracking_results(path)` loads the full time series into memory\n",
    "- `HandTrackingResult` objects provide direct access to landmark positions, wrist transforms, and helper methods such as `get_wrist_position_device()`\n",
    "\n",
    "In the next cell we load the CSV file and inspect a sample entry to understand the data layout before visualizing it alongside SLAM outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== MPS - Hand Tracking ===\")\n",
    "\n",
    "hand_tracking_results_file = os.path.join(\n",
    "    mps_folder_path, \"hand_tracking\", \"hand_tracking_results.csv\"\n",
    ")\n",
    "hand_tracking_results = mps.hand_tracking.read_hand_tracking_results(\n",
    "    hand_tracking_results_file\n",
    ")\n",
    "\n",
    "if hand_tracking_results:\n",
    "    if len(hand_tracking_results) > 10:\n",
    "        sample = hand_tracking_results[10] # get stable hand tracking result, since first hand tracking result in example vrs might be empty\n",
    "    else:\n",
    "        sample = hand_tracking_results[0]\n",
    "    sample_ts_us = int(sample.tracking_timestamp.total_seconds() * 1e6)\n",
    "    print(f\"Sample tracking timestamp: {sample_ts_us} us\")\n",
    "    print(f\"Total number of hand tracking results: {len(hand_tracking_results)}\")\n",
    "\n",
    "    for handedness, hand in ((\"Left\", sample.left_hand), (\"Right\", sample.right_hand)):\n",
    "        if hand is None:\n",
    "            print(f\"  {handedness} hand: not available in this sample\")\n",
    "            continue\n",
    "\n",
    "        landmarks = hand.landmark_positions_device\n",
    "        landmark_count = len(landmarks) if landmarks is not None else 0\n",
    "        print(f\"  {handedness} hand confidence: {hand.confidence:.2f}\")\n",
    "        print(f\"  {handedness} hand landmark count: {landmark_count}\")\n",
    "        wrist_position = hand.get_wrist_position_device()\n",
    "        palm_position = hand.get_palm_position_device()\n",
    "        print(\n",
    "            f\"  {handedness} wrist position (device frame): {wrist_position}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"  {handedness} palm position (device frame): {palm_position}\"\n",
    "        )\n",
    "else:\n",
    "    print(\"hand_tracking_results is empty.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [MPS] Visualization\n",
    "\n",
    "In the following code snippet, we demonstrate how to visualize both the MPS SLAM outputs and hand tracking results in a 3D view.\n",
    "\n",
    "We first prepare a short trajectory segment, extract the semi-dense point cloud with per-frame observations, and attach the nearest hand tracking sample for each timestamp. Finally, we plot everything in Rerun to explore trajectories, points, and articulated hands together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Hand Tracking Data\n",
    "\n",
    "In this step we restrict the hand tracking results to the same time window used for the trajectory segment. Filtering the list up front keeps the visualization loop fast and ensures that the nearest hand pose we display stays in sync with the device motion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to slice the hand tracking timeline to the selected trajectory window.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# A helper coloring function\n",
    "def color_from_zdepth(z_depth_m: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Map z-depth (meters, along the camera's forward axis) to a bright Viridis-like RGB color.\n",
    "    - If z_depth_m <= 0 (point is behind the camera), return white [255, 255, 255].\n",
    "    - Near (0.2 m) -> yellow, Far (3.0 m) -> purple.\n",
    "    Returns an array of shape (3,) with dtype=uint8.\n",
    "    \"\"\"\n",
    "    if not np.isfinite(z_depth_m) or z_depth_m <= 0.0:\n",
    "        return np.array([0, 0, 0], dtype=np.uint8)\n",
    "\n",
    "    NEAR_METERS, FAR_METERS = 0.2, 5.0\n",
    "\n",
    "    # Normalize to [0,1], then flip so near → bright (yellow), far → dark (purple)\n",
    "    clamped = min(max(float(z_depth_m), NEAR_METERS), FAR_METERS)\n",
    "    normalized_position = (clamped - NEAR_METERS) / (FAR_METERS - NEAR_METERS + 1e-12)\n",
    "    gradient_position = 1.0 - normalized_position\n",
    "\n",
    "    # Viridis-like anchor colors: purple → blue → teal → green → yellow\n",
    "    color_stops = [\n",
    "        (68, 1, 84),\n",
    "        (59, 82, 139),\n",
    "        (33, 145, 140),\n",
    "        (94, 201, 98),\n",
    "        (253, 231, 37),\n",
    "    ]\n",
    "\n",
    "    # Locate segment and blend between its endpoints\n",
    "    segment_count = len(color_stops) - 1\n",
    "    continuous_index = gradient_position * segment_count\n",
    "    lower_segment_index = int(continuous_index)\n",
    "\n",
    "    if lower_segment_index >= segment_count:\n",
    "        red, green, blue = color_stops[-1]\n",
    "    else:\n",
    "        segment_fraction = continuous_index - lower_segment_index\n",
    "        r0, g0, b0 = color_stops[lower_segment_index]\n",
    "        r1, g1, b1 = color_stops[lower_segment_index + 1]\n",
    "        red   = r0 + segment_fraction * (r1 - r0)\n",
    "        green = g0 + segment_fraction * (g1 - g0)\n",
    "        blue  = b0 + segment_fraction * (b1 - b0)\n",
    "\n",
    "    return np.array([int(red), int(green), int(blue)], dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Preparing MPS SLAM results for visualization ===\")\n",
    "\n",
    "# Check if we have valid SLAM data to visualize\n",
    "if not closed_loop_trajectory or not semidense_points:\n",
    "    raise RuntimeError(\"Warning: This tutorial requires valid MPS SLAM data to run.\")\n",
    "\n",
    "# -----------\n",
    "# Prepare Trajectory data\n",
    "# -----------\n",
    "# Select a short segment of trajectory (e.g., first 5000 samples, subsampled by 50)\n",
    "segment_length = min(50000, len(closed_loop_trajectory))\n",
    "trajectory_segment = closed_loop_trajectory[:segment_length:50]\n",
    "timestamp_to_pose = {\n",
    "    pose.tracking_timestamp: pose for pose in trajectory_segment\n",
    "}\n",
    "print(f\"Finished preparing a trajectory of length {len(trajectory_segment)}... \")\n",
    "\n",
    "# -----------\n",
    "# Prepare Semidense point data\n",
    "# -----------\n",
    "# Filter the semidense point cloud by confidence and limit max point count, and extract the point positions\n",
    "filtered_semidense_point_cloud_data = filter_points_from_confidence(semidense_points)\n",
    "points_positions = np.array(\n",
    "    [\n",
    "        point.position_world for point in filtered_semidense_point_cloud_data\n",
    "    ]\n",
    ")\n",
    "print(f\"Finished preparing filtered semidense points cloud of {len(filtered_semidense_point_cloud_data)} points... \")\n",
    "\n",
    "# -----------\n",
    "# Prepare Semidense observation data\n",
    "# -----------\n",
    "# Based on RGB observations, create a per-timestamp point position list, and color them according to its distance from RGB camera\n",
    "point_uid_to_position = {\n",
    "    point.uid: np.array(point.position_world) for point in filtered_semidense_point_cloud_data\n",
    "}\n",
    "\n",
    "# A helper function that creates a easier-to-query mapping to obtain observations according to timestamps\n",
    "slam_1_serial = vrs_data_provider.get_device_calibration().get_camera_calib(\"slam-front-left\").get_serial_number()\n",
    "timestamp_to_point_positions = defaultdict(list)  # t_ns -> [position, position, ...]\n",
    "timestamp_to_point_colors = defaultdict(list) # t_ns -> [color, color, ...]\n",
    "\n",
    "for obs in semidense_observations:\n",
    "    # Only add observations for SLAM_1 camera, and if the timestamp is in the chosen trajectory segment\n",
    "    if (\n",
    "        obs.camera_serial == slam_1_serial and\n",
    "        obs.frame_capture_timestamp in timestamp_to_pose and\n",
    "        obs.point_uid in point_uid_to_position):\n",
    "        # Insert point position\n",
    "        obs_timestamp = obs.frame_capture_timestamp\n",
    "        point_position = point_uid_to_position[obs.point_uid]\n",
    "        timestamp_to_point_positions[obs_timestamp].append(point_position)\n",
    "\n",
    "        # Insert point color\n",
    "        T_world_device = timestamp_to_pose[obs_timestamp].transform_world_device\n",
    "        point_in_device = T_world_device.inverse() @ point_position\n",
    "        point_z_depth = point_in_device.squeeze()[2]\n",
    "        point_color = color_from_zdepth(point_z_depth)\n",
    "        timestamp_to_point_colors[obs_timestamp].append(point_color)\n",
    "\n",
    "from itertools import islice\n",
    "print(\"Finished preparing semidense points observations: \")\n",
    "for timestamp, points in islice(timestamp_to_point_positions.items(), 5):\n",
    "    print(\n",
    "        f\"\\t timestamp {int(timestamp.total_seconds() * 1e9)} ns has {len(points)} observed points in slam-front-left view. \"\n",
    "    )\n",
    "print(\"\\t ...\")\n",
    "\n",
    "# Hand tracking data is prepared in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Preparing MPS Hand Tracking data ===\")\n",
    "\n",
    "hand_tracking_results_segment: list[mps.hand_tracking.HandTrackingResult] = []\n",
    "\n",
    "if not hand_tracking_results:\n",
    "    print(\"No hand tracking results loaded; visualization will skip hands.\")\n",
    "else:\n",
    "    if trajectory_segment:\n",
    "        segment_start_ns = int(\n",
    "            trajectory_segment[0].tracking_timestamp.total_seconds() * 1e9\n",
    "        )\n",
    "        segment_end_ns = int(\n",
    "            trajectory_segment[-1].tracking_timestamp.total_seconds() * 1e9\n",
    "        )\n",
    "\n",
    "        hand_tracking_results_segment = [\n",
    "            result\n",
    "            for result in hand_tracking_results\n",
    "            if segment_start_ns\n",
    "            <= int(result.tracking_timestamp.total_seconds() * 1e9)\n",
    "            <= segment_end_ns\n",
    "        ]\n",
    "\n",
    "    if hand_tracking_results_segment:\n",
    "        print(\n",
    "            \"Finished preparing hand tracking results: \"\n",
    "            f\"{len(hand_tracking_results_segment)} samples overlap with the trajectory segment.\"\n",
    "        )\n",
    "    else:\n",
    "        hand_tracking_results_segment = hand_tracking_results\n",
    "        print(\n",
    "            \"Hand tracking results do not overlap with the selected trajectory segment; \"\n",
    "            \"visualization will reuse the nearest available hand pose.\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rerun as rr\n",
    "import numpy as np\n",
    "from projectaria_tools.utils.rerun_helpers import (\n",
    "    AriaGlassesOutline,\n",
    "    ToTransform3D,\n",
    "    ToBox3D,\n",
    "    create_hand_skeleton_from_landmarks,\n",
    ")\n",
    "from projectaria_tools.core.mps.utils import (\n",
    "    filter_points_from_confidence,\n",
    "    get_nearest_pose,\n",
    ")\n",
    "\n",
    "HAND_COLORS = {\n",
    "    \"left\": np.array([102, 204, 255], dtype=np.uint8),\n",
    "    \"right\": np.array([255, 153, 102], dtype=np.uint8),\n",
    "}\n",
    "HAND_LANDMARK_RADIUS = 1e-2\n",
    "HAND_KEYPOINT_RADIUS = 1.5e-2\n",
    "HAND_SKELETON_RADIUS = 7e-3\n",
    "\n",
    "\n",
    "def log_hand_tracking_result(hand_result):\n",
    "    rr.log(\"world/device/hand-tracking\", rr.Clear.recursive())\n",
    "    if hand_result is None:\n",
    "        return\n",
    "\n",
    "    for label in (\"left\", \"right\"):\n",
    "        hand = getattr(hand_result, f\"{label}_hand\")\n",
    "        if hand is None or hand.landmark_positions_device is None:\n",
    "            continue\n",
    "\n",
    "        landmarks = np.array(hand.landmark_positions_device, dtype=np.float32)\n",
    "        if landmarks.size == 0:\n",
    "            continue\n",
    "\n",
    "        color = HAND_COLORS[label]\n",
    "        colors = np.repeat(color[np.newaxis, :], landmarks.shape[0], axis=0)\n",
    "\n",
    "        rr.log(\n",
    "            f\"world/device/hand-tracking/{label}/landmarks\",\n",
    "            rr.Points3D(\n",
    "                positions=landmarks,\n",
    "                colors=colors,\n",
    "                radii=HAND_LANDMARK_RADIUS,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        skeleton = create_hand_skeleton_from_landmarks(landmarks)\n",
    "        if skeleton:\n",
    "            rr.log(\n",
    "                f\"world/device/hand-tracking/{label}/skeleton\",\n",
    "                rr.LineStrips3D(\n",
    "                    skeleton,\n",
    "                    colors=[HAND_COLORS[label].tolist()] * len(skeleton),\n",
    "                    radii=HAND_SKELETON_RADIUS,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        wrist = hand.get_wrist_position_device()\n",
    "        palm = hand.get_palm_position_device()\n",
    "        keypoints = [\n",
    "            np.array(position, dtype=np.float32)\n",
    "            for position in (wrist, palm)\n",
    "            if position is not None\n",
    "        ]\n",
    "        if keypoints:\n",
    "            rr.log(\n",
    "                f\"world/device/hand-tracking/{label}/keypoints\",\n",
    "                rr.Points3D(\n",
    "                    positions=keypoints,\n",
    "                    colors=np.repeat(color[np.newaxis, :], len(keypoints), axis=0),\n",
    "                    radii=HAND_KEYPOINT_RADIUS,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "\n",
    "print(\"=== Visualizing MPS Results in 3D ===\")\n",
    "\n",
    "# Initialize Rerun\n",
    "rr.init(\"MPS Visualization\")\n",
    "\n",
    "# Set up the 3D scene\n",
    "rr.log(\"world\", rr.ViewCoordinates.RIGHT_HAND_Z_UP, static=True)\n",
    "\n",
    "# Log point cloud\n",
    "rr.log(\n",
    "    \"world/semidense_points\",\n",
    "    rr.Points3D(\n",
    "        positions=points_positions,\n",
    "        colors=[255, 255, 255, 125],\n",
    "        radii=0.001\n",
    "    ),\n",
    "    static=True\n",
    ")\n",
    "\n",
    "# Aria glass outline for visualization purpose\n",
    "device_calib = vrs_data_provider.get_device_calibration()\n",
    "aria_glasses_point_outline = AriaGlassesOutline(\n",
    "    device_calib, use_cad_calib=True\n",
    ")\n",
    "\n",
    "# Plot Closed loop trajectory\n",
    "closed_loop_traj_cached_full = []\n",
    "observation_points_cached = None\n",
    "observation_colors_cached = None\n",
    "hand_tracking_timestamps_ns = [\n",
    "    int(result.tracking_timestamp.total_seconds() * 1e9)\n",
    "    for result in hand_tracking_results_segment\n",
    "]\n",
    "hand_tracking_index = 0\n",
    "latest_hand_tracking_result = None\n",
    "\n",
    "for closed_loop_pose in trajectory_segment:\n",
    "    capture_timestamp_ns = int(closed_loop_pose.tracking_timestamp.total_seconds() * 1e9)\n",
    "    rr.set_time_nanos(\"device_time\", capture_timestamp_ns)\n",
    "\n",
    "    if hand_tracking_results_segment:\n",
    "        while (\n",
    "            hand_tracking_index < len(hand_tracking_results_segment)\n",
    "            and hand_tracking_timestamps_ns[hand_tracking_index] <= capture_timestamp_ns\n",
    "        ):\n",
    "            latest_hand_tracking_result = hand_tracking_results_segment[hand_tracking_index]\n",
    "            hand_tracking_index += 1\n",
    "    else:\n",
    "        latest_hand_tracking_result = None\n",
    "\n",
    "    T_world_device = closed_loop_pose.transform_world_device\n",
    "\n",
    "    # Log device pose as a coordinate frame\n",
    "    rr.log(\n",
    "        \"world/device\",\n",
    "        ToTransform3D(\n",
    "            T_world_device,\n",
    "            axis_length=0.05,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    log_hand_tracking_result(latest_hand_tracking_result)\n",
    "\n",
    "    # Plot Aria glass outline\n",
    "    rr.log(\n",
    "        \"world/device/glasses_outline\",\n",
    "        rr.LineStrips3D(\n",
    "            aria_glasses_point_outline,\n",
    "            colors=[150,200,40],\n",
    "            radii=5e-3,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Plot gravity direction vector\n",
    "    rr.log(\n",
    "        \"world/vio_gravity\",\n",
    "        rr.Arrows3D(\n",
    "            origins=[T_world_device.translation()[0]],\n",
    "            vectors=[\n",
    "                closed_loop_pose.gravity_world * 1e-2\n",
    "            ],  # length converted from 9.8 meter -> 10 cm\n",
    "            colors=[101,67,33],\n",
    "            radii=5e-3,\n",
    "        ),\n",
    "        static=False,\n",
    "    )\n",
    "\n",
    "    # Update cached results for observations. Cache is needed because observation has a much lower freq than high-freq trajectory.\n",
    "    if closed_loop_pose.tracking_timestamp in timestamp_to_point_positions.keys():\n",
    "        observation_points_cached = timestamp_to_point_positions[closed_loop_pose.tracking_timestamp]\n",
    "        observation_colors_cached = timestamp_to_point_colors[closed_loop_pose.tracking_timestamp]\n",
    "    if observation_points_cached is not None:\n",
    "        rr.log(\n",
    "            \"world/semidense_observations\",\n",
    "            rr.Points3D(\n",
    "            positions = observation_points_cached,\n",
    "            colors = observation_colors_cached,\n",
    "            radii=0.01\n",
    "            ),\n",
    "            static = False\n",
    "        )\n",
    "\n",
    "\n",
    "    # Plot the entire VIO trajectory that are cached so far\n",
    "    closed_loop_traj_cached_full.append(T_world_device.translation()[0])\n",
    "    rr.log(\n",
    "        \"world/vio_trajectory\",\n",
    "        rr.LineStrips3D(\n",
    "            closed_loop_traj_cached_full,\n",
    "            colors=[173, 216, 255],\n",
    "            radii=5e-3,\n",
    "        ),\n",
    "        static=False,\n",
    "    )\n",
    "\n",
    "rr.notebook_show()"
   ]
  }
 ],
 "metadata": {
  "fileHeader": "",
  "fileUid": "450dd897-9f2e-40f4-8f64-d43f63f83bf6",
  "isAdHoc": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
