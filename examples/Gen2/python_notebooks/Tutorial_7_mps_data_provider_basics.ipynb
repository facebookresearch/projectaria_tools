{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fccb05e",
   "metadata": {},
   "source": [
    "# Tutorial 7: Machine Perception Services (MPS) for Aria Gen2\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Machine Perception Services, or MPS, is a post-processing cloud service that we provide to Aria users. \n",
    "It runs a set of priprietary, Spatial AI machine perception algorithms, that are designed for Project Aria glasses. \n",
    "MPS is designed to provide superior accuracy and robustness compared to off-the-shelf open algorithms. \n",
    "\n",
    "We are excited to share that we have extended MPS to Aria-Gen2 users. \n",
    "Currently, the supported MPS algorithms for Aria Gen2 include: \n",
    "- **SLAM Single Sequence Trajectory**: generates device trajectories, semidense-point cloud data, online calibration.  \n",
    "\n",
    "This tutorial focuses on demonstrating how to load and visualize the MPS results. \n",
    "\n",
    "**What you'll learn:**\n",
    "\n",
    "- How to load MPS output data, and definitions of the data types. \n",
    "- How to visualize the MPS data together with Aria VRS files. \n",
    "\n",
    "**Prerequisites**\n",
    "- Complete Tutorial 1 (VrsDataProvider Basics) to understand basic data provider concepts\n",
    "- Complete Tutorial 2 (Device Calibration) to understand how to properly use calibration in Aria data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3643bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from projectaria_tools.core import data_provider, mps\n",
    "import os\n",
    "\n",
    "# Load local VRS file\n",
    "vrs_file_path = \"path/to/your/recording.vrs\"\n",
    "vrs_data_provider = data_provider.create_vrs_data_provider(vrs_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e195ee",
   "metadata": {},
   "source": [
    "## MPS - SLAM\n",
    "\n",
    "### [MPS - SLAM] Output Files\n",
    "MPS output result files are categorized into sub-folders by algorithms. \n",
    "For SLAM algorithm output, it generates the following files: \n",
    "- `closed_loop_trajectory.csv`\n",
    "- `open_loop_trajectory.csv`\n",
    "- `semidense_observations.csv.gz`\n",
    "- `semidense_points.csv.gz`\n",
    "- `online_calibration.jsonl`\n",
    "- `summary.json`\n",
    "\n",
    "Please refer to the [MPS Wiki page](https://facebookresearch.github.io/projectaria_tools/docs/data_formats/mps/slam) for details of each file. \n",
    "\n",
    "### [MPS - SLAM] Closed vs Open Loop trajectory\n",
    "\n",
    "MPS SLAM algorithm outputs 2 trajectory files (see [wiki page](https://facebookresearch.github.io/projectaria_tools/docs/data_formats/mps/slam/mps_trajectory) for data type definitions): \n",
    "- **Open loop trajectory**: High-frequency (1kHz) odometry from visual-inertial odometry (VIO), accurate over short periods but drifts over time and distance.\n",
    "- **Closed loop trajectory**: High-frequency (1kHz) pose from mapping with loop closure corrections, reducing drift but possibly less accurate locally over short spans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56124fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from projectaria_tools.core.mps.utils import (\n",
    "    filter_points_from_confidence,\n",
    "    get_nearest_pose,\n",
    ")\n",
    "\n",
    "print(\"=== MPS - Closed loop trajectory ===\")\n",
    "\n",
    "# Load MPS closed-loop trajectory data\n",
    "mps_folder_path = \"path/to/your/mps/folder/\"\n",
    "closed_loop_trajectory_file = os.path.join(\n",
    "    mps_folder_path, \"slam\", \"closed_loop_trajectory.csv\"\n",
    ")\n",
    "closed_loop_trajectory = mps.read_closed_loop_trajectory(closed_loop_trajectory_file)\n",
    "\n",
    "# Print out the content of the first sample in closed_loop_trajectory\n",
    "if closed_loop_trajectory:\n",
    "    sample = closed_loop_trajectory[0]\n",
    "    print(\"ClosedLoopTrajectoryPose sample:\")\n",
    "    print(f\"  tracking_timestamp: {int(sample.tracking_timestamp.total_seconds() * 1e6)} us\")\n",
    "    print(f\"  utc_timestamp: {int(sample.utc_timestamp.total_seconds() * 1e6)} us\")\n",
    "    print(f\"  transform_world_device:\\n{sample.transform_world_device}\")\n",
    "    print(f\"  device_linear_velocity_device: {sample.device_linear_velocity_device}\")\n",
    "    print(f\"  angular_velocity_device: {sample.angular_velocity_device}\")\n",
    "    print(f\"  quality_score: {sample.quality_score}\")\n",
    "    print(f\"  gravity_world: {sample.gravity_world}\")\n",
    "    print(f\"  graph_uid: {sample.graph_uid}\")\n",
    "else:\n",
    "    print(\"closed_loop_trajectory is empty.\")\n",
    "\n",
    "\n",
    "print(\"=== MPS - Open loop trajectory ===\")\n",
    "\n",
    "# Load MPS open-loop trajectory data\n",
    "open_loop_trajectory_file = os.path.join(\n",
    "    mps_folder_path, \"slam\", \"open_loop_trajectory.csv\"\n",
    ")\n",
    "open_loop_trajectory = mps.read_open_loop_trajectory(open_loop_trajectory_file)\n",
    "\n",
    "# Print out the content of the first sample in open_loop_trajectory\n",
    "if open_loop_trajectory:\n",
    "    sample = open_loop_trajectory[0]\n",
    "    print(\"OpenLoopTrajectoryPose sample:\")\n",
    "    print(f\"  tracking_timestamp: {int(sample.tracking_timestamp.total_seconds() * 1e6)} us\")\n",
    "    print(f\"  utc_timestamp: {int(sample.utc_timestamp.total_seconds() * 1e6)} us\")\n",
    "    print(f\"  transform_odometry_device:\\n{sample.transform_odometry_device}\")\n",
    "    print(f\"  device_linear_velocity_odometry: {sample.device_linear_velocity_odometry}\")\n",
    "    print(f\"  angular_velocity_device: {sample.angular_velocity_device}\")\n",
    "    print(f\"  quality_score: {sample.quality_score}\")\n",
    "    print(f\"  gravity_odometry: {sample.gravity_odometry}\")\n",
    "    print(f\"  session_uid: {sample.session_uid}\")\n",
    "else:\n",
    "    print(\"open_loop_trajectory is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58abddc",
   "metadata": {},
   "source": [
    "### [MPS - SLAM] Semi-dense Point Cloud and Observations\n",
    "\n",
    "MPS SLAM algorithm outputs 2 files related to semi-dense point cloud (see [wiki page](https://facebookresearch.github.io/projectaria_tools/docs/data_formats/mps/slam/mps_pointcloud) for data type definitions): \n",
    "- `semidense_points.csv.gz`: Global points in the world coordinate frame. \n",
    "- `semidense_observations.csv.gz`: Point observations for each camera, at each timestamp.\n",
    "\n",
    "Note that semidense point files are normally large, therefore loading them may take some time. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e882ecda",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== MPS - Semi-dense Point Cloud ===\")\n",
    "\n",
    "# Load MPS semi-dense point cloud data\n",
    "semidense_points_file = os.path.join(\n",
    "    mps_folder_path, \"slam\", \"semidense_points.csv.gz\"\n",
    ")\n",
    "semidense_points = mps.read_global_point_cloud(semidense_points_file)\n",
    "\n",
    "# Print out the content of the first sample in semidense_points\n",
    "if semidense_points:\n",
    "    sample = semidense_points[0]\n",
    "    print(\"GlobalPointPosition sample:\")\n",
    "    print(f\"  uid: {sample.uid}\")\n",
    "    print(f\"  graph_uid: {sample.graph_uid}\")\n",
    "    print(f\"  position_world: {sample.position_world}\")\n",
    "    print(f\"  inverse_distance_std: {sample.inverse_distance_std}\")\n",
    "    print(f\"  distance_std: {sample.distance_std}\")\n",
    "    print(f\"Total number of semi-dense points: {len(semidense_points)}\")\n",
    "else:\n",
    "    print(\"semidense_points is empty.\")\n",
    "\n",
    "# Filter semidense points by inv_dep or depth. \n",
    "# The filter will KEEP points with (inv_dep or depth < threshold)\n",
    "filtered_semidense_points = filter_points_from_confidence(raw_points = semidense_points, threshold_invdep = 1e-3, threshold_dep = 5e-2)\n",
    "print(f\"Filtering semidense points from a total of {len(semidense_points)} points down to {len(filtered_semidense_points)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2558426-4246-40a5-afe4-1e8301d8616e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== MPS - Semi-dense Point Observations ===\")\n",
    "\n",
    "# Load MPS semi-dense point observations data\n",
    "semidense_observations_file = os.path.join(\n",
    "    mps_folder_path, \"slam\", \"semidense_observations.csv.gz\"\n",
    ")\n",
    "semidense_observations = mps.read_point_observations(semidense_observations_file)\n",
    "\n",
    "# Print out the content of the first sample in semidense_observations\n",
    "if semidense_observations:\n",
    "    sample = semidense_observations[0]\n",
    "    print(\"PointObservation sample:\")\n",
    "    print(f\"  point_uid: {sample.point_uid}\")\n",
    "    print(f\"  frame_capture_timestamp: {int(sample.frame_capture_timestamp.total_seconds() * 1e6)} us\")\n",
    "    print(f\"  camera_serial: {sample.camera_serial}\")\n",
    "    print(f\"  uv: {sample.uv}\")\n",
    "    print(f\"Total number of point observations: {len(semidense_observations)}\")\n",
    "else:\n",
    "    print(\"semidense_observations is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562d48b4",
   "metadata": {},
   "source": [
    "### [MPS - SLAM] Visualization\n",
    "\n",
    "In the following code snippet, we demonstrate how to visualize the MPS SLAM results in a 3D view. \n",
    "\n",
    "We first prepare a short trajectory segment, then extract the semidense points position, along with timestamp-mapped observations for visualization purpose. Finally we plot everything in Rerun. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae260b7-716d-4a08-ba9e-07a5184d61bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# A helper coloring function\n",
    "def color_from_zdepth(z_depth_m: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Map z-depth (meters, along the camera's forward axis) to a bright Viridis-like RGB color.\n",
    "    - If z_depth_m <= 0 (point is behind the camera), return white [255, 255, 255].\n",
    "    - Near (0.2 m) -> yellow, Far (3.0 m) -> purple.\n",
    "    Returns an array of shape (3,) with dtype=uint8.\n",
    "    \"\"\"\n",
    "    if not np.isfinite(z_depth_m) or z_depth_m <= 0.0:\n",
    "        return np.array([0, 0, 0], dtype=np.uint8)\n",
    "\n",
    "    NEAR_METERS, FAR_METERS = 0.2, 5.0\n",
    "\n",
    "    # Normalize to [0,1], then flip so near → bright (yellow), far → dark (purple)\n",
    "    clamped = min(max(float(z_depth_m), NEAR_METERS), FAR_METERS)\n",
    "    normalized_position = (clamped - NEAR_METERS) / (FAR_METERS - NEAR_METERS + 1e-12)\n",
    "    gradient_position = 1.0 - normalized_position\n",
    "\n",
    "    # Viridis-like anchor colors: purple → blue → teal → green → yellow\n",
    "    color_stops = [\n",
    "        (68, 1, 84),\n",
    "        (59, 82, 139),\n",
    "        (33, 145, 140),\n",
    "        (94, 201, 98),\n",
    "        (253, 231, 37),\n",
    "    ]\n",
    "\n",
    "    # Locate segment and blend between its endpoints\n",
    "    segment_count = len(color_stops) - 1\n",
    "    continuous_index = gradient_position * segment_count\n",
    "    lower_segment_index = int(continuous_index)\n",
    "\n",
    "    if lower_segment_index >= segment_count:\n",
    "        red, green, blue = color_stops[-1]\n",
    "    else:\n",
    "        segment_fraction = continuous_index - lower_segment_index\n",
    "        r0, g0, b0 = color_stops[lower_segment_index]\n",
    "        r1, g1, b1 = color_stops[lower_segment_index + 1]\n",
    "        red   = r0 + segment_fraction * (r1 - r0)\n",
    "        green = g0 + segment_fraction * (g1 - g0)\n",
    "        blue  = b0 + segment_fraction * (b1 - b0)\n",
    "\n",
    "    return np.array([int(red), int(green), int(blue)], dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad11348f-186a-4d08-b480-710238da4f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Preparing MPS SLAM results for visualization ===\")\n",
    "\n",
    "# Check if we have valid SLAM data to visualize\n",
    "if not closed_loop_trajectory or not semidense_points:\n",
    "    raise RuntimeError(\"Warning: This tutorial requires valid MPS SLAM data to run.\")\n",
    "\n",
    "# ----------- \n",
    "# Prepare Trajectory data\n",
    "# -----------\n",
    "# Select a short segment of trajectory (e.g., first 5000 samples, subsampled by 50)\n",
    "segment_length = min(50000, len(closed_loop_trajectory))\n",
    "trajectory_segment = closed_loop_trajectory[:segment_length:50]\n",
    "timestamp_to_pose = {\n",
    "    pose.tracking_timestamp: pose for pose in trajectory_segment\n",
    "}\n",
    "print(f\"Finished preparing a trajectory of length {len(trajectory_segment)}... \")\n",
    "\n",
    "# ----------- \n",
    "# Prepare Semidense point data\n",
    "# -----------\n",
    "# Filter the semidense point cloud by confidence and limit max point count, and extract the point positions\n",
    "filtered_semidense_point_cloud_data = filter_points_from_confidence(semidense_points)\n",
    "points_positions = np.array(\n",
    "    [\n",
    "        point.position_world for point in filtered_semidense_point_cloud_data\n",
    "    ]\n",
    ")\n",
    "print(f\"Finished preparing filtered semidense points cloud of {len(filtered_semidense_point_cloud_data)} points... \")\n",
    "\n",
    "# ----------- \n",
    "# Prepare Semidense observation data\n",
    "# -----------\n",
    "# Based on RGB observations, create a per-timestamp point position list, and color them according to its distance from RGB camera\n",
    "point_uid_to_position = {\n",
    "    point.uid: np.array(point.position_world) for point in filtered_semidense_point_cloud_data\n",
    "}\n",
    "\n",
    "# A helper function that creates a easier-to-query mapping to obtain observations according to timestamps\n",
    "slam_1_serial = vrs_data_provider.get_device_calibration().get_camera_calib(\"slam-front-left\").get_serial_number()\n",
    "timestamp_to_point_positions = defaultdict(list)  # t_ns -> [position, position, ...]\n",
    "timestamp_to_point_colors = defaultdict(list) # t_ns -> [color, color, ...]\n",
    "\n",
    "for obs in semidense_observations:\n",
    "    # Only add observations for SLAM_1 camera, and if the timestamp is in the chosen trajectory segment\n",
    "    if (\n",
    "        obs.camera_serial == slam_1_serial and \n",
    "        obs.frame_capture_timestamp in timestamp_to_pose and \n",
    "        obs.point_uid in point_uid_to_position):\n",
    "        # Insert point position\n",
    "        obs_timestamp = obs.frame_capture_timestamp\n",
    "        point_position = point_uid_to_position[obs.point_uid]\n",
    "        timestamp_to_point_positions[obs_timestamp].append(point_position)\n",
    "\n",
    "        # Insert point color\n",
    "        T_world_device = timestamp_to_pose[obs_timestamp].transform_world_device\n",
    "        point_in_device = T_world_device.inverse() @ point_position\n",
    "        point_z_depth = point_in_device.squeeze()[2]\n",
    "        point_color = color_from_zdepth(point_z_depth)\n",
    "        timestamp_to_point_colors[obs_timestamp].append(point_color)\n",
    "\n",
    "from itertools import islice\n",
    "print(f\"Finished preparing semidense points observations: \")\n",
    "for timestamp, points in islice(timestamp_to_point_positions.items(), 5):\n",
    "    print(f\"\\t timestamp {int(timestamp.total_seconds() * 1e9)} ns has {len(points)} observed points in slam-front-left view. \")\n",
    "print(f\"\\t ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda9ee2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rerun as rr\n",
    "import numpy as np\n",
    "from projectaria_tools.utils.rerun_helpers import (\n",
    "    AriaGlassesOutline,\n",
    "    ToTransform3D,\n",
    "    ToBox3D,\n",
    ")\n",
    "from projectaria_tools.core.mps.utils import (\n",
    "    filter_points_from_confidence,\n",
    "    get_nearest_pose,\n",
    ")\n",
    "\n",
    "print(\"=== Visualizing MPS SLAM Results in 3D ===\")\n",
    "\n",
    "# Initialize Rerun\n",
    "rr.init(\"MPS SLAM Visualization\")\n",
    "rr.notebook_show()  # open the in-notebook viewer first, then stream logs\n",
    "\n",
    "# Set up the 3D scene\n",
    "rr.log(\"world\", rr.ViewCoordinates.RIGHT_HAND_Z_UP, static=True)\n",
    "\n",
    "# Log point cloud\n",
    "rr.log(\n",
    "    \"world/semidense_points\",\n",
    "    rr.Points3D(\n",
    "        positions=points_positions,\n",
    "        colors=[255, 255, 255, 125],\n",
    "        radii=0.001\n",
    "    ),\n",
    "    static=True\n",
    ")\n",
    "\n",
    "# Aria glass outline for visualization purpose\n",
    "device_calib = vrs_data_provider.get_device_calibration()\n",
    "aria_glasses_point_outline = AriaGlassesOutline(\n",
    "    device_calib, use_cad_calib=True\n",
    ")\n",
    "\n",
    "# Plot Closed loop trajectory \n",
    "closed_loop_traj_cached_full = []\n",
    "observation_points_cached = None\n",
    "observation_colors_cached = None\n",
    "for closed_loop_pose in trajectory_segment:\n",
    "    capture_timestamp_ns = int(closed_loop_pose.tracking_timestamp.total_seconds() * 1e9)\n",
    "    rr.set_time_nanos(\"device_time\", capture_timestamp_ns)\n",
    "    \n",
    "    T_world_device = closed_loop_pose.transform_world_device\n",
    "\n",
    "    # Log device pose as a coordinate frame\n",
    "    rr.log(\n",
    "        \"world/device\",\n",
    "        ToTransform3D(\n",
    "            T_world_device,\n",
    "            axis_length=0.05,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Plot Aria glass outline\n",
    "    rr.log(\n",
    "        \"world/device/glasses_outline\",\n",
    "        rr.LineStrips3D(\n",
    "            aria_glasses_point_outline,\n",
    "            colors=[150,200,40],\n",
    "            radii=5e-3,\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    # Plot gravity direction vector\n",
    "    rr.log(\n",
    "        \"world/vio_gravity\",\n",
    "        rr.Arrows3D(\n",
    "            origins=[T_world_device.translation()[0]],\n",
    "            vectors=[\n",
    "                closed_loop_pose.gravity_world * 1e-2\n",
    "            ],  # length converted from 9.8 meter -> 10 cm\n",
    "            colors=[101,67,33],\n",
    "            radii=5e-3,\n",
    "        ),\n",
    "        static=False,\n",
    "    )\n",
    "\n",
    "    # Update cached results for observations. Cache is needed because observation has a much lower freq than high-freq trajectory. \n",
    "    if closed_loop_pose.tracking_timestamp in timestamp_to_point_positions.keys():\n",
    "        observation_points_cached = timestamp_to_point_positions[closed_loop_pose.tracking_timestamp]\n",
    "        observation_colors_cached = timestamp_to_point_colors[closed_loop_pose.tracking_timestamp]\n",
    "    if observation_points_cached is not None:\n",
    "        rr.log(\n",
    "            \"world/semidense_observations\", \n",
    "            rr.Points3D(\n",
    "            positions = observation_points_cached,\n",
    "            colors = observation_colors_cached,\n",
    "            radii=0.01\n",
    "            ),\n",
    "            static = False\n",
    "        )\n",
    "    \n",
    "    \n",
    "    # Plot the entire VIO trajectory that are cached so far\n",
    "    closed_loop_traj_cached_full.append(T_world_device.translation()[0])\n",
    "    rr.log(\n",
    "        \"world/vio_trajectory\",\n",
    "        rr.LineStrips3D(\n",
    "            closed_loop_traj_cached_full,\n",
    "            colors=[173, 216, 255],\n",
    "            radii=5e-3,\n",
    "        ),\n",
    "        static=False,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
