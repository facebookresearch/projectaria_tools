{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16ef947e",
   "metadata": {},
   "source": [
    "# ADT Tutorial\n",
    "This tutorial will walk through the steps to fetch groundtruth data for an ADT sequence and visualize all the together with the Aria raw sensor data. All data being visualized includes:\n",
    "* Raw camera data\n",
    "* Object 2d bounding box\n",
    "* Object 6DoF pose and 3d bounding box \n",
    "* Segmentation image \n",
    "* Depth image \n",
    "* Synthetic image\n",
    "* Eye gaze \n",
    "* Human skeleton\n",
    "\n",
    "\n",
    "### Notebook stuck?\n",
    "Note that because of Jupyter and Plotly issues, sometimes the code may stuck at visualization. We recommend **restart the kernels** and try again to see if the issue is resolved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadaec3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import plotly.graph_objects as go\n",
    "from math import tan\n",
    "import random \n",
    "\n",
    "from projectaria_tools import utils\n",
    "from projectaria_tools.core.stream_id import StreamId\n",
    "from projectaria_tools.core import calibration\n",
    "from projectaria_tools.projects.adt import (\n",
    "   AriaDigitalTwinDataProvider,\n",
    "   AriaDigitalTwinSkeletonProvider,\n",
    "   AriaDigitalTwinDataPathsProvider,\n",
    "   bbox3d_to_line_coordinates,\n",
    "   bbox2d_to_image_coordinates,\n",
    "   utils as adt_utils,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89655959",
   "metadata": {},
   "source": [
    "### Download the example sequence\n",
    "\n",
    "Follow the wiki: to download the ADT example sequence. \n",
    "\n",
    "\n",
    "### Choose a sequence\n",
    "**sequence_path**: Specify the name of the sequence that we will load. You should see a gt_metadata.json in your sequence folder. If you are following the wiki, the sequence name we choose is Apartment_release_golden_skeleton_seq100_10s_sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc1d284",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_path = os.path.expanduser('~') + \"/Documents/projectaria_tools_adt_data/Apartment_release_golden_skeleton_seq100_10s_sample\"\n",
    "print(\"sequence_path: \", sequence_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f032e05",
   "metadata": {},
   "source": [
    "### Get sequence information\n",
    "\n",
    "ADT has sequences of multi-person activities. Each person is identified by the Aria serial number he wears. Use the AriaDigitalTwinDataPathsProvider to get all person ids, a.k.a Aria serial numbers \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274a31c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_provider = AriaDigitalTwinDataPathsProvider(sequence_path)\n",
    "all_device_serials = paths_provider.get_device_serial_numbers()\n",
    "sequence_name = os.path.basename(sequence_path)\n",
    "print(\"all devices for sequence \", sequence_name, \":\")\n",
    "for idx, device_serial in enumerate(all_device_serials):\n",
    "    print(\"device number - \", idx, \": \", device_serial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2e6dee",
   "metadata": {},
   "source": [
    "### Get data files for the target sequence\n",
    "\n",
    "Select a device / person and get all ADT data for their sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed4551b",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_device_number = 0\n",
    "data_paths = paths_provider.get_datapaths_by_device_num(selected_device_number)\n",
    "print(data_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bdca4d",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "**Expected warnings**: The AriaDigitalTwinDataProvider loads 4 VRS files: \n",
    "\n",
    "    1. Raw Aria recording (stream ids: 211-1, 214-1, 247-1, 1201-1/2, 1202-1/2, 1203-1) \n",
    "    2. Synthetic twin recording (stream ids: 214-1, 1201-1/2, 1202-2) \n",
    "    3. Segmentation images (stream ids: 400-1/2/3)\n",
    "    4. Depth images (stream ids: 345-1/2/3)\n",
    "\n",
    "VRS files 1 and 2 have their own calibrations embeded into the VRS file at record time. Since VRS files 2 and 3 are annotations of the data in VRS 1, these do not contain a calibration to avoid duplicating the existing calibration in the raw data. For manipulating image data from VRS files 2 and 3, you should use the calibration from VRS file 1. For a full list of stream ids and their descriptions, see [StreamId.h](https://github.com/facebookresearch/vrs/blob/main/vrs/StreamId.h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c3f339",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"loading ground truth data...\")\n",
    "gt_provider = AriaDigitalTwinDataProvider(data_paths)\n",
    "print(\"done loading ground truth data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6a506d",
   "metadata": {},
   "source": [
    "### Get Info of Instances in the sequence\n",
    "In ADT, an instance can be either an object or a person (skeleton). Each instance has a unique id that is used for indexing the 3D bboxes, 2D bboxes and the value of a segmentation pixel (segmentation images contain an instance id in each pixel of the image). By using instance id, you can check the instance information in the instance info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424706c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_ids = gt_provider.get_instance_ids()\n",
    "print(\"there are {} object in this sequence\".format(len(instances_ids)))\n",
    "# load a random instance information\n",
    "random_id = int(len(instances_ids) / 2)\n",
    "# print(\"\\ninstance info from randomly selected instance: \")\n",
    "print('\\n' +  '\\033[1m' + 'Instance info from randomly selected instance: ' + '\\033[0m')\n",
    "print(gt_provider.get_instance_info_by_id(instances_ids[random_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4999c8",
   "metadata": {},
   "source": [
    "### Select a target object for the rest of tutorial\n",
    "Some example objects observed in the example sequence and select one of them, e.g.: \n",
    "\n",
    "* 4404207983027294: ChoppingBoard\n",
    "* 4587554268035732: Mug_2\n",
    "* 4233561046743320: FacebookPortal\n",
    "* 7379153972126671: WhiteVase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8944c419",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target_obj_id = 4404207983027294\n",
    "\n",
    "print('\\n' +  '\\033[1m' + 'The object you have chosen is: ' + '\\033[0m')\n",
    "print(gt_provider.get_instance_info_by_id(target_obj_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d9d801",
   "metadata": {},
   "source": [
    "### Get raw Aria sensor frame\n",
    "**stream id**: ID of the camera you want to view. Aria has 3 cameras, the IDs of each camera are as follows: 214-1 for RGB, 1201-1 for SLAM LEFT, 1201-2 for SLAM RIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0594dc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_id = StreamId(\"214-1\")\n",
    "img_timestamps_ns = gt_provider.get_aria_device_capture_timestamps_ns(stream_id)\n",
    "print(\"There are {} frames\".format(len(img_timestamps_ns)))\n",
    "\n",
    "# choose the frame in the middle of the sequence\n",
    "select_timestamps_ns = img_timestamps_ns[int(len(img_timestamps_ns)/2)]\n",
    "\n",
    "# fetch the raw data \n",
    "print(\"loading image with timestamp: \", select_timestamps_ns, \" ns\")\n",
    "image_with_dt = gt_provider.get_aria_image_by_timestamp_ns(select_timestamps_ns, stream_id)\n",
    "\n",
    "# check image is valid. It's always possible that the data retrieval fails, therefore all \n",
    "# returned data not only contains dt, but also contains an is_valid() function, or returns \n",
    "# an optional variable. \n",
    "assert image_with_dt.is_valid(), \"Image not valid!\"    \n",
    "\n",
    "# since all image data and some GT data are discrete, each time we query discrete data \n",
    "# from the data providers we return an object with dt. Dt is the difference between \n",
    "# returned data time and the query time. For this case, since we are querying for images\n",
    "# using the real image times we already fetched, dt should be zero. Note that if we iterated\n",
    "# through GT timestamps, they may not correspond to the exact same time points as the camera \n",
    "# data. We align GT data to SLAM camera timestamps, so if querying RGB images using GT \n",
    "# timestamps, dt will not be zero\n",
    "print(\"image_time - query_time: \", image_with_dt.dt_ns(), \" ns\")\n",
    "\n",
    "# convert image to numpy array\n",
    "image = image_with_dt.data().to_numpy_array()\n",
    "\n",
    "# pad SLAM camera gray-scale image to 3 channel for color visualization\n",
    "image = np.repeat(image[..., np.newaxis], 3, axis=2) if len(image.shape) < 3 else image\n",
    "plt.imshow(image); plt.xticks([]); plt.yticks([]); "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da5a0cd",
   "metadata": {},
   "source": [
    "### Get and draw 2D bounding box on Aria sensor frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5c0374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all observed 2D bboxes in the selected frame\n",
    "bbox2d_with_dt = gt_provider.get_object_2d_boundingboxes_by_timestamp_ns(select_timestamps_ns, stream_id)\n",
    "\n",
    "# check if the result is valid\n",
    "if not bbox2d_with_dt.is_valid():\n",
    "    print(\"2D bounding box is not available\")\n",
    "print(\"groundtruth_time - query_time = \", bbox2d_with_dt.dt_ns(), \"ns\")\n",
    "bbox2d_all_objects = bbox2d_with_dt.data()\n",
    "    \n",
    "# get bbox of the target object   \n",
    "bbox2d_target_obj = bbox2d_all_objects[target_obj_id].box_range\n",
    "print(\"[xmin, xmax, ymin, ymax]:\", bbox2d_target_obj)\n",
    "\n",
    "# setup plot\n",
    "rect = patches.Rectangle((bbox2d_target_obj[0], bbox2d_target_obj[2]), \n",
    "                         bbox2d_target_obj[1] - bbox2d_target_obj[0], \n",
    "                         bbox2d_target_obj[3] - bbox2d_target_obj[2], \n",
    "                         linewidth=1, edgecolor='r', facecolor=\"none\")\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(image);\n",
    "ax.add_patch(rect)\n",
    "ax.axis('off')\n",
    "\n",
    "# draw and output object information\n",
    "target_obj_info = None\n",
    "if gt_provider.has_instance_id(target_obj_id):\n",
    "    target_obj_info = gt_provider.get_instance_info_by_id(target_obj_id)\n",
    "    plt.text(bbox2d_target_obj[1], bbox2d_target_obj[2], target_obj_info.name, c=\"r\", fontsize=12)\n",
    "    print('\\n' + '\\033[1m' + 'selected object information: ' + '\\033[0m')\n",
    "    print(str(target_obj_info))\n",
    "    \n",
    "plt.show(); "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10627a2",
   "metadata": {},
   "source": [
    "### Get and draw 3D bounding box on Aria sensor frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e6d59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get object poses and Aria poses of the selected frame\n",
    "bbox3d_with_dt = gt_provider.get_object_3d_boundingboxes_by_timestamp_ns(select_timestamps_ns)\n",
    "if not bbox3d_with_dt.is_valid():\n",
    "    print(\"3D bounding box is not available\")\n",
    "bbox3d = bbox3d_with_dt.data()[target_obj_id]\n",
    "print(\"3D bbounding box dt: groundtruth_time - query_time = \", bbox3d_with_dt.dt_ns(), \"ns\")\n",
    "print(\"AABB [xmin, xmax, ymin, ymax, zmin, zmax]: \", bbox3d.aabb)\n",
    "\n",
    "# get the Aria pose \n",
    "aria3dpose_with_dt = gt_provider.get_aria_3d_pose_by_timestamp_ns(select_timestamps_ns)\n",
    "if not aria3dpose_with_dt.is_valid():\n",
    "    print(\"aria 3d pose is not available\")\n",
    "aria3dpose = aria3dpose_with_dt.data()\n",
    "print(\"Aria pose dt: groundtruth_time - query_time = \", aria3dpose_with_dt.dt_ns(), \"ns\")\n",
    "\n",
    "# now to project 3D bbox to Aria camera\n",
    "# get 6DoF object pose with repect to the target camera\n",
    "transform_cam_device = gt_provider.get_aria_transform_device_camera(stream_id).inverse()\n",
    "transform_cam_scene = transform_cam_device.to_matrix() @ aria3dpose.transform_scene_device.inverse().to_matrix()\n",
    "transform_cam_obj = transform_cam_scene @ bbox3d.transform_scene_object.to_matrix()\n",
    "\n",
    "# get projection function\n",
    "cam_calibration = gt_provider.get_aria_camera_calibration(stream_id)\n",
    "assert cam_calibration is not None, \"no camera calibration\"\n",
    "\n",
    "# get projected bbox\n",
    "reprojected_bbox = adt_utils.project_3d_bbox_to_image(bbox3d.aabb, transform_cam_obj, cam_calibration)\n",
    "if reprojected_bbox:\n",
    "    # plot\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(image)\n",
    "    ax.axis('off')\n",
    "    ax.add_patch(plt.Polygon(reprojected_bbox, linewidth=1, edgecolor='y', facecolor=\"none\")) \n",
    "    if target_obj_info:\n",
    "        plt.text(bbox2d_target_obj[1], bbox2d_target_obj[2], target_obj_info.name, c=\"y\", fontsize=12)\n",
    "    plt.show()\n",
    "else:\n",
    "    print('\\033[1m' + '\\033[91m' + 'Try another object!' + '\\033[0m')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31e9c96",
   "metadata": {},
   "source": [
    "### Get and draw segmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b4206a",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_with_dt = gt_provider.get_segmentation_image_by_timestamp_ns(select_timestamps_ns, stream_id)\n",
    "\n",
    "# check if the result is valid\n",
    "assert segmentation_with_dt.is_valid(), \"segmentation not valid for input timestamp!\"\n",
    "print(\"groundtruth_time - query_time = \", segmentation_with_dt.dt_ns(), \"ns\")\n",
    "segmentation_for_viz = segmentation_with_dt.data().get_visualizable().to_numpy_array()\n",
    "plt.imshow(segmentation_for_viz)\n",
    "\n",
    "# Access segmentation raw data\n",
    "print(segmentation_with_dt.data().get_width())\n",
    "segmentation_data = segmentation_with_dt.data().to_numpy_array()\n",
    "itemindex = np.where(segmentation_data == target_obj_id)\n",
    "\n",
    "row = itemindex[0][0]\n",
    "col = itemindex[1][0]\n",
    "        \n",
    "plt.plot(col, row, '+', c=\"black\", mew=1, ms=20); plt.xticks([]); plt.yticks([]); \n",
    "if target_obj_info:\n",
    "    plt.text(col, row, target_obj_info.name, c=\"red\", bbox=dict(fill=False, edgecolor='none', linewidth=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a62e8c4",
   "metadata": {},
   "source": [
    "### Get and draw depth map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed2ee36",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_with_dt = gt_provider.get_depth_image_by_timestamp_ns(select_timestamps_ns, stream_id)\n",
    "\n",
    "# check if the result is valid\n",
    "if not depth_with_dt.is_valid():\n",
    "    print(\"depth map not valid for input timestamp!\")\n",
    "print(\"groundtruth_time - query_time = \", depth_with_dt.dt_ns(), \"ns\")\n",
    "\n",
    "# draw image\n",
    "depth_image = depth_with_dt.data()\n",
    "depth_for_vis = depth_with_dt.data().get_visualizable().to_numpy_array()\n",
    "depth_for_vis = np.repeat(depth_for_vis[..., np.newaxis], 3, axis=2)\n",
    "plt.imshow(depth_for_vis)\n",
    "\n",
    "# Access depth raw data\n",
    "row = int(depth_image.get_height() / 3) \n",
    "col = int(depth_image.get_width() / 2)\n",
    "depth_mm = depth_image.at(col, row)\n",
    "plt.plot(col, row, '+', c=\"black\", mew=1, ms=20); plt.xticks([]); plt.yticks([]); \n",
    "plt.text(col, row, str(depth_mm / 1e3) + \"m\", c = \"red\", bbox=dict(fill=False, edgecolor='none', linewidth=2));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac4614f",
   "metadata": {},
   "source": [
    "### Get and draw synthetic image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6fbb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_with_dt = gt_provider.get_synthetic_image_by_timestamp_ns(int(select_timestamps_ns), stream_id)\n",
    "\n",
    "# check if the result is valid\n",
    "# TODO: change back to assert when all datasets have synthetic\n",
    "# assert synthetic_with_dt.is_valid(), \"synthetic image not valid for input timestamp!\"\n",
    "if synthetic_with_dt.is_valid():\n",
    "    print(\"groundtruth_time - query_time = \", synthetic_with_dt.dt_ns(), \"ns\")\n",
    "    synthetic_image = synthetic_with_dt.data().to_numpy_array()\n",
    "    synthetic_image = np.repeat(synthetic_image[..., np.newaxis], 3, axis=2) if len(synthetic_image.shape) < 3 else synthetic_image\n",
    "    plt.imshow(synthetic_image); plt.xticks([]); plt.yticks([]);    \n",
    "else:\n",
    "    print(\"synthetic image not valid for input timestamp!\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dad99e2",
   "metadata": {},
   "source": [
    "### Get and draw eye gaze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff43817",
   "metadata": {},
   "outputs": [],
   "source": [
    "eye_gaze_with_dt = gt_provider.get_eyegaze_by_timestamp_ns(select_timestamps_ns)\n",
    "assert eye_gaze_with_dt.is_valid(), \"Eye gaze not available\"\n",
    "\n",
    "# Project the gaze center in CPF frame into camera sensor plane, with multiplication performed in homogenous coordinates\n",
    "eye_gaze = eye_gaze_with_dt.data()\n",
    "gaze_center_in_cpf = np.array([tan(eye_gaze.yaw), tan(eye_gaze.pitch), 1.0], dtype = np.float64) * eye_gaze.depth\n",
    "transform_cpf_sensor = gt_provider.raw_data_provider_ptr().get_device_calibration().get_transform_cpf_sensor(cam_calibration.get_label())\n",
    "gaze_center_in_camera = transform_cpf_sensor.inverse().to_matrix() @ np.hstack((gaze_center_in_cpf, 1)).T\n",
    "gaze_center_in_camera = gaze_center_in_camera[:3] / gaze_center_in_camera[3:]\n",
    "gaze_center_in_pixels = cam_calibration.project(gaze_center_in_camera)\n",
    "\n",
    "# Draw a solid circle plus a cross at the projected gaze center location\n",
    "if gaze_center_in_pixels is not None:\n",
    "    circle = patches.Circle(gaze_center_in_pixels.flatten(), radius = 10.0, facecolor=\"red\")\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(image);\n",
    "    ax.add_patch(circle)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Plot the cross\n",
    "    u, v = gaze_center_in_pixels.flatten()\n",
    "    plt.plot(u, v, '+', c=\"red\", mew=1, ms=20); plt.xticks([]); plt.yticks([]); \n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"Eye gaze center projected to {gaze_center_in_pixels}, which is out of camera sensor plane.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e94f9c",
   "metadata": {},
   "source": [
    "### Draw 3D Bounding Boxes in 3D for all objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ac9b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get object 3d bounding box\n",
    "bbox3d_with_dt = gt_provider.get_object_3d_boundingboxes_by_timestamp_ns(select_timestamps_ns)\n",
    "assert bbox3d_with_dt.is_valid(), \"3D bounding box is not available\"\n",
    "bboxes3d = bbox3d_with_dt.data()\n",
    "print(\"groundtruth_time - query_time = \", bbox3d_with_dt.dt_ns(), \"ns\")\n",
    "\n",
    "# transform AABB to OBB and visualize\n",
    "traces = []\n",
    "obbs = []\n",
    "for obj_id in bboxes3d:\n",
    "    bbox3d = bboxes3d[obj_id]\n",
    "    aabb = bbox3d.aabb\n",
    "    aabb_coords = bbox3d_to_line_coordinates(aabb)\n",
    "    obb = np.zeros(shape=(len(aabb_coords), 3))\n",
    "    for i in range(0, len(aabb_coords)):\n",
    "        aabb_pt = aabb_coords[i]\n",
    "        aabb_pt_homo = np.append(aabb_pt, [1])\n",
    "        obb_pt = (bbox3d.transform_scene_object.to_matrix()@aabb_pt_homo)[0:3]\n",
    "        obb[i] = obb_pt\n",
    "    obbs.append(obb) \n",
    "    obj_name = str(obj_id)\n",
    "    if gt_provider.has_instance_id(obj_id):\n",
    "        obj_name = gt_provider.get_instance_info_by_id(obj_id).name\n",
    "    traces.append(go.Scatter3d(\n",
    "        x=obb[:, 0],\n",
    "        y=obb[:, 1],\n",
    "        z=obb[:, 2],\n",
    "        name=obj_name,\n",
    "        mode=\"lines\",\n",
    "        line=go.scatter3d.Line(width=5),\n",
    "    ))\n",
    "    \n",
    "layout = go.Layout(scene=dict(dragmode='orbit', aspectmode='data', xaxis_visible=False, yaxis_visible=False,zaxis_visible=False))\n",
    "figure = go.Figure(data=traces, layout=layout)\n",
    "figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b896c95d",
   "metadata": {},
   "source": [
    "### Get and draw skeleton in 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3f4d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all available skeletons in a sequence\n",
    "skeleton_ids = gt_provider.get_skeleton_ids()\n",
    "skeleton_info = gt_provider.get_instance_info_by_id(skeleton_ids[0])\n",
    "print(\"skeleton \", skeleton_info.name, \" wears \", skeleton_info.associated_device_serial)\n",
    "\n",
    "skeleton_with_dt = gt_provider.get_skeleton_by_timestamp_ns(select_timestamps_ns, skeleton_ids[0] )\n",
    "assert skeleton_with_dt.is_valid(), \"skeleton is not valid\"\n",
    "print(\"groundtruth_time - query_time = \", skeleton_with_dt.dt_ns(), \"ns\")\n",
    "skeleton = skeleton_with_dt.data()\n",
    "joints = skeleton.joints\n",
    "\n",
    "traces = []\n",
    "# draw skeleton\n",
    "joint_labels = AriaDigitalTwinSkeletonProvider.get_joint_labels()\n",
    "joint_connections = AriaDigitalTwinSkeletonProvider.get_joint_connections()\n",
    "for i in range(0, len(joint_connections)):\n",
    "    joint_1 = joints[joint_connections[i][0]]\n",
    "    joint_2 = joints[joint_connections[i][1]]\n",
    "    traces.append(go.Scatter3d(\n",
    "        x=[joint_1[0], joint_2[0]],\n",
    "        y=[joint_1[1], joint_2[1]],\n",
    "        z=[joint_1[2], joint_2[2]],\n",
    "        name=joint_labels[joint_connections[i][0]],\n",
    "        mode=\"lines\",\n",
    "        line=go.scatter3d.Line(width=5),\n",
    "    ))\n",
    "# draw objects together with skeleton\n",
    "for obb in obbs:\n",
    "    # skip too high objects to focus on skeleton\n",
    "    if max(obb[:, 1]) > 3:\n",
    "        continue\n",
    "    traces.append(go.Scatter3d(\n",
    "        x=obb[:, 0],\n",
    "        y=obb[:, 1],\n",
    "        z=obb[:, 2],\n",
    "        mode=\"lines\",\n",
    "        line=dict(color=['rgba(128, 128, 128, 0.6)']*len(obb), width=5),\n",
    "        showlegend=False\n",
    "    ))\n",
    "layout = go.Layout(scene=dict(dragmode='orbit', aspectmode='data', xaxis_visible=False, yaxis_visible=False,zaxis_visible=False))\n",
    "figure = go.Figure(data=traces, layout=layout)\n",
    "figure.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa0ac23",
   "metadata": {},
   "source": [
    "### Rectify ADT data\n",
    "Sometimes you may want to rectify ADT data coming from Aria's fisheye cameras. The instruction below shows how to rectify a raw Aria sensor image, the 2D bounding box and segmentation. The same step can be applied to 3D bounding boxes, depth images and synthetic images easily. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250b18f4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Rectify the RGB image\n",
    "# get source calibration - Aria original camera model\n",
    "sensor_name = gt_provider.raw_data_provider_ptr().get_label_from_stream_id(stream_id)\n",
    "device_calib = gt_provider.raw_data_provider_ptr().get_device_calibration()\n",
    "src_calib = device_calib.get_camera_calib(sensor_name)\n",
    "\n",
    "# create output calibration: a pin-hole rectificied image size 512x512 and focal length 280\n",
    "dst_calib = calibration.get_linear_camera_calibration(512, 512, 280, sensor_name)\n",
    "\n",
    "# rectify image\n",
    "rectified_image = calibration.distort_by_calibration(image, dst_calib, src_calib)\n",
    "\n",
    "# visualize input and results\n",
    "plt.figure()\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "fig.suptitle(f\"Image undistortion (focal length = {dst_calib.get_focal_lengths()})\") \n",
    "axes[0].imshow(image)\n",
    "axes[0].title.set_text(f\"sensor image ({sensor_name})\"); \n",
    "axes[0].tick_params(left=False, right=False, labelleft=False, labelbottom=False, bottom=False)\n",
    "axes[1].imshow(rectified_image)\n",
    "axes[1].title.set_text(f\"undistorted image ({sensor_name})\")\n",
    "axes[1].tick_params(left=False, right=False, labelleft=False, labelbottom=False, bottom=False)\n",
    "\n",
    "### Rectify 2D bbox \n",
    "# get bbox    \n",
    "bbox2d_target_obj = bbox2d_all_objects[target_obj_id].box_range\n",
    "bbox2d_draw = patches.Rectangle((bbox2d_target_obj[0], bbox2d_target_obj[2]), \n",
    "                                 bbox2d_target_obj[1] - bbox2d_target_obj[0], \n",
    "                                 bbox2d_target_obj[3] - bbox2d_target_obj[2], \n",
    "                                 linewidth=1, edgecolor='r', facecolor=\"none\")\n",
    "bbox2d_coords = bbox2d_to_image_coordinates(bbox2d_target_obj)\n",
    "rect_bbox2d_xs = []\n",
    "rect_bbox2d_ys = []\n",
    "for bbox2d_coord in bbox2d_coords:\n",
    "    unprojected_bbox2d_ray = src_calib.unproject_no_checks(bbox2d_coord)\n",
    "    rect_bbox2d_coord = dst_calib.project(unprojected_bbox2d_ray)\n",
    "    rect_bbox2d_xs.append(rect_bbox2d_coord[0])\n",
    "    rect_bbox2d_ys.append(rect_bbox2d_coord[1])    \n",
    "rect_bbox2d = [min(rect_bbox2d_xs), max(rect_bbox2d_xs), min(rect_bbox2d_ys), max(rect_bbox2d_ys)]\n",
    "rect_bbox2d_draw = patches.Rectangle((rect_bbox2d[0], rect_bbox2d[2]), \n",
    "                                      rect_bbox2d[1] - rect_bbox2d[0], \n",
    "                                      rect_bbox2d[3] - rect_bbox2d[2], \n",
    "                                      linewidth=1, edgecolor='r', facecolor=\"none\")\n",
    "\n",
    "# visualize original 2D bbox and rectified 2D bbox\n",
    "axes[0].add_patch(bbox2d_draw)\n",
    "axes[1].add_patch(rect_bbox2d_draw)\n",
    "\n",
    "### Rectify segmentation image\n",
    "rectified_segmentation_for_viz = calibration.distort_by_calibration(segmentation_for_viz, dst_calib, src_calib)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axes[0].imshow(segmentation_for_viz)\n",
    "axes[0].title.set_text(\"segmentation\")\n",
    "axes[0].tick_params(left=False, right=False, labelleft=False, labelbottom=False, bottom=False)\n",
    "axes[1].imshow(rectified_segmentation_for_viz)\n",
    "axes[1].title.set_text(\"undistorted segmentation\")\n",
    "axes[1].tick_params(left=False, right=False, labelleft=False, labelbottom=False, bottom=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
