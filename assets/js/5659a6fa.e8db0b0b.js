"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[6646],{28453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>r});var s=t(96540);const i={},a=s.createContext(i);function o(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),s.createElement(a.Provider,{value:n},e.children)}},70865:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>r,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"open_datasets/aria_digital_twin_dataset/data_format","title":"Data Format","description":"The Aria Digital Twin Dataset (ADT) provides real world and synthetic raw Project Aria data, derived data generated by ADT Ground Truth data processing services as well as derived data generated by Project Aria\'s Machine Perception Services (MPS).","source":"@site/docs/open_datasets/aria_digital_twin_dataset/data_format.mdx","sourceDirName":"open_datasets/aria_digital_twin_dataset","slug":"/open_datasets/aria_digital_twin_dataset/data_format","permalink":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_format","draft":false,"unlisted":false,"editUrl":"https://github.com/facebookresearch/projectaria_tools/tree/main/website/docs/open_datasets/aria_digital_twin_dataset/data_format.mdx","tags":[],"version":"current","sidebarPosition":40,"frontMatter":{"sidebar_position":40,"title":"Data Format"},"sidebar":"tutorialSidebar","previous":{"title":"Object Models","permalink":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/object_models"},"next":{"title":"Data Loader","permalink":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_loader"}}');var i=t(74848),a=t(28453);const o={sidebar_position:40,title:"Data Format"},r="ADT Data Format",d={},c=[{value:"Sequence structure",id:"sequence-structure",level:2},{value:"Timestamps Mapping Data",id:"timestamps-mapping-data",level:3},{value:"Ground Truth Data",id:"ground-truth-data",level:2},{value:"Aligning Ground Truth and MPS Data",id:"aligning-ground-truth-and-mps-data",level:3},{value:"Skeleton Data and Availability",id:"skeleton-data-and-availability",level:2},{value:"Ground Truth Data Format",id:"ground-truth-data-format",level:2},{value:"2d_bounding_box.csv or 2d_bounding_box_with_skeleton.csv",id:"2d_bounding_boxcsv-or-2d_bounding_box_with_skeletoncsv",level:3},{value:"3d_bounding_box.csv",id:"3d_bounding_boxcsv",level:3},{value:"aria_trajectory.csv",id:"aria_trajectorycsv",level:3},{value:"eyegaze.csv",id:"eyegazecsv",level:3},{value:"scene_objects.csv",id:"scene_objectscsv",level:3},{value:"instances.json",id:"instancesjson",level:3},{value:"Skeleton_T.json or Skeleton_C.json",id:"skeleton_tjson-or-skeleton_cjson",level:3},{value:"skeleton_aria_association.json",id:"skeleton_aria_associationjson",level:3},{value:"video.vrs",id:"videovrs",level:3},{value:"depth_images.vrs",id:"depth_imagesvrs",level:3},{value:"segmentations.vrs",id:"segmentationsvrs",level:3}];function l(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"adt-data-format",children:"ADT Data Format"})}),"\n",(0,i.jsxs)(n.p,{children:["The Aria Digital Twin Dataset (ADT) provides real world and synthetic raw Project Aria data, derived data generated by ADT Ground Truth data processing services as well as derived data generated by Project Aria's ",(0,i.jsx)(n.a,{href:"/projectaria_tools/docs/data_formats/mps/mps_summary",children:"Machine Perception Services (MPS)"}),"."]}),"\n",(0,i.jsx)(n.p,{children:"All ADT data is recorded with up to two users in a single scene, however, sometimes only one user is wearing an Aria device.\nFor multi-person recordings, we break each Aria recording and all associated ground truth data into separate sequences.\nThis allows you to filter for any single Aria device recording, and all tooling is designed to operate on a single sequence data."}),"\n",(0,i.jsxs)(n.admonition,{title:"Subsequences have been removed",type:"info",children:[(0,i.jsx)(n.p,{children:"In versions V1.X of ADT, we grouped concurrent recordings into a single sequence with two sub-sequences. We have removed the concept of subsequences in V2.0 (June 2024).\nIf you have data with versions prior to V2.0, we recommend you re-download the sequences."}),(0,i.jsx)(n.admonition,{type:"note",children:(0,i.jsx)(n.p,{children:"All tooling is still backwards compatible. You should still be able to use old data, but is not recommended."})})]}),"\n",(0,i.jsx)(n.p,{children:":::"}),"\n",(0,i.jsx)(n.h2,{id:"sequence-structure",children:"Sequence structure"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"|Sequence1Name|\n    \u251c\u2500\u2500video.vrs  # Aria recording data\n    \u251c\u2500\u2500instances.json  # metadata of all instances in a sequence. An instance can be an object or a skeleton\n    \u251c\u2500\u2500aria_trajectory.csv  # 6DoF Aria trajectory\n    \u251c\u2500\u25002d_bounding_box.csv  # 2D bounding box data for instances in three Aria sensors: RGB camera, left SLAM camera, right SLAM camera\n    \u251c\u2500\u25003d_bounding_box.csv  # 3D AABB of each object\n    \u251c\u2500\u2500scene_objects.csv    # 6 DoF poses of objects\n    \u251c\u2500\u2500eyegaze.csv          # Eye gaze\n    \u251c\u2500\u2500synthetic_video.vrs  # Synthetic rendering of video.vrs\n    \u251c\u2500\u2500depth_images.vrs     # Depth images of video.vrs\n    \u251c\u2500\u2500segmentations.vrs    # Instance segmentations of video.vrs\n    \u251c\u2500\u2500skeleton_aria_association.json [optional]  # File showing association between Aria devices and skeletons, if they exist. Omitted if a sequence does not have skeleton ground truth.\n    \u251c\u2500\u2500Skeleton_*.json [optional]   # Body skeleton data. * is the skeleton name. Omitted if a sequence does not have skeleton ground truth\n    \u251c\u2500\u25002d_bounding_box_with_skeleton.csv [optional]  # 2D bounding box data with body mesh occlusions. Omitted if a sequence does not have skeleton ground truth\n    \u251c\u2500\u2500depth_images_with_skeleton.vrs [optional]  # Depth images with body mesh occlusions. Omitted if a sequence does not have skeleton ground truth\n    \u251c\u2500\u2500segmentations_with_skeleton.vrs [optional]  # Segmentations with body mesh occlusions. Omitted if a sequence does not have skeleton ground truth\n    \u251c\u2500\u2500metadata.json # stores important information about the sequence\n    \u251c\u2500\u2500MPS # Go to Data Formats/MPS Output for more information about the data in this directory\n        \u251c\u2500\u2500 eye_gaze\n            \u251c\u2500\u2500 general_eye_gaze.csv\n            \u251c\u2500\u2500 summary.json\n        \u251c\u2500\u2500 slam\n            \u251c\u2500\u2500 alignment_results.json # Alignment results between the MPS closed loop trajectory and the ADT GT trajectory\n            \u251c\u2500\u2500 closed_loop_trajectory.csv\n            \u251c\u2500\u2500 open_loop_trajectory.csv\n            \u251c\u2500\u2500 online_calibration.csv\n            \u251c\u2500\u2500 semidense_observations.csv.gz\n            \u251c\u2500\u2500 semidense_points.csv.gz\n            \u251c\u2500\u2500 summary.json\n"})}),"\n",(0,i.jsx)(n.admonition,{title:"SkeletonMetaData.json name change",type:"note",children:(0,i.jsxs)(n.p,{children:["Prior to ",(0,i.jsx)(n.a,{href:"https://github.com/facebookresearch/projectaria_tools/releases/tag/1.1.0",children:"v1.1"})," of the dataset, ",(0,i.jsx)(n.code,{children:"skeleton_aria_association.json"})," was called ",(0,i.jsx)(n.code,{children:"SkeletonMetaData.json"}),"."]})}),"\n",(0,i.jsx)(n.h3,{id:"timestamps-mapping-data",children:"Timestamps Mapping Data"}),"\n",(0,i.jsxs)(n.p,{children:["Project Aria glasses recording concurrently in the same location leverage ",(0,i.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/SMPTE_timecode",children:"SMPTE timecode"})," to receive a synchronized time clock with sub-millisecond accuracy."]}),"\n",(0,i.jsxs)(n.p,{children:["The mapping between device time and timecode clock for each sequence is stored in the VRS file as a Time Domain Mapping Class. Go to ",(0,i.jsx)(n.a,{href:"/projectaria_tools/docs/data_formats/aria_vrs/timestamps_in_aria_vrs",children:"Timestamps in Aria VRS Files"})," for more information about how Aria sensor data is timestamped."]}),"\n",(0,i.jsxs)(n.p,{children:["Go to ",(0,i.jsx)(n.a,{href:"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/advanced_tutorials/multiperson_synchronization",children:"Multiperson Synchronization"})," for how to get synchronized ground truth data in a multi-person sequence."]}),"\n",(0,i.jsx)(n.h2,{id:"ground-truth-data",children:"Ground Truth Data"}),"\n",(0,i.jsxs)(n.p,{children:["You can use the ",(0,i.jsx)(n.code,{children:"AriaDigitalTwinDataPathProvider"})," to load a sequence and select a subsequence.\n",(0,i.jsx)(n.code,{children:"AriaDigitalTwinDataPathProvider"})," will manage all the ground truth files in a subsequence folder (not the MPS files)."]}),"\n",(0,i.jsx)(n.h3,{id:"aligning-ground-truth-and-mps-data",children:"Aligning Ground Truth and MPS Data"}),"\n",(0,i.jsx)(n.p,{children:"The alignment_results.json file in mps/slam directory contains the alignment results between the MPS closed loop trajectory and the ADT GT trajectory. The alignment results have already been applied to the closed loop trajectory and the semidense pointcloud to convert from the SLAM frame to the ADT frame, ensuring all ADT data is expressed in the same coordinate frame for all sequences."}),"\n",(0,i.jsx)(n.h2,{id:"skeleton-data-and-availability",children:"Skeleton Data and Availability"}),"\n",(0,i.jsx)(n.p,{children:"Not all ADT sequences have skeleton tracking. For those sequences with skeleton tracking enabled, we use the marker measurements from the bodysuit to generate a 3D mesh estimate of the wearer which is then used in our ground truth generation pipeline to calculate 2D bounding boxes, segmentation images and depth images."}),"\n",(0,i.jsx)(n.p,{children:"In these cases, ADT provides two sets of ground truth data: one with skeleton occlusion, one without."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"segmentations.vrs"})," vs. ",(0,i.jsx)(n.code,{children:"segmentations_with_skeleton.vrs"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"depth_images.vrs"})," vs. ",(0,i.jsx)(n.code,{children:"depth_images_with_skeleton.vrs"})]}),"\n",(0,i.jsx)(n.li,{children:"'2d_bounding_box.csv' vs. '2d_bounding_box_with_skeleton.csv'"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["You can use ",(0,i.jsx)(n.code,{children:"AriaDigitalTwinDataPathsProvider"})," to switch between these two sets."]}),"\n",(0,i.jsx)(n.h2,{id:"ground-truth-data-format",children:"Ground Truth Data Format"}),"\n",(0,i.jsxs)(n.p,{children:["Our data loader loads all this data into a single class with useful tools for accessing data. For more information on the data classes returned by the loader, go to the ",(0,i.jsx)(n.a,{href:"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_loader",children:"Data Loader page"}),"."]}),"\n",(0,i.jsx)(n.h3,{id:"2d_bounding_boxcsv-or-2d_bounding_box_with_skeletoncsv",children:"2d_bounding_box.csv or 2d_bounding_box_with_skeleton.csv"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Column"}),(0,i.jsx)(n.th,{children:"Type"}),(0,i.jsx)(n.th,{children:"Description"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"stream_id"}),(0,i.jsx)(n.td,{children:"string"}),(0,i.jsx)(n.td,{children:"camera stream id associated with the bounding box image"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"object_uid"}),(0,i.jsx)(n.td,{children:"uint64_t"}),(0,i.jsx)(n.td,{children:"id of the instance (object or skeleton)"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"timestamp[ns]"}),(0,i.jsx)(n.td,{children:"int64_t"}),(0,i.jsx)(n.td,{children:"timestamp of the image in nanoseconds"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"x_min[pixel]"}),(0,i.jsx)(n.td,{children:"int"}),(0,i.jsx)(n.td,{children:"minimum dimension in the x axis"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"x_max[pixel]"}),(0,i.jsx)(n.td,{children:"int"}),(0,i.jsx)(n.td,{children:"maximum dimension in the x axis"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"y_min[pixel]"}),(0,i.jsx)(n.td,{children:"int"}),(0,i.jsx)(n.td,{children:"minimum dimension in the y axis"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"y_max[pixel]"}),(0,i.jsx)(n.td,{children:"int"}),(0,i.jsx)(n.td,{children:"maximum dimension in the y axis"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"visibility_ratio[%]"}),(0,i.jsx)(n.td,{children:"double"}),(0,i.jsx)(n.td,{children:"percentage of the object that is visible (0: not visible, 1: fully visible)"})]})]})]}),"\n",(0,i.jsx)(n.h3,{id:"3d_bounding_boxcsv",children:"3d_bounding_box.csv"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Column"}),(0,i.jsx)(n.th,{children:"Type"}),(0,i.jsx)(n.th,{children:"Description"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"object_uid"}),(0,i.jsx)(n.td,{children:"uint64_t"}),(0,i.jsx)(n.td,{children:"id of the instance (object or skeleton)"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"timestamp[ns]"}),(0,i.jsx)(n.td,{children:"int64_t"}),(0,i.jsx)(n.td,{children:"timestamp of the image in nanoseconds. -1 means the instance is static"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"p_local_obj_xmin[m]"}),(0,i.jsx)(n.td,{children:"double"}),(0,i.jsx)(n.td,{children:"minimum dimension in the x axis (in meters) of the bounding box"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"p_local_obj_xmax[m]"}),(0,i.jsx)(n.td,{children:"double"}),(0,i.jsx)(n.td,{children:"maximum dimension in the x axis (in meters) of the bounding box"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"p_local_obj_ymin[m]"}),(0,i.jsx)(n.td,{children:"double"}),(0,i.jsx)(n.td,{children:"minimum dimension in the y axis (in meters) of the bounding box"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"p_local_obj_ymax[m]"}),(0,i.jsx)(n.td,{children:"double"}),(0,i.jsx)(n.td,{children:"maximum dimension in the y axis (in meters) of the bounding box"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"p_local_obj_zmin[m]"}),(0,i.jsx)(n.td,{children:"double"}),(0,i.jsx)(n.td,{children:"minimum dimension in the z axis (in meters) of the bounding box"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"p_local_obj_zmax[m]"}),(0,i.jsx)(n.td,{children:"double"}),(0,i.jsx)(n.td,{children:"maximum dimension in the z axis (in meters) of the bounding box"})]})]})]}),"\n",(0,i.jsx)(n.h3,{id:"aria_trajectorycsv",children:"aria_trajectory.csv"}),"\n",(0,i.jsxs)(n.p,{children:["ADT uses the same trajectory format as ",(0,i.jsx)(n.a,{href:"/projectaria_tools/docs/data_formats/mps/slam/mps_trajectory#closed-loop-trajectory",children:"closed loop trajectory in MPS"}),"."]}),"\n",(0,i.jsx)(n.p,{children:"While the data structure is the same, the file is generated by the ADT ground truth system, not by MPS."}),"\n",(0,i.jsx)(n.h3,{id:"eyegazecsv",children:"eyegaze.csv"}),"\n",(0,i.jsxs)(n.p,{children:["ADT uses the same ",(0,i.jsx)(n.a,{href:"/projectaria_tools/docs/data_formats/mps/mps_eye_gaze#eye-gaze-data-format",children:"eye gaze format as MPS"}),"."]}),"\n",(0,i.jsx)(n.p,{children:"Unlike MPS outputs, the ground truth eyegaze.csv contains depth mapping estimated by the ADT ground truth system."}),"\n",(0,i.jsx)(n.h3,{id:"scene_objectscsv",children:"scene_objects.csv"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Column"}),(0,i.jsx)(n.th,{children:"Type"}),(0,i.jsx)(n.th,{children:"Description"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"object_uid"}),(0,i.jsx)(n.td,{children:"uint64_t"}),(0,i.jsx)(n.td,{children:"id of the instance (object or skeleton)"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"timestamp[ns]"}),(0,i.jsx)(n.td,{children:"int64_t"}),(0,i.jsx)(n.td,{children:"timestamp of the image in nanoseconds. -1 means the instance is static"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"t_wo_x[m]"}),(0,i.jsx)(n.td,{children:"double"}),(0,i.jsx)(n.td,{children:"x translation from object frame to world (scene) frame (in meters)"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"t_wo_y[m]"}),(0,i.jsx)(n.td,{children:"double"}),(0,i.jsx)(n.td,{children:"y translation from object frame to world (scene) frame (in meters)"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"t_wo_z[m]"}),(0,i.jsx)(n.td,{children:"double"}),(0,i.jsx)(n.td,{children:"z translation from object frame to world (scene) frame (in meters)"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"q_wo_w"}),(0,i.jsx)(n.td,{children:"double"}),(0,i.jsx)(n.td,{children:"w component of quaternion from object frame to world (scene) frame"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"q_wo_x"}),(0,i.jsx)(n.td,{children:"double"}),(0,i.jsx)(n.td,{children:"x component of quaternion from object frame to world (scene) frame"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"q_wo_y"}),(0,i.jsx)(n.td,{children:"double"}),(0,i.jsx)(n.td,{children:"y component of quaternion from object frame to world (scene) frame"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"q_wo_z"}),(0,i.jsx)(n.td,{children:"double"}),(0,i.jsx)(n.td,{children:"z component of quaternion from object frame to world (scene) frame"})]})]})]}),"\n",(0,i.jsx)(n.h3,{id:"instancesjson",children:"instances.json"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'{\n    "IID1": {\n    "instance_id": IID1,\n    "instance_name": "XXXX",\n    "prototype_name": "XXXX",\n    "category": "XXXX",\n    "category_uid": XXXX,\n    "motion_type": "static/dynamic",\n    "instance_type": "object/human",\n    "rigidity": "rigid/deformable",\n    "rotational_symmetry": {\n      "is_annotated": true/false\n    },\n    "canonical_pose": {\n      "up_vector": [\n        x,\n        y,\n        z\n      ],\n      "front_vector": [\n        x,\n        y,\n        z\n      ]\n    }\n  },\n  ...\n}\n'})}),"\n",(0,i.jsx)(n.h3,{id:"skeleton_tjson-or-skeleton_cjson",children:"Skeleton_T.json or Skeleton_C.json"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'{\n  "dt_optitrack_minus_device_ns": {\n    "1WM103600M1292": XXXXX\n  },\n  "frames": [\n    {\n      "markers": [\n        [\n          mx1\n          my1\n          mz1\n        ],\n        ...\n       ],\n       "joints": [\n         [\n          jx1\n          jy1\n          jz1\n         ],\n        ...\n       ],\n       "timestamp_ns": tsns1\n    },\n    ...\n  ]\n}\n'})}),"\n",(0,i.jsx)(n.h3,{id:"skeleton_aria_associationjson",children:"skeleton_aria_association.json"}),"\n",(0,i.jsx)(n.p,{children:"This file shows the skeleton info including name, Id, and associated Aria device for each human in the sequence."}),"\n",(0,i.jsx)(n.p,{children:"Because it's possible to have a person wearing a bodysuit that is not wearing an Aria device, it's possible to have a skeleton with no associated AriaDeviceSerial."}),"\n",(0,i.jsx)(n.p,{children:"It's also possible to have an Aria wearer with no bodysuit, which means there may be an empty skeleton Id and a name associated with an Aria device."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'{\n    "SkeletonMetadata": [\n        {\n            "AssociatedDeviceSerial": "AriaSerial1/NONE",\n            "SkeletonId": ID1,\n            "SkeletonName": "SkeletonName1/NONE"\n        },\n        ...\n    ]\n}\n'})}),"\n",(0,i.jsx)(n.h3,{id:"videovrs",children:"video.vrs"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"video.vrs"})," contains the raw sensor recording from the Aria device."]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"/projectaria_tools/docs/tech_spec/hardware_spec",children:"Aria Hardware Specifications"})," shows the sensors used to make recordings","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Images were all recorded at 30 fps"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"depth_imagesvrs",children:"depth_images.vrs"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"depth_images.vrs1"})," contains 3 streams of images corresponding to the exact streams in ",(0,i.jsx)(n.code,{children:"video.vrs"}),"."]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Each depth image is the same size as their corresponding raw image, where the pixel contents are integers expressing the depth in the camera\u2019s Z-axis, in units of mm.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"This should not to be confused with ASE depth images, which describe the depth along each pixel ray"}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.li,{children:"Depth data is calculated using ADT\u2019s ground truth system"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"segmentationsvrs",children:"segmentations.vrs"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"segmentations.vrs"})," contains 3 streams of images corresponding to the exact streams in ",(0,i.jsx)(n.code,{children:"video.vrs"}),"."]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Each segmentation image is the same size as their corresponding raw image, where the pixel contents are integers expressing the Instance Id that was observed by that pixel"}),"\n",(0,i.jsx)(n.li,{children:"Segmentation data is calculated using ADT\u2019s ground truth system"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(l,{...e})}):l(e)}}}]);