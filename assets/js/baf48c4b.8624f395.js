"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[3199],{15888:(e,a,r)=>{r.r(a),r.d(a,{assets:()=>c,contentTitle:()=>n,default:()=>p,frontMatter:()=>i,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"ARK/mps/mps","title":"Machine Perception Services (MPS)","description":"To accelerate research with Project Aria, we provide several Spatial AI machine perception capabilities that help form the foundation for future Contextualized AI applications and analysis of egocentric data. These capabilities are powered by a set of proprietary machine perception algorithms, designed for Project Aria glasses, that provide superior accuracy and robustness on Aria data compared to off-the-shelf open source algorithms.","source":"@site/docs/ARK/mps/mps.mdx","sourceDirName":"ARK/mps","slug":"/ARK/mps/","permalink":"/projectaria_tools/docs/ARK/mps/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebookresearch/projectaria_tools/tree/main/website/docs/ARK/mps/mps.mdx","tags":[],"version":"current","sidebarPosition":10,"frontMatter":{"sidebar_position":10,"title":"Machine Perception Services (MPS)"},"sidebar":"tutorialSidebar","previous":{"title":"SDK Troubleshooting & Known Issues","permalink":"/projectaria_tools/docs/ARK/sdk/sdk_troubleshooting"},"next":{"title":"Request MPS","permalink":"/projectaria_tools/docs/ARK/mps/request_mps/"}}');var t=r(74848),o=r(28453);const i={sidebar_position:10,title:"Machine Perception Services (MPS)"},n="Project Aria Machine Perception Services",c={},d=[{value:"Current MPS offerings",id:"current-mps-offerings",level:2},{value:"SLAM services",id:"slam-services",level:2},{value:"6DoF trajectory",id:"6dof-trajectory",level:3},{value:"Semi-dense point cloud",id:"semi-dense-point-cloud",level:3},{value:"Online sensor calibration",id:"online-sensor-calibration",level:3},{value:"Multi-SLAM",id:"multi-slam",level:3},{value:"Eye Gaze services",id:"eye-gaze-services",level:2},{value:"Hand Tracking",id:"hand-tracking",level:2},{value:"About MPS Data Loader APIs",id:"about-mps-data-loader-apis",level:2},{value:"Questions &amp; Feedback",id:"questions--feedback",level:2}];function l(e){const a={a:"a",admonition:"admonition",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(a.header,{children:(0,t.jsx)(a.h1,{id:"project-aria-machine-perception-services",children:"Project Aria Machine Perception Services"})}),"\n",(0,t.jsx)(a.p,{children:"To accelerate research with Project Aria, we provide several Spatial AI machine perception capabilities that help form the foundation for future Contextualized AI applications and analysis of egocentric data. These capabilities are powered by a set of proprietary machine perception algorithms, designed for Project Aria glasses, that provide superior accuracy and robustness on Aria data compared to off-the-shelf open source algorithms."}),"\n",(0,t.jsxs)(a.p,{children:["Some MPS data is provided as part of open data releases. Go to the ",(0,t.jsx)(a.a,{href:"/projectaria_tools/docs/open_datasets/",children:"Open Datasets section"})," for documentation about each release."]}),"\n",(0,t.jsxs)(a.p,{children:["For research partners with access to the Aria Research Kit, Machine Perception Services (MPS) are offered as post-processing of VRS files via a cloud service. Use the ",(0,t.jsx)(a.a,{href:"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli",children:"MPS CLI"})," to request derived data from any VRS file that contains necessary sensor data."]}),"\n",(0,t.jsx)(a.admonition,{type:"note",children:(0,t.jsxs)(a.p,{children:["When research partners submit data for processing, the data is only used to serve MPS requests. Partner data is not made available to Meta researchers or Meta\u2019s affiliates. Go to ",(0,t.jsx)(a.a,{href:"/projectaria_tools/docs/ARK/mps/mps_processing",children:"MPS Data Lifecycle"})," for more details about how partner data is processed and stored."]})}),"\n",(0,t.jsx)(a.h2,{id:"current-mps-offerings",children:"Current MPS offerings"}),"\n",(0,t.jsxs)(a.p,{children:["The following MPS can be requested, as long as the data has been recorded with a compatible Recording Profile. Go to the ",(0,t.jsx)(a.a,{href:"/projectaria_tools/docs/tech_spec/recording_profiles",children:"Recording Profiles"})," for information about each profile."]}),"\n",(0,t.jsx)(a.p,{children:"MPS offerings are grouped into SLAM, Eye Gaze and Hand Tracking services."}),"\n",(0,t.jsx)(a.h2,{id:"slam-services",children:"SLAM services"}),"\n",(0,t.jsxs)(a.p,{children:["To get these outputs the ",(0,t.jsx)(a.a,{href:"/projectaria_tools/docs/tech_spec/recording_profiles",children:"recording profile"})," must have SLAM cameras + IMU enabled."]}),"\n",(0,t.jsx)(a.h3,{id:"6dof-trajectory",children:"6DoF trajectory"}),"\n",(0,t.jsx)(a.p,{children:"MPS provides two types of high frequency (1kHz) trajectories:"}),"\n",(0,t.jsxs)(a.ul,{children:["\n",(0,t.jsxs)(a.li,{children:[(0,t.jsx)(a.a,{href:"/projectaria_tools/docs/data_formats/mps/slam/mps_trajectory#open-loop-trajectory",children:"Open loop trajectory"})," - local odometry estimation from visual-inertial odometry (VIO)"]}),"\n",(0,t.jsxs)(a.li,{children:[(0,t.jsx)(a.a,{href:"/projectaria_tools/docs/data_formats/mps/slam/mps_trajectory#closed-loop-trajectory",children:"Closed loop trajectory"})," - created via batch optimization, using multi-sensors' input (SLAM, IMU, barometer, Wi-Fi and GPS), fully optimized and provides poses in a consistent frame of reference."]}),"\n"]}),"\n",(0,t.jsx)(a.h3,{id:"semi-dense-point-cloud",children:"Semi-dense point cloud"}),"\n",(0,t.jsxs)(a.p,{children:[(0,t.jsx)(a.a,{href:"/projectaria_tools/docs/data_formats/mps/slam/mps_pointcloud",children:"Semi-dense point cloud"})," data supports researchers who need static scene 3D reconstructions, reliable 2D images tracks or a representative visualization of the environment."]}),"\n",(0,t.jsx)(a.h3,{id:"online-sensor-calibration",children:"Online sensor calibration"}),"\n",(0,t.jsxs)(a.p,{children:["The ",(0,t.jsx)(a.a,{href:"/projectaria_tools/docs/data_formats/mps/slam/mps_calibration",children:"time-varying intrinsic and extrinsic calibrations"})," of cameras and IMUs are estimated at the frequency of the SLAM (mono scene) cameras by our multi-sensor state estimation pipeline."]}),"\n",(0,t.jsx)(a.h3,{id:"multi-slam",children:"Multi-SLAM"}),"\n",(0,t.jsxs)(a.p,{children:[(0,t.jsx)(a.a,{href:"/projectaria_tools/docs/data_formats/mps/slam/mps_multi_slam",children:"Multi-SLAM"})," can be requested on two or more recordings. It creates all of the above SLAM output, in a shared co-ordinate frame."]}),"\n",(0,t.jsxs)(a.p,{children:["Multi-SLAM can only be requested using the ",(0,t.jsx)(a.a,{href:"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli",children:"MPS CLI SDK"}),"."]}),"\n",(0,t.jsx)(a.h2,{id:"eye-gaze-services",children:"Eye Gaze services"}),"\n",(0,t.jsxs)(a.p,{children:[(0,t.jsx)(a.a,{href:"/projectaria_tools/docs/data_formats/mps/mps_eye_gaze",children:"Eye Gaze data"})," is generated using Aria's Eye Tracking (ET) camera images to estimate the direction the user is looking. These outputs can be generated for any recording that had ",(0,t.jsx)(a.a,{href:"/projectaria_tools/docs/tech_spec/recording_profiles",children:"ET cameras enabled"}),"."]}),"\n",(0,t.jsx)(a.p,{children:"In March 2024, we updated our eye gaze model to support depth estimation. We do this by providing left and right eye gaze directions (yaw values) along with the depth at which these gaze directions intersect (translation values)."}),"\n",(0,t.jsxs)(a.p,{children:["If you have made a recording with ",(0,t.jsx)(a.a,{href:"/projectaria_tools/docs/ARK/mps/eye_gaze_calibration",children:"In-Session Eye Gaze Calibration"}),", you will receive a second .csv file with calibrated eye gaze outputs."]}),"\n",(0,t.jsx)(a.h2,{id:"hand-tracking",children:"Hand Tracking"}),"\n",(0,t.jsxs)(a.p,{children:["To compute hand tracking outputs, the ",(0,t.jsx)(a.a,{href:"/projectaria_tools/docs/tech_spec/recording_profiles",children:"recording profile"})," must have SLAM cameras enabled."]}),"\n",(0,t.jsxs)(a.p,{children:[(0,t.jsx)(a.a,{href:"/projectaria_tools/docs/data_formats/mps/hand_tracking/",children:"Hand Tracking data"})," is created by using SLAM camera images to estimate the hand movement of the wearer. The 21 landmarks (incl. wrist and palm) are given in the device frame in meters."]}),"\n",(0,t.jsx)(a.h2,{id:"about-mps-data-loader-apis",children:"About MPS Data Loader APIs"}),"\n",(0,t.jsxs)(a.p,{children:["Please refer to our ",(0,t.jsx)(a.a,{href:"/projectaria_tools/docs/data_utilities/core_code_snippets/mps#load-mps-output",children:"MPS data loader APIs"})," (C++ and Python support) to load the MPS outputs into your application. The ",(0,t.jsx)(a.a,{href:"/projectaria_tools/docs/data_utilities/visualization/visualization_cpp#mps-static-scene-visualizer",children:"visualization guide"})," shows how to visualize all the MPS outputs."]}),"\n",(0,t.jsx)(a.h2,{id:"questions--feedback",children:"Questions & Feedback"}),"\n",(0,t.jsxs)(a.p,{children:["If you have feedback you'd like to provide, be it overall trends and experiences or where we can improve, we'd love to hear from you. Go to our ",(0,t.jsx)(a.a,{href:"/projectaria_tools/docs/support",children:"Support page"})," for different ways to get in touch."]})]})}function p(e={}){const{wrapper:a}={...(0,o.R)(),...e.components};return a?(0,t.jsx)(a,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}},28453:(e,a,r)=>{r.d(a,{R:()=>i,x:()=>n});var s=r(96540);const t={},o=s.createContext(t);function i(e){const a=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(a):{...a,...e}},[a,e])}function n(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:i(e.components),s.createElement(o.Provider,{value:a},e.children)}}}]);