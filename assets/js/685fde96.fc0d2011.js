"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[8684],{28453:(e,t,a)=>{a.d(t,{R:()=>i,x:()=>r});var n=a(96540);const s={},o=n.createContext(s);function i(e){const t=n.useContext(o);return n.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),n.createElement(o.Provider,{value:t},e.children)}},63278:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>d,contentTitle:()=>r,default:()=>h,frontMatter:()=>i,metadata:()=>n,toc:()=>c});const n=JSON.parse('{"id":"open_datasets/hot3d","title":"HOT3D Dataset","description":"HOT3D is a new benchmark dataset for vision-based understanding of 3D hand-object interactions. This dataset contains over 800 minutes of egocentric recordings, with 33 diverse hand-held objects, capturing over one million multi-view frames of hand-object interactions.","source":"@site/docs/open_datasets/hot3d.mdx","sourceDirName":"open_datasets","slug":"/open_datasets/hot3d","permalink":"/projectaria_tools/docs/open_datasets/hot3d","draft":false,"unlisted":false,"editUrl":"https://github.com/facebookresearch/projectaria_tools/tree/main/website/docs/open_datasets/hot3d.mdx","tags":[],"version":"current","sidebarPosition":65,"frontMatter":{"sidebar_position":65,"title":"HOT3D Dataset"},"sidebar":"tutorialSidebar","previous":{"title":"DTC Object Explorer","permalink":"/projectaria_tools/docs/open_datasets/object_explorer"},"next":{"title":"Aria Everyday Objects Dataset","permalink":"/projectaria_tools/docs/open_datasets/aria_everyday_objects/"}}');var s=a(74848),o=a(28453);const i={sidebar_position:65,title:"HOT3D Dataset"},r="HOT3D Dataset",d={},c=[{value:"Getting Started",id:"getting-started",level:2},{value:"HOT3D Research Challenges",id:"hot3d-research-challenges",level:2}];function l(e){const t={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.header,{children:(0,s.jsx)(t.h1,{id:"hot3d-dataset",children:"HOT3D Dataset"})}),"\n",(0,s.jsx)(t.p,{children:"HOT3D is a new benchmark dataset for vision-based understanding of 3D hand-object interactions. This dataset contains over 800 minutes of egocentric recordings, with 33 diverse hand-held objects, capturing over one million multi-view frames of hand-object interactions."}),"\n",(0,s.jsx)(t.p,{children:"The dataset contains:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"Synchronized multi-view egocentric videos from Project Aria glasses and Quest 3 VR headset"}),"\n",(0,s.jsx)(t.li,{children:"High-quality 3D pose annotations of hands and objects"}),"\n",(0,s.jsx)(t.li,{children:"3D object models with PBR materials"}),"\n",(0,s.jsx)(t.li,{children:"2D bounding boxes"}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.a,{href:"/projectaria_tools/docs/data_formats/mps/mps_eye_gaze",children:"Eye Gaze MPS data"})," (Aria only)"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.a,{href:"/projectaria_tools/docs/data_formats/mps/slam/mps_pointcloud",children:"Semi-Dense Point Cloud MPS data"})," (Aria only)"]}),"\n"]}),"\n",(0,s.jsx)(t.p,{children:"HOT3D uses its own specific downloader, available in the HOT3D GitHub repository, enabling you to download Quest3, Aria and object models data."}),"\n",(0,s.jsx)(t.h2,{id:"getting-started",children:"Getting Started"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.a,{href:"https://www.projectaria.com/datasets/hot3D/",children:"https://www.projectaria.com/datasets/hot3D/"})," - find out more about the dataset and get access to it."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.a,{href:"https://arxiv.org/pdf/2406.09598",children:"Introducing HOT3D: An Egocentric Dataset for 3D Hand and Object Tracking"})," - research paper."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.a,{href:"https://github.com/facebookresearch/hot3d",children:"HOT3D GitHub repository"})," - install HOT3D Python tooling that will enable you to download and visualize HOT3D data.","\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:["Use the ",(0,s.jsx)(t.a,{href:"https://github.com/facebookresearch/hot3d/blob/main/hot3d/HOT3D_Tutorial.ipynb",children:"HOT3D Jupyter notebook tutorial"})," to get to know the downloader and visualizers."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(t.h2,{id:"hot3d-research-challenges",children:"HOT3D Research Challenges"}),"\n",(0,s.jsx)(t.p,{children:"HOT3D data is used in the following research challenges:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:(0,s.jsx)(t.a,{href:"https://github.com/facebookresearch/hand_tracking_toolkit?tab=readme-ov-file#evaluation",children:"Multiview Egocentric Hand Tracking Challenge"})}),"\n",(0,s.jsx)(t.li,{children:(0,s.jsx)(t.a,{href:"https://bop.felk.cvut.cz/challenges/bop-challenge-2024/",children:"BOP: Benchmark for 6D Object Pose Estimation"})}),"\n"]})]})}function h(e={}){const{wrapper:t}={...(0,o.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(l,{...e})}):l(e)}}}]);