"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[9263],{28453:(e,n,s)=>{s.d(n,{R:()=>r,x:()=>o});var t=s(96540);const a={},i=t.createContext(a);function r(e){const n=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),t.createElement(i.Provider,{value:n},e.children)}},34438:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>d,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"open_datasets/aria_synthetic_environments_dataset/ase_data_format","title":"Data Format","description":"This page provides an overview of Aria Synthetic Environments (ASE) data formats and organization.","source":"@site/docs/open_datasets/aria_synthetic_environments_dataset/ase_data_format.mdx","sourceDirName":"open_datasets/aria_synthetic_environments_dataset","slug":"/open_datasets/aria_synthetic_environments_dataset/ase_data_format","permalink":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_data_format","draft":false,"unlisted":false,"editUrl":"https://github.com/facebookresearch/projectaria_tools/tree/main/website/docs/open_datasets/aria_synthetic_environments_dataset/ase_data_format.mdx","tags":[],"version":"current","sidebarPosition":30,"frontMatter":{"sidebar_position":30,"title":"Data Format"},"sidebar":"tutorialSidebar","previous":{"title":"Dataset Download","permalink":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_download_dataset"},"next":{"title":"Data Tools and Visualization","permalink":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_data_tools"}}');var a=s(74848),i=s(28453);const r={sidebar_position:30,title:"Data Format"},o="ASE Data Format",d={},c=[{value:"Overall Data Organization",id:"overall-data-organization",level:2},{value:"Aria RGB Sensor - Image, Depth and Instance Segmentation",id:"aria-rgb-sensor---image-depth-and-instance-segmentation",level:2},{value:"ASE Scene Language Format",id:"ase-scene-language-format",level:2},{value:"Trajectory and Semi-Dense Map Points",id:"trajectory-and-semi-dense-map-points",level:2}];function l(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"ase-data-format",children:"ASE Data Format"})}),"\n",(0,a.jsx)(n.p,{children:"This page provides an overview of Aria Synthetic Environments (ASE) data formats and organization."}),"\n",(0,a.jsxs)(n.p,{children:["Using the code snippets and tools listed in ",(0,a.jsx)(n.a,{href:"ase_data_tools",children:"Data Tools and Visualization"}),", researchers should be able to quickly onboard this data into ML pipelines."]}),"\n",(0,a.jsx)(n.h2,{id:"overall-data-organization",children:"Overall Data Organization"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Each scene has its own subdirectory with a unique ID (0-100K)"}),"\n",(0,a.jsx)(n.li,{children:"Each scene directory contains separate files and directories for each type of data"}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"<sceneID>\n\u251c\u2500\u2500 rgb\n\u2502   \u2514\u2500\u2500 vignette0000000.jpg\n\u2502   \u2514\u2500\u2500 vignette0000001.jpg\n\u2502   ...\n\u2502   \u2514\u2500\u2500 vignette0xxn.jpg\n\u251c\u2500\u2500 depth\n\u2502   \u2514\u2500\u2500 depth0000000.jpg\n\u2502   \u2514\u2500\u2500 depth0000001.jpg\n\u2502   ...\n\u2502   \u2514\u2500\u2500 depth0xxn.jpg\n\u251c\u2500\u2500 instances\n\u2502   \u2514\u2500\u2500 instance0000000.jpg\n\u2502   \u2514\u2500\u2500 instance0000001.jpg\n\u2502   ...\n\u2502   \u2514\u2500\u2500 instance0xxn.jpg\n\u251c\u2500\u2500 ase_scene_language.txt\n\u251c\u2500\u2500 trajectory.txt\n\u251c\u2500\u2500 semidense_points.csv.gz\n\u251c\u2500\u2500 semidense_observations.csv.gz\n\u2514\u2500\u2500 object_instances_to_classes.json\n"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"rgb"})," - ",(0,a.jsx)(n.strong,{children:"2D RGB fisheye images"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Synthetically generated Aria RGB images at 10 FPS"}),"\n",(0,a.jsx)(n.li,{children:"Each image is saved in JPEG format"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"depth"})," - ",(0,a.jsx)(n.strong,{children:"2D depth maps"})," (16 bit)","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Each depth image is the same size as the corresponding synthetic RGB image, where the pixel contents are integers expressing the depth along the pixel\u2019s ray direction, in units of mm.","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"This should not be confused with ADT depth images, which describe the depth in the camera\u2019s Z-axis"}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.li,{children:"Each image is saved in PNG format"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"instances"})," - ",(0,a.jsx)(n.strong,{children:"2D segmentation maps"})," (16 bit)","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Each segmentation image is the same size as the corresponding synthetic RGB image, where the pixel contents are integers expressing the object Id that was observed by the pixel"}),"\n",(0,a.jsx)(n.li,{children:"Each image is saved as PNG format"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"ase_scene_language.txt"})," - ",(0,a.jsx)(n.strong,{children:"3D floor plan definition"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Describes the scene in the form of a language."}),"\n",(0,a.jsx)(n.li,{children:"Each row is a command which includes its own set of parameters. A set of such commands describe the geomtery of the scene specified."}),"\n",(0,a.jsxs)(n.li,{children:["Go to ",(0,a.jsx)(n.a,{href:"#scene_language",children:"ASE scene language format below"})," for more details"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"trajectory.txt"})," - ",(0,a.jsx)(n.strong,{children:"Ground-truth trajectory"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Go to ",(0,a.jsx)(n.a,{href:"/docs/data_formats/mps/slam/mps_trajectory",children:"MPS Output - Trajectory"})," for how the data is structured","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["While the file structure is the same, please note, this is the ground truth trajectory, not an output generated by ",(0,a.jsx)(n.a,{href:"/docs/ARK/mps",children:"MPS"})]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"semidense_points.csv.gz"})," - ",(0,a.jsx)(n.strong,{children:"Semi-dense map points"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Go to ",(0,a.jsx)(n.a,{href:"/docs/data_formats/mps/slam/mps_pointcloud",children:"MPS Output - Semi-Dense Point Cloud"})," for how the data is structured"]}),"\n",(0,a.jsx)(n.li,{children:"Produced by MPS run on synthetic SLAM (mono scene) camera data"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"semidense_observations.csv.gz"})," - ",(0,a.jsx)(n.strong,{children:"Semi-dense map observations"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Go to ",(0,a.jsx)(n.a,{href:"/docs/data_formats/mps/slam/mps_pointcloud",children:"MPS Output - Semi-Dense Point Cloud"})," for how the data is structured"]}),"\n",(0,a.jsx)(n.li,{children:"Produced by MPS run on synthetic SLAM (mono scene) camera data"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"object_instances_to_classes.json"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Per-scene mappings from the object instance image IDs to object classes"}),"\n",(0,a.jsxs)(n.li,{children:["Given an instance image pixel value/object ID, one will then be able to look up the class from this mapping","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://github.com/facebookresearch/projectaria_tools/issues/1",children:"How to convert them to point clouds based on depth images and RGB images"})}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"aria-rgb-sensor---image-depth-and-instance-segmentation",children:"Aria RGB Sensor - Image, Depth and Instance Segmentation"}),"\n",(0,a.jsx)(n.p,{children:"For each frame from the RGB sensor we provide:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"A vignetted sensor image"}),"\n",(0,a.jsx)(n.li,{children:"Simulated 16 bit metric depth (mm) in PNG image format"}),"\n",(0,a.jsx)(n.li,{children:"A segmentation image (16 bit PNG)"}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["The images in each folder are in sync. This means there will be same number of images in each folder. We also provide example data visualizers to load these images and/or associate them.\n",(0,a.jsx)(n.img,{alt:"Image: sample_rgb_depth_instance_images.png",src:s(71295).A+"",width:"2696",height:"882"})]}),"\n",(0,a.jsx)("div",{id:"scene_language"}),"\n",(0,a.jsx)(n.h2,{id:"ase-scene-language-format",children:"ASE Scene Language Format"}),"\n",(0,a.jsx)(n.p,{children:"The ASE Scene Language format is set of hand-designed procedural commands in pure text form. To handle commonly encountered static indoor layout elements, we use three commands:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"make_wall"})," - the full set of parameters specifies a gravity-aligned oriented box"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"make_door"})," - specify box-based cutouts from walls"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"make_window"})," - specify box-based cutouts from wall"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Each command includes its own set of parameters, as described below. Given the command\u2019s full set of parameters, a geometry is completely specified."}),"\n",(0,a.jsxs)(n.p,{children:["A single scene is described via a sequence of multiple commands stored in ",(0,a.jsx)(n.code,{children:"ase_scene_language.txt"}),". The sequence length is arbitrary and follows no specific ordering. The interpretation of the command and its arguments is carried out by a customized interpreter responsible for parsing the sequence and generating a 3D mesh of the scene."]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Image: language_format.png",src:s(77778).A+"",width:"1254",height:"461"})}),"\n",(0,a.jsx)(n.h2,{id:"trajectory-and-semi-dense-map-points",children:"Trajectory and Semi-Dense Map Points"}),"\n",(0,a.jsxs)(n.p,{children:["Ground-truth trajectory data provides poses for each frame generated from a simulation at 10 FPS.\nWe are follow the same trajectory format as ",(0,a.jsx)(n.a,{href:"/docs/data_formats/mps/slam/mps_trajectory#closed-loop-trajectory",children:"the closed loop trajectory"})," used by ",(0,a.jsx)(n.a,{href:"/docs/ARK/mps",children:"Machine Perception Services (MPS)"}),"."]}),"\n",(0,a.jsxs)(n.p,{children:["For semi-dense map point clouds and their observations, we follow the same ",(0,a.jsx)(n.a,{href:"/docs/data_formats/mps/slam/mps_pointcloud",children:"point cloud points and observations format as MPS"}),". The semi-dense map point cloud is generated using same algorithm as MPS, with the addition of ground-truth trajectory and simulated SLAM camera images."]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(l,{...e})}):l(e)}},71295:(e,n,s)=>{s.d(n,{A:()=>t});const t=s.p+"assets/images/sample_rgb_depth_instance_images-a53f1903a7bf8f4eb8a2970134211e1c.png"},77778:(e,n,s)=>{s.d(n,{A:()=>t});const t=s.p+"assets/images/language_format-639fb56c1943ec56f331239d4af59e2b.png"}}]);