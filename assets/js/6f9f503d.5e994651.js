"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[468],{28453:(e,t,a)=>{a.d(t,{R:()=>n,x:()=>o});var s=a(96540);const i={},r=s.createContext(i);function n(e){const t=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:n(e.components),s.createElement(r.Provider,{value:t},e.children)}},88757:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>c,contentTitle:()=>o,default:()=>p,frontMatter:()=>n,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"open_datasets/aria_everyday_activities_dataset/aea_data_format","title":"Data Format","description":"The Aria Everyday Activities dataset contains multiple activity sequences for one to two Project Aria glasses users. We created recordings using scripts to represent all day activities with always on sensing.","source":"@site/docs/open_datasets/aria_everyday_activities_dataset/aea_data_format.mdx","sourceDirName":"open_datasets/aria_everyday_activities_dataset","slug":"/open_datasets/aria_everyday_activities_dataset/aea_data_format","permalink":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_data_format","draft":false,"unlisted":false,"editUrl":"https://github.com/facebookresearch/projectaria_tools/tree/main/website/docs/open_datasets/aria_everyday_activities_dataset/aea_data_format.mdx","tags":[],"version":"current","sidebarPosition":30,"frontMatter":{"sidebar_position":30,"title":"Data Format"},"sidebar":"tutorialSidebar","previous":{"title":"Dataset Download","permalink":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_download_dataset"},"next":{"title":"Visualizer","permalink":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_visualizers"}}');var i=a(74848),r=a(28453);const n={sidebar_position:30,title:"Data Format"},o="AEA Data Format",c={},d=[{value:"SLAM output",id:"slam-output",level:2},{value:"Speech to Text annotation",id:"speech-to-text-annotation",level:2},{value:"Timestamps Mapping Data",id:"timestamps-mapping-data",level:2}];function l(e){const t={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",p:"p",pre:"pre",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.header,{children:(0,i.jsx)(t.h1,{id:"aea-data-format",children:"AEA Data Format"})}),"\n",(0,i.jsxs)(t.p,{children:["The Aria Everyday Activities dataset contains multiple activity sequences for one to two Project Aria glasses users. We created recordings using ",(0,i.jsx)(t.a,{href:"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_scripts",children:"scripts"})," to represent all day activities with always on sensing."]}),"\n",(0,i.jsx)(t.p,{children:"Each Aria glasses recording is stored as its own sequence with all data related to that recording self contained within that sequence folder. An example sequence folder would look like this:"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"loc1_script1_seq1_rec1\n   \u251c\u2500\u2500 recording.vrs\n   \u251c\u2500\u2500 metadata.json\n   \u251c\u2500\u2500 speech.csv\n   \u251c\u2500\u2500  MPS\n       \u251c\u2500\u2500 eye_gaze\n           \u251c\u2500\u2500 general_eye_gaze.csv\n           \u251c\u2500\u2500 summary.json\n       \u251c\u2500\u2500 slam\n           \u251c\u2500\u2500 closed_loop_trajectory.csv\n           \u251c\u2500\u2500 open_loop_trajectory.csv\n           \u251c\u2500\u2500 online_calibration.csv\n           \u251c\u2500\u2500 semidense_observations.csv.gz\n           \u251c\u2500\u2500 semidense_points.csv.gz\n           \u251c\u2500\u2500 summary.json\n\n"})}),"\n",(0,i.jsx)(t.h2,{id:"slam-output",children:"SLAM output"}),"\n",(0,i.jsxs)(t.p,{children:["The SLAM outputs were created in a shared coordinate frame. The ",(0,i.jsx)(t.a,{href:"/projectaria_tools/docs/data_formats/mps/slam/mps_multi_slam",children:"Multi-SLAM data format page"})," contains more information about this output. Please note the file structure is slightly different (vrs_to_multi_slam.json is not necessary) compared to ",(0,i.jsx)(t.a,{href:"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli",children:"MPS CLI"})," Multi-SLAM requests, as the shared coordinate frame is organized by location."]}),"\n",(0,i.jsx)(t.h2,{id:"speech-to-text-annotation",children:"Speech to Text annotation"}),"\n",(0,i.jsx)(t.p,{children:"Speech to Text annotation provides text strings generated by Automatic Speech Recognition (ASR) with timestamps and confidence rating. The ASR annotation used an in-house proprietary system. Similar results can be acquired via open-source ASR solutions."}),"\n",(0,i.jsxs)(t.p,{children:["Table 2: ",(0,i.jsxs)("em",{children:[(0,i.jsx)(t.code,{children:"speech.csv"})," Structure"]})]}),"\n",(0,i.jsxs)("table",{children:[(0,i.jsxs)("tr",{children:[(0,i.jsx)("td",{children:(0,i.jsx)("strong",{children:"startTime_ns"})}),(0,i.jsx)("td",{children:(0,i.jsx)("strong",{children:"endTime_ns"})}),(0,i.jsx)("td",{children:(0,i.jsx)("strong",{children:"written"})}),(0,i.jsx)("td",{children:(0,i.jsx)("strong",{children:"confidence"})})]}),(0,i.jsxs)("tr",{children:[(0,i.jsx)("td",{children:"54040"}),(0,i.jsx)("td",{children:"55040"}),(0,i.jsx)("td",{children:"I\u2019m"}),(0,i.jsx)("td",{children:"0.25608"})]}),(0,i.jsxs)("tr",{children:[(0,i.jsx)("td",{children:"72920"}),(0,i.jsx)("td",{children:"73920"}),(0,i.jsx)("td",{children:"looking"}),(0,i.jsx)("td",{children:"0.84339"})]})]}),"\n",(0,i.jsx)(t.h2,{id:"timestamps-mapping-data",children:"Timestamps Mapping Data"}),"\n",(0,i.jsxs)(t.p,{children:["Project Aria glasses and multi-view devices operating in proximity to each other (<100m) can leverage ",(0,i.jsx)(t.a,{href:"https://en.wikipedia.org/wiki/SMPTE_timecode",children:"SMPTE timecode"})," to receive a synchronized time clock with sub-millisecond accuracy."]}),"\n",(0,i.jsxs)(t.p,{children:["The mapping between device time and timecode clock for each sequence is stored in the VRS file as a Time Domain Mapping Class. Go to ",(0,i.jsx)(t.a,{href:"/projectaria_tools/docs/data_formats/aria_vrs/timestamps_in_aria_vrs",children:"Timestamps in Aria VRS Files"})," for more information about how Aria sensor data is timestamped."]}),"\n",(0,i.jsxs)(t.p,{children:["To translate the local timestamp of an arbitrary piece of data to the timecode time domain, you can interpolate between device timestamps in the time domain mapping data. An implementation of this mechanism is already provided in ",(0,i.jsx)(t.a,{href:"https://github.com/facebookresearch/projectaria_tools/blob/main/core/data_provider/VrsDataProvider.h#L299-L309",children:"VrsDataProvider"}),". To synchronize data from a secondary device, you can query that second VRS with this timecode time. This functionality is also already implemented in the VrsDataProvider class."]})]})}function p(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(l,{...e})}):l(e)}}}]);