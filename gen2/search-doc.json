[{"title":"Welcome to Aria Research Kit","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/ark","content":"","keywords":"","version":"Next"},{"title":"What is Aria Gen 2?‚Äã","type":1,"pageTitle":"Welcome to Aria Research Kit","url":"/projectaria_tools/gen2/ark#what-is-aria-gen-2","content":" Aria Gen 2 is a state-of-the-art research device featuring an advanced sensor suite that includes:  Four computer vision (CV) cameras with wide field of view and HDR imagingRGB point-of-view (POV) camera for high-resolution egocentric visionOn-device VIO for real-time 3D tracking and mappingOn-device Eye tracking system for precise gaze estimation and attention modelingOn-device 21-keypoint hand tracking for articulated hand pose estimationSpatial and contact microphones for high-fidelity audio captureIMU, barometer, magnetometer, and GNSS for accurate localization and motion trackingAmbient Light Sensor (ALS) with UV channel for indoor/outdoor context awarenessPPG sensor for heart rate monitoring  See Hardware Specifications for more details.  ","version":"Next","tagName":"h2"},{"title":"What You Can Do with ARK‚Äã","type":1,"pageTitle":"Welcome to Aria Research Kit","url":"/projectaria_tools/gen2/ark#what-you-can-do-with-ark","content":" ","version":"Next","tagName":"h2"},{"title":"üéÆ Control Your Device‚Äã","type":1,"pageTitle":"Welcome to Aria Research Kit","url":"/projectaria_tools/gen2/ark#-control-your-device","content":" Use the Client SDK to programmatically control Aria Gen 2 devices:  Start and stop recordingsStream sensor data in real-timeAccess on-device machine perception outputs (VIO, hand tracking, eye gaze)Configure recording profiles for different research applicationsAutomate device workflows with Python scripts or CLI tools  ","version":"Next","tagName":"h3"},{"title":"üì± Mobile Companion App‚Äã","type":1,"pageTitle":"Welcome to Aria Research Kit","url":"/projectaria_tools/gen2/ark#-mobile-companion-app","content":" Pair your device with the Companion App to:  Configure and manage your Aria Gen 2 glassesStart recordings with a simple interfaceMonitor device status and battery lifeManage recordings from your phone  ","version":"Next","tagName":"h3"},{"title":"üîç On-Device Machine Perception‚Äã","type":1,"pageTitle":"Welcome to Aria Research Kit","url":"/projectaria_tools/gen2/ark#-on-device-machine-perception","content":" Run real-time algorithms directly on Aria Gen 2:  Visual Inertial Odometry (VIO) at 10Hz (and 800Hz high-frequency output)Eye gaze tracking at up to 90Hz with pupil diameter and vergence depthHand tracking at 30Hz with 21 joint landmarks per hand  ","version":"Next","tagName":"h3"},{"title":"üß† Machine Perception Services (MPS)‚Äã","type":1,"pageTitle":"Welcome to Aria Research Kit","url":"/projectaria_tools/gen2/ark#-machine-perception-services-mps","content":" Leverage cloud-based processing to generate rich outputs from your recordings:  3D trajectory and semi-dense point cloudHand tracking  All optimized for all-day wearability with energy-efficient hardware acceleration.  ","version":"Next","tagName":"h3"},{"title":"üõ†Ô∏è Data Analysis & Visualization‚Äã","type":1,"pageTitle":"Welcome to Aria Research Kit","url":"/projectaria_tools/gen2/ark#Ô∏è-data-analysis--visualization","content":" Use the comprehensive toolset to:  Validate recording quality with VRS Health CheckVisualize sensor data streamsExport data in standard formatsBuild custom analysis pipelines  ","version":"Next","tagName":"h3"},{"title":"Getting Started‚Äã","type":1,"pageTitle":"Welcome to Aria Research Kit","url":"/projectaria_tools/gen2/ark#getting-started","content":" Choose your path based on your role and objectives:  ","version":"Next","tagName":"h2"},{"title":"For Developers‚Äã","type":1,"pageTitle":"Welcome to Aria Research Kit","url":"/projectaria_tools/gen2/ark#for-developers","content":" Start with the Client SDK to control your device programmatically. Install the Python SDK, authenticate your device, and begin streaming or recording data within minutes.  ","version":"Next","tagName":"h3"},{"title":"For Researchers‚Äã","type":1,"pageTitle":"Welcome to Aria Research Kit","url":"/projectaria_tools/gen2/ark#for-researchers","content":" Explore the Device Manual to understand the hardware capabilities and sensor specifications. Learn about recording profiles and on-device machine perception in the On-Device Machine Perception guide.  ","version":"Next","tagName":"h3"},{"title":"For First-Time Users‚Äã","type":1,"pageTitle":"Welcome to Aria Research Kit","url":"/projectaria_tools/gen2/ark#for-first-time-users","content":" Begin with the Companion App for a simple way to set up and use your Aria Gen 2 glasses. Then explore the Client SDK when you're ready for more advanced control.  ","version":"Next","tagName":"h3"},{"title":"For Data Scientists‚Äã","type":1,"pageTitle":"Welcome to Aria Research Kit","url":"/projectaria_tools/gen2/ark#for-data-scientists","content":" Check out Machine Perception Services (MPS) to learn how to process your recordings and generate rich perception signals for your research datasets.  ","version":"Next","tagName":"h3"},{"title":"Need Help?‚Äã","type":1,"pageTitle":"Welcome to Aria Research Kit","url":"/projectaria_tools/gen2/ark#need-help","content":" Visit the Support page for troubleshooting guides, FAQs, and contact information.    Ready to dive in? Pick a section from the navigation menu to begin your journey with Aria Research Kit. ","version":"Next","tagName":"h2"},{"title":"Authentication Example","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/auth-example","content":"","keywords":"","version":"Next"},{"title":"Quick Start‚Äã","type":1,"pageTitle":"Authentication Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/auth-example#quick-start","content":" Run the authentication example script:  python ~/Downloads/projectaria_client_sdk_samples_gen2/device_auth.py   When prompted, open the Companion App on your mobile device and approve the authentication request.    ","version":"Next","tagName":"h2"},{"title":"What This Example Does‚Äã","type":1,"pageTitle":"Authentication Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/auth-example#what-this-example-does","content":" The script performs the following steps:  Creates a device client to manage connectionsConfigures the client to connect to the first available deviceInitiates the authentication processWaits for user approval via the Companion AppEstablishes a connection to verify authentication    ","version":"Next","tagName":"h2"},{"title":"Code Walkthrough‚Äã","type":1,"pageTitle":"Authentication Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/auth-example#code-walkthrough","content":" ","version":"Next","tagName":"h2"},{"title":"Step 1: Import Required Modules‚Äã","type":1,"pageTitle":"Authentication Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/auth-example#step-1-import-required-modules","content":" import time import aria.sdk_gen2 as sdk_gen2   The aria.sdk_gen2 module provides all the classes and functions needed to interact with Aria Gen2 devices.    ","version":"Next","tagName":"h3"},{"title":"Step 2: Create Device Client‚Äã","type":1,"pageTitle":"Authentication Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/auth-example#step-2-create-device-client","content":" device_client = sdk_gen2.DeviceClient()   The DeviceClient object manages authentication and device connections. It handles the communication between your PC and the device.    ","version":"Next","tagName":"h3"},{"title":"Step 3: Configure the Client‚Äã","type":1,"pageTitle":"Authentication Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/auth-example#step-3-configure-the-client","content":" config = sdk_gen2.DeviceClientConfig() device_client.set_client_config(config)   What this does:  Creates a default configuration that connects to the first available deviceIf you want to connect to a specific device, you can set config.device_serial to the device's serial number  Example with specific device:  config = sdk_gen2.DeviceClientConfig() config.device_serial = &quot;1M0YDB5H7B0020&quot; # Replace with your device serial device_client.set_client_config(config)     ","version":"Next","tagName":"h3"},{"title":"Step 4: Initiate Authentication‚Äã","type":1,"pageTitle":"Authentication Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/auth-example#step-4-initiate-authentication","content":" print(&quot;Authenticating device. Please open the Aria app and accept the pairing request&quot;) device_client.authenticate() time.sleep(5)   What happens here:  The authenticate() method starts the authentication processThe device sends a pairing request to the Companion App on your mobile deviceThe script waits 5 seconds to give you time to approve the request  Important:  Keep the Companion App open and in the foregroundCheck for a notification or prompt in the appVerify the hash code matches between your PC terminal and the Companion AppApprove the authentication request    ","version":"Next","tagName":"h3"},{"title":"Step 5: Connect and Verify‚Äã","type":1,"pageTitle":"Authentication Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/auth-example#step-5-connect-and-verify","content":" try: device = device_client.connect() print(f&quot;Device authentication successful to device {device.connection_id()}&quot;) except Exception: print(&quot;Failed to authenticate and connect to device&quot;) return   What this does:  Attempts to establish a connection to the authenticated deviceIf successful, returns a Device object that you can use for recording, streaming, etc.The device.connection_id() provides the device identifier    ","version":"Next","tagName":"h3"},{"title":"Complete Example Code‚Äã","type":1,"pageTitle":"Authentication Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/auth-example#complete-example-code","content":" Here's the full authentication script:  import time import aria.sdk_gen2 as sdk_gen2 def device_auth(): device_client = sdk_gen2.DeviceClient() config = sdk_gen2.DeviceClientConfig() device_client.set_client_config(config) print( &quot;Authenticating device. Please open the Aria app and accept the pairing request&quot; ) device_client.authenticate() time.sleep(5) try: device = device_client.connect() print(f&quot;Device authentication successful to device {device.connection_id()}&quot;) except Exception: print(f&quot;Failed to authenticate and connect to device&quot;) return if __name__ == &quot;__main__&quot;: device_auth()     ","version":"Next","tagName":"h2"},{"title":"Expected Output‚Äã","type":1,"pageTitle":"Authentication Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/auth-example#expected-output","content":" When running the script, you should see output similar to:  Authenticating device. Please open the Aria app and accept the pairing request [AriaGen2SDK:DeviceClient][INFO]: Client hash is: f30a36a9f3842bd3fc16c75b525... [AriaGen2SDK:DeviceClient][INFO]: Waiting for authentication approval... Device authentication successful to device 1M0YDB5H7B0020     ","version":"Next","tagName":"h2"},{"title":"Troubleshooting‚Äã","type":1,"pageTitle":"Authentication Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/auth-example#troubleshooting","content":" ","version":"Next","tagName":"h2"},{"title":"Authentication Times Out‚Äã","type":1,"pageTitle":"Authentication Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/auth-example#authentication-times-out","content":" Problem: The script waits but never completes authentication.  Solutions:  Ensure the Companion App is running in the foregroundCheck that your mobile device has an active internet connectionVerify notifications are enabled for the Companion AppCheck that you approved the correct pairing request (hash codes should match)    ","version":"Next","tagName":"h3"},{"title":"Device Not Found‚Äã","type":1,"pageTitle":"Authentication Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/auth-example#device-not-found","content":" Problem: Error message indicates no device found.  Solutions:  Verify the device is connected via USBRun aria-gen2 device list to check connectivityEnsure aria-doctor has been runCheck USB cable and port connections    ","version":"Next","tagName":"h3"},{"title":"Key Concepts‚Äã","type":1,"pageTitle":"Authentication Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/auth-example#key-concepts","content":" ","version":"Next","tagName":"h2"},{"title":"Authentication vs Connection‚Äã","type":1,"pageTitle":"Authentication Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/auth-example#authentication-vs-connection","content":" Authentication: A one-time security handshake between device and PC (requires Companion App approval)Connection: Establishing a session with an already-authenticated device (no approval needed)  Once authenticated, you can connect to the device repeatedly without re-authenticating.  ","version":"Next","tagName":"h3"},{"title":"Device Object‚Äã","type":1,"pageTitle":"Authentication Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/auth-example#device-object","content":" The Device object returned by connect() is your interface for all device operations:  device = device_client.connect() # Now you can use device for: # - Recording: device.start_recording() # - Streaming: device.start_streaming() # - Commands: device.render_tts(&quot;Hello&quot;) # - Info: device.connection_id()     ","version":"Next","tagName":"h3"},{"title":"Next Steps‚Äã","type":1,"pageTitle":"Authentication Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/auth-example#next-steps","content":" Learn how to connect to a specific deviceStart recording data from your deviceExplore real-time streamingReview all Python SDK examples ","version":"Next","tagName":"h2"},{"title":"Connection Example","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/connection-example","content":"","keywords":"","version":"Next"},{"title":"Quick Start‚Äã","type":1,"pageTitle":"Connection Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/connection-example#quick-start","content":" Run the connection example script:  python ~/Downloads/projectaria_client_sdk_samples_gen2/device_connect.py   The script connects to the first available device and displays its information.    ","version":"Next","tagName":"h2"},{"title":"What This Example Does‚Äã","type":1,"pageTitle":"Connection Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/connection-example#what-this-example-does","content":" The script performs the following steps:  Creates a device clientConfigures the client to connect to a specific device (or first available)Establishes a connectionDisplays device information    ","version":"Next","tagName":"h2"},{"title":"Code Walkthrough‚Äã","type":1,"pageTitle":"Connection Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/connection-example#code-walkthrough","content":" ","version":"Next","tagName":"h2"},{"title":"Step 1: Import Required Modules‚Äã","type":1,"pageTitle":"Connection Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/connection-example#step-1-import-required-modules","content":" import argparse import aria.sdk_gen2 as sdk_gen2     ","version":"Next","tagName":"h3"},{"title":"Step 2: Parse Command-Line Arguments‚Äã","type":1,"pageTitle":"Connection Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/connection-example#step-2-parse-command-line-arguments","content":" def parse_args() -&gt; argparse.Namespace: parser = argparse.ArgumentParser() parser.add_argument( &quot;--serial&quot;, dest=&quot;serial&quot;, type=str, default=&quot;&quot;, required=False, help=&quot;Serial number of the device which will be connected. (e.g. 1M0YDB5H7B0020)&quot;, ) return parser.parse_args()   Available Options:  --serial: Specify a device by its serial number (e.g., 1M0YDB5H7B0020)If nothing is specified, connects to the first available device    ","version":"Next","tagName":"h3"},{"title":"Step 3: Create and Configure Device Client‚Äã","type":1,"pageTitle":"Connection Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/connection-example#step-3-create-and-configure-device-client","content":" device_client = sdk_gen2.DeviceClient() config = sdk_gen2.DeviceClientConfig() config.device_serial = serial device_client.set_client_config(config)   How Configuration Works:  Default (no arguments): Connects to the first available deviceBy Serial: Targets a specific device by its unique serial number    ","version":"Next","tagName":"h3"},{"title":"Step 4: Establish Connection‚Äã","type":1,"pageTitle":"Connection Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/connection-example#step-4-establish-connection","content":" try: device = device_client.connect() print(f&quot;Successfully connected to device {device.connection_id()}&quot;) except Exception as e: print(f&quot;Failed to connect to device: {e}&quot;) return   What Happens:  The connect() method establishes a session with the deviceReturns a Device object that provides access to all device featuresThrows an exception if connection fails    ","version":"Next","tagName":"h3"},{"title":"Complete Example Code‚Äã","type":1,"pageTitle":"Connection Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/connection-example#complete-example-code","content":" Here's the full connection script:  import argparse import aria.sdk_gen2 as sdk_gen2 def device_connect(serial): # Set up the device client to initiate connection to the device device_client = sdk_gen2.DeviceClient() # Set up the device client config to specify the device to be connected to e.g. device serial number. # If nothing is specified, the first device in the list of connected devices will be connected to config = sdk_gen2.DeviceClientConfig() config.device_serial = serial device_client.set_client_config(config) # try to connect to the device try: device = device_client.connect() print(f&quot;Successfully connected to device {device.connection_id()}&quot;) except Exception: print(&quot;Failed to connect to device.&quot;) def parse_args() -&gt; argparse.Namespace: parser = argparse.ArgumentParser() parser.add_argument( &quot;--serial&quot;, dest=&quot;serial&quot;, type=str, default=&quot;&quot;, required=False, help=&quot;Serial number of the device which will be connected. (e.g. 1M0YDB5H7B0020)&quot;, ) return parser.parse_args() if __name__ == &quot;__main__&quot;: args = parse_args() device_connect(args.serial)     ","version":"Next","tagName":"h2"},{"title":"Usage Examples‚Äã","type":1,"pageTitle":"Connection Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/connection-example#usage-examples","content":" ","version":"Next","tagName":"h2"},{"title":"Connect to First Available Device‚Äã","type":1,"pageTitle":"Connection Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/connection-example#connect-to-first-available-device","content":" python ~/Downloads/projectaria_client_sdk_samples_gen2/device_connect.py   Output:  Successfully connected to device 1M0YDB5H7B0020     ","version":"Next","tagName":"h3"},{"title":"Connect by Serial Number‚Äã","type":1,"pageTitle":"Connection Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/connection-example#connect-by-serial-number","content":" python ~/Downloads/projectaria_client_sdk_samples_gen2/device_connect.py --serial 1M0YDB5H7B0020   When to Use:  Multiple devices are connectedYou need to ensure you're using a specific deviceScripts that require consistent device selection    ","version":"Next","tagName":"h3"},{"title":"Finding Device Information‚Äã","type":1,"pageTitle":"Connection Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/connection-example#finding-device-information","content":" ","version":"Next","tagName":"h2"},{"title":"Get Device Serial Number‚Äã","type":1,"pageTitle":"Connection Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/connection-example#get-device-serial-number","content":" Use the CLI to list all available devices:  aria-gen2 device list # Output example: # [AriaGen2Cli:App][INFO]: 1M0YDB5H7B0020     ","version":"Next","tagName":"h3"},{"title":"Troubleshooting‚Äã","type":1,"pageTitle":"Connection Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/connection-example#troubleshooting","content":" ","version":"Next","tagName":"h2"},{"title":"Connection Failed: Device Not Found‚Äã","type":1,"pageTitle":"Connection Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/connection-example#connection-failed-device-not-found","content":" Problem: Script cannot find or connect to the device.  Solutions:  Check device is connected: aria-gen2 device list Verify authentication: aria-gen2 auth check Run aria-doctor: aria-doctor Check USB connection: Try a different USB portUse a high-quality USB cableEnsure device is powered on    ","version":"Next","tagName":"h3"},{"title":"Connection Failed: Wrong Serial Number‚Äã","type":1,"pageTitle":"Connection Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/connection-example#connection-failed-wrong-serial-number","content":" Problem: Specified serial number doesn't match any device.  Solutions:  List available devices: aria-gen2 device list Use the correct serial number from the outputSerial numbers are case-sensitive    ","version":"Next","tagName":"h3"},{"title":"Connection Modes‚Äã","type":1,"pageTitle":"Connection Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/connection-example#connection-modes","content":" ","version":"Next","tagName":"h2"},{"title":"USB Connection (Default)‚Äã","type":1,"pageTitle":"Connection Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/connection-example#usb-connection-default","content":" How it works:  Device connected via USB cableMost reliable connection method  Usage:  # Connects via USB to first available device config = sdk_gen2.DeviceClientConfig()     ","version":"Next","tagName":"h3"},{"title":"Key Concepts‚Äã","type":1,"pageTitle":"Connection Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/connection-example#key-concepts","content":" ","version":"Next","tagName":"h2"},{"title":"DeviceClientConfig‚Äã","type":1,"pageTitle":"Connection Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/connection-example#deviceclientconfig","content":" The configuration object controls how the client connects to devices:  config = sdk_gen2.DeviceClientConfig() # Option 1: Connect to first available device (default) # Leave config empty # Option 2: Connect by serial number config.device_serial = &quot;1M0YDB5H7B0020&quot; # Apply configuration device_client.set_client_config(config)     ","version":"Next","tagName":"h3"},{"title":"Device Object‚Äã","type":1,"pageTitle":"Connection Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/connection-example#device-object","content":" Once connected, the Device object provides:  device = device_client.connect() # Device information device.connection_id() # Get connection identifier # Device operations (covered in other examples) device.start_recording() # Start recording device.start_streaming() # Start streaming device.render_tts(text) # Text-to-speech     ","version":"Next","tagName":"h3"},{"title":"Next Steps‚Äã","type":1,"pageTitle":"Connection Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/connection-example#next-steps","content":" Learn how to control recording with your connected deviceExplore real-time streamingSend text-to-speech commandsReview all Python SDK examples ","version":"Next","tagName":"h2"},{"title":"Python SDK Examples","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/python-interface","content":"","keywords":"","version":"Next"},{"title":"Overview‚Äã","type":1,"pageTitle":"Python SDK Examples","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/python-interface#overview","content":" The Python SDK enables you to:  Authenticate devices programmatically with your PCEstablish connections to devices via USB or wirelesslyControl recording - Start, stop, and download recordingsManage streaming - Stream data with custom callbacks for real-time processingSend commands - Control device features like text-to-speech    ","version":"Next","tagName":"h2"},{"title":"Getting Started‚Äã","type":1,"pageTitle":"Python SDK Examples","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/python-interface#getting-started","content":" ","version":"Next","tagName":"h2"},{"title":"Prerequisites‚Äã","type":1,"pageTitle":"Python SDK Examples","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/python-interface#prerequisites","content":" Before using the Python SDK, ensure you have:  Installed the ClientSDK (see Get Started guide)Activated your virtual environmentYour device connected and authenticated  ","version":"Next","tagName":"h3"},{"title":"Export Example Code‚Äã","type":1,"pageTitle":"Python SDK Examples","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/python-interface#export-example-code","content":" The SDK includes example scripts that demonstrate common use cases. Extract them using:  # Export example codes python -m aria.extract_sdk_samples --output ~/Downloads/   This creates a projectaria_client_sdk_samples_gen2 directory containing all example scripts:  ls ~/Downloads/projectaria_client_sdk_samples_gen2 device_auth.py # Authentication example device_connect.py # Connection example device_record.py # Recording example device_streaming.py # Streaming example device_tts.py # Text-to-speech example     ","version":"Next","tagName":"h3"},{"title":"Available Examples‚Äã","type":1,"pageTitle":"Python SDK Examples","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/python-interface#available-examples","content":" The following example pages provide detailed walkthroughs of each script:  ","version":"Next","tagName":"h2"},{"title":"1. Authentication Example‚Äã","type":1,"pageTitle":"Python SDK Examples","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/python-interface#1-authentication-example","content":" Learn how to programmatically authenticate your device with your PC. This is the first step before using any device features.  Topics covered:  Setting up DeviceClientInitiating authenticationHandling authentication confirmation    ","version":"Next","tagName":"h3"},{"title":"2. Connection Example‚Äã","type":1,"pageTitle":"Python SDK Examples","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/python-interface#2-connection-example","content":" Understand how to establish a connection to a specific device using serial numbers or IP addresses.  Topics covered:  Configuring device selectionEstablishing connectionsHandling connection errors    ","version":"Next","tagName":"h3"},{"title":"3. Recording Example‚Äã","type":1,"pageTitle":"Python SDK Examples","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/python-interface#3-recording-example","content":" Learn how to control recording operations, including starting, stopping, and downloading recordings.  Topics covered:  Setting up recording configurationsManaging recording sessionsDownloading recordings to your local machine    ","version":"Next","tagName":"h3"},{"title":"4. Streaming Example‚Äã","type":1,"pageTitle":"Python SDK Examples","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/python-interface#4-streaming-example","content":" Explore real-time data streaming with custom callback functions for processing sensor data.  Topics covered:  Configuring streaming profiles and interfacesImplementing data callbacks (image, audio, IMU, eye gaze, hand tracking, VIO)Recording streaming data to VRS files    ","version":"Next","tagName":"h3"},{"title":"5. Text-to-Speech Example‚Äã","type":1,"pageTitle":"Python SDK Examples","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/python-interface#5-text-to-speech-example","content":" Learn how to send text-to-speech commands to your device for audio feedback.  Topics covered:  Sending TTS commandsControlling device audio output    ","version":"Next","tagName":"h3"},{"title":"Common Python SDK Patterns‚Äã","type":1,"pageTitle":"Python SDK Examples","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/python-interface#common-python-sdk-patterns","content":" ","version":"Next","tagName":"h2"},{"title":"Basic Device Connection Pattern‚Äã","type":1,"pageTitle":"Python SDK Examples","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/python-interface#basic-device-connection-pattern","content":" Most scripts follow this basic pattern:  import aria.sdk_gen2 as sdk_gen2 # Create device client device_client = sdk_gen2.DeviceClient() # Configure client (optional - defaults to first available device) config = sdk_gen2.DeviceClientConfig() config.device_serial = &quot;1M0YDB5H7B0020&quot; # Optional: specify device device_client.set_client_config(config) # Connect to device device = device_client.connect() print(f&quot;Connected to device: {device.connection_id()}&quot;) # Use device for recording, streaming, etc.     ","version":"Next","tagName":"h3"},{"title":"Error Handling Pattern‚Äã","type":1,"pageTitle":"Python SDK Examples","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/python-interface#error-handling-pattern","content":" Always wrap device operations in try-except blocks:  try: device = device_client.connect() print(f&quot;Successfully connected to device {device.connection_id()}&quot;) except Exception as e: print(f&quot;Failed to connect: {e}&quot;) return     ","version":"Next","tagName":"h3"},{"title":"Configuration Pattern‚Äã","type":1,"pageTitle":"Python SDK Examples","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/python-interface#configuration-pattern","content":" Device operations (recording, streaming) typically require configuration:  # Recording configuration recording_config = sdk_gen2.RecordingConfig() recording_config.profile_name = &quot;profile8&quot; recording_config.recording_name = &quot;my_recording&quot; device.set_recording_config(recording_config) # Streaming configuration streaming_config = sdk_gen2.HttpStreamingConfig() streaming_config.profile_name = &quot;mp_streaming_demo&quot; device.set_streaming_config(streaming_config)     ","version":"Next","tagName":"h3"},{"title":"Next Steps‚Äã","type":1,"pageTitle":"Python SDK Examples","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/python-interface#next-steps","content":" Start with basics: Begin with the Authentication ExampleProgress through examples: Work through each example in orderExperiment: Modify the examples to fit your use caseBuild custom applications: Use the patterns to create your own scripts    ","version":"Next","tagName":"h2"},{"title":"Additional Resources‚Äã","type":1,"pageTitle":"Python SDK Examples","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/python-interface#additional-resources","content":" CLI Commands - Command-line interface referenceRecording Guide - Detailed recording documentationStreaming Guide - Detailed streaming documentationTroubleshooting - Common issues and solutions ","version":"Next","tagName":"h2"},{"title":"Recording Example","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/recording-example","content":"","keywords":"","version":"Next"},{"title":"Quick Start‚Äã","type":1,"pageTitle":"Recording Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/recording-example#quick-start","content":" Run the recording example script:  python ~/Downloads/projectaria_client_sdk_samples_gen2/device_record.py   The script will start a recording, wait for user input to stop, then download the recording.    ","version":"Next","tagName":"h2"},{"title":"What This Example Does‚Äã","type":1,"pageTitle":"Recording Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/recording-example#what-this-example-does","content":" The script performs the following operations:  Connects to the deviceConfigures recording settings (profile, name)Starts recording on the deviceWaits for user to press Enter to stopStops the recordingLists available recordingsDownloads the most recent recording to your local machine    ","version":"Next","tagName":"h2"},{"title":"Code Walkthrough‚Äã","type":1,"pageTitle":"Recording Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/recording-example#code-walkthrough","content":" ","version":"Next","tagName":"h2"},{"title":"Step 1: Import Required Modules‚Äã","type":1,"pageTitle":"Recording Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/recording-example#step-1-import-required-modules","content":" import argparse import time import aria.sdk_gen2 as sdk_gen2     ","version":"Next","tagName":"h3"},{"title":"Step 2: Parse Command-Line Arguments‚Äã","type":1,"pageTitle":"Recording Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/recording-example#step-2-parse-command-line-arguments","content":" def parse_args() -&gt; argparse.Namespace: parser = argparse.ArgumentParser() parser.add_argument( &quot;--output&quot;, dest=&quot;output_path&quot;, type=str, default=&quot;&quot;, required=False, help=&quot;Output directory to save the recording&quot;, ) parser.add_argument( &quot;--duration&quot;, dest=&quot;duration&quot;, type=int, default=10, required=False, help=&quot;Recording duration in seconds (default: 10)&quot;, ) return parser.parse_args()   Available Options:  --duration: Duration of the recording--output: Directory where the recording will be downloaded    ","version":"Next","tagName":"h3"},{"title":"Step 3: Connect to Device‚Äã","type":1,"pageTitle":"Recording Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/recording-example#step-3-connect-to-device","content":" device_client = sdk_gen2.DeviceClient() config = sdk_gen2.DeviceClientConfig() device_client.set_client_config(config) try: device = device_client.connect() print(f&quot;Successfully connected to device {device.connection_id()}&quot;) except Exception: print(&quot;Failed to connect to device and record&quot;) return   This establishes a connection to the first available device.    ","version":"Next","tagName":"h3"},{"title":"Step 4: Configure Recording‚Äã","type":1,"pageTitle":"Recording Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/recording-example#step-4-configure-recording","content":" recording_config = sdk_gen2.RecordingConfig() recording_config.recording_name = &quot;example_recording&quot; recording_config.profile_name = &quot;profile9&quot; device.set_recording_config(recording_config)   RecordingConfig Options:  profile_name: The sensor configuration profile (profile8, profile10)recording_name: A descriptive name for your recording  For profile details, see the Profiles Technical Specification.    ","version":"Next","tagName":"h3"},{"title":"Step 5: Start Recording‚Äã","type":1,"pageTitle":"Recording Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/recording-example#step-5-start-recording","content":" uuid = device.start_recording() print(f&quot;Start recording for {duration} seconds with uuid: {uuid}&quot;) print(&quot;Recording started. Press Enter to stop recording...&quot;) input()   What happens:  start_recording() begins capturing sensor data on the deviceThe recording continues until stop_recording() is calledData is stored on the device's internal storage    ","version":"Next","tagName":"h3"},{"title":"Step 6: Stop Recording‚Äã","type":1,"pageTitle":"Recording Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/recording-example#step-6-stop-recording","content":" print(&quot;Stopping recording...&quot;) device.stop_recording() print(&quot;Recording stopped&quot;)   The stop_recording() method stops data capture and saves the recording with a unique UUID.    ","version":"Next","tagName":"h3"},{"title":"Step 7: List Recordings‚Äã","type":1,"pageTitle":"Recording Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/recording-example#step-7-list-recordings","content":" device.list_recordings()     ","version":"Next","tagName":"h3"},{"title":"Step 8: Download Recording‚Äã","type":1,"pageTitle":"Recording Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/recording-example#step-8-download-recording","content":" device.download_recording(uuid=uuid, output_path=output_path)   Recording Information:  uuid: Unique identifier for the recording  Download Process:  download_recording(uuid, output_path) downloads the recording from device to PCDownloaded files are in VRS formatDownload time depends on recording size    ","version":"Next","tagName":"h3"},{"title":"Complete Example Code‚Äã","type":1,"pageTitle":"Recording Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/recording-example#complete-example-code","content":" Here's the full recording script:  import argparse import time import aria.sdk_gen2 as sdk_gen2 def device_record(duration, output_path): device_client = sdk_gen2.DeviceClient() config = sdk_gen2.DeviceClientConfig() device_client.set_client_config(config) # try to connect to the device try: device = device_client.connect() print(f&quot;Successfully connected to device {device.connection_id()}&quot;) # Set recording config with profile name print(&quot;Setup recording config&quot;) recording_config = sdk_gen2.RecordingConfig() recording_config.recording_name = &quot;example_recording&quot; recording_config.profile_name = &quot;profile9&quot; device.set_recording_config(recording_config) # Start and stop recording uuid = device.start_recording() print(f&quot;Start recording for {duration} seconds with uuid: {uuid}&quot;) time.sleep(duration) device.stop_recording() # list existing recordings on device print(&quot;List recordings&quot;) device.list_recordings() # download all recordings print(f&quot;Download recordings {uuid}&quot;) device.download_recording(uuid=uuid, output_path=output_path) except Exception: print(&quot;Failed to connect to device and record&quot;) return def parse_args() -&gt; argparse.Namespace: parser = argparse.ArgumentParser() parser.add_argument( &quot;--output&quot;, dest=&quot;output_path&quot;, type=str, default=&quot;&quot;, required=False, help=&quot;Output directory to save the recording&quot;, ) parser.add_argument( &quot;--duration&quot;, dest=&quot;duration&quot;, type=int, default=10, required=False, help=&quot;Recording duration in seconds (default: 10)&quot;, ) return parser.parse_args() if __name__ == &quot;__main__&quot;: args = parse_args() device_record(args.duration, args.output_path)     ","version":"Next","tagName":"h2"},{"title":"Recording Profiles‚Äã","type":1,"pageTitle":"Recording Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/recording-example#recording-profiles","content":" For complete profile specifications, see the Profiles Technical Specification.    ","version":"Next","tagName":"h2"},{"title":"Working with Downloaded Recordings‚Äã","type":1,"pageTitle":"Recording Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/recording-example#working-with-downloaded-recordings","content":" ","version":"Next","tagName":"h2"},{"title":"Visualize VRS Files‚Äã","type":1,"pageTitle":"Recording Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/recording-example#visualize-vrs-files","content":" After downloading, visualize recordings using the viewer:  aria_rerun_viewer --vrs ~/Documents/recordings/&lt;recording_name&gt;.vrs   For more on data processing, see the Project Aria Tools documentation.    ","version":"Next","tagName":"h3"},{"title":"Troubleshooting‚Äã","type":1,"pageTitle":"Recording Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/recording-example#troubleshooting","content":" ","version":"Next","tagName":"h2"},{"title":"Recording Won't Start‚Äã","type":1,"pageTitle":"Recording Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/recording-example#recording-wont-start","content":" Problem: start_recording() fails or hangs.  Solutions:  Check device connection: device = device_client.connect() Stop existing recording: device.stop_recording() time.sleep(2) device.start_recording()     ","version":"Next","tagName":"h3"},{"title":"Next Steps‚Äã","type":1,"pageTitle":"Recording Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/recording-example#next-steps","content":" Learn about real-time streaming for live data accessSend text-to-speech commands to the deviceReview all Python SDK examplesExplore Machine Perception Services for offline processing ","version":"Next","tagName":"h2"},{"title":"Streaming Example","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/streaming-example","content":"","keywords":"","version":"Next"},{"title":"Quick Start‚Äã","type":1,"pageTitle":"Streaming Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/streaming-example#quick-start","content":" Run the streaming example script:  python ~/Downloads/projectaria_client_sdk_samples_gen2/device_streaming.py   To save streaming data to a VRS file:  python ~/Downloads/projectaria_client_sdk_samples_gen2/device_streaming.py --record-to-vrs /path     ","version":"Next","tagName":"h2"},{"title":"What This Example Does‚Äã","type":1,"pageTitle":"Streaming Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/streaming-example#what-this-example-does","content":" The script performs the following operations:  Connects to the deviceConfigures streaming settings (profile, interface)Starts streaming on the deviceSets up a streaming receiver with custom callbacksProcesses real-time data from all sensors (cameras, IMU, audio, machine perception)Optionally records streaming data to a VRS file    ","version":"Next","tagName":"h2"},{"title":"Code Walkthrough‚Äã","type":1,"pageTitle":"Streaming Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/streaming-example#code-walkthrough","content":" ","version":"Next","tagName":"h2"},{"title":"Step 1: Import Required Modules‚Äã","type":1,"pageTitle":"Streaming Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/streaming-example#step-1-import-required-modules","content":" import argparse import signal import sys import aria.sdk_gen2 as sdk_gen2 import aria.stream_receiver as receiver from projectaria_tools.core.mps import EyeGaze, hand_tracking from projectaria_tools.core.sensor_data import ( AudioData, AudioDataRecord, FrontendOutput, ImageData, ImageDataRecord, MotionData, )   Key Modules:  aria.sdk_gen2: Main SDK for device controlaria.stream_receiver: Receives and processes streaming dataprojectaria_tools.core: Data structures for sensor data and machine perception    ","version":"Next","tagName":"h3"},{"title":"Step 2: Parse Command-Line Arguments‚Äã","type":1,"pageTitle":"Streaming Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/streaming-example#step-2-parse-command-line-arguments","content":" def parse_args() -&gt; argparse.Namespace: parser = argparse.ArgumentParser() parser.add_argument( &quot;--record-to-vrs&quot;, dest=&quot;record_to_vrs&quot;, type=str, default=&quot;&quot;, required=False, help=&quot;Output directory to save the received streaming into VRS&quot;, ) return parser.parse_args()   Available Options:  --record-to-vrs: Optional path to save streaming data as a VRS fileIf not specified, data is only processed in callbacks (not saved)  Important: Data drops can occur with poor streaming connections. The saved VRS file will reflect any data drops that occurred during streaming.    ","version":"Next","tagName":"h3"},{"title":"Step 3: Connect to Device and Start Streaming‚Äã","type":1,"pageTitle":"Streaming Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/streaming-example#step-3-connect-to-device-and-start-streaming","content":" # Create device client device_client = sdk_gen2.DeviceClient() # Establish connection to the device config = sdk_gen2.DeviceClientConfig() device_client.set_client_config(config) device = device_client.connect() # Set recording config with profile name streaming_config = sdk_gen2.HttpStreamingConfig() streaming_config.profile_name = &quot;profile9&quot; device.set_streaming_config(streaming_config) # Start and stop recording device.start_streaming()   Streaming Configuration:  profile_name: Use mp_streaming_demo for smooth visualization with all machine perception datastreaming_interface: Current support: USB_NCM: USB connection (default)    ","version":"Next","tagName":"h3"},{"title":"Step 4: Define Data Callbacks‚Äã","type":1,"pageTitle":"Streaming Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/streaming-example#step-4-define-data-callbacks","content":" Callbacks are functions that process data as it arrives from the device. Here's how to implement callbacks for each data type:  Image Callback‚Äã  Processes all camera streams: RGB (1x), SLAM (4x), and Eye Tracking (2x):  def image_callback(image_data: ImageData, image_record: ImageDataRecord): &quot;&quot;&quot;Called for each image frame from any camera.&quot;&quot;&quot; image_array = image_data.to_numpy_array() timestamp_ns = image_record.capture_timestamp_ns print(f&quot;Received image: shape={image_array.shape}, timestamp={timestamp_ns} ns&quot;) # Example: Process the image # - image_array is a numpy array you can process with OpenCV, PIL, etc. # - image_record contains metadata like timestamp, camera ID, exposure     Audio Callback‚Äã  Processes audio data from all 8 microphone channels:  def audio_callback(audio_data: AudioData, audio_record: AudioDataRecord, num_channels: int): &quot;&quot;&quot;Called for each audio data packet.&quot;&quot;&quot; num_samples = len(audio_data.data) num_timestamps = len(audio_record.capture_timestamps_ns) print(f&quot;Received audio: samples={num_samples}, timestamps={num_timestamps}, channels={num_channels}&quot;) # Example: Process audio # - audio_data.data contains the raw audio samples # - audio_record.capture_timestamps_ns contains per-sample timestamps     IMU Callback‚Äã  Processes high-frequency IMU data from both IMU sensors at 800Hz:  def imu_callback(imu_data: MotionData, sensor_label: str): &quot;&quot;&quot;Called for each IMU data sample from imu-left or imu-right.&quot;&quot;&quot; accel = imu_data.accel_msec2 # Acceleration in m/s¬≤ gyro = imu_data.gyro_radsec # Gyroscope in rad/s print(f&quot;Received {sensor_label}: accel={accel}, gyro={gyro}&quot;) # Example: Process IMU data # - Use for motion tracking.     Eye Gaze Callback‚Äã  Processes eye tracking data with gaze direction and depth:  def eyegaze_callback(eyegaze_data: EyeGaze): &quot;&quot;&quot;Called for each eye gaze estimate.&quot;&quot;&quot; timestamp_sec = eyegaze_data.tracking_timestamp.total_seconds() yaw_rad = eyegaze_data.yaw pitch_rad = eyegaze_data.pitch depth_m = eyegaze_data.depth print(f&quot;Eye Gaze: timestamp={timestamp_sec}s, yaw={yaw_rad:.3f}, pitch={pitch_rad:.3f}, depth={depth_m:.3f}m&quot;) # Example: Use eye gaze data # - Track where user is looking # - Estimate focus depth # - Build attention maps     Hand Tracking Callback‚Äã  Processes hand pose estimates for both left and right hands:  def handtracking_callback(handtracking_data: hand_tracking.HandTrackingResult): &quot;&quot;&quot;Called for each hand tracking estimate.&quot;&quot;&quot; timestamp_sec = handtracking_data.tracking_timestamp.total_seconds() print(f&quot;Hand Tracking: timestamp={timestamp_sec}s&quot;) # Process left hand if handtracking_data.left_hand is not None: left_hand = handtracking_data.left_hand print(f&quot; Left hand confidence: {left_hand.confidence:.3f}&quot;) print(f&quot; Left wrist position: {left_hand.get_wrist_position_device()}&quot;) print(f&quot; Left palm position: {left_hand.get_palm_position_device()}&quot;) if left_hand.wrist_and_palm_normal_device is not None: normals = left_hand.wrist_and_palm_normal_device print(f&quot; Left wrist normal: {normals.wrist_normal_device}&quot;) print(f&quot; Left palm normal: {normals.palm_normal_device}&quot;) else: print(&quot; Left hand: No data&quot;) # Process right hand (similar to left hand) if handtracking_data.right_hand is not None: right_hand = handtracking_data.right_hand print(f&quot; Right hand confidence: {right_hand.confidence:.3f}&quot;) print(f&quot; Right wrist position: {right_hand.get_wrist_position_device()}&quot;) print(f&quot; Right palm position: {right_hand.get_palm_position_device()}&quot;) if right_hand.wrist_and_palm_normal_device is not None: normals = right_hand.wrist_and_palm_normal_device print(f&quot; Right wrist normal: {normals.wrist_normal_device}&quot;) print(f&quot; Right palm normal: {normals.palm_normal_device}&quot;) else: print(&quot; Right hand: No data&quot;)     VIO Callback‚Äã  Processes Visual-Inertial Odometry data (6-DOF pose estimates):  def vio_callback(vio_data: FrontendOutput): &quot;&quot;&quot;Called for each VIO pose estimate.&quot;&quot;&quot; timestamp_ns = vio_data.capture_timestamp_ns rotation = vio_data.transform_odometry_bodyimu.rotation().log() translation = vio_data.transform_odometry_bodyimu.translation() print(f&quot;VIO: timestamp={timestamp_ns}ns, rotation={rotation}, translation={translation}&quot;) # Example: Use VIO data # - Track device position and orientation # - Build 3D maps # - Enable AR applications     ","version":"Next","tagName":"h3"},{"title":"Step 5: Set Up the Streaming Receiver‚Äã","type":1,"pageTitle":"Streaming Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/streaming-example#step-5-set-up-the-streaming-receiver","content":" The streaming receiver listens for data and dispatches it to your callbacks:  def setup_streaming_receiver(device, record_to_vrs): &quot;&quot;&quot;Configure and start the streaming receiver.&quot;&quot;&quot; # Configure server config = sdk_gen2.HttpServerConfig() config.address = &quot;0.0.0.0&quot; # Listen on all interfaces config.port = 6768 # Default streaming port # Create receiver stream_receiver = receiver.StreamReceiver() stream_receiver.set_server_config(config) # Optional: Record to VRS if record_to_vrs != &quot;&quot;: stream_receiver.record_to_vrs(record_to_vrs) # Register all callbacks stream_receiver.register_slam_callback(image_callback) stream_receiver.register_rgb_callback(image_callback) stream_receiver.register_audio_callback(audio_callback) stream_receiver.register_eye_gaze_callback(eyegaze_callback) stream_receiver.register_hand_pose_callback(handtracking_callback) stream_receiver.register_vio_callback(vio_callback) # Start receiving data stream_receiver.start_server() return stream_receiver   Important Configuration Notes:  Port 6768: Ensure this port is open and not blocked by firewallVPN: Disable VPN to allow streaming data to be received0.0.0.0: Listens on all network interfaces (required for device to connect)    ","version":"Next","tagName":"h3"},{"title":"Step 6: Main Function‚Äã","type":1,"pageTitle":"Streaming Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/streaming-example#step-6-main-function","content":" Tie everything together:  if __name__ == &quot;__main__&quot;: args = parse_args() # Connect and start streaming device = device_streaming() # Set up receiver with callbacks stream_receiver = setup_streaming_receiver(device, args.record_to_vrs) # Keep running until interrupted print(&quot;Streaming... Press Ctrl+C to stop&quot;) try: signal.pause() # Wait for interrupt signal except KeyboardInterrupt: print(&quot;\\nStopping streaming...&quot;) device.stop_streaming() print(&quot;Streaming stopped&quot;)     ","version":"Next","tagName":"h3"},{"title":"Complete Example Code‚Äã","type":1,"pageTitle":"Streaming Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/streaming-example#complete-example-code","content":" Here's the full streaming script structure:  import argparse import time import aria.sdk_gen2 as sdk_gen2 import aria.stream_receiver as receiver from projectaria_tools.core.mps import EyeGaze, hand_tracking, OpenLoopTrajectoryPose from projectaria_tools.core.sensor_data import ( AudioData, AudioDataRecord, FrontendOutput, ImageData, ImageDataRecord, MotionData, ) # Set up the device client to initiate connection to the device device_client = sdk_gen2.DeviceClient() def device_streaming(): # Set up the device client config to specify the device to be connected to e.g. device serial number. # If nothing is specified, the first device in the list of connected devices will be connected to config = sdk_gen2.DeviceClientConfig() device_client.set_client_config(config) device = device_client.connect() # Set recording config with profile name streaming_config = sdk_gen2.HttpStreamingConfig() streaming_config.profile_name = &quot;profile9&quot; device.set_streaming_config(streaming_config) # Start and stop recording device.start_streaming() return device def image_callback(image_data: ImageData, image_record: ImageDataRecord): print( f&quot;Received image data of size {image_data.to_numpy_array().shape} with timestamp {image_record.capture_timestamp_ns} ns&quot; ) def audio_callback( audio_data: AudioData, audio_record: AudioDataRecord, num_channels: int ): print( f&quot;Received audio data with {len(audio_data.data)} samples and {len(audio_record.capture_timestamps_ns)} timestamps and num channels {num_channels}&quot; ) def imu_callback(imu_data: MotionData, sensor_label: str): print( f&quot;Received {sensor_label} accel data {imu_data.accel_msec2} and gyro {imu_data.gyro_radsec}&quot; ) def eyegaze_callback(eyegaze_data: EyeGaze): print( f&quot;Received EyeGaze data at timestamp {eyegaze_data.tracking_timestamp.total_seconds()} sec &quot; f&quot;with yaw={eyegaze_data.yaw:.3f} rad, pitch={eyegaze_data.pitch:.3f} rad, &quot; f&quot;depth={eyegaze_data.depth:.3f} m&quot; ) def handtracking_callback(handtracking_data: hand_tracking.HandTrackingResult): print( f&quot;Received HandTracking data at timestamp {handtracking_data.tracking_timestamp.total_seconds()} sec&quot; ) # Check left hand data if handtracking_data.left_hand is not None: left_hand = handtracking_data.left_hand print(f&quot; Left hand confidence: {left_hand.confidence:.3f}&quot;) print(f&quot; Left wrist position: {left_hand.get_wrist_position_device()}&quot;) print(f&quot; Left palm position: {left_hand.get_palm_position_device()}&quot;) if left_hand.wrist_and_palm_normal_device is not None: normals = left_hand.wrist_and_palm_normal_device print(f&quot; Left wrist normal: {normals.wrist_normal_device}&quot;) print(f&quot; Left palm normal: {normals.palm_normal_device}&quot;) else: print(&quot; Left hand: No data&quot;) # Check right hand data if handtracking_data.right_hand is not None: right_hand = handtracking_data.right_hand print(f&quot; Right hand confidence: {right_hand.confidence:.3f}&quot;) print(f&quot; Right wrist position: {right_hand.get_wrist_position_device()}&quot;) print(f&quot; Right palm position: {right_hand.get_palm_position_device()}&quot;) if right_hand.wrist_and_palm_normal_device is not None: normals = right_hand.wrist_and_palm_normal_device print(f&quot; Right wrist normal: {normals.wrist_normal_device}&quot;) print(f&quot; Right palm normal: {normals.palm_normal_device}&quot;) else: print(&quot; Right hand: No data&quot;) def vio_callback(vio_data: FrontendOutput): print( f&quot;Received VIO data at timestamp {vio_data.capture_timestamp_ns} with transform_odometry_bodyimu: {vio_data.transform_odometry_bodyimu.rotation().log()} and {vio_data.transform_odometry_bodyimu.translation()} ns&quot; ) def calib_callback(calib_json_str: str): print(f&quot;Received calibration: {calib_json_str}&quot;) def setup_streaming_receiver(device, record_to_vrs): # setup the server to receive streaming data from the device # IP address : 0.0.0.0 means that the server is listening on all available interfaces # Port : 6768 is the port number that the server is listening on config = sdk_gen2.HttpServerConfig() config.address = &quot;0.0.0.0&quot; config.port = 6768 # setup the receiver stream_receiver = receiver.StreamReceiver() stream_receiver.set_server_config(config) if record_to_vrs != &quot;&quot;: stream_receiver.record_to_vrs(record_to_vrs) # register callbacks for each type of data stream_receiver.register_slam_callback(image_callback) stream_receiver.register_rgb_callback(image_callback) stream_receiver.register_audio_callback(audio_callback) stream_receiver.register_eye_gaze_callback(eyegaze_callback) stream_receiver.register_hand_pose_callback(handtracking_callback) stream_receiver.register_vio_callback(vio_callback) # start the server stream_receiver.start_server() time.sleep(10) # stop streaming and terminate the server device.stop_streaming() time.sleep(2) def parse_args() -&gt; argparse.Namespace: parser = argparse.ArgumentParser() parser.add_argument( &quot;--record-to-vrs&quot;, dest=&quot;record_to_vrs&quot;, type=str, default=&quot;&quot;, required=False, help=&quot;Output directory to save the received streaming into VRS&quot;, ) return parser.parse_args() if __name__ == &quot;__main__&quot;: args = parse_args() # setup device to start streaming device = device_streaming() # setup streaming receiver to receive streaming data with callbacks setup_streaming_receiver(device, args.record_to_vrs)     ","version":"Next","tagName":"h2"},{"title":"Usage Examples‚Äã","type":1,"pageTitle":"Streaming Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/streaming-example#usage-examples","content":" ","version":"Next","tagName":"h2"},{"title":"Basic Streaming with Console Output‚Äã","type":1,"pageTitle":"Streaming Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/streaming-example#basic-streaming-with-console-output","content":" python ~/Downloads/projectaria_client_sdk_samples_gen2/device_streaming.py   What happens:  Connects to device and starts streamingPrints real-time data from all sensors to consoleData is NOT saved    ","version":"Next","tagName":"h3"},{"title":"Streaming and Recording to VRS‚Äã","type":1,"pageTitle":"Streaming Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/streaming-example#streaming-and-recording-to-vrs","content":" python ~/Downloads/projectaria_client_sdk_samples_gen2/device_streaming.py \\ --record-to-vrs ~/Downloads/streaming_capture.vrs   What happens:  Streams data with real-time callbacksSimultaneously records all data to VRS fileVRS file can be played back later with aria_rerun_viewer    ","version":"Next","tagName":"h3"},{"title":"Troubleshooting‚Äã","type":1,"pageTitle":"Streaming Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/streaming-example#troubleshooting","content":" ","version":"Next","tagName":"h2"},{"title":"No Data in Callbacks‚Äã","type":1,"pageTitle":"Streaming Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/streaming-example#no-data-in-callbacks","content":" Problem: Streaming starts but callbacks are never called.  Solutions:  Check port 6768 is open: # On Linux/macOS sudo lsof -i :6768 Disable VPN: VPNs block streaming dataDisconnect from VPN/Lighthouse Check firewall: Ensure firewall allows port 6768Add exception for Python script Verify server address: Must be 0.0.0.0 to listen on all interfaces Run aria_doctor: aria_doctor     ","version":"Next","tagName":"h3"},{"title":"High Data Drop Rate‚Äã","type":1,"pageTitle":"Streaming Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/streaming-example#high-data-drop-rate","content":" Problem: VRS file shows many dropped frames.  Solutions:  Close other applications consuming bandwidthCheck system resources (CPU, memory)Reduce callback processing timeIncrease message queue size    ","version":"Next","tagName":"h3"},{"title":"Next Steps‚Äã","type":1,"pageTitle":"Streaming Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/streaming-example#next-steps","content":" Learn about text-to-speech commandsReview all Python SDK examplesExplore the Streaming Guide for CLI streamingCheck Troubleshooting for common issues ","version":"Next","tagName":"h2"},{"title":"Text-to-Speech Example","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/text-to-speech-example","content":"","keywords":"","version":"Next"},{"title":"Quick Start‚Äã","type":1,"pageTitle":"Text-to-Speech Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/text-to-speech-example#quick-start","content":" Run the text-to-speech example script:  python ~/Downloads/projectaria_client_sdk_samples_gen2/device_tts.py --text &quot;Hello from Aria&quot;   The device will speak the text through its built-in speakers.    ","version":"Next","tagName":"h2"},{"title":"What This Example Does‚Äã","type":1,"pageTitle":"Text-to-Speech Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/text-to-speech-example#what-this-example-does","content":" The script performs the following operations:  Connects to the deviceSends a text-to-speech command with your specified textThe device converts the text to speech and plays it through the speakersReturns once the command is sent (device plays audio asynchronously)    ","version":"Next","tagName":"h2"},{"title":"Code Walkthrough‚Äã","type":1,"pageTitle":"Text-to-Speech Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/text-to-speech-example#code-walkthrough","content":" ","version":"Next","tagName":"h2"},{"title":"Step 1: Import Required Modules‚Äã","type":1,"pageTitle":"Text-to-Speech Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/text-to-speech-example#step-1-import-required-modules","content":" import argparse import aria.sdk_gen2 as sdk_gen2     ","version":"Next","tagName":"h3"},{"title":"Step 2: Parse Command-Line Arguments‚Äã","type":1,"pageTitle":"Text-to-Speech Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/text-to-speech-example#step-2-parse-command-line-arguments","content":" def parse_args() -&gt; argparse.Namespace: parser = argparse.ArgumentParser() parser.add_argument( &quot;--text&quot;, dest=&quot;text&quot;, type=str, default=&quot;&quot;, required=True, help=&quot;TTS text to rendered by the device.&quot;, ) return parser.parse_args()   Required Argument:  --text: The text string you want the device to speak    ","version":"Next","tagName":"h3"},{"title":"Step 3: Connect to Device‚Äã","type":1,"pageTitle":"Text-to-Speech Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/text-to-speech-example#step-3-connect-to-device","content":" device_client = sdk_gen2.DeviceClient() config = sdk_gen2.DeviceClientConfig() device_client.set_client_config(config) try: device = device_client.connect() print(f&quot;Connected to device {device.connection_id()}&quot;) except Exception as e: print(f&quot;Failed to connect: {e}&quot;) return   This establishes a connection to the first available device.    ","version":"Next","tagName":"h3"},{"title":"Step 4: Send Text-to-Speech Command‚Äã","type":1,"pageTitle":"Text-to-Speech Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/text-to-speech-example#step-4-send-text-to-speech-command","content":" text = args.text print(f&quot;Sending text-to-speech: '{text}'&quot;) device.render_tts(text) print(&quot;Text-to-speech command sent successfully&quot;)   How it works:  The render_tts(text) method sends the text to the deviceThe device processes the text and generates speechAudio is played through the device's built-in speakersThe function returns immediately (audio plays asynchronously)    ","version":"Next","tagName":"h3"},{"title":"Complete Example Code‚Äã","type":1,"pageTitle":"Text-to-Speech Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/text-to-speech-example#complete-example-code","content":" Here's the full text-to-speech script:  import argparse import aria.sdk_gen2 as sdk_gen2 def parse_args() -&gt; argparse.Namespace: parser = argparse.ArgumentParser() parser.add_argument( &quot;--text&quot;, dest=&quot;text&quot;, type=str, default=&quot;&quot;, required=True, help=&quot;TTS text to rendered by the device.&quot;, ) return parser.parse_args() if __name__ == &quot;__main__&quot;: args = parse_args() # Set up the device client to initiate connection to the device device_client = sdk_gen2.DeviceClient() # Set up the device client config to specify the device to be connected to e.g. device serial number. # If nothing is specified, the first device in the list of connected devices will be connected to config = sdk_gen2.DeviceClientConfig() device_client.set_client_config(config) device = device_client.connect() print(f&quot;Connected to device: {device.connection_id()}&quot;) print(f&quot;Rendering TTS: {args.text}&quot;) device.render_tts(text=args.text)     ","version":"Next","tagName":"h2"},{"title":"Usage Examples‚Äã","type":1,"pageTitle":"Text-to-Speech Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/text-to-speech-example#usage-examples","content":" ","version":"Next","tagName":"h2"},{"title":"Simple Message‚Äã","type":1,"pageTitle":"Text-to-Speech Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/text-to-speech-example#simple-message","content":" python ~/Downloads/projectaria_client_sdk_samples_gen2/device_tts.py --text &quot;Recording started&quot;     ","version":"Next","tagName":"h3"},{"title":"Multi-Word Message‚Äã","type":1,"pageTitle":"Text-to-Speech Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/text-to-speech-example#multi-word-message","content":" python ~/Downloads/projectaria_client_sdk_samples_gen2/device_tts.py --text &quot;Please look at the target&quot;     ","version":"Next","tagName":"h3"},{"title":"Instructions‚Äã","type":1,"pageTitle":"Text-to-Speech Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/text-to-speech-example#instructions","content":" python ~/Downloads/projectaria_client_sdk_samples_gen2/device_tts.py --text &quot;Turn left at the next intersection&quot;     ","version":"Next","tagName":"h3"},{"title":"Numbers and Punctuation‚Äã","type":1,"pageTitle":"Text-to-Speech Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/text-to-speech-example#numbers-and-punctuation","content":" python ~/Downloads/projectaria_client_sdk_samples_gen2/device_tts.py --text &quot;You have 3 notifications. Check your device.&quot;   The TTS engine handles numbers and punctuation naturally.    ","version":"Next","tagName":"h3"},{"title":"Troubleshooting‚Äã","type":1,"pageTitle":"Text-to-Speech Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/text-to-speech-example#troubleshooting","content":" ","version":"Next","tagName":"h2"},{"title":"TTS Command Sent But No Audio‚Äã","type":1,"pageTitle":"Text-to-Speech Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/text-to-speech-example#tts-command-sent-but-no-audio","content":" Problem: The render_tts() command succeeds but device doesn't speak.  Solutions:  Check device volume: Ensure device audio is not mutedCheck volume level in Companion App Check device status: Verify device is powered on and functioningCheck battery level (low battery may affect audio) Verify connection: device = device_client.connect() print(f&quot;Connected: {device.connection_id()}&quot;) Test with simple message: device.render_tts(&quot;Test&quot;)     ","version":"Next","tagName":"h3"},{"title":"Connection Lost During TTS‚Äã","type":1,"pageTitle":"Text-to-Speech Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/text-to-speech-example#connection-lost-during-tts","content":" Problem: Device disconnects when sending TTS.  Solutions:  Verify USB connection is stableCheck authentication: aria_gen2 auth check Reconnect if needed: try: device.render_tts(text) except Exception as e: print(f&quot;Connection lost: {e}&quot;) device = device_client.connect() # Reconnect device.render_tts(text) # Retry     ","version":"Next","tagName":"h3"},{"title":"Next Steps‚Äã","type":1,"pageTitle":"Text-to-Speech Example","url":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/text-to-speech-example#next-steps","content":" Review all Python SDK examplesLearn about recording controlExplore streaming with callbacksCheck troubleshooting for common issues ","version":"Next","tagName":"h2"},{"title":"Recording Control","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/ark/client-sdk/recording","content":"","keywords":"","version":"Next"},{"title":"Available Recording Profiles‚Äã","type":1,"pageTitle":"Recording Control","url":"/projectaria_tools/gen2/ark/client-sdk/recording#available-recording-profiles","content":" Before starting a recording, you need to select a recording profile that defines the sensor configuration, data rates, resolutions, and encoding formats. Project Aria Gen2 offers several pre-defined profiles:  profile8: General purpose recording profile - Recommended for most use cases with balanced sensor configurationprofile9: General purpose streaming profile - Optimized for real-time streaming with lower bandwidthprofile10: Recording profile with RGB 3MP 30Hz - Higher quality RGB capture for detailed visual datamp_streaming_demo: Streaming profile for visualization - Optimized for real-time streaming visualization  You can also create custom recording profiles to define your own sensor configurations. For detailed specifications of each profile including sensor rates, resolutions, and encoding formats, visit the Profiles Technical Specification page.  ","version":"Next","tagName":"h2"},{"title":"Recording Commands‚Äã","type":1,"pageTitle":"Recording Control","url":"/projectaria_tools/gen2/ark/client-sdk/recording#recording-commands","content":" ","version":"Next","tagName":"h2"},{"title":"Start Recording‚Äã","type":1,"pageTitle":"Recording Control","url":"/projectaria_tools/gen2/ark/client-sdk/recording#start-recording","content":" Start a new recording on your Aria Gen2 device with a specified profile and optional recording name.  aria_gen2 recording start --profile &lt;profile_name&gt; --recording-name &lt;name&gt;   Parameters:  --profile (required): The recording profile to use (e.g., profile8, profile9, profile10)--recording-name (optional): A custom name for the recording (default: timestamp-based name)  Example:  # Start recording with profile8 and a custom name aria_gen2 recording start --profile profile8 --recording-name my_experiment_01 # Start recording with profile10 for high-quality RGB capture aria_gen2 recording start --profile profile10 --recording-name high_res_capture   tip Choose the profile based on your research needs. Use profile8 for general-purpose recording with all sensors enabled, or profile10 if you need higher resolution RGB data at 30Hz.    ","version":"Next","tagName":"h3"},{"title":"Stop Recording‚Äã","type":1,"pageTitle":"Recording Control","url":"/projectaria_tools/gen2/ark/client-sdk/recording#stop-recording","content":" Stop the currently active recording on the device.  aria_gen2 recording stop   info The recording will be automatically saved on the device with a unique UUID that you can use to reference it later.    ","version":"Next","tagName":"h3"},{"title":"List Recordings‚Äã","type":1,"pageTitle":"Recording Control","url":"/projectaria_tools/gen2/ark/client-sdk/recording#list-recordings","content":" List all recordings stored on the device. This command displays the UUID, name, timestamp, and size of each recording.  aria_gen2 recording list   Example:  # List all recordings on the device aria_gen2 recording list # Expected output [AriaGen2Cli:App][INFO]: Setting up connection to device ... [AriaGen2Cli:App][INFO]: Connected to device 1M0YCB5G9W002M [AriaGen2Cli:App][INFO]: Listing recordings... [RecordingClientWrapper][INFO]: Start time Recording uuid Size (bytes) Type [RecordingClientWrapper][INFO]: 2025-10-12 16:51:16 PDT 4b5d47d4-f1e4-46d6-b852-670adae9d848 11724800 prototype   tip Use this command to find the UUID of the recording you want to download or delete.    ","version":"Next","tagName":"h3"},{"title":"Get Recording Information‚Äã","type":1,"pageTitle":"Recording Control","url":"/projectaria_tools/gen2/ark/client-sdk/recording#get-recording-information","content":" Get detailed information about a specific recording using its UUID.  aria_gen2 recording info -u &lt;uuid&gt;   Parameters:  -u or --uuid (required): The UUID of the recording  Example:  # Get information about a specific recording aria_gen2 recording info -u 12345678-1234-5678-1234-567812345678     ","version":"Next","tagName":"h3"},{"title":"Download Recording‚Äã","type":1,"pageTitle":"Recording Control","url":"/projectaria_tools/gen2/ark/client-sdk/recording#download-recording","content":" Download a specific recording from the device to your local machine. The recording is downloaded in VRS (Virtual Reality Stream) format.  aria_gen2 recording download -u &lt;uuid&gt; -o &lt;output_directory&gt;   Parameters:  -u or --uuid (required): The UUID of the recording to download-o or --output (optional): The output directory where the recording will be saved  Example:  # Download a recording to the Downloads folder aria_gen2 recording download -u 12345678-1234-5678-1234-567812345678 -o ~/Downloads/ # The file will be saved as ~/Downloads/&lt;recording_name&gt;.vrs   info Downloaded recordings are in VRS format. You can visualize them using the aria_rerun_viewer tool (see Visualizing Recordings below).    ","version":"Next","tagName":"h3"},{"title":"Download All Recordings‚Äã","type":1,"pageTitle":"Recording Control","url":"/projectaria_tools/gen2/ark/client-sdk/recording#download-all-recordings","content":" Download all recordings from the device to your local machine. Each recording will be named with its UUID.  aria_gen2 recording download-all -o &lt;output_directory&gt;   Parameters:  -o or --output (optional): The output directory where recordings will be saved  Example:  # Download all recordings to the Downloads folder aria_gen2 recording download-all -o ~/Downloads/ # Files will be saved as ~/Downloads/&lt;uuid&gt;.vrs   warning This command will download all recordings on the device, which may take considerable time depending on the number and size of recordings.    ","version":"Next","tagName":"h3"},{"title":"Delete Recording‚Äã","type":1,"pageTitle":"Recording Control","url":"/projectaria_tools/gen2/ark/client-sdk/recording#delete-recording","content":" Delete a specific recording from the device to free up storage space.  aria_gen2 recording delete -u &lt;uuid&gt;   Parameters:  -u or --uuid (required): The UUID of the recording to delete  Example:  # Delete a specific recording aria_gen2 recording delete -u 12345678-1234-5678-1234-567812345678   danger This action is irreversible. Make sure you have downloaded the recording before deleting it if you need to keep the data.    ","version":"Next","tagName":"h3"},{"title":"Delete All Recordings‚Äã","type":1,"pageTitle":"Recording Control","url":"/projectaria_tools/gen2/ark/client-sdk/recording#delete-all-recordings","content":" Delete all recordings from the device to free up storage space.  aria_gen2 recording delete-all   Example:  # Delete all recordings from the device aria_gen2 recording delete-all   danger This action is irreversible and will delete ALL recordings on the device. Make sure you have downloaded all necessary recordings before using this command.    ","version":"Next","tagName":"h3"},{"title":"Complete Workflow Example‚Äã","type":1,"pageTitle":"Recording Control","url":"/projectaria_tools/gen2/ark/client-sdk/recording#complete-workflow-example","content":" Here's a typical workflow for recording, downloading, and visualizing data:  # 1. Start a recording with profile8 aria_gen2 recording start --profile profile8 --recording-name my_research_session # 2. Perform your research activity... # (Let the recording run while you collect data) # 3. Stop the recording aria_gen2 recording stop # 4. List recordings to find the UUID aria_gen2 recording list # 5. Download the recording (using UUID from step 4) aria_gen2 recording download -u &lt;uuid&gt; -o ~/Downloads/ # 6. Visualize the recording aria_rerun_viewer --vrs ~/Downloads/my_research_session.vrs # 7. (Optional) Delete the recording from the device to free up space aria_gen2 recording delete -u &lt;uuid&gt;     ","version":"Next","tagName":"h2"},{"title":"Visualizing Recordings‚Äã","type":1,"pageTitle":"Recording Control","url":"/projectaria_tools/gen2/ark/client-sdk/recording#visualizing-recordings","content":" After downloading a recording, you can visualize it using the viewer.  aria_rerun_viewer --vrs ~/Downloads/&lt;recording_name&gt;.vrs   For more details of this VRS visualization tool, visit Project Aria Tools viewer  The viewer provides an interactive visualization of all sensor streams including cameras, IMU data, eye tracking, and more.  ","version":"Next","tagName":"h2"},{"title":"Example Visualization‚Äã","type":1,"pageTitle":"Recording Control","url":"/projectaria_tools/gen2/ark/client-sdk/recording#example-visualization","content":"      ","version":"Next","tagName":"h3"},{"title":"Advanced features‚Äã","type":1,"pageTitle":"Recording Control","url":"/projectaria_tools/gen2/ark/client-sdk/recording#advanced-features","content":" ","version":"Next","tagName":"h2"},{"title":"Custom Recording Profiles‚Äã","type":1,"pageTitle":"Recording Control","url":"/projectaria_tools/gen2/ark/client-sdk/recording#custom-recording-profiles","content":" In addition to the pre-defined profiles, you can create and use custom recording profiles to define your own sensor configurations, data rates, tailored to your specific research needs.  Using a Custom Profile:  aria_gen2 recording start --json-profile &lt;path_to_custom_profile.json&gt; --recording-name &lt;name&gt;     ","version":"Next","tagName":"h3"},{"title":"Best Practices‚Äã","type":1,"pageTitle":"Recording Control","url":"/projectaria_tools/gen2/ark/client-sdk/recording#best-practices","content":" Choose the right profile: Select the profile that matches your research needs. See the Profiles Technical Specification for detailed comparisonsName your recordings: Use descriptive recording names to easily identify recordings laterMonitor storage: Regularly download and delete recordings to avoid running out of storage space on the deviceVerify recordings: Use aria_gen2 recording list to verify recordings were saved correctly before deleting themDownload before deleting: Always download recordings before deleting them from the device to prevent data loss    ","version":"Next","tagName":"h2"},{"title":"Next Steps‚Äã","type":1,"pageTitle":"Recording Control","url":"/projectaria_tools/gen2/ark/client-sdk/recording#next-steps","content":" Learn about Streaming to capture real-time data from your deviceExplore the Python SDK for programmatic controlReview Recording Examples for advanced use casesCheck the CLI Technical Specs for all available commands and options ","version":"Next","tagName":"h2"},{"title":"Introduction to ClientSDK","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/ark/client-sdk/start","content":"","keywords":"","version":"Next"},{"title":"Key Features‚Äã","type":1,"pageTitle":"Introduction to ClientSDK","url":"/projectaria_tools/gen2/ark/client-sdk/start#key-features","content":" Device Management: Core classes and utilities for connecting to, authenticating, and controlling Aria Gen2 devicesStreaming Support: Components for starting and managing data streams, including on-device machine perception data (VIO, hand pose, and eye gaze) and other sensor dataRecording Support: Components for starting and managing recording on-deviceSecurity: Built-in support for streaming certificates and security optionsPython &amp; CLI Tools: Includes Python SDK and command-line utilities for scripting and automationExample Code: Ready-to-use examples to help developers get started quickly  ","version":"Next","tagName":"h2"},{"title":"Prerequisites‚Äã","type":1,"pageTitle":"Introduction to ClientSDK","url":"/projectaria_tools/gen2/ark/client-sdk/start#prerequisites","content":" Before getting started, ensure you have:  Hardware: Aria Gen2 deviceMobile App: Aria Companion App (installed and paired with your device)Connection: USB cable for device-to-PC connection  ","version":"Next","tagName":"h2"},{"title":"Supported Platforms‚Äã","type":1,"pageTitle":"Introduction to ClientSDK","url":"/projectaria_tools/gen2/ark/client-sdk/start#supported-platforms","content":" OS / Platform\tOS / Distro Details\tAria Gen2 Support (projectaria-tools ‚â•2.0)\tSupported Python VersionsLinux (x64)\tFedora 40/41; Ubuntu 22.04 LTS (jammy) / 24.04 LTS ( Noble Numbat)\t‚úÖ Supported\t3.10 ‚Äì 3.12 macOS (Apple Silicon / ARM64)\tmacOS 14+ (Sonoma or newer) on M1/M2/M3\tüöß Planned\t3.10 ‚Äì 3.12 macOS (Intel)\tmacOS 13+ (Ventura or newer)\tüöß Planned\t3.10 ‚Äì 3.12 Windows (x64)\tMSVC 2019/2022\tüöß Planned\t3.10 ‚Äì 3.12  Ubuntu 22.04 Known Issue If you encounter a GLIBCXX_3.4.31 not found error when running CLI commands on Ubuntu 22.04, you'll need to update your C++ standard library. See the GLIBCXX troubleshooting guide for the solution.    ","version":"Next","tagName":"h2"},{"title":"Installation‚Äã","type":1,"pageTitle":"Introduction to ClientSDK","url":"/projectaria_tools/gen2/ark/client-sdk/start#installation","content":" ","version":"Next","tagName":"h2"},{"title":"Step 1: Install Python‚Äã","type":1,"pageTitle":"Introduction to ClientSDK","url":"/projectaria_tools/gen2/ark/client-sdk/start#step-1-install-python","content":" To use ClientSDK, you'll need Python 3.10 - 3.12.  Python 3 download pageTo check what version of Python 3 you have, use python3 --version  ","version":"Next","tagName":"h3"},{"title":"Step 2: Create a Virtual Environment‚Äã","type":1,"pageTitle":"Introduction to ClientSDK","url":"/projectaria_tools/gen2/ark/client-sdk/start#step-2-create-a-virtual-environment","content":" Linux &amp; macOS rm -rf $HOME/projectaria_gen2_python_env python3 -m venv $HOME/projectaria_gen2_python_env source $HOME/projectaria_gen2_python_env/bin/activate   ","version":"Next","tagName":"h3"},{"title":"Step 3: Install ClientSDK Python Package‚Äã","type":1,"pageTitle":"Introduction to ClientSDK","url":"/projectaria_tools/gen2/ark/client-sdk/start#step-3-install-clientsdk-python-package","content":" pip install projectaria-client-sdk==2.0.0   ","version":"Next","tagName":"h3"},{"title":"Device Authentication‚Äã","type":1,"pageTitle":"Introduction to ClientSDK","url":"/projectaria_tools/gen2/ark/client-sdk/start#device-authentication","content":" You need to authenticate your device with your PC to allow secure connection via USB. Before proceeding, ensure you have paired the device with your mobile Companion App.  ","version":"Next","tagName":"h2"},{"title":"Step 1: Verify Device Setup‚Äã","type":1,"pageTitle":"Introduction to ClientSDK","url":"/projectaria_tools/gen2/ark/client-sdk/start#step-1-verify-device-setup","content":" After setting up the device with the Companion App, connect your device via USB and run the following command to ensure the device can be reached:  aria_doctor   info This step opens the necessary ports on your PC for device discovery and configures your system to allow unblocked internet connection.  ","version":"Next","tagName":"h3"},{"title":"Step 2: Check Device Connection‚Äã","type":1,"pageTitle":"Introduction to ClientSDK","url":"/projectaria_tools/gen2/ark/client-sdk/start#step-2-check-device-connection","content":" Verify the device connection to the PC via USB network interface:  # Connect device to the computer and check if the device appears aria_gen2 device list # Expected output [AriaGen2Cli:App][INFO]: 1M0YDB5H7B0020   warning If you cannot see your device listed in the terminal, please visit the troubleshooting page for help.  ","version":"Next","tagName":"h3"},{"title":"Step 3: Authenticate Device‚Äã","type":1,"pageTitle":"Introduction to ClientSDK","url":"/projectaria_tools/gen2/ark/client-sdk/start#step-3-authenticate-device","content":" Authenticate your device to this PC to start using ClientSDK:  # Authenticate device to connect with the PC aria_gen2 auth pair # Expected output [AriaGen2SDK:DeviceClient][INFO]: Client hash is: f30a36a9f3842bd3fc16c75b525.... please check against the companion app pairing request [AriaGen2Cli:App][INFO]: Waiting for authentication approval on your mobile Companion App... [AriaGen2Cli:App][INFO]: Successfully authenticated the device   You will need to approve the authentication from your mobile Companion App to allow the connection, as shown here:     warning If you cannot see the authentication request in your Companion App, please visit the troubleshooting page for help.  ","version":"Next","tagName":"h3"},{"title":"Step 4: Verify Authentication‚Äã","type":1,"pageTitle":"Introduction to ClientSDK","url":"/projectaria_tools/gen2/ark/client-sdk/start#step-4-verify-authentication","content":" Once authenticated, you can check your authentication status using the command below:  # Check authentication status aria_gen2 auth check # Expected output [AriaGen2Cli:App][INFO]: Setting up connection to device ... [AriaGen2Cli:App][INFO]: Connected to device 1M0YDB5H7B0020 [AriaGen2Cli:App][INFO]: Device 1M0YDB5H7B0020 is successfully authenticated.   tip You only need to authenticate once per device and PC combination.    ","version":"Next","tagName":"h3"},{"title":"Troubleshooting Connection and Authentication‚Äã","type":1,"pageTitle":"Introduction to ClientSDK","url":"/projectaria_tools/gen2/ark/client-sdk/start#troubleshooting-connection-and-authentication","content":" If you're experiencing issues connecting to your device or authenticating, try the following solutions:  ","version":"Next","tagName":"h2"},{"title":"Device Not Detected‚Äã","type":1,"pageTitle":"Introduction to ClientSDK","url":"/projectaria_tools/gen2/ark/client-sdk/start#device-not-detected","content":" If your device is not showing up when running aria_gen2 device list, follow these steps:  1. Enable Ports Using aria_doctor‚Äã  Run aria_doctor to configure your system and enable the necessary ports:  aria_doctor   This command will:  Open required ports on your PC for device discoveryConfigure your system to allow proper USB Ethernet connectionSet up internet access when connected via USB  info If you cannot connect to the internet when connected to the device via USB, running aria_doctor will typically resolve this issue.  2. Check USB Network in Companion App‚Äã  Ensure USB network is enabled in the Companion App:  Open the Aria Companion App on your mobile deviceNavigate to your paired device settingsVerify that USB Network is toggled ONIf it's off, enable it and reconnect the USB cable  tip The USB Network toggle must be enabled for the device to communicate with your PC over USB.  3. Update USB Ethernet Security Settings‚Äã  When streaming via USB, the USB connection appears as an Ethernet configuration. You may need to adjust the security settings for this connection:  Linux:  Check network connections with nmcli device statusLook for a new USB or Ethernet deviceEnsure NetworkManager is managing the connectionIn Security tab, disable 802-1x Security for the USB Network  4. Disable VPN‚Äã  Your local PC may be connected to a VPN that blocks the connection. VPNs can interfere with the USB network interface used by the device.  Steps to resolve:  Disconnect from any active VPN connectionsRun aria_gen2 device list again to check if the device is detectedIf streaming, restart the streaming viewer after disabling VPN  # After disabling VPN, verify device connection aria_gen2 device list   warning Some corporate VPNs may automatically reconnect. You may need to temporarily disable auto-connect features while working with the device.    ","version":"Next","tagName":"h3"},{"title":"Authentication Issues‚Äã","type":1,"pageTitle":"Introduction to ClientSDK","url":"/projectaria_tools/gen2/ark/client-sdk/start#authentication-issues","content":" If authentication is failing or you cannot see the authentication request in the Companion App:  1. Verify Device Connection‚Äã  Ensure the device is properly connected and detected:  # Check if device is listed aria_gen2 device list # If not listed, run aria_doctor aria_doctor   2. Check Companion App Connection‚Äã  Ensure the Companion App is running on your mobile deviceVerify the device is paired with the Companion AppCheck that your mobile device has an active internet connectionMake sure the Companion App is running in the foreground  3. Retry Authentication‚Äã  # Stop any existing authentication attempts # Press Ctrl+C if a command is hanging # Retry authentication aria_gen2 auth pair   4. Reset Connection‚Äã  If authentication continues to fail, try resetting the connection:  # Disconnect and reconnect the USB cable # Wait a few seconds after reconnecting # Run aria_doctor again aria_doctor # Check device connection aria_gen2 device list # Retry authentication aria_gen2 auth pair     ","version":"Next","tagName":"h3"},{"title":"Additional Troubleshooting Steps‚Äã","type":1,"pageTitle":"Introduction to ClientSDK","url":"/projectaria_tools/gen2/ark/client-sdk/start#additional-troubleshooting-steps","content":" If you're still experiencing issues:  Try a Different USB Port: Use a different USB port on your computer, preferably USB 3.0+Try a Different USB Cable: Use a high-quality USB cable that supports data transferRestart the Device: Power off and restart your Aria Gen2 deviceRestart Your Computer: Sometimes a system restart can resolve network configuration issuesCheck Device Battery: Ensure your device has sufficient battery chargeUpdate ClientSDK: Ensure you have the latest version of the ClientSDK installed  # Update ClientSDK to the latest version pip install --upgrade projectaria-client-sdk   For more detailed troubleshooting, visit the troubleshooting page.    ","version":"Next","tagName":"h3"},{"title":"Available CLI Controls‚Äã","type":1,"pageTitle":"Introduction to ClientSDK","url":"/projectaria_tools/gen2/ark/client-sdk/start#available-cli-controls","content":" The ClientSDK provides a powerful CLI to control your Aria Gen2 device. You can use the packaged CLI to:  Recording: Control the device to start, stop, list, and download recordings in VRS formatStreaming: Control the device to start and stop streamingDevice: General device controls including obtaining device information, connecting to Wi-Fi, sending text-to-speech, and more  For detailed instructions and command reference, refer to the Technical Specs page.  ","version":"Next","tagName":"h2"},{"title":"Next Steps‚Äã","type":1,"pageTitle":"Introduction to ClientSDK","url":"/projectaria_tools/gen2/ark/client-sdk/start#next-steps","content":" Now that you have the ClientSDK installed and your device authenticated, you can:  Explore the Recording guide to learn how to capture dataCheck out the Streaming documentation to stream data in real-timeVisit the Technical Specs for comprehensive CLI command referenceReview the Python SDK API Reference for detailed Python SDK API documentation ","version":"Next","tagName":"h2"},{"title":"Control Recording","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/ark/companion-app/recording","content":"","keywords":"","version":"Next"},{"title":"Start a recording‚Äã","type":1,"pageTitle":"Control Recording","url":"/projectaria_tools/gen2/ark/companion-app/recording#start-a-recording","content":" Navigate to the dashboard tab in the bottom left of your app (home icon).Select Custom recording. The recording setup screen will appear. From here, you can name your recording, add notes, and select a recording profile.    Tap recording profile to select a profile. If you have a default profile set, it will auto-populate. Otherwise, you can select a profile from the list.    (Optional) when selecting a profile, you can make it the default profile for future recordings.Tap Begin new recording and you will see your glasses LED light up once recording has started.    Once you've finished recording, you can end the recording by tapping the Complete recording button in the app or by pressing the action button on your glasses. If you don't want to save your recording to the glasses, tap Discard recording.  ","version":"Next","tagName":"h2"},{"title":"Sensor integrity‚Äã","type":1,"pageTitle":"Control Recording","url":"/projectaria_tools/gen2/ark/companion-app/recording#sensor-integrity","content":" You can view the health of the sensors you are recording with by tapping the Data Integrity and Status button during a recording.    ","version":"Next","tagName":"h3"},{"title":"Consecutive recordings‚Äã","type":1,"pageTitle":"Control Recording","url":"/projectaria_tools/gen2/ark/companion-app/recording#consecutive-recordings","content":" Note that if you run multiple consecutive recordings, the app will automatically name them with a suffix of the number of recordings you've made. The number will automatically reset if you update the name field on a new recording.  ","version":"Next","tagName":"h3"},{"title":"Manage recordings‚Äã","type":1,"pageTitle":"Control Recording","url":"/projectaria_tools/gen2/ark/companion-app/recording#manage-recordings","content":" Tap the Recordings tab in the bottom of your app (second from the left) to view recordings stored on your glasses.    ","version":"Next","tagName":"h2"},{"title":"View recording details‚Äã","type":1,"pageTitle":"Control Recording","url":"/projectaria_tools/gen2/ark/companion-app/recording#view-recording-details","content":" Tap a recording to view its details. From here, you can view the recording's name, date, duration, thumbnails, and other useful metadata.    ","version":"Next","tagName":"h3"},{"title":"Rename a recording‚Äã","type":1,"pageTitle":"Control Recording","url":"/projectaria_tools/gen2/ark/companion-app/recording#rename-a-recording","content":" Tap the Edit button to rename a recording. You can also add or edit notes on this screen. Tap Save to save your changes.    ","version":"Next","tagName":"h3"},{"title":"Delete a recording‚Äã","type":1,"pageTitle":"Control Recording","url":"/projectaria_tools/gen2/ark/companion-app/recording#delete-a-recording","content":" Tap the Delete button to permanently delete a recording from your glasses.  ","version":"Next","tagName":"h3"},{"title":"Next Steps‚Äã","type":1,"pageTitle":"Control Recording","url":"/projectaria_tools/gen2/ark/companion-app/recording#next-steps","content":" Now that you can control recording on your glasses, learn how to set up record and stream over the command line, or download recordings to your computer using the ClientSDK. ","version":"Next","tagName":"h2"},{"title":"Get Started","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/ark/companion-app/start","content":"","keywords":"","version":"Next"},{"title":"Companion App key features‚Äã","type":1,"pageTitle":"Get Started","url":"/projectaria_tools/gen2/ark/companion-app/start#companion-app-key-features","content":" Fully wireless operationCheck glasses status (temperature, GPS, battery, etc.)Handle and select between multiple paired Aria Gen 2 glassesUpdate glasses to the latest firmwareSelect and start recording profiles directly from the appData quality signals during recording and streamingEye Gaze CalibrationManage security certificates for SDK/CLI access  ","version":"Next","tagName":"h2"},{"title":"Get the app‚Äã","type":1,"pageTitle":"Get Started","url":"/projectaria_tools/gen2/ark/companion-app/start#get-the-app","content":" The current version of the Companion App is v238. Go to ARK Software Release Notes to learn more about the latest features.  iOSAndroid iOS‚Äã Requirements‚Äã If you're using iOS, your mobile device will need: OS version must be iOS 14 or above(Optional) TrueDepth camera (iPhone X or later) needed for eye-tracking calibration Download and Install‚Äã The Aria Mobile Companion app is available on iOS as a beta app through TestFlight. On your phone, scan this QR code: to open the Aria Testflight invitation link and download the app. Updating the app‚Äã Open the TestFlight appNext to the Aria app, select Update  If you encounter problems ordering your device through the Companion App, please contact AriaOps@meta.com.  ","version":"Next","tagName":"h2"},{"title":"Log in‚Äã","type":1,"pageTitle":"Get Started","url":"/projectaria_tools/gen2/ark/companion-app/start#log-in","content":" Open the Aria app and log in with your provided account. Tap Log in with Meta, then Continue with email. Review the Project Aria Research Community Guidelines and Health &amp; Safety information.    Set up glasses  Follow the instructions in the app to pair your Aria Gen 2 glasses. You'll need to set up Wi-Fi in order to check for any software updates and complete the glasses setup.    Need help? Email AriaOps@meta.com.  ","version":"Next","tagName":"h2"},{"title":"Updating glasses‚Äã","type":1,"pageTitle":"Get Started","url":"/projectaria_tools/gen2/ark/companion-app/start#updating-glasses","content":" ","version":"Next","tagName":"h2"},{"title":"Automatic updates‚Äã","type":1,"pageTitle":"Get Started","url":"/projectaria_tools/gen2/ark/companion-app/start#automatic-updates","content":" OS updates occur automatically when glasses are plugged in and connected to Wi-Fi.    ","version":"Next","tagName":"h3"},{"title":"Manual updates‚Äã","type":1,"pageTitle":"Get Started","url":"/projectaria_tools/gen2/ark/companion-app/start#manual-updates","content":" From the dashboard tab (home icon), tap your glasses name at the top of the brown card.Scroll to Glasses OS.    Select Check for Updates.If an update is available, your glasses will reboot after updating.  ","version":"Next","tagName":"h3"},{"title":"App Dashboard‚Äã","type":1,"pageTitle":"Get Started","url":"/projectaria_tools/gen2/ark/companion-app/start#app-dashboard","content":" Once your glasses are set up, you'll be taken to the dashboard. The dashboard is the main tab of the companion app and shows the status of your glasses, including battery level, Wi-Fi, and audio pairing state.    ","version":"Next","tagName":"h2"},{"title":"Next steps‚Äã","type":1,"pageTitle":"Get Started","url":"/projectaria_tools/gen2/ark/companion-app/start#next-steps","content":" Make your first recordingSet up the ClientSDK ","version":"Next","tagName":"h2"},{"title":"Accessories and Fitment","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/ark/device/accessories_fitment","content":"","keywords":"","version":"Next"},{"title":"Aria Welcome Kit Contents‚Äã","type":1,"pageTitle":"Accessories and Fitment","url":"/projectaria_tools/gen2/ark/device/accessories_fitment#aria-welcome-kit-contents","content":" Case:Safely stores your Aria glasses and accessories for transport. Microfiber Lens Cloth:Use this cloth to gently clean and dry your Aria glasses. (Note: The glasses are not waterproof.) Lens cleaning wipes are also suitable. Quick Start Guide:Provides essential information about the device and Companion App. Full details are also available on our wiki. Accessories Guide:Explains all included accessories and their usage. Regulatory Insert:Contains compliance and safety information. Circle and Wedge Fitment Accessories:These help improve comfort and prevent slippage. For application instructions, refer to the Accessories Guide.    For more details, please consult the included guides or contact our Support team.  ","version":"Next","tagName":"h2"},{"title":"Fitment‚Äã","type":1,"pageTitle":"Accessories and Fitment","url":"/projectaria_tools/gen2/ark/device/accessories_fitment#fitment","content":" Aria Gen 2 boasts superior wearability, characterized by enhanced comfort and fit, while accommodating a wider range of face morphologies. To ensure you have an optimal physical and functional fit, we‚Äôve introduced eight size variations of the device‚Äîaccounting for a number of human factors including head breadth and nose bridge variation.     To determine your Aria size, please follow the instructions and fill your measurements in the form below:  ","version":"Next","tagName":"h2"},{"title":"Aria Gen 2 Self-Sizing Form‚Äã","type":1,"pageTitle":"Accessories and Fitment","url":"/projectaria_tools/gen2/ark/device/accessories_fitment#aria-gen-2-self-sizing-form","content":"","version":"Next","tagName":"h3"},{"title":"Streaming Control","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/ark/client-sdk/streaming","content":"","keywords":"","version":"Next"},{"title":"Overview‚Äã","type":1,"pageTitle":"Streaming Control","url":"/projectaria_tools/gen2/ark/client-sdk/streaming#overview","content":" Streaming enables you to access real-time data from your device:  Real-time Data Access: Receive sensor data as it's capturedMachine Perception: Access on-device VIO (Visual Inertial Odometry), eye gaze, and hand pose dataLive Visualization: View streaming data in real-time using the streaming viewerCustom Processing: Build applications that process streaming data    ","version":"Next","tagName":"h2"},{"title":"Choose Your Streaming Method‚Äã","type":1,"pageTitle":"Streaming Control","url":"/projectaria_tools/gen2/ark/client-sdk/streaming#choose-your-streaming-method","content":" Select the streaming method that best fits your use case:  Method\tBest For\tPros\tConsUSB Streaming\tExtended sessions, highest quality\tMost reliable, no thermal issues\tRequires cable connection  Getting Started New to streaming? Start with USB Streaming - it's the simplest and most reliable method.    ","version":"Next","tagName":"h2"},{"title":"USB Streaming‚Äã","type":1,"pageTitle":"Streaming Control","url":"/projectaria_tools/gen2/ark/client-sdk/streaming#usb-streaming","content":" Stream data over USB connection for maximum bandwidth and reliability. This is the recommended method for getting started and for extended streaming sessions.  ","version":"Next","tagName":"h2"},{"title":"When to Use USB Streaming‚Äã","type":1,"pageTitle":"Streaming Control","url":"/projectaria_tools/gen2/ark/client-sdk/streaming#when-to-use-usb-streaming","content":" Extended sessions: No thermal concerns - stream as long as you needHighest quality: Maximum bandwidth and lowest latencySimple setup: Just connect and stream, no network configurationDevelopment: Best for testing and development work  ","version":"Next","tagName":"h3"},{"title":"Quick Start‚Äã","type":1,"pageTitle":"Streaming Control","url":"/projectaria_tools/gen2/ark/client-sdk/streaming#quick-start","content":" # 1. Connect device via USB and verify connection aria_gen2 device list # 2. Start streaming aria_gen2 streaming start # 3. In another terminal, visualize the stream aria_streaming_viewer --real-time --interpolate # 4. When done, stop streaming aria_gen2 streaming stop   ","version":"Next","tagName":"h3"},{"title":"USB Streaming Details‚Äã","type":1,"pageTitle":"Streaming Control","url":"/projectaria_tools/gen2/ark/client-sdk/streaming#usb-streaming-details","content":" Requirements:  Device connected via USBDevice authenticated (see Get Started guide)High-quality USB 3.0+ cable recommended  Command:  aria_gen2 streaming start   Default Profile:The mp_streaming_demo profile is used by default, which provides smooth visualization with VIO, eye gaze, and hand pose data.  ","version":"Next","tagName":"h3"},{"title":"USB Troubleshooting‚Äã","type":1,"pageTitle":"Streaming Control","url":"/projectaria_tools/gen2/ark/client-sdk/streaming#usb-troubleshooting","content":" Device Not Found:  # Check device connectivity aria_gen2 device list # If not found, ensure device is authenticated aria_gen2 auth check   Common Issues:  USB cable: Try a different high-quality USB 3.0+ cableUSB port: Try a different USB port on your computerAuthentication: Run aria_gen2 auth pair if device is not authenticatedRecording active: Stop any active recording with aria_gen2 recording stop    ","version":"Next","tagName":"h3"},{"title":"Streaming Commands Reference‚Äã","type":1,"pageTitle":"Streaming Control","url":"/projectaria_tools/gen2/ark/client-sdk/streaming#streaming-commands-reference","content":" ","version":"Next","tagName":"h2"},{"title":"Start Streaming‚Äã","type":1,"pageTitle":"Streaming Control","url":"/projectaria_tools/gen2/ark/client-sdk/streaming#start-streaming","content":" aria_gen2 streaming start [OPTIONS]   Options:  Option\tValues\tDefault\tDescription--profile\tProfile name\tmp_streaming_demo\tStreaming profile  Examples:  # USB streaming (default) aria_gen2 streaming start   ","version":"Next","tagName":"h3"},{"title":"Stop Streaming‚Äã","type":1,"pageTitle":"Streaming Control","url":"/projectaria_tools/gen2/ark/client-sdk/streaming#stop-streaming","content":" Connect device via USB, then stop streaming:  aria_gen2 streaming stop   ","version":"Next","tagName":"h3"},{"title":"Visualize Streaming Data‚Äã","type":1,"pageTitle":"Streaming Control","url":"/projectaria_tools/gen2/ark/client-sdk/streaming#visualize-streaming-data","content":" Open the streaming viewer in a separate terminal while streaming is active:  aria_streaming_viewer --real-time --interpolate   The viewer shows:  Live camera feeds (RGB, SLAM, eye tracking)Machine perception data (VIO trajectory, eye gaze, hand pose)Sensor readings (IMU, magnetometer, etc.)Performance metrics (frame rates, latency)  Example Visualization‚Äã       ","version":"Next","tagName":"h3"},{"title":"Streaming vs Recording‚Äã","type":1,"pageTitle":"Streaming Control","url":"/projectaria_tools/gen2/ark/client-sdk/streaming#streaming-vs-recording","content":" Choose the right tool for your needs:  Feature\tStreaming\tRecordingReal-time Access\tYes\tNo Data Saved on Device\tNo\tYes Machine Perception\tVIO, Eye Gaze, Hand Pose\tVIO, Eye Gaze, Hand Pose Connection Required\tDuring capture\tNo Use Case\tLive monitoring, real-time processing\tData collection for offline analysis Best For\tDevelopment, live demos, debugging\tResearch data collection, post-processing  tip Use streaming when you need to see data in real-time or process it liveUse recording when you need to save data for later analysis or offline processing    ","version":"Next","tagName":"h2"},{"title":"Best Practices‚Äã","type":1,"pageTitle":"Streaming Control","url":"/projectaria_tools/gen2/ark/client-sdk/streaming#best-practices","content":" ","version":"Next","tagName":"h2"},{"title":"Choosing the Right Method‚Äã","type":1,"pageTitle":"Streaming Control","url":"/projectaria_tools/gen2/ark/client-sdk/streaming#choosing-the-right-method","content":" Development &amp; Testing: Use USB streaming for reliability and easeDemos &amp; Monitoring: Use USB streaming with long cables for mobility during short sessionsField Work: Use hotspot streaming when no network is availableLong Sessions: Always use USB streaming (no thermal concerns)  ","version":"Next","tagName":"h3"},{"title":"General Tips‚Äã","type":1,"pageTitle":"Streaming Control","url":"/projectaria_tools/gen2/ark/client-sdk/streaming#general-tips","content":" Start Simple: Begin with USB streaming to verify everything worksTest First: Run a short streaming session before important capturesMonitor Temperature: Watch for device heating during wireless streamingKeep Device Charged: Wireless streaming consumes more batteryClose Applications: Free up system resources for better performanceUse Quality Cables: USB 3.0+ cables for best USB streaming performance  ","version":"Next","tagName":"h3"},{"title":"Thermal Management‚Äã","type":1,"pageTitle":"Streaming Control","url":"/projectaria_tools/gen2/ark/client-sdk/streaming#thermal-management","content":" If device gets hot: Stop streaming and let it cool before continuingExtended sessions: Always prefer USB streaming    ","version":"Next","tagName":"h3"},{"title":"Next Steps‚Äã","type":1,"pageTitle":"Streaming Control","url":"/projectaria_tools/gen2/ark/client-sdk/streaming#next-steps","content":" Learn about Recording to save data for offline analysisCheck out Streaming Examples to build custom streaming applicationsVisit the CLI Technical Specs for complete command reference ","version":"Next","tagName":"h2"},{"title":"Project Aria Machine Perception Services","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/ark/mps","content":"","keywords":"","version":"Next"},{"title":"Current MPS offerings‚Äã","type":1,"pageTitle":"Project Aria Machine Perception Services","url":"/projectaria_tools/gen2/ark/mps#current-mps-offerings","content":" The following MPS can be requested, as long as the data has been recorded with a compatible Recording Profile. Go to the Recording Profiles for information about each profile.  MPS offerings are grouped into SLAM and Hand Tracking services.  ","version":"Next","tagName":"h2"},{"title":"SLAM services‚Äã","type":1,"pageTitle":"Project Aria Machine Perception Services","url":"/projectaria_tools/gen2/ark/mps#slam-services","content":" To get these outputs the recording profile must have CV cameras + IMU enabled.  ","version":"Next","tagName":"h2"},{"title":"6DoF trajectory‚Äã","type":1,"pageTitle":"Project Aria Machine Perception Services","url":"/projectaria_tools/gen2/ark/mps#6dof-trajectory","content":" MPS provides two types of high frequency (1kHz) trajectories:  Open loop trajectory - local odometry estimation from visual-inertial odometry (VIO)Closed loop trajectory - created via batch optimization, using multi-sensors' input (SLAM, IMU, barometer, Wi-Fi and GPS), fully optimized and provides poses in a consistent frame of reference.  ","version":"Next","tagName":"h3"},{"title":"Semi-dense point cloud‚Äã","type":1,"pageTitle":"Project Aria Machine Perception Services","url":"/projectaria_tools/gen2/ark/mps#semi-dense-point-cloud","content":" Semi-dense point cloud data supports researchers who need static scene 3D reconstructions, reliable 2D images tracks or a representative visualization of the environment.  ","version":"Next","tagName":"h3"},{"title":"Online sensor calibration‚Äã","type":1,"pageTitle":"Project Aria Machine Perception Services","url":"/projectaria_tools/gen2/ark/mps#online-sensor-calibration","content":" The time-varying intrinsic and extrinsic calibrations of cameras and IMUs are estimated at the frequency of the SLAM (mono scene) cameras by our multi-sensor state estimation pipeline.  ","version":"Next","tagName":"h3"},{"title":"Hand Tracking services‚Äã","type":1,"pageTitle":"Project Aria Machine Perception Services","url":"/projectaria_tools/gen2/ark/mps#hand-tracking-services","content":" Coming Soon.  Cloud Hand Tracking service is currently available for Aria gen1 devices only. For Aria Gen2, please refer to On device Hand Tracking.  ","version":"Next","tagName":"h2"},{"title":"About MPS Data Loader APIs‚Äã","type":1,"pageTitle":"Project Aria Machine Perception Services","url":"/projectaria_tools/gen2/ark/mps#about-mps-data-loader-apis","content":" Please refer to our MPS data loader APIs in python, to load and visualize the MPS outputs into your application.  ","version":"Next","tagName":"h2"},{"title":"Questions & Feedback‚Äã","type":1,"pageTitle":"Project Aria Machine Perception Services","url":"/projectaria_tools/gen2/ark/mps#questions--feedback","content":" If you have feedback you'd like to provide, be it overall trends and experiences or where we can improve, we'd love to hear from you. Go to our Support page for different ways to get in touch. ","version":"Next","tagName":"h2"},{"title":"MPS Data Lifecycle","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/ark/mps/mps_data_lifecycle","content":"","keywords":"","version":"Next"},{"title":"Overview‚Äã","type":1,"pageTitle":"MPS Data Lifecycle","url":"/projectaria_tools/gen2/ark/mps/mps_data_lifecycle#overview","content":" Researchers can upload data collected by Project Aria glasses to Meta for cloud-based Machine Perception Services (MPS) processing.  This page provides information about how all Aria sequences submitted to Meta for MPS are processed, handled and stored.  ","version":"Next","tagName":"h2"},{"title":"How sequences are processed‚Äã","type":1,"pageTitle":"MPS Data Lifecycle","url":"/projectaria_tools/gen2/ark/mps/mps_data_lifecycle#how-sequences-are-processed","content":" Raw Aria sequences (VRS files) are uploaded to secure cloud storage via MPS CLI.The data is only uploaded to Meta servers to serve MPS requests and is deleted after 30 daysThe MPS output is saved in the cloud User account that requested the MPS gets the token necessary to access MPS outputsThis derived data is persisted in the cloud Raw data is deleted from the cloud Meta‚Äôs data management processes mandate that this raw data cannot be stored for more than 30 days  Figure 1: MPS Processing Lifecycle  ","version":"Next","tagName":"h2"},{"title":"Data storage and use‚Äã","type":1,"pageTitle":"MPS Data Lifecycle","url":"/projectaria_tools/gen2/ark/mps/mps_data_lifecycle#data-storage-and-use","content":" Partner data is only used to serve MPS requests. Partner data is not¬†available to Meta researchers or Meta‚Äôs affiliates.Raw partner data (VRS files) is stored for no more than 30 days.The whole process is automated and only engineers in the core MPS team can access the pipeline under explicit permission from the customer.All MPS output (trajectories, semi-dense point clouds etc.)¬†continue to be stored in secure cloud storage, so that users can re-download the data at any time. MPS output is not¬†available to Meta researchers or Meta‚Äôs affiliates. Only the user account that requested the MPS output gets the token necessary to download the derived data The MPS pipeline generates¬†statistics about how the algorithms are performing¬†as well as the console logs from processing. These aggregated statistics are used by the Project Aria MPS team to help improve our offerings. ","version":"Next","tagName":"h2"},{"title":"On-device Machine Perception","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/ark/on_device_mp","content":"","keywords":"","version":"Next"},{"title":"Visual Inertial Odometry (VIO)‚Äã","type":1,"pageTitle":"On-device Machine Perception","url":"/projectaria_tools/gen2/ark/on_device_mp#visual-inertial-odometry-vio","content":" One of the key features of Aria Gen 2 is its ability to track the glasses in six degrees of freedom (6DOF) within a spatial frame of reference using Visual Inertial Odometry (VIO), by fusing the sensor data from four CV cameras and two IMUs. This allows for seamless navigation and mapping of the environment, opening up new possibilities for research in contextual AI and robotics. The VIO output is generated at 10Hz with the following output:  3-DOF position3-DOF linear velocity3-DOF orientation in quaternion form3-DOF angular velocityEstimated direction of gravity for the odometry frame  Additionally, Aria Gen2 also produces high-frequency VIO output (the fields of the output are the same regular VIO) at IMU rate (800Hz), by performing IMU pre-integration on top of the regular 10Hz VIO output. The high-frequency VIO output can be useful for applications where low-latency VIO poses are needed.  ","version":"Next","tagName":"h2"},{"title":"Eye Tracking‚Äã","type":1,"pageTitle":"On-device Machine Perception","url":"/projectaria_tools/gen2/ark/on_device_mp#eye-tracking","content":" Aria Gen 2 also boasts an advanced camera-based eye tracking system that tracks the wearer‚Äôs gaze. The advanced gaze signal enables a deeper understanding of the wearer‚Äôs visual attention and intentions, unlocking new possibilities for human-computer interaction. This system generates the following eye tracking outputs for each eye, up to 90Hz:  The origin and direction of the individual gaze rayThe 3-DOF position of the entrance pupilThe diameter of the pupilWhether the eye is blinking  Additionally, the system also produces the following signals for the combined gaze estimated from both eyes, including:  The original and direction of the combined gaze rayVergence depth of the combined gazeDistance between the left/right eye pupils, a.k.a, IPD (Inter Pupillary Distance)  ","version":"Next","tagName":"h2"},{"title":"Hand Tracking‚Äã","type":1,"pageTitle":"On-device Machine Perception","url":"/projectaria_tools/gen2/ark/on_device_mp#hand-tracking","content":" Aria Gen 2 also features a hand detection and tracking solution that tracks the wearer‚Äôs hand in 3D space. This produces articulated hand-joint poses in the device frame of reference, facilitating accurate hand annotations for datasets and enabling applications such as dexterous robot hand manipulation that require high precision. The hand tracking pipeline generates the following outputs at 30Hz for each hand (left and right):  3-DOF position of the wrist3-DOF rotation of the wrist3-DOF positions of the 21 finger joint landmarks ","version":"Next","tagName":"h2"},{"title":"Aria Gen 2 Glasses Manual","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/ark/device/manual","content":"","keywords":"","version":"Next"},{"title":"Sensors‚Äã","type":1,"pageTitle":"Aria Gen 2 Glasses Manual","url":"/projectaria_tools/gen2/ark/device/manual#sensors","content":" Aria Gen2 introduces a state-of-the-art sensor suite that expands the range of research applications:     CV Cameras: Four computer vision (CV) cameras provide a wide field of view and high-dynamic-range (HDR) imaging, enabling robust scene understanding and perception even in challenging lighting conditions.RGB Camera: RGB point-of-view (POV) camera for high-resolution egocentric vision and ML applicationsEye Tracking system: Enable precise gaze estimation and user intent modeling.Spatial Microphones: Capture high-fidelity audio for spatial awareness and voice interaction.Contact Microphone (Nosepad): Isolates the wearer‚Äôs voice from bystanders, improving voice command reliability in noisy environments.IMU, Barometer, Magnetometer, GNSS: Support accurate localization, motion tracking, and environmental sensing.Ambient Light Sensor (ALS) with UV Channel: Distinguishes between indoor and outdoor lighting, opening new avenues for context-aware research.PPG Sensor (Nosepad): Measures heart rate, supporting physiological and affective computing studies.Multi-device time alignment using SubGHz radio based custom protocols allow for multiple Aria Gen 2 device to align their timebases to 10‚Äôs of microseconds accuracy.  These new sensors, especially the ALS with UV channel, contact-mics and the physiological sensors in the nosepad, unlock research opportunities in context-aware computing, health monitoring, and robust multimodal perception.  For details, see the Hardware Specifications.    ","version":"Next","tagName":"h2"},{"title":"Hardwares‚Äã","type":1,"pageTitle":"Aria Gen 2 Glasses Manual","url":"/projectaria_tools/gen2/ark/device/manual#hardwares","content":" Aria Gen 2 glasses are equipped with a suite of hardware controls, visual indicators, and audio feedback to help you operate and understand the device‚Äôs status.       ","version":"Next","tagName":"h2"},{"title":"Controls and Hardware Features‚Äã","type":1,"pageTitle":"Aria Gen 2 Glasses Manual","url":"/projectaria_tools/gen2/ark/device/manual#controls-and-hardware-features","content":" The glasses include several built-in controls:  Privacy Switch: Controls whether the device can record data. When set to the rear and the red underneath is visible, recording is disabled and any ongoing recordings are deleted.Action Button: Starts and ends recording sessions. This button can be customized to perform other actions (e.g., start a livestream).Power Button: Turns the device on and off.USB Port: Uses a standard USB-C cable.Volume Control: Adjusts the speaker volume.    ","version":"Next","tagName":"h3"},{"title":"Visual and Audio Indicators‚Äã","type":1,"pageTitle":"Aria Gen 2 Glasses Manual","url":"/projectaria_tools/gen2/ark/device/manual#visual-and-audio-indicators","content":" Aria Gen 2 uses LEDs and speakers to communicate device status to both the wearer and bystanders.  LED Indicators‚Äã  There are three main LEDs:  Inner LED: Visible only to the wearer, primarily communicates device state and events.Bystander LED: Visible to bystanders, indicates when the device is recording or streaming data from sensors.Power LED: Located above the power button on the inner right arm, indicates battery and charging status (not visible while wearing).  Audio Feedback‚Äã  Earcons: Non-verbal sounds (e.g., camera shutter, error beeps) indicate device states.TTS (Text-to-Speech): Spoken messages (e.g., ‚ÄúGlasses shutting down‚Äù, ‚ÄúBattery ten percent: charge soon‚Äù) provide status updates.    ","version":"Next","tagName":"h3"},{"title":"Device States and Indicators‚Äã","type":1,"pageTitle":"Aria Gen 2 Glasses Manual","url":"/projectaria_tools/gen2/ark/device/manual#device-states-and-indicators","content":" The following table summarizes how Aria Gen 2 communicates its various states:  Inward LED\tPower LED\tOutward LED\tDevice Status ExplanationOff\tOff\tOff\tDevice is off or in standby Off\tSolid blue\tOff\tBattery charging or fully charged (system running) Off\tSolid orange\tOff\tFastboot mode Off\tBlinking red\tOff\tLow battery if powering on or charging Off\tSolid blue (1s)\tOff\tStatus check (short-press power button) Off\tPulsing blue\tOff\tSystem shutting down (with earcon) Pulsing green\tOff\tOff\tUploading Solid green (3s)\tSolid blue (3s)\tOff\tBoot complete (both LEDs solid for 3s, then off if not charging) Blinking orange (4x)\tOff\tOff\tWarning: battery low (&lt;&lt;&lt;20%), thermal throttling, low storage, or other warning Blinking red (4x)\tOff\tOff\tError: battery critical (‚â§‚â§‚â§5%) or other error Solid white (1s)\tOff\tOff\tDevice On Notification (DON): user puts on glasses, earcon plays Pulsing white\tOff\tOff\tSystem warm booting (waking from sleep) Blinking white, then solid white\tOff\tBlinking white (2x), then solid white\tRecording/Streaming active: inward LED blinks then solid; outward LED blinks then solid Pulsing white\tPulsing blue\tOff\tOS booting up    ","version":"Next","tagName":"h3"},{"title":"Power and Charging‚Äã","type":1,"pageTitle":"Aria Gen 2 Glasses Manual","url":"/projectaria_tools/gen2/ark/device/manual#power-and-charging","content":" Only use a USB-C cable to charge your glasses. When first plugged in, a red flashing light inside the right arm indicates the battery is &lt;&lt;&lt;5% but charging.  To turn the device on, press and hold the power button for 3 seconds. When booting is complete, the power LED will turn solid blue for 3 seconds before fading. You can also turn on your glasses by plugging them into a power source; wait for the blue LED before disconnecting the charger.  To turn the device off, hold the power button for 3 seconds and release it. The blue LED will start blinking and then turn off after a short delay, and you will hear an earcon confirming shutdown.    ","version":"Next","tagName":"h3"},{"title":"Resetting the Device‚Äã","type":1,"pageTitle":"Aria Gen 2 Glasses Manual","url":"/projectaria_tools/gen2/ark/device/manual#resetting-the-device","content":" Force Reset‚Äã  A force reset immediately reboots your device, preserving all data. Use this if the device is unresponsive.  Toggle the privacy switch forward (orange not visible).Hold both the action and power buttons for 3 seconds.Wait for the device to reboot.  Factory Reset‚Äã  A factory reset erases all local data, including recordings.  Toggle the privacy switch to the rear (orange visible).Tap (do not hold) the power and action buttons simultaneously.The device will reboot and the status LED will flash blue.The device is now ready to be paired.  Warning: Factory reset will delete all local data. ","version":"Next","tagName":"h3"},{"title":"Getting Started","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/ark/mps/start","content":"","keywords":"","version":"Next"},{"title":"Overview‚Äã","type":1,"pageTitle":"Getting Started","url":"/projectaria_tools/gen2/ark/mps/start#overview","content":" The Project Aria Machine Perceptions Services Command Line Interface (MPS CLI) is a command line tool used to request and receive Machine Perception Services. MPS CLI Guide page provides basic information to get you started with the MPS CLI.  The MPS inputs can be a file or directory, and multiple inputs can be listed in a single command.  The MPS CLI has two modes:  Single Process each recording individuallyOutput is always saved next to the input VRS fileThe most common way to request MPS Multi This mode is only available for Aria Gen1 at this time. Support for Aria Gen2 is coming soon.  Non-UI options available This tutorial uses the the MPS CLI UI, but all processes also work without using the UI and can be integrated into automated workflows. See the Command Line Reference in the Technical Specs for more details.  ","version":"Next","tagName":"h2"},{"title":"Install Project Aria MPS‚Äã","type":1,"pageTitle":"Getting Started","url":"/projectaria_tools/gen2/ark/mps/start#install-project-aria-mps","content":" Aria MPS is available as a Python package on PyPI. You can install it using pip. And we strongly recommend installing it under a python virtual environment:  Linux &amp; macOSWindows rm -rf $HOME/projectaria_tools_python_env python3 -m venv $HOME/projectaria_tools_python_env source $HOME/projectaria_tools_python_env/bin/activate python3 -m pip install projectaria-mps   ","version":"Next","tagName":"h2"},{"title":"Quick Start - Running the tool‚Äã","type":1,"pageTitle":"Getting Started","url":"/projectaria_tools/gen2/ark/mps/start#quick-start---running-the-tool","content":" aria_mps single -i &lt;path to vrs file or directory&gt; [options]   Note that if the above command emits some error, please refer to the troubleshooting section.  Project Aria MPS CLI is only available if you install the pip installation version of Project Aria MPS. This installation has been designed to be simple to use, even if you are not familiar with programming languages.  Dependencies Project Aria MPS pypi package automatically installs the projectaria-tools package and projectaria-vrs-health-check package  ","version":"Next","tagName":"h2"},{"title":"Supported Platforms‚Äã","type":1,"pageTitle":"Getting Started","url":"/projectaria_tools/gen2/ark/mps/start#supported-platforms","content":" OS / Platform\tOS / Distro Details\tAria Gen2 Support (projectaria-tools ‚â•2.0)\tSupported Python VersionsLinux (x64)\tFedora 40/41; Ubuntu 20.04 LTS (focal) / 22.04 LTS (jammy) / 24.04 LTS ( Noble Numbat)\t‚úÖ Supported\t3.9 ‚Äì 3.12 macOS (Apple Silicon / ARM64)\tmacOS 14+ (Sonoma or newer) on M1/M2/M3\t‚úÖ Supported\t3.10 ‚Äì 3.12 macOS (Intel)\tmacOS 13+ (Ventura or newer)\t‚úÖ Supported\t3.9 ‚Äì 3.12 Windows (x64)\tMSVC 2019/2022\tüöß Planned\t3.10 ‚Äì 3.12  ","version":"Next","tagName":"h2"},{"title":"Download the MPS CLI sample dataset‚Äã","type":1,"pageTitle":"Getting Started","url":"/projectaria_tools/gen2/ark/mps/start#download-the-mps-cli-sample-dataset","content":" To try out the following commands on VRS files:  Download the sample gen2 vrs fileMove it to a directory called Example in your downloads directory  info You may also wish to use your own recordings.  ","version":"Next","tagName":"h2"},{"title":"Request MPS for all VRS files in the ‚ÄúExample‚Äù directory and it‚Äôs subdirectories:‚Äã","type":1,"pageTitle":"Getting Started","url":"/projectaria_tools/gen2/ark/mps/start#request-mps-for-all-vrs-files-in-the-example-directory-and-its-subdirectories","content":" aria_mps single -i ~/Downloads/Example/   You'll be prompted to enter your username and then authenticate either via password on managed account login. Use the Project Aria credentials you use to sign into the Mobile Companion app.    Once the request has been processed, the MPS output will be downloaded next to the original VRS file. In this example, a recording in the Example directory called gen2_sample2.vrs was used to generate MPS.   ‚îî‚îÄ‚îÄ Example ‚îú‚îÄ‚îÄ mps_gen2_sample2_vrs ‚îÇ ‚îú‚îÄ‚îÄ slam ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ closed_loop_trajectory.csv ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ online_calibration.jsonl ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ open_loop_trajectory.csv ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ semidense_observations.csv.gz ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ semidense_points.csv.gz ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ summary.json ‚îÇ ‚îî‚îÄ‚îÄ vrs_health_check.json ‚îî‚îÄ‚îÄ gen2_sample2.vrs   Go to MPS Data Format Basics for more details about the folder structure.  ","version":"Next","tagName":"h2"},{"title":"Exit the MPS CLI‚Äã","type":1,"pageTitle":"Getting Started","url":"/projectaria_tools/gen2/ark/mps/start#exit-the-mps-cli","content":" To quit the MPS CLI, press CTRL + Q. The CLI will ask for confirmation before quitting.  If you quit the request tool while the files are uploading the uploads will stop.If you resubmit the request the uploads will resume where they left off, progress won‚Äôt be lost for upto 24 hrs.  If you quit the request tool once the files have been uploaded, the MPS processes will continue. Once processing is complete, and the request tool is open, MPS files will be automatically downloaded to your VRS files‚Äô location.  ","version":"Next","tagName":"h2"},{"title":"Working with MPS data‚Äã","type":1,"pageTitle":"Getting Started","url":"/projectaria_tools/gen2/ark/mps/start#working-with-mps-data","content":" You may find the following resources helpful when working with MPS data:  MPS Data FormatsVisualize MPS Using Python ","version":"Next","tagName":"h2"},{"title":"MPS Service Versions","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/ark/mps/mps_versioning","content":"","keywords":"","version":"Next"},{"title":"Overview‚Äã","type":1,"pageTitle":"MPS Service Versions","url":"/projectaria_tools/gen2/ark/mps/mps_versioning#overview","content":" The versioning system for MPS services adheres to the following format:  Major Number: Indicates significant changes that are not backward compatible. This number is incremented when there are major updates or changes in the API that could affect existing functionalities.Minor Number: Represents minor updates or improvements that are backward compatible. This number is incremented when additional features or minor improvements are introduced.Bugfix Number: Used for small fixes and patches to address issues without affecting the overall functionality. This number is incremented when bugs are fixed.  ","version":"Next","tagName":"h2"},{"title":"Current MPS Services and Versions:‚Äã","type":1,"pageTitle":"MPS Service Versions","url":"/projectaria_tools/gen2/ark/mps/mps_versioning#current-mps-services-and-versions","content":" ","version":"Next","tagName":"h2"},{"title":"SLAM (Simultaneous Localization and Mapping) Service‚Äã","type":1,"pageTitle":"MPS Service Versions","url":"/projectaria_tools/gen2/ark/mps/mps_versioning#slam-simultaneous-localization-and-mapping-service","content":" Current version: 2.0.0  Description: Project Aria's MPS SLAM service provides the trajectory in 6DoF, a semi-dense point cloud, and the online sensor calibration.  ","version":"Next","tagName":"h3"},{"title":"Hand Tracking Service‚Äã","type":1,"pageTitle":"MPS Service Versions","url":"/projectaria_tools/gen2/ark/mps/mps_versioning#hand-tracking-service","content":" Coming Soon  ","version":"Next","tagName":"h3"},{"title":"Summary‚Äã","type":1,"pageTitle":"MPS Service Versions","url":"/projectaria_tools/gen2/ark/mps/mps_versioning#summary","content":" This versioning system ensures that each MPS service is consistently updated and maintained, providing reliable and efficient performance for Project Aria glasses. ","version":"Next","tagName":"h3"},{"title":"Support and Troubleshooting","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/ark/support","content":"","keywords":"","version":"Next"},{"title":"Support Channels‚Äã","type":1,"pageTitle":"Support and Troubleshooting","url":"/projectaria_tools/gen2/ark/support#support-channels","content":" ","version":"Next","tagName":"h2"},{"title":"For Academic Partners‚Äã","type":1,"pageTitle":"Support and Troubleshooting","url":"/projectaria_tools/gen2/ark/support#for-academic-partners","content":" If you're an academic partner with Aria Research Kit access, you have multiple support channels available:  üì± Project Aria Discord (Recommended)‚Äã  Best for: Discussion, feedback, and user support  The Project Aria Discord community is the fastest way to get help from both the Aria team and fellow researchers.  Ask technical questionsShare tips and best practicesDiscuss research use casesGet quick responses from the community  Access: Discord invitation links are sent to your email when you receive ARK access.  üí¨ Academic Partners Feedback and Support Workplace Group‚Äã  Best for: Discussion, feedback, and user support  Connect with other academic partners and the Aria team on Workplace.  Share detailed feedbackDiscuss research methodologiesNetwork with other academic usersAccess announcements and updates  Access: Workplace group invitation links are sent to your email when you receive ARK access.  ‚úâÔ∏è Email Support‚Äã  Email: AriaOps@meta.com  Best for:  Formal feedback or feature requestsIssues requiring private communicationAccount or access-related questions    ","version":"Next","tagName":"h3"},{"title":"For Corporate Partners‚Äã","type":1,"pageTitle":"Support and Troubleshooting","url":"/projectaria_tools/gen2/ark/support#for-corporate-partners","content":" If you're a corporate partner with Aria Research Kit access:  ‚úâÔ∏è Email Support‚Äã  Email: AriaOps@meta.com  For:  User support and technical assistanceFeedback and feature requestsAccount or access-related questions    ","version":"Next","tagName":"h3"},{"title":"Getting Access to Support Channels‚Äã","type":1,"pageTitle":"Support and Troubleshooting","url":"/projectaria_tools/gen2/ark/support#getting-access-to-support-channels","content":" ","version":"Next","tagName":"h2"},{"title":"Discord and Workplace Access‚Äã","type":1,"pageTitle":"Support and Troubleshooting","url":"/projectaria_tools/gen2/ark/support#discord-and-workplace-access","content":" If you have received ARK access but haven't received invitations to Discord or Workplace:  Check your email (including spam/junk folders) for invitation linksVerify your email address matches the one used for ARK registrationContact us at AriaOps@meta.com if you haven't received invitations within 24 hours of receiving ARK access    ","version":"Next","tagName":"h3"},{"title":"Before Requesting Support‚Äã","type":1,"pageTitle":"Support and Troubleshooting","url":"/projectaria_tools/gen2/ark/support#before-requesting-support","content":" To help us assist you more efficiently, please try these steps first:  ","version":"Next","tagName":"h2"},{"title":"1. Check the Documentation‚Äã","type":1,"pageTitle":"Support and Troubleshooting","url":"/projectaria_tools/gen2/ark/support#1-check-the-documentation","content":" Many common questions are answered in our documentation:  Device Manual - Hardware controls, indicators, and operationClient SDK Troubleshooting - Common SDK issues and solutionsMPS Troubleshooting - MPS processing issuesTechnical Specifications - Detailed hardware and software specs  ","version":"Next","tagName":"h3"},{"title":"2. Review Known Issues‚Äã","type":1,"pageTitle":"Support and Troubleshooting","url":"/projectaria_tools/gen2/ark/support#2-review-known-issues","content":" Check for known issues and workarounds:  GitHub Issues - Open issues and bug reportsDiscord/Workplace announcements - Recent updates and known problems  ","version":"Next","tagName":"h3"},{"title":"3. Verify Your Setup‚Äã","type":1,"pageTitle":"Support and Troubleshooting","url":"/projectaria_tools/gen2/ark/support#3-verify-your-setup","content":" Common setup issues to check:  Device Issues:  ‚úÖ Device is fully charged‚úÖ Privacy switch is in correct position‚úÖ Device firmware is up to date‚úÖ Companion App is updated to latest version  SDK Issues:  ‚úÖ Using supported Python version (3.9-3.12)‚úÖ Client SDK is installed correctly‚úÖ Device authentication is completed (aria-gen2 auth check)‚úÖ Device is connected via USB or WiFi  Data Issues:  ‚úÖ VRS file is not corrupted‚úÖ Using compatible version of Project Aria Tools‚úÖ Sufficient storage space available    ","version":"Next","tagName":"h3"},{"title":"When Requesting Support‚Äã","type":1,"pageTitle":"Support and Troubleshooting","url":"/projectaria_tools/gen2/ark/support#when-requesting-support","content":" To help us resolve your issue quickly, please provide:  ","version":"Next","tagName":"h2"},{"title":"Essential Information‚Äã","type":1,"pageTitle":"Support and Troubleshooting","url":"/projectaria_tools/gen2/ark/support#essential-information","content":" Rage shake when issue just happens Clear description of the issue What you were trying to doWhat actually happenedError messages (exact text or screenshots) Environment details Operating system and versionPython versionClient SDK version (pip show projectaria-client-sdk)Project Aria Tools version (pip show projectaria-tools) Steps to reproduce Detailed steps to reproduce the issueWhether it happens consistently or intermittently Device information (if applicable) Device serial numberFirmware versionRecording profile usedBattery level during issue    ","version":"Next","tagName":"h3"},{"title":"Response Times‚Äã","type":1,"pageTitle":"Support and Troubleshooting","url":"/projectaria_tools/gen2/ark/support#response-times","content":" Discord: Community-driven, typically responses within hours during business days  Workplace: Responses within 1-2 business days  Email: Responses within 2-3 business days for standard support requests  Urgent Issues: For critical issues affecting large deployments or research deadlines, mark your email as &quot;Urgent&quot; and we'll prioritize accordingly.    ","version":"Next","tagName":"h2"},{"title":"Feedback and Feature Requests‚Äã","type":1,"pageTitle":"Support and Troubleshooting","url":"/projectaria_tools/gen2/ark/support#feedback-and-feature-requests","content":" We value your feedback! If you have suggestions for:  New features or improvementsDocumentation updates or clarificationsTools or utilitiesDataset requests  Please share through:  Discord (academic partners) - for discussion and community inputWorkplace (academic partners) - for detailed proposalsEmail (all partners) - for formal feature requests  Include:  Description of the feature/improvementUse case or research applicationExpected benefitsPriority level    ","version":"Next","tagName":"h2"},{"title":"Privacy and Security‚Äã","type":1,"pageTitle":"Support and Troubleshooting","url":"/projectaria_tools/gen2/ark/support#privacy-and-security","content":" When requesting support:  ‚úÖ Share device logs and error messages‚úÖ Share anonymized data samples if needed‚ùå Do NOT share personal participant data‚ùå Do NOT share sensitive research information‚ùå Do NOT share authentication credentials  If your issue involves sensitive data, mention this in your support request and we'll provide secure communication channels.    ","version":"Next","tagName":"h2"},{"title":"Emergency Support‚Äã","type":1,"pageTitle":"Support and Troubleshooting","url":"/projectaria_tools/gen2/ark/support#emergency-support","content":" For critical issues that are blocking your research:  Email AriaOps@meta.com with &quot;URGENT&quot; in subject lineClearly explain the impact and timeline constraintsProvide all relevant details from the &quot;When Requesting Support&quot; section above  We'll prioritize urgent requests affecting:  Active research studies with participantsConference or publication deadlinesLarge-scale data collection campaignsCritical demonstrations or presentations    Need help? Choose your support channel above and don't hesitate to reach out. We're here to help you succeed with Aria Research Kit! ","version":"Next","tagName":"h2"},{"title":"MPS Troubleshooting","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/ark/support/mps","content":"","keywords":"","version":"Next"},{"title":"Overview‚Äã","type":1,"pageTitle":"MPS Troubleshooting","url":"/projectaria_tools/gen2/ark/support/mps#overview","content":" This page provides troubleshooting support for issues relating to Project Aria's Machine Perception Services (MPS). If you can't find a solution for your problem on this page, go to the Support page for how to contact our team.  This page covers:  General MPS questionsMPS CLI specific troubleshooting and error codes  ","version":"Next","tagName":"h2"},{"title":"General MPS questions‚Äã","type":1,"pageTitle":"MPS Troubleshooting","url":"/projectaria_tools/gen2/ark/support/mps#general-mps-questions","content":" ","version":"Next","tagName":"h2"},{"title":"How do I learn more about MPS data?‚Äã","type":1,"pageTitle":"MPS Troubleshooting","url":"/projectaria_tools/gen2/ark/support/mps#how-do-i-learn-more-about-mps-data","content":" The MPS Data Formats section of the wiki provides more information about each MPS output and links to visualizers you can use with this data.  ","version":"Next","tagName":"h3"},{"title":"MPS CLI specific issues‚Äã","type":1,"pageTitle":"MPS Troubleshooting","url":"/projectaria_tools/gen2/ark/support/mps#mps-cli-specific-issues","content":" ","version":"Next","tagName":"h2"},{"title":"How do I request support?‚Äã","type":1,"pageTitle":"MPS Troubleshooting","url":"/projectaria_tools/gen2/ark/support/mps#how-do-i-request-support","content":" When requesting support, please include:  Log file MPS CLI logs File path is shown on the bottom left side of the MPS CLI UIStored in /tmp/logs/projectaria/mps/ by default Supplemental debugging files, if available. These will be stored in output folders: vrs_health_check.jsonsummary.json Screenshot showing the error codes  Please don‚Äôt send us any raw data Raw data is generally not necessary, but if it is, User Support will send you instructions for where to upload it.  ","version":"Next","tagName":"h3"},{"title":"Authentication issue on macOS‚Äã","type":1,"pageTitle":"MPS Troubleshooting","url":"/projectaria_tools/gen2/ark/support/mps#authentication-issue-on-macos","content":" If you encounter a permissions error while trying to sign in to the MPS CLI, try the following command (updated with your Python version):  export SSL_CERT_FILE=/etc/ssl/cert.pem   ","version":"Next","tagName":"h3"},{"title":"Terminal display issues in macOS‚Äã","type":1,"pageTitle":"MPS Troubleshooting","url":"/projectaria_tools/gen2/ark/support/mps#terminal-display-issues-in-macos","content":" Some users have reported that the MPS CLI UI has display issues in Terminal. Using a different terminal app may help. Why doesn't Textual look good on macOS provides further information about this issue.  ","version":"Next","tagName":"h3"},{"title":"MPS CLI error codes‚Äã","type":1,"pageTitle":"MPS Troubleshooting","url":"/projectaria_tools/gen2/ark/support/mps#mps-cli-error-codes","content":" ","version":"Next","tagName":"h3"},{"title":"Error codes‚Äã","type":1,"pageTitle":"MPS Troubleshooting","url":"/projectaria_tools/gen2/ark/support/mps#error-codes","content":" Error Code\tDescription\tAll 1xx codes are local client errors 100\tMulti-Recording processing error. Another recording in the group had a failure, so the processing was halted. We need the full group of recordings to successfully process and generate multi-slam MPS output. The recording that fails will show its own error code, processing then stops and and sets the other recordings to this error code. 101\tSomething failed on your computer. When you see this error, we suggest inspecting the local log for this run. If you need to reach out to support, please include this log file. 102\tThere were health check failures during processing. Check the vrs_health_check.json and vrs_health_check_slam.json for more information about what went wrong. 103\tMulti-Recording processing error. Duplicate file detected. Duplicate recordings are not allowed for multi-SLAM processing. 104\tMPS CLI failed to encrypt the file. Check error logs for more information. 105\tInput-output mismatch error. This error happens when you try to run multi-SLAM processing but pass an output directory that already contains output or intermediate artifacts from a different multi-SLAM job. 106\tThe output directory used for multi-SLAM processing is not empty. Non-empty output directory is only allowed if the output directory contains output or artifacts from a previous run of the same group of recordings. 108\tThe server failed to query the status of the request. Retrying should usually fix this issue. 109\tError during computing the hash id of the file. Check error logs for more information. 110\tError when checking existing outputs on the disk. Check error logs for more information. 111\tError when checking previously submitted MPS requests for a file. Check error logs for more information. 112\tError when checking if the recording was previously uploaded. Check error logs for more information. 113\tError uploading the VRS file. Check error logs for more information. 114\tError submitting MPS request to the server. Check error logs for more information. Retrying should fix the issue. 115\tError when checking the processing status of the MPS request. Check error logs for more information. 116\tError when downloading the MPS results. Check error logs for more information. 117\tThe requested service is not supported on the device generation where the recording was made. All other error codes\tServer side failure. Reach out to support with the error code and log file. ","version":"Next","tagName":"h3"},{"title":"Configuration","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/ark/vrs_health_check/configuration_and_thresholds","content":"","keywords":"","version":"Next"},{"title":"Listing Available Configurations‚Äã","type":1,"pageTitle":"Configuration","url":"/projectaria_tools/gen2/ark/vrs_health_check/configuration_and_thresholds#listing-available-configurations","content":" To see all available threshold configurations:  run_vrs_health_check --list-configurations   Example output:  Available VrsHealthCheck configurations: AriaGen2_Default, AriaGen2_Location, AriaGen1_Default, AriaGen1_Location   ","version":"Next","tagName":"h2"},{"title":"Viewing Configuration Details‚Äã","type":1,"pageTitle":"Configuration","url":"/projectaria_tools/gen2/ark/vrs_health_check/configuration_and_thresholds#viewing-configuration-details","content":" To see the detailed checks and thresholds for a specific configuration:  run_vrs_health_check --show-configuration-json AriaGen2_Default   This displays the complete JSON structure with all threshold values for that configuration.  ","version":"Next","tagName":"h2"},{"title":"Selecting Configuration for Output‚Äã","type":1,"pageTitle":"Configuration","url":"/projectaria_tools/gen2/ark/vrs_health_check/configuration_and_thresholds#selecting-configuration-for-output","content":" To control which configuration's results are reported as the final status, use the --choose-configuration flag:  run_vrs_health_check --path recording.vrs \\ --output-json results.json \\ --choose-configuration AriaGen2_Location   ‚ö†Ô∏è Important Note: The --choose-configuration flag does not limit which configuration's checks are performed. All available configurations are evaluated during every run and the results are reported in the output.json file. This flag only controls:  Final Exit Code: Which configuration's result determines the tool's exit status (PASS/WARN/FAIL)Terminal Output: Which configuration's detailed results are displayed in the console  This design ensures you get comprehensive analysis data for all configurations while allowing you to focus on the specific threshold set most relevant to your use case.  Understanding Checks  Checks are quality validation rules applied to VRS data streams. Each check evaluates a specific metric against predefined thresholds. There are two fundamental types of checks: Numeric Checks and Boolean Checks.  ","version":"Next","tagName":"h2"},{"title":"1. Numeric Checks‚Äã","type":1,"pageTitle":"Configuration","url":"/projectaria_tools/gen2/ark/vrs_health_check/configuration_and_thresholds#1-numeric-checks","content":" Numeric checks compare measured values (like frame rates, error counts, time intervals) against numerical thresholds.  Supported Check Types:  LE (Less than or Equal): Pass if measured_value &lt;= thresholdLT (Less than): Pass if measured_value &lt; thresholdGE (Greater than or Equal): Pass if measured_value &gt;= thresholdGT (Greater than): Pass if measured_value &gt; threshold  Structure:  { &quot;check_type&quot;: &quot;LE|LT|GE|GT&quot;, &quot;fail_threshold&quot;: &lt;number&gt;, // Optional: value that triggers FAIL &quot;warn_threshold&quot;: &lt;number&gt; // Optional: value that triggers WARN }   Evaluation Logic:  PASS: Value meets all thresholdsWARN: Value exceeds warn_threshold but not fail_thresholdFAIL: Value exceeds fail_threshold  ","version":"Next","tagName":"h2"},{"title":"2. Boolean Checks‚Äã","type":1,"pageTitle":"Configuration","url":"/projectaria_tools/gen2/ark/vrs_health_check/configuration_and_thresholds#2-boolean-checks","content":" Boolean checks verify true/false conditions like calibration validity or file integrity.  Supported Check Types:  EQ_TRUE: Pass if value is trueEQ_FALSE: Pass if value is false  Structure:  { &quot;check_type&quot;: &quot;EQ_TRUE|EQ_FALSE&quot;, &quot;fail_if_mismatch&quot;: true|false // Whether mismatch causes FAIL or WARN }   Evaluation Logic:  PASS: Value matches expected booleanFAIL/WARN: Value doesn't match (severity depends on fail_if_mismatch)   ","version":"Next","tagName":"h2"},{"title":"Companion App Rage Shake","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/ark/support/rageshake","content":"","keywords":"","version":"Next"},{"title":"What is Rage Shake?‚Äã","type":1,"pageTitle":"Companion App Rage Shake","url":"/projectaria_tools/gen2/ark/support/rageshake#what-is-rage-shake","content":" Rage Shake is a built-in diagnostic feature in the Aria Companion App that automatically:  ‚úÖ Captures device logs and diagnostics‚úÖ Creates a support task directly with the Aria team‚úÖ Includes relevant device information‚úÖ Streamlines the troubleshooting process  This is the fastest way to get help with device or Companion App problems.  ","version":"Next","tagName":"h3"},{"title":"How to Use Rage Shake‚Äã","type":1,"pageTitle":"Companion App Rage Shake","url":"/projectaria_tools/gen2/ark/support/rageshake#how-to-use-rage-shake","content":" Ensure Aria glasses are connected to WiFIOpen the Aria Companion App on your mobile deviceShake your phone vigorously (like you're frustrated - hence &quot;rage&quot; shake!)A prompt will appear asking if you want to send a bug reportConfirm to automatically submit logs and create a support taskYou're done! The Aria team will receive your logs and device information automatically  ","version":"Next","tagName":"h3"},{"title":"When to Use Rage Shake‚Äã","type":1,"pageTitle":"Companion App Rage Shake","url":"/projectaria_tools/gen2/ark/support/rageshake#when-to-use-rage-shake","content":" Use Rage Shake for issues such as:  üîß Companion App crashes or freezesüîß Device not connecting to Companion Appüîß Recording failuresüîß Device not respondingüîß Firmware update issuesüîß Unexpected device behaviorüîß Pairing or authentication problemsüîß Any other device or app-related issues  ","version":"Next","tagName":"h3"},{"title":"Important Notes‚Äã","type":1,"pageTitle":"Companion App Rage Shake","url":"/projectaria_tools/gen2/ark/support/rageshake#important-notes","content":" Internet connection required: Make sure your mobile device and Aria glasses are connected to the internetAutomatic submission: Logs are sent automatically - you don't need to follow up with additional support channels unless requestedPrivacy: Only device logs and technical diagnostics are sent - no personal data or recordingsResponse: The Aria team will review your logs and may reach out if additional information is needed  ","version":"Next","tagName":"h3"},{"title":"After Using Rage Shake‚Äã","type":1,"pageTitle":"Companion App Rage Shake","url":"/projectaria_tools/gen2/ark/support/rageshake#after-using-rage-shake","content":" After submitting a Rage Shake report:  Note the approximate time you submitted itContinue with your work if possibleIf the issue is urgent or blocking, also reach out via Discord or email mentioning you've submitted a Rage Shake reportThe team will investigate and may contact you for follow-up ","version":"Next","tagName":"h3"},{"title":"ClientSDK Troubleshooting","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/ark/support/sdk","content":"","keywords":"","version":"Next"},{"title":"Quick Diagnostic Commands‚Äã","type":1,"pageTitle":"ClientSDK Troubleshooting","url":"/projectaria_tools/gen2/ark/support/sdk#quick-diagnostic-commands","content":" Before diving into specific issues, use these commands to check your device status:  # Check device connectivity aria_gen2 device list # Check authentication status aria_gen2 auth check # Verify system configuration aria_doctor     ","version":"Next","tagName":"h2"},{"title":"Customization - Override Check Thresholds","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/ark/vrs_health_check/customization","content":"","keywords":"","version":"Next"},{"title":"Method 1: JSON Override File‚Äã","type":1,"pageTitle":"Customization - Override Check Thresholds","url":"/projectaria_tools/gen2/ark/vrs_health_check/customization#method-1-json-override-file","content":" Create a JSON file with your custom thresholds and use --override-check-file:  Example override file (custom_thresholds.json):  { &quot;configuration_name&quot;: &quot;UserOverride&quot;, &quot;configuration_to_override&quot;: &quot;AriaGen2_Default&quot;, &quot;checks_and_thresholds&quot;: { &quot;Camera Data (SLAM)&quot;: { &quot;ratio_dropped_over_expected&quot;: { &quot;fail_threshold&quot;: 0.05, &quot;warn_threshold&quot;: 0.02 } }, &quot;IMU Data (SLAM)&quot;: { &quot;non_monotonic&quot;: { &quot;ignore&quot;: true } } } }   Usage:  run_vrs_health_check --path recording.vrs \\ --output-json results.json \\ --override-check-file custom_thresholds.json \\ --choose-configuration UserOverride   ","version":"Next","tagName":"h2"},{"title":"Method 2: Command Line Overrides‚Äã","type":1,"pageTitle":"Customization - Override Check Thresholds","url":"/projectaria_tools/gen2/ark/vrs_health_check/customization#method-2-command-line-overrides","content":" For quick adjustments, use --override-checks with comma-separated overrides:  Syntax:  --override-checks &quot;stream_name.check_name.threshold_type=value,...&quot;   Examples:  # Override camera drop threshold --override-checks &quot;Camera Data (SLAM).ratio_dropped_over_expected.fail_threshold=0.05&quot; # Multiple overrides --override-checks &quot;Camera Data (SLAM).ratio_dropped_over_expected.fail_threshold=0.05,IMU Data (SLAM).non_monotonic.ignore=true&quot; # Specify base configuration to override --configuration-to-override AriaGen2_Default --override-checks &quot;...&quot;   Usage:  run_vrs_health_check --path recording.vrs \\ --output-json result.json \\ --configuration-to-override AriaGen2_Default \\ --override-checks &quot;Pose Data Class (hand).non_monotonic.ignore=true&quot; \\ --choose-configuration UserOverride   ","version":"Next","tagName":"h2"},{"title":"Disabling Checks‚Äã","type":1,"pageTitle":"Customization - Override Check Thresholds","url":"/projectaria_tools/gen2/ark/vrs_health_check/customization#disabling-checks","content":" To completely disable a check, set ignore=true:  JSON method:  &quot;IMU Data (SLAM)&quot;: { &quot;non_monotonic&quot;: { &quot;ignore&quot;: true } }   Command line method:  --override-checks &quot;IMU Data (SLAM).non_monotonic.ignore=true&quot;   Best Practices for Overriding  Use JSON files for complex, reusable configuration changesUse command line overrides for quick testing or one-off adjustmentsDocument your custom thresholds and the reasoning behind themTest your overrides with known good and bad recordings to validate effectivenessStart with existing configurations as a base rather than creating from scratch ","version":"Next","tagName":"h3"},{"title":"Device Connection Issues‚Äã","type":1,"pageTitle":"ClientSDK Troubleshooting","url":"/projectaria_tools/gen2/ark/support/sdk#device-connection-issues","content":" ","version":"Next","tagName":"h2"},{"title":"Device Not Detected‚Äã","type":1,"pageTitle":"ClientSDK Troubleshooting","url":"/projectaria_tools/gen2/ark/support/sdk#device-not-detected","content":" If your device is not showing up when running aria_gen2 device list, follow these solutions in order:  Solution 1: Run aria_doctor‚Äã  The aria_doctor command configures your system for device communication:  aria_doctor   This command will:  Open required ports on your PC for device discoveryConfigure your system to allow proper USB Ethernet connectionSet up internet access when connected via USB  tip If you cannot connect to the internet when connected to the device via USB, running aria_doctor typically resolves this issue.    Solution 2: Update USB Ethernet Security Settings‚Äã  When connecting via USB, the connection appears as an Ethernet configuration. You may need to adjust security settings:  Linux Instructions:  Check network connections: nmcli device status Look for a new USB or Ethernet deviceEnsure NetworkManager is managing the connectionDisable Security in USB networking interface    Solution 3: Disable VPN‚Äã  VPNs can interfere with the USB network interface used by the device:  Steps to resolve:  Disconnect from any active VPN or Lighthouse connectionsRun aria_gen2 device list to check if the device is detectedIf successful, streaming and device control should work  # After disabling VPN, verify device connection aria_gen2 device list   warning Some corporate VPNs may automatically reconnect. You may need to temporarily disable auto-connect features while working with the device.    Solution 4: Check USB Connection‚Äã  Try these steps:  Different USB Port: Use a different USB port on your computer, preferably USB 3.0+Different USB Cable: Use a high-quality USB cable that supports data transferDirect Connection: Avoid USB hubs if possible - connect directly to computerCheck Cable: Ensure the cable supports data transfer (not just charging)    Solution 5: Restart and Reset‚Äã  If the above solutions don't work:  Disconnect and reconnect USB cable - Wait a few seconds between disconnect and reconnectRestart the device - Power off and restart your Aria Gen2 device from Companion AppRestart your computer - Sometimes a system restart resolves network configuration issuesRun aria_doctor again after restart  # After restart, verify setup aria_doctor aria_gen2 device list     Solution 6: Check Device Status‚Äã  Battery: Ensure your device has sufficient battery chargePower State: Verify the device is powered onFirmware: Check if device firmware is up to date via Companion App    ","version":"Next","tagName":"h3"},{"title":"Authentication Issues‚Äã","type":1,"pageTitle":"ClientSDK Troubleshooting","url":"/projectaria_tools/gen2/ark/support/sdk#authentication-issues","content":" ","version":"Next","tagName":"h2"},{"title":"Cannot Authenticate Device‚Äã","type":1,"pageTitle":"ClientSDK Troubleshooting","url":"/projectaria_tools/gen2/ark/support/sdk#cannot-authenticate-device","content":" If authentication is failing when running aria_gen2 auth pair:  Solution 1: Verify Device Connection‚Äã  Ensure the device is properly connected and detected:  # Check if device is listed aria_gen2 device list # If not listed, run aria_doctor aria_doctor   If device is not listed, see Device Connection Issues above.    Solution 2: Check Companion App Connection‚Äã  Authentication requires approval through the Companion App. Verify:  Companion App is running on your mobile deviceDevice is paired with the Companion AppMobile device has an active internet connectionCompanion App is running in the foreground (not background)Notifications are enabled for the Companion App    Solution 3: Retry Authentication‚Äã  # Stop any existing authentication attempts # Press Ctrl+C if a command is hanging # Retry authentication aria_gen2 auth pair   When the command runs:  Check your Companion App for the authentication requestVerify the hash code shown matches between PC and Companion AppApprove the authentication in the Companion AppWait for confirmation on PC    Solution 4: Reset Connection‚Äã  If authentication continues to fail:  # 1. Disconnect and reconnect the USB cable # Wait a few seconds after reconnecting # 2. Run aria_doctor again aria_doctor # 3. Check device connection aria_gen2 device list # 4. Retry authentication aria_gen2 auth pair     Solution 5: Check Authentication Status‚Äã  If you've previously authenticated, verify the status:  aria_gen2 auth check # Expected output if authenticated: # [AriaGen2Cli:App][INFO]: Device &lt;serial&gt; is successfully authenticated.   tip You only need to authenticate once per device and PC combination. If already authenticated, you don't need to authenticate again.    ","version":"Next","tagName":"h3"},{"title":"Recording Issues‚Äã","type":1,"pageTitle":"ClientSDK Troubleshooting","url":"/projectaria_tools/gen2/ark/support/sdk#recording-issues","content":" ","version":"Next","tagName":"h2"},{"title":"Cannot Start Recording‚Äã","type":1,"pageTitle":"ClientSDK Troubleshooting","url":"/projectaria_tools/gen2/ark/support/sdk#cannot-start-recording","content":" Symptoms: Recording fails to start or returns an error  Solutions:  Check device connection: aria_gen2 device list Stop existing recording: aria_gen2 recording stop Check storage space: aria_gen2 recording list If device is full, download and delete old recordings Verify authentication: aria_gen2 auth check Try a different profile: # A wrong profile name might be provided, try profile8 aria_gen2 recording start --profile profile8 --recording-name test_recording     ","version":"Next","tagName":"h3"},{"title":"Cannot Download Recording‚Äã","type":1,"pageTitle":"ClientSDK Troubleshooting","url":"/projectaria_tools/gen2/ark/support/sdk#cannot-download-recording","content":" Symptoms: Download fails or hangs  Solutions:  Verify USB connection: aria_gen2 device list Check recording UUID: aria_gen2 recording list Ensure you're using the correct UUID Verify output directory exists: # Create directory if it doesn't exist Check disk space on your computer Try downloading to a different location: aria_gen2 recording download -u &lt;uuid&gt; -o /path/to/different/folder/ For large files: Be patient - downloads can take several minutes depending on recording size    ","version":"Next","tagName":"h3"},{"title":"Cannot Delete Recording‚Äã","type":1,"pageTitle":"ClientSDK Troubleshooting","url":"/projectaria_tools/gen2/ark/support/sdk#cannot-delete-recording","content":" Symptoms: Delete command fails  Solutions:  Stop any active recording: aria_gen2 recording stop Verify device connection: aria_gen2 device list Check recording UUID: aria_gen2 recording list Retry delete command: aria_gen2 recording delete -u &lt;uuid&gt;     ","version":"Next","tagName":"h3"},{"title":"USB Streaming Issues‚Äã","type":1,"pageTitle":"ClientSDK Troubleshooting","url":"/projectaria_tools/gen2/ark/support/sdk#usb-streaming-issues","content":" ","version":"Next","tagName":"h2"},{"title":"Streaming Won't Start‚Äã","type":1,"pageTitle":"ClientSDK Troubleshooting","url":"/projectaria_tools/gen2/ark/support/sdk#streaming-wont-start","content":" Common Causes:  Device is recordingAnother streaming session is activeDevice battery is lowUSB connection issue  Solutions:  # 1. Stop any active recording aria_gen2 recording stop # 2. Stop any active streaming aria_gen2 streaming stop # 3. Check device status aria_gen2 device list # 4. Try starting streaming again aria_gen2 streaming start     ","version":"Next","tagName":"h3"},{"title":"Poor USB Streaming Performance‚Äã","type":1,"pageTitle":"ClientSDK Troubleshooting","url":"/projectaria_tools/gen2/ark/support/sdk#poor-usb-streaming-performance","content":" Symptoms:  Dropped framesHigh latencyChoppy visualization  Solutions:  USB Connection: Use a high-quality USB 3.0+ cableTry a different USB port, preferably USB 3.0+ (usually blue-colored)Avoid USB hubs - connect directly to computerTry a different USB cable System Resources: Close unnecessary applications to free up CPU and memoryDisable other programs using camera/video (Zoom, Teams, etc.)Check system resource usage (CPU, RAM) Device Temperature: If device is hot, take a break to let it cool downEnsure good ventilation around the device Profile Selection: Use mp_streaming_demo profile for smooth performance: aria_gen2 streaming start --profile mp_streaming_demo     ","version":"Next","tagName":"h3"},{"title":"Viewer Not Showing Data (USB)‚Äã","type":1,"pageTitle":"ClientSDK Troubleshooting","url":"/projectaria_tools/gen2/ark/support/sdk#viewer-not-showing-data-usb","content":" Common Causes:  Streaming not started on deviceViewer started before streaming beganPort configuration issue  Solutions:  # 1. Ensure streaming is running aria_gen2 streaming stop aria_gen2 streaming start # 2. Restart the viewer # Press Ctrl+C to stop the current viewer, then: aria_streaming_viewer   Additional Steps:  Ensure aria_doctor has been runCheck firewall settings aren't blocking viewerTry restarting both streaming and viewer with fresh terminals    ","version":"Next","tagName":"h3"},{"title":"General Issues‚Äã","type":1,"pageTitle":"ClientSDK Troubleshooting","url":"/projectaria_tools/gen2/ark/support/sdk#general-issues","content":" ","version":"Next","tagName":"h2"},{"title":"Cannot Stop Streaming/Recording‚Äã","type":1,"pageTitle":"ClientSDK Troubleshooting","url":"/projectaria_tools/gen2/ark/support/sdk#cannot-stop-streamingrecording","content":" Symptoms: Stop command hangs or doesn't respond  Solutions:  Connect device via USB (if streaming wirelessly) Force stop: Press Ctrl+C to interrupt the hanging commandTry the stop command again: aria_gen2 streaming stop # or aria_gen2 recording stop Restart device: Power off and restart the deviceReconnect via USBTry stop command again    ","version":"Next","tagName":"h3"},{"title":"Commands Hang or Timeout‚Äã","type":1,"pageTitle":"ClientSDK Troubleshooting","url":"/projectaria_tools/gen2/ark/support/sdk#commands-hang-or-timeout","content":" Symptoms: CLI commands don't complete or timeout  Solutions:  Check device connection: aria_gen2 device list Run aria_doctor: aria_doctor Check for VPN: Disable VPNRetry command Restart device and computer: Sometimes network state needs resetReconnect after restart Check USB connection: Try different USB portTry different USB cable    ","version":"Next","tagName":"h3"},{"title":"High Device Temperature‚Äã","type":1,"pageTitle":"ClientSDK Troubleshooting","url":"/projectaria_tools/gen2/ark/support/sdk#high-device-temperature","content":" Symptoms: Device feels hot to touch  When This Happens:  During long recording sessionsDuring high frame rate profiles (e.g. profile8)In warm environments  Solutions:  Stop streaming/recording: aria_gen2 streaming stop # or aria_gen2 recording stop Let device cool: Remove from hot environmentsEnsure good ventilationWait 10-15 minutes Switch to USB for streaming: USB streaming generates less heatBetter for extended sessions Reduce session length: Keep wireless streaming under 30 minutesTake breaks between sessions  tip This is normal behavior during intensive operations. The device has thermal protection that will automatically stop operations if it gets too hot. This protects the device hardware.    ","version":"Next","tagName":"h3"},{"title":"Cannot Update ClientSDK‚Äã","type":1,"pageTitle":"ClientSDK Troubleshooting","url":"/projectaria_tools/gen2/ark/support/sdk#cannot-update-clientsdk","content":" Symptoms: pip install fails or hangs  Solutions:  Ensure virtual environment is activated: source $HOME/projectaria_gen2_python_env/bin/activate Update pip first: python3 -m pip install --upgrade pip Clear pip cache: pip cache purge Reinstall: pip uninstall projectaria-client-sdk pip install projectaria-client-sdk==2.0.0 Check internet connection: Ensure you can access PyPITry with VPN disabledCheck firewall settings    ","version":"Next","tagName":"h3"},{"title":"GLIBCXX Error on Ubuntu 22 (Linux)‚Äã","type":1,"pageTitle":"ClientSDK Troubleshooting","url":"/projectaria_tools/gen2/ark/support/sdk#glibcxx-error-on-ubuntu-22-linux","content":" Symptoms: When running CLI commands on Ubuntu 22, you may see an error like:  GLIBCXX_3.4.31 not found   Cause: The system's C++ standard library (libstdc++6) is outdated and doesn't have the required version.  Solution:  Update the C++ standard library by installing a newer compiler toolchain:  # 1. Check current libstdc++6 version sudo apt list --installed | grep libstdc++6 # 2. Add the Ubuntu toolchain PPA repository sudo add-apt-repository ppa:ubuntu-toolchain-r/test # 3. Update package list sudo apt update # 4. Install the newer g++ compiler (which includes the updated library) sudo apt install g++-13   After installation, try running your CLI command again. The updated libstdc++6 library should now be available.  info This issue is specific to Ubuntu 22.04 and earlier versions. Ubuntu 24 and later typically have the required library version by default.    ","version":"Next","tagName":"h3"},{"title":"Getting Additional Help‚Äã","type":1,"pageTitle":"ClientSDK Troubleshooting","url":"/projectaria_tools/gen2/ark/support/sdk#getting-additional-help","content":" If you've tried the solutions above and still experience issues:  ","version":"Next","tagName":"h2"},{"title":"Gather Diagnostic Information‚Äã","type":1,"pageTitle":"ClientSDK Troubleshooting","url":"/projectaria_tools/gen2/ark/support/sdk#gather-diagnostic-information","content":" Before seeking help, gather this information:  # Device information aria_gen2 device list # Authentication status aria_gen2 auth check # System configuration aria_doctor # Python and package versions python3 --version pip list | grep projectaria # run diagnostic to collect error logs aria_diagnostics   Linux Users If aria_diagnostics fails to run, you may need to install net-tools: sudo apt install net-tools   ","version":"Next","tagName":"h3"},{"title":"Contact Support‚Äã","type":1,"pageTitle":"ClientSDK Troubleshooting","url":"/projectaria_tools/gen2/ark/support/sdk#contact-support","content":" Include diagnostic information when reporting issuesDescribe steps to reproduce the problemInclude diagnostics.log  ","version":"Next","tagName":"h3"},{"title":"Additional Resources‚Äã","type":1,"pageTitle":"ClientSDK Troubleshooting","url":"/projectaria_tools/gen2/ark/support/sdk#additional-resources","content":" Get Started Guide - Initial setup and authenticationRecording Guide - Recording troubleshootingStreaming Guide - Streaming troubleshootingCLI Technical Specs - Complete command reference ","version":"Next","tagName":"h3"},{"title":"Welcome to the Open Science Initiative","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/research-tools","content":"","keywords":"","version":"Next"},{"title":"What is OSI?‚Äã","type":1,"pageTitle":"Welcome to the Open Science Initiative","url":"/projectaria_tools/gen2/research-tools#what-is-osi","content":" OSI makes egocentric AI research accessible by open-sourcing three key components:  ","version":"Next","tagName":"h2"},{"title":"üìä Public Datasets‚Äã","type":1,"pageTitle":"Welcome to the Open Science Initiative","url":"/projectaria_tools/gen2/research-tools#-public-datasets","content":" High-quality, multimodal egocentric datasets collected with Aria glasses, featuring:  Raw sensor streams from multiple synchronized cameras, IMUs, audio, and moreOn-device machine perception outputs (VIO, eye gaze, hand tracking)Offline machine perception annotations from cloud-based processingRich semantic annotations from state-of-the-art perception algorithms  Our datasets capture real-world scenarios with time-synchronized multi-participant recordings, enabling research in computer vision, multimodal learning, robotics, and contextual AI.  ","version":"Next","tagName":"h3"},{"title":"üõ†Ô∏è projectaria-tools‚Äã","type":1,"pageTitle":"Welcome to the Open Science Initiative","url":"/projectaria_tools/gen2/research-tools#Ô∏è-projectaria-tools","content":" A comprehensive Python/C++ library for working with Aria data:  Load and visualize VRS (Video Recording Storage) filesAccess device calibration and sensor dataWork with machine perception outputs (VIO, eye gaze, hand tracking)Process and analyze multimodal sensor streamsExport data to standard formats  ","version":"Next","tagName":"h3"},{"title":"ü§ñ Open Models‚Äã","type":1,"pageTitle":"Welcome to the Open Science Initiative","url":"/projectaria_tools/gen2/research-tools#-open-models","content":" Pre-trained machine learning models and algorithms for egocentric perception:  Coming soon...  ","version":"Next","tagName":"h3"},{"title":"What You Can Do with OSI‚Äã","type":1,"pageTitle":"Welcome to the Open Science Initiative","url":"/projectaria_tools/gen2/research-tools#what-you-can-do-with-osi","content":" ","version":"Next","tagName":"h2"},{"title":"üì• Access Research Data‚Äã","type":1,"pageTitle":"Welcome to the Open Science Initiative","url":"/projectaria_tools/gen2/research-tools#-access-research-data","content":" Download and explore publicly available egocentric datasets:  Multi-sensor time-synchronized recordingsGround truth annotations from advanced perception algorithmsPre-processed machine perception outputsCalibration data for all sensors  ","version":"Next","tagName":"h3"},{"title":"üî¨ Analyze Egocentric Data‚Äã","type":1,"pageTitle":"Welcome to the Open Science Initiative","url":"/projectaria_tools/gen2/research-tools#-analyze-egocentric-data","content":" Use Project Aria Tools to:  Load and visualize VRS recordings in PythonAccess raw sensor data (RGB, CV cameras, IMU, audio, etc.)Work with device calibration and coordinate transformationsQuery time-synchronized sensor streams efficientlyVisualize 3D trajectories and point clouds with Rerun  ","version":"Next","tagName":"h3"},{"title":"üß™ Build and Train Models‚Äã","type":1,"pageTitle":"Welcome to the Open Science Initiative","url":"/projectaria_tools/gen2/research-tools#-build-and-train-models","content":" Leverage open datasets and tools to:  Train computer vision models on egocentric dataDevelop multimodal perception algorithmsResearch human attention and gaze patternsStudy hand-object interactionsExplore social dynamics in multi-participant scenarios  ","version":"Next","tagName":"h3"},{"title":"üìä Benchmark Your Algorithms‚Äã","type":1,"pageTitle":"Welcome to the Open Science Initiative","url":"/projectaria_tools/gen2/research-tools#-benchmark-your-algorithms","content":" Compare your methods against:  Baseline perception algorithmsPre-trained open modelsPublished research resultsState-of-the-art egocentric AI systems  ","version":"Next","tagName":"h3"},{"title":"Getting Started‚Äã","type":1,"pageTitle":"Welcome to the Open Science Initiative","url":"/projectaria_tools/gen2/research-tools#getting-started","content":" Choose your starting point based on your research goals:  ","version":"Next","tagName":"h2"},{"title":"For Dataset Users‚Äã","type":1,"pageTitle":"Welcome to the Open Science Initiative","url":"/projectaria_tools/gen2/research-tools#for-dataset-users","content":" Begin with the Aria Gen 2 Pilot Dataset to explore available data. Then follow the Download Guide to access the dataset and start your research.  ","version":"Next","tagName":"h3"},{"title":"For Tool Users‚Äã","type":1,"pageTitle":"Welcome to the Open Science Initiative","url":"/projectaria_tools/gen2/research-tools#for-tool-users","content":" Install Project Aria Tools to work with Aria data. The library provides Python APIs and tutorials for loading, visualizing, and processing VRS files.  ","version":"Next","tagName":"h3"},{"title":"For Algorithm Developers‚Äã","type":1,"pageTitle":"Welcome to the Open Science Initiative","url":"/projectaria_tools/gen2/research-tools#for-algorithm-developers","content":" Explore the Open Models section to access pre-trained models and algorithms. Use them as baselines or building blocks for your own research.  ","version":"Next","tagName":"h3"},{"title":"For First-Time Users‚Äã","type":1,"pageTitle":"Welcome to the Open Science Initiative","url":"/projectaria_tools/gen2/research-tools#for-first-time-users","content":" Start with the Python Tutorials to learn the basics:  VrsDataProvider Basics - Load and access Aria dataDevice Calibration - Work with sensor calibrationQueued Sensor Data - Efficiently stream multi-sensor dataEye Tracking &amp; Hand Tracking - Access on-device perception outputsMPS Data Loading - Visualize machine perception results  ","version":"Next","tagName":"h3"},{"title":"Research Impact‚Äã","type":1,"pageTitle":"Welcome to the Open Science Initiative","url":"/projectaria_tools/gen2/research-tools#research-impact","content":" OSI enables research in:  Egocentric Vision: First-person computer vision and scene understandingMultimodal AI: Fusion of vision, audio, IMU, and physiological signalsHuman-AI Interaction: Gaze-based interfaces and attention modelingRobotics: Manipulation learning from human demonstrationsSocial Computing: Multi-participant interaction analysisContextual AI: Understanding user context and intentAugmented Reality: Spatial computing and scene reconstruction  ","version":"Next","tagName":"h2"},{"title":"Community & Support‚Äã","type":1,"pageTitle":"Welcome to the Open Science Initiative","url":"/projectaria_tools/gen2/research-tools#community--support","content":" Join a growing community of researchers working with Aria data:  Access open-source code and tutorialsContribute to Project Aria Tools developmentShare your research and findingsCollaborate on new datasets and models  ","version":"Next","tagName":"h2"},{"title":"Citation‚Äã","type":1,"pageTitle":"Welcome to the Open Science Initiative","url":"/projectaria_tools/gen2/research-tools#citation","content":" If you use OSI datasets or tools in your research, please cite the relevant papers and acknowledge the Project Aria platform.    Ready to explore? Choose a starting point above to begin your journey with the Open Science Initiative. ","version":"Next","tagName":"h2"},{"title":"Installation and Quick Start","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/ark/vrs_health_check/installation","content":"","keywords":"","version":"Next"},{"title":"Installing VRS Health Check‚Äã","type":1,"pageTitle":"Installation and Quick Start","url":"/projectaria_tools/gen2/ark/vrs_health_check/installation#installing-vrs-health-check","content":" Vrs Health Check is available as a Python package on PyPI. You can install it using pip. And we strongly recommend installing it under a python virtual environment:  Linux &amp; macOSWindows rm -rf $HOME/projectaria_gen2_python_env python3 -m venv $HOME/projectaria_gen2_python_env source $HOME/projectaria_gen2_python_env/bin/activate python3 -m pip install projectaria-vrs-health-check   ","version":"Next","tagName":"h2"},{"title":"Quick Start - Running the tool‚Äã","type":1,"pageTitle":"Installation and Quick Start","url":"/projectaria_tools/gen2/ark/vrs_health_check/installation#quick-start---running-the-tool","content":" run_vrs_health_check --path &lt;VRS_FILE&gt; --output-json &lt;RESULTS_FILE&gt; [OPTIONS]   Note that if the above command emits some error, please refer to the following troubleshooting section.  ","version":"Next","tagName":"h2"},{"title":"Available Options:‚Äã","type":1,"pageTitle":"Installation and Quick Start","url":"/projectaria_tools/gen2/ark/vrs_health_check/installation#available-options","content":" üìÅ Basic Options‚Äã  Option\tDescription--path\tPath to VRS file or directory containing VRS files --output-json\tPath to save detailed JSON results  üñ•Ô∏è Terminal Output Control‚Äã  Option\tDescription--print-stats\tPrint detailed statistics to console --disable-logging\tSuppress diagnostic log messages (quiet mode)  ‚öôÔ∏è Configuration Management‚Äã  Option\tDescription--list-configurations\tList all available threshold configurations --show-configuration-json &lt;CONFIG&gt;\tShow details of a specific configuration --choose-configuration &lt;CONFIG&gt;\tSelect a configuration to output its results  üîß Custom Threshold Overrides‚Äã  Option\tDescription--override-check-file &lt;FILE&gt;\tJSON file with custom threshold overrides --override-checks &lt;OVERRIDES&gt;\tComma-separated threshold overrides --configuration-to-override &lt;CONFIG&gt;\tBase configuration for overrides  ","version":"Next","tagName":"h2"},{"title":"Expected Outputs‚Äã","type":1,"pageTitle":"Installation and Quick Start","url":"/projectaria_tools/gen2/ark/vrs_health_check/installation#expected-outputs","content":" The VRS Health Check tool provides two primary outputs:  ","version":"Next","tagName":"h2"},{"title":"1. Exit Code‚Äã","type":1,"pageTitle":"Installation and Quick Start","url":"/projectaria_tools/gen2/ark/vrs_health_check/installation#1-exit-code","content":" The tool returns an exit code indicating the overall health check result:  Exit Code\tStatus\tDescription0\tPASS\tHealth check passed successfully 1\tWARN\tHealth check passed with warnings 2\tFAIL\tHealth check failed Others 64\tEX_USAGE\tCommand line usage error 66\tEX_NOINPUT\tCannot open input file or setup failure  ","version":"Next","tagName":"h3"},{"title":"2. JSON Results File‚Äã","type":1,"pageTitle":"Installation and Quick Start","url":"/projectaria_tools/gen2/ark/vrs_health_check/installation#2-json-results-file","content":" The --output-json parameter creates a detailed JSON file containing:  Overall status: per-configuration check result : pass / warn / fail.Failed check list: List of checks that result in fail.Warn check list: List of checks that result in warn.Performed checks with details: Complete list of all performed checks, with details including value, threshold, and result.Unperformed checks: List of VRS stream statistics that are computed, but not used for threshold checking.See the following for an example output json file:  { &quot;AriaGen2_Default&quot;: { &quot;final_result&quot;: &quot;fail&quot;, &quot;failed_checks&quot;: [ &quot;RGB Camera Class #1.time_error&quot; ], &quot;warn_checks&quot;: [], &quot;performed_checks_with_details&quot;: { &quot;Ambient Light Sensor (ALS) Data Class #1&quot;: { &quot;bad&quot;: { &quot;check_type&quot;: &quot;LE&quot;, &quot;fail_threshold&quot;: 0.0, &quot;result&quot;: &quot;pass&quot;, &quot;value&quot;: 0.0 }, &quot;end_offset_from_file_us&quot;: { &quot;check_type&quot;: &quot;LE&quot;, &quot;fail_threshold&quot;: 3000000.0, &quot;result&quot;: &quot;pass&quot;, &quot;value&quot;: 14221.0, &quot;warn_threshold&quot;: 1500000.0 }, &quot;non_monotonic&quot;: { &quot;check_type&quot;: &quot;LE&quot;, &quot;fail_threshold&quot;: 0.0, &quot;result&quot;: &quot;pass&quot;, &quot;value&quot;: 0.0 }, ... }, &quot;Barometer Data Class #1&quot;: { &quot;bad&quot;: { &quot;check_type&quot;: &quot;LE&quot;, &quot;fail_threshold&quot;: 0.0, &quot;result&quot;: &quot;pass&quot;, &quot;value&quot;: 0.0 }, ... }, ... }, &quot;unperformed_checks&quot;: { &quot;Ambient Light Sensor (ALS) Data Class #1&quot;: { &quot;dropped&quot;: 0, &quot;expected&quot;: 368, &quot;largest_deviation_from_period_us&quot;: 1577, &quot;processed&quot;: 372, &quot;time_error&quot;: 0 }, ... } }, &quot;AriaGen2_Location&quot;: { &quot;final_result&quot;: &quot;fail&quot;, &quot;failed_checks&quot;: [ &quot;RGB Camera Class #1.time_error&quot; ], &quot;warn_checks&quot;: [], &quot;performed_checks_with_details&quot;: { ... }, &quot;unperformed_checks&quot;: { ... } } }   ","version":"Next","tagName":"h3"},{"title":"Troubleshoot‚Äã","type":1,"pageTitle":"Installation and Quick Start","url":"/projectaria_tools/gen2/ark/vrs_health_check/installation#troubleshoot","content":" ","version":"Next","tagName":"h2"},{"title":"run_vrs_health_check exists but running the command emits some error‚Äã","type":1,"pageTitle":"Installation and Quick Start","url":"/projectaria_tools/gen2/ark/vrs_health_check/installation#run_vrs_health_check-exists-but-running-the-command-emits-some-error","content":" This is likely due to that you have previously installed projectaria-tools (version &lt; 2.0) python package globally, instead installing in a virtual Python environment. This leads to that the Python tool with the same name projectaria_tools becoming a globally visible binary tool.  This can be validated by running:  which run_vrs_health_check   if you see it pointing to anywhere other than ~/$YOUR_NEW_VHC_PYTHON_VENV/bin/run_vrs_health_check, then this is the culprit.  To resolve, simply upgrade projectaria-tools to &gt;= 2.0.0, it will resolve this naming conflict. ","version":"Next","tagName":"h3"},{"title":"Aria Gen 2 Pilot Dataset","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/content","content":"","keywords":"","version":"Next"},{"title":"Dataset Content‚Äã","type":1,"pageTitle":"Aria Gen 2 Pilot Dataset","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/content#dataset-content","content":" The Aria Gen 2 pilot dataset comprises four primary data content types:  raw sensor streams acquired directly from Aria Gen 2 devices and recorded by Profile 8real-time machine perception outputs generated on-device via embedded algorithms during data collectionoffline machine perception results produced by Machine Perception Services (MPS) during post-processing; andoutputs from additional offline perception algorithms. See below for details.  Content (1) and (2) are obtained natively from the device, whereas (3) and (4) are derived through post-hoc processing.  ","version":"Next","tagName":"h2"},{"title":"Additional Perception Algorithms‚Äã","type":1,"pageTitle":"Aria Gen 2 Pilot Dataset","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/content#additional-perception-algorithms","content":" Algorithm\tDescription\tOutputDirectional Automatic Speech Recognition (ASR)\tDistinguishes between wearer and others, generating timestamped transcripts for all sequences. Enables analysis of conversational dynamics and social context.\tTimestamped transcripts of speech. Heart Rate Estimation\tUses PPG sensors to estimate continuous heart rate, reflecting physical activity and physiological state. Coverage for over 95% of recording duration.\tTimestamped heart rate in beats per minute. Hand-Object Interaction Recognition\tSegments left/right hands and interacted objects, enabling analysis of manipulation patterns and object usage.\tSegmentation masks for hands and objects per RGB image. 3D Object Detection (Egocentric Voxel Lifting)\tDetects 2D and 3D bounding boxes for objects in indoor scenes using multi-camera data. Supports spatial understanding and scene reconstruction.\t2D and 3D bounding boxes with class prediction. Depth Estimation (Foundation Stereo)\tGenerates depth maps from overlapping CV cameras, enabling research in 3D scene understanding and object localization.\tDepth images, rectified CV images, and corresponding camera intrinsics/extrinsics.    ","version":"Next","tagName":"h3"},{"title":"Resources‚Äã","type":1,"pageTitle":"Aria Gen 2 Pilot Dataset","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/content#resources","content":" For more information about the Aria Gen 2 Pilot Dataset:  üìÑ ArXiv Paper: https://arxiv.org/abs/2510.16134üåê Project Website: https://www.projectaria.com/datasets/gen2pilot/üíª GitHub Repository: https://github.com/facebookresearch/projectaria_gen2_pilot_datasetüîç Dataset Explorer: https://explorer.projectaria.com/gen2pilot    ","version":"Next","tagName":"h2"},{"title":"Citation‚Äã","type":1,"pageTitle":"Aria Gen 2 Pilot Dataset","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/content#citation","content":" If you use the Aria Gen 2 Pilot Dataset in your research, please cite the following:  @misc{kong2025ariagen2pilot, title ={Aria Gen 2 Pilot Dataset}, author ={Chen Kong and James Fort and Aria Kang and Jonathan Wittmer and Simon Green and Tianwei Shen and Yipu Zhao and Cheng Peng and Gustavo Solaira and Andrew Berkovich and Nikhil Raina and Vijay Baiyya and Evgeniy Oleinik and Eric Huang and Fan Zhang and Julian Straub and Mark Schwesinger and Luis Pesqueira and Xiaqing Pan and Jakob Julian Engel and Carl Ren and Mingfei Yan and Richard Newcombe}, year ={2025}, eprint ={2510.16134}, archivePrefix ={arXiv}, primaryClass ={cs.CV}, url ={https://arxiv.org/abs/2510.16134}, }  ","version":"Next","tagName":"h2"},{"title":"Download and Usage","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/download","content":"","keywords":"","version":"Next"},{"title":"Preview the Dataset‚Äã","type":1,"pageTitle":"Download and Usage","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/download#preview-the-dataset","content":" Before downloading, you can explore the dataset using our interactive Dataset Explorer:  üîó Dataset Explorer: https://explorer.projectaria.com/gen2pilot  ","version":"Next","tagName":"h2"},{"title":"What You Can Preview‚Äã","type":1,"pageTitle":"Download and Usage","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/download#what-you-can-preview","content":" The Dataset Explorer provides:  MP4 Video Previews: Watch video clips from RGB camera streamInteractive 3D Visualization: Explore 30-second segments with Rerun visualization showing: Raw sensor data from all camerasOn-device machine perception outputs (VIO, eye gaze, hand tracking)MPS results (3D trajectories, semi-dense point clouds)Additional algorithm outputs (ASR, heart rate, depth, etc.)  This allows you to understand the data structure and quality before downloading the full dataset.  ","version":"Next","tagName":"h3"},{"title":"Download the Dataset‚Äã","type":1,"pageTitle":"Download and Usage","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/download#download-the-dataset","content":" ","version":"Next","tagName":"h2"},{"title":"Via Dataset Explorer (Recommended)‚Äã","type":1,"pageTitle":"Download and Usage","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/download#via-dataset-explorer-recommended","content":" Visit https://explorer.projectaria.com/gen2pilotBrowse the available sequencesFollow the download instructions provided on the explorer pageAccept the license agreement and terms of use  The Dataset Explorer provides detailed instructions for downloading individual sequences or the entire dataset.  ","version":"Next","tagName":"h3"},{"title":"Dataset Size‚Äã","type":1,"pageTitle":"Download and Usage","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/download#dataset-size","content":" The complete Aria Gen 2 Pilot Dataset includes:  12 sequences (4 participants per sequence)Raw VRS files with all sensor streamsOn-device machine perception outputsOffline MPS resultsAdditional perception algorithm outputs (ASR, heart rate, depth, etc.)  Please ensure you have sufficient storage space before downloading.  ","version":"Next","tagName":"h3"},{"title":"Install the Dataset Tools‚Äã","type":1,"pageTitle":"Download and Usage","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/download#install-the-dataset-tools","content":" To work with the Aria Gen 2 Pilot Dataset, install the projectaria-gen2-pilot-dataset Python package.  ","version":"Next","tagName":"h2"},{"title":"System Requirements‚Äã","type":1,"pageTitle":"Download and Usage","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/download#system-requirements","content":" Python: 3.8 - 3.12 (Python 3.12 recommended)Operating Systems: Linux (x64): Fedora 40/41, Ubuntu 20.04/22.04 LTSmacOS (ARM64): macOS 14+ (Apple Silicon)macOS (Intel): macOS 13+  ","version":"Next","tagName":"h3"},{"title":"Installation Steps‚Äã","type":1,"pageTitle":"Download and Usage","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/download#installation-steps","content":" Linux &amp; macOS # Deactivate conda if active conda deactivate # Remove existing environment if it exists rm -rf ~/projectaria_gen2_python_env # Create new Python virtual environment python3.12 -m venv ~/projectaria_gen2_python_env # Activate the environment source ~/projectaria_gen2_python_env/bin/activate # Upgrade pip python3 -m pip install --upgrade pip # Install the package with all dependencies python3 -m pip install 'projectaria-gen2-pilot-dataset[all]'   ","version":"Next","tagName":"h3"},{"title":"What Gets Installed‚Äã","type":1,"pageTitle":"Download and Usage","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/download#what-gets-installed","content":" The projectaria-gen2-pilot-dataset[all] package includes:  Core data loaders: Access VRS files, MPS outputs, and algorithm resultsVisualization tools: Rerun integration for 3D visualizationAll dependencies: Including projectaria-tools for working with Aria dataTutorial notebooks: Example code for loading and analyzing the dataset  ","version":"Next","tagName":"h3"},{"title":"Verify Installation‚Äã","type":1,"pageTitle":"Download and Usage","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/download#verify-installation","content":" After installation, verify it works by checking the package version:  python3 -c &quot;import projectaria_gen2_pilot_dataset&quot;   ","version":"Next","tagName":"h3"},{"title":"Source Code‚Äã","type":1,"pageTitle":"Download and Usage","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/download#source-code","content":" The dataset tools are open-source and available on GitHub:  üîó GitHub Repository  You can:  View the source codeReport issuesContribute improvementsAccess example scripts and notebooks  ","version":"Next","tagName":"h2"},{"title":"Dataset License‚Äã","type":1,"pageTitle":"Download and Usage","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/download#dataset-license","content":" Please review and comply with the dataset license agreement available on the Dataset Explorer. By downloading and using this dataset, you agree to the terms of use.  ","version":"Next","tagName":"h2"},{"title":"Citation‚Äã","type":1,"pageTitle":"Download and Usage","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/download#citation","content":" If you use the Aria Gen 2 Pilot Dataset in your research, please cite: To be updated. ","version":"Next","tagName":"h2"},{"title":"Aria Gen 2 Pilot Dataset Format","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/format","content":"","keywords":"","version":"Next"},{"title":"File format‚Äã","type":1,"pageTitle":"Aria Gen 2 Pilot Dataset Format","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/format#file-format","content":" closed_loop_trajectory.csv, open_loop_trajectory.csv, semidense_observations.csv.gz, semidense_points.csv.gz, online_calibration.jsonl and hand_tracking_results.csv follow MPS data format.  ","version":"Next","tagName":"h2"},{"title":"diarization_results.csv‚Äã","type":1,"pageTitle":"Aria Gen 2 Pilot Dataset Format","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/format#diarization_resultscsv","content":" Column\tType\tDescriptionstart_timestamp_ns\tint\tTimestamp, in nanoseconds, in device time domain. end_timestamp_ns\tint\tTimestamp, in nanoseconds, in device time domain. speaker\tstring\tUnique identifier of the speaker content\tstring\tThe ASR results in text.  ","version":"Next","tagName":"h3"},{"title":"2d_bounding_box.csv‚Äã","type":1,"pageTitle":"Aria Gen 2 Pilot Dataset Format","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/format#2d_bounding_boxcsv","content":" Column\tType\tDescriptionstream_id\tstring\tcamera stream id associated with the bounding box image object_uid\tuint64_t\tid of the object instance timestamp[ns]\tint64_t\ttimestamp of the image in nanoseconds x_min[pixel]\tint\tminimum dimension in the x axis x_max[pixel]\tint\tmaximum dimension in the x axis y_min[pixel]\tint\tminimum dimension in the y axis y_max[pixel]\tint\tmaximum dimension in the y axis visibility_ratio[%]\tdouble\tpercentage of the object that is visible (0: not visible, 1: fully visible). NOTE: for EVL this is estimated from semidense points and is NOT accurate and fills -1.  ","version":"Next","tagName":"h3"},{"title":"3d_bounding_box.csv‚Äã","type":1,"pageTitle":"Aria Gen 2 Pilot Dataset Format","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/format#3d_bounding_boxcsv","content":" Column\tType\tDescriptionobject_uid\tuint64_t\tid of the instance timestamp[ns]\tint64_t\ttimestamp of the image in nanoseconds. -1 means the instance is static p_local_obj_xmin[m]\tdouble\tminimum dimension in the x axis (in meters) of the bounding box p_local_obj_xmax[m]\tdouble\tmaximum dimension in the x axis (in meters) of the bounding box p_local_obj_ymin[m]\tdouble\tminimum dimension in the y axis (in meters) of the bounding box p_local_obj_ymax[m]\tdouble\tmaximum dimension in the y axis (in meters) of the bounding box p_local_obj_zmin[m]\tdouble\tminimum dimension in the z axis (in meters) of the bounding box p_local_obj_zmax[m]\tdouble\tmaximum dimension in the z axis (in meters) of the bounding box  ","version":"Next","tagName":"h3"},{"title":"instances.json‚Äã","type":1,"pageTitle":"Aria Gen 2 Pilot Dataset Format","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/format#instancesjson","content":" { object_id:{ &quot;canonical_pose&quot;:{&quot;front_vector&quot;:unit_vector,&quot;up_vector&quot;:unit_vector}, &quot;category&quot;:xxx, &quot;category_uid&quot;:xxx, &quot;instance_id&quot;:xxx, &quot;instance_name&quot;:text description, &quot;instance_type&quot;:&quot;object&quot;, &quot;motion_type&quot;:&quot;static&quot;, &quot;prototype_name&quot;:text description,\u000b&quot;rigidity&quot;: &quot;rigid&quot;, &quot;rotational_symmetry&quot;:{&quot;is_annotated&quot;:false} }, ... # example &quot;12&quot;:{ &quot;canonical_pose&quot;:{&quot;front_vector&quot;:[0,0,1],&quot;up_vector&quot;:[0,1,0]},&quot; category&quot;:&quot;Screen&quot;, &quot;category_uid&quot;:10, &quot;instance_id&quot;:12, &quot;instance_name&quot;:&quot;screen, television, laptop screen (not keyboard), tablet screen, computer monitor, display, not mobile phone screen&quot;, &quot;instance_type&quot;:&quot;object&quot;, &quot;motion_type&quot;:&quot;static&quot;, &quot;prototype_name&quot;:&quot;screen, television, laptop screen (not keyboard), tablet screen, computer monitor, display, not mobile phone screen&quot;, &quot;rigidity&quot;:&quot;rigid&quot;, &quot;rotational_symmetry&quot;:{&quot;is_annotated&quot;:false} }, ... }   ","version":"Next","tagName":"h3"},{"title":"scene\\_objects.csv‚Äã","type":1,"pageTitle":"Aria Gen 2 Pilot Dataset Format","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/format#scene_objectscsv","content":" Column\tType\tDescriptionobject_uid\tuint64_t\tid of the object instance timestamp[ns]\tint64_t\ttimestamp of the image in nanoseconds. -1 means the instance is static t_wo_x[m]\tdouble\tx translation from object frame to world (scene) frame (in meters) t_wo_y[m]\tdouble\ty translation from object frame to world (scene) frame (in meters) t_wo_z[m]\tdouble\tz translation from object frame to world (scene) frame (in meters) q_wo_w\tdouble\tw component of quaternion from object frame to world (scene) frame q_wo_x\tdouble\tx component of quaternion from object frame to world (scene) frame q_wo_y\tdouble\ty component of quaternion from object frame to world (scene) frame q_wo_z\tdouble\tz component of quaternion from object frame to world (scene) frame  ","version":"Next","tagName":"h3"},{"title":"heart_rate_results.csv‚Äã","type":1,"pageTitle":"Aria Gen 2 Pilot Dataset Format","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/format#heart_rate_resultscsv","content":" Column\tType\tDescriptiontimestamp_ns\tint\tTimestamp, in nanoseconds, in device time domain. heart_rate_bpm\tint\tThe estimated heart rate (beats per minutes) at the specific timestamp  ","version":"Next","tagName":"h3"},{"title":"Depth Folder‚Äã","type":1,"pageTitle":"Aria Gen 2 Pilot Dataset Format","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/format#depth-folder","content":" Depth output consists of the following files in a single directory named ‚Äúdepth‚Äù:  Rectified depth maps as 512 x 512, 16-bit grayscale PNG images. The pixel contents are integers expressing the depth along the pixel‚Äôs ray direction, in units of mm. This is the same format used in ASE. depth_{:08d}.png Matching rectified front-left SLAM camera images as 8-bit grayscale PNGs. image_{:08d}.png A JSON file containing camera transforms and intrinsics for the rectified pinhole camera, for each frame. pinhole_camera_parameters.json  Example JSON:  [ { &quot;T_world_camera&quot;: { &quot;QuaternionXYZW&quot;: [ -0.56967133283615112, 0.35075613856315613, 0.60195386409759521, 0.4360002875328064 ], &quot;Translation&quot;: [ -0.0005431128665804863, 0.0053895660676062107, -0.0027622696943581104 ] }, &quot;camera&quot;: { &quot;ModelName&quot;: &quot;Linear:fu,fv,u0,v0&quot;, &quot;Parameters&quot;: [ 306.38043212890625, 306.38043212890625, 254.6942138671875, 257.29779052734375 ] }, &quot;index&quot;: 0, &quot;frameTimestampNs&quot;: 34234234234, } ]   ","version":"Next","tagName":"h3"},{"title":"hand_object_interaction_results.json‚Äã","type":1,"pageTitle":"Aria Gen 2 Pilot Dataset Format","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/format#hand_object_interaction_resultsjson","content":" Standard COCO json format, with category_id 1 (left hand) 2 (right hand) 3 (hand interacting object). Example Json file:  [ { &quot;segmentation&quot;: { &quot;size&quot;: [ 1512, 2016 ], &quot;counts&quot;: &quot;TUk`13U_10L8L0K7N0UJOYlN9eS1G[lN9aS11okN=mS1CSlN=kS1GokNa0a1kNRl0d0]ROa0a1kNnk0]1aPOhNSN`1Q5dNYl0T1cPOhNSN`1Q5dNYl0T1cPOhNSN`1Q5dNQl0T2mnNT3V1QMko0KonNT3V1QMio0Z7jnNRIVQ1n6jnNRIVQ1`:00N200L400M3000000N200N200N2000000N200000002N00000N4N0N200N4N0000002N00000N4N0000002N004L004L0000002N002N00000000000020N02N002N0020N02N0020N03M002N002N00f0ZO004L002N00000N4N000000000000000000000000000000000002L2000000000000000002N000N20000000000000000000000000000002N00000000000000000000N2000N20N20000000N2000000000N202N00000000000000000000000000000000000000000000000N20000000000000000N202N00000000000N20000000N200000000000000N200N20000000000000N02000000000000000000000N202N000N202L202L202L200N20000N200N200N202I504hNT102iNU103K202H606XOb004mJYmNTMPS1l2PmNTMRS1f2VmNRMnR1n2RmNRMPS1c2amNaLSS1_3mlNaLdU1^Ok50?VO;06^O&lt;0de]n0&quot; }, &quot;bbox&quot;: [ 1050.1774193548385, 738.1073369565217, 325.16129032258095, 535.0801630434783 ], &quot;score&quot;: 1.0, &quot;image_id&quot;: 2620886, # timestamp_ns = image_id * 1e6 &quot;category_id&quot;: 3 }, ... ]```  ","version":"Next","tagName":"h3"},{"title":"Aria Gen2 Pilot Dataset Tutorial - MPS Data Loading","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/mps_loading","content":"","keywords":"","version":"Next"},{"title":"Introduction‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - MPS Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/mps_loading#introduction","content":" Machine Perception Services, or MPS, is a post-processing cloud service that we provide to Aria users. It runs a set of proprietary Spatial AI machine perception algorithms that are designed for Project Aria glasses. MPS is designed to provide superior accuracy and robustness compared to off-the-shelf open algorithms.  Currently, the supported MPS algorithms for Aria Gen2 include:  SLAM: Single Sequence Trajectory and Semi-dense point cloud generation.Hand Tracking: 21 landmarks, wrist to device transformation, wrist and palm positions and normals.  This tutorial focuses on demonstrating how to load and visualize the MPS results.  What You'll Learn  How to load MPS output data, and definitions of the data types.How to visualize the MPS dataUnderstanding the difference between MPS and on-device perception outputs  ","version":"Next","tagName":"h2"},{"title":"Dataset Structure‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - MPS Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/mps_loading#dataset-structure","content":" The Aria Gen2 Pilot Dataset comprises four primary data content types:  Raw sensor streams acquired directly from Aria Gen2 devicesReal-time machine perception outputs generated on-device via embedded algorithms during data collectionOffline machine perception results produced by Machine Perception Services (MPS) during post-processing (focus of this tutorial)Outputs from additional offline perception algorithms  Each sequence folder contains an mps/ directory with the following structure:  mps/ ‚îú‚îÄ‚îÄ slam/ ‚îÇ ‚îú‚îÄ‚îÄ closed_loop_trajectory.csv ‚îÇ ‚îú‚îÄ‚îÄ open_loop_trajectory.csv ‚îÇ ‚îú‚îÄ‚îÄ semidense_observations.csv.gz ‚îÇ ‚îú‚îÄ‚îÄ semidense_points.csv.gz ‚îÇ ‚îú‚îÄ‚îÄ online_calibration.jsonl ‚îÇ ‚îî‚îÄ‚îÄ summary.json ‚îî‚îÄ‚îÄ hand_tracking/ ‚îú‚îÄ‚îÄ hand_tracking_results.csv ‚îî‚îÄ‚îÄ summary.json   ","version":"Next","tagName":"h2"},{"title":"Initialize Data Provider‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - MPS Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/mps_loading#initialize-data-provider","content":" ‚ö†Ô∏è Important: Update the sequence_path below to point to your downloaded Aria Gen2 Pilot Dataset sequence folder.  import numpy as np from aria_gen2_pilot_dataset import AriaGen2PilotDataProvider from projectaria_tools.core.sensor_data import TimeDomain from projectaria_tools.core import mps from aria_gen2_pilot_dataset.visualization.plot_style import get_plot_style, PlotEntity, PlotStyle from projectaria_tools.utils.rerun_helpers import ( create_hand_skeleton_from_landmarks, ToTransform3D, ) import rerun as rr import rerun.blueprint as rrb   # TODO: Update this path to your dataset location sequence_path = &quot;path/to/your/sequence_folder&quot; pilot_data_provider = AriaGen2PilotDataProvider(sequence_path)   ","version":"Next","tagName":"h2"},{"title":"MPS - SLAM‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - MPS Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/mps_loading#mps---slam","content":" ","version":"Next","tagName":"h2"},{"title":"[MPS - SLAM] Output Files‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - MPS Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/mps_loading#mps---slam-output-files","content":" MPS output result files are categorized into sub-folders by algorithm. For SLAM algorithm output, it generates the following files:  closed_loop_trajectory.csvopen_loop_trajectory.csvsemidense_observations.csv.gzsemidense_points.csv.gzonline_calibration.jsonlsummary.json  Please refer to the MPS Wiki page for details of each file.  ","version":"Next","tagName":"h3"},{"title":"[MPS - SLAM] Semi-dense Point Cloud and Observations‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - MPS Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/mps_loading#mps---slam-semi-dense-point-cloud-and-observations","content":" The MPS SLAM algorithm outputs 2 files related to semi-dense point cloud (see wiki page for data type definitions):  semidense_points.csv.gz: Global points in the world coordinate frame.semidense_observations.csv.gz: Point observations for each camera, at each timestamp.  Note that semidense point files are normally large, therefore loading them may take some time.  Point Cloud Data Types‚Äã  GlobalPointPosition contains:  uid: Unique identifier for the 3D pointgraph_uid: Identifier linking point to pose graphposition_world: 3D position in world coordinate frameinverse_distance_std: Inverse distance standard deviation (quality metric)distance_std: Distance standard deviation (quality metric)  PointObservation contains:  point_uid: Links observation to 3D pointframe_capture_timestamp: When the observation was capturedcamera_serial: Serial number of the observing camerauv: 2D pixel coordinates of the observation  Filtering Point Cloud by Confidence‚Äã  You can filter the semi-dense point cloud using confidence thresholds based on inverse_distance_std or distance_std to improve quality. The AriaGen2PilotDataProvider provides a convenient method get_mps_semidense_point_cloud_filtered() for this purpose.  print(&quot;=== MPS - Semi-dense Point Cloud ===&quot;) semi_dense_point_cloud = pilot_data_provider.get_mps_semidense_point_cloud() semi_dense_point_cloud_filtered = pilot_data_provider.get_mps_semidense_point_cloud_filtered(filter_confidence=True, max_point_count=50000) # Print out the content of the first sample in semidense_points if semi_dense_point_cloud: sample = semi_dense_point_cloud[0] print(&quot;GlobalPointPosition sample:&quot;) print(f&quot; uid: {sample.uid}&quot;) print(f&quot; graph_uid: {sample.graph_uid}&quot;) print(f&quot; position_world: {sample.position_world}&quot;) print(f&quot; inverse_distance_std: {sample.inverse_distance_std}&quot;) print(f&quot; distance_std: {sample.distance_std}&quot;) print(f&quot;Total number of semi-dense points: {len(semi_dense_point_cloud)}&quot;) print(f&quot;Total number of filtered semi-dense points: {len(semi_dense_point_cloud_filtered)}&quot;) else: print(&quot;semidense_points is empty.&quot;)   ","version":"Next","tagName":"h3"},{"title":"[MPS - SLAM] Closed vs Open Loop Trajectory‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - MPS Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/mps_loading#mps---slam-closed-vs-open-loop-trajectory","content":" The MPS SLAM algorithm outputs 2 trajectory files open_loop_trajectory.csv and closed_loop_trajectory.csv (see wiki page for data type definitions):  Open loop trajectory: High-frequency (1kHz) odometry from visual-inertial odometry (VIO), accurate over short periods but drifts over time and distance. Closed loop trajectory: High-frequency (1kHz) pose from mapping with loop closure corrections, reducing drift but possibly less accurate locally over short spans. For closed loop, an additional interpolation API is provided.  Key Differences‚Äã  Aspect\tOpen Loop (VIO)\tClosed Loop (SLAM)Coordinate Frame\tOdometry frame\tGlobal world frame Drift\tAccumulates over time\tMinimized with loop closure Accuracy\tGood for short periods\tHigher global accuracy Use Case\tReal-time odometry\tHigh-quality reconstruction  Data Types‚Äã  ClosedLoopTrajectoryPose contains:  tracking_timestamp: Device timestamp when pose was computedtransform_world_device: 6DOF pose in world coordinate framedevice_linear_velocity_device: Linear velocity in device frameangular_velocity_device: Angular velocity in device framequality_score: Pose estimation quality (higher = better)gravity_world: Gravity vector in world frame  OpenLoopTrajectoryPose contains:  tracking_timestamp: Device timestamp when pose was computedtransform_odometry_device: 6DOF pose in odometry coordinate framedevice_linear_velocity_odometry: Linear velocity in odometry frameangular_velocity_device: Angular velocity in device framequality_score: Pose estimation quality (higher = better)gravity_odometry: Gravity vector in odometry frame  print(&quot;=== MPS - query whole trajectory ===&quot;) mps_closed_loop_trajectory = pilot_data_provider.get_mps_closed_loop_trajectory() mps_closed_loop_trajectory_duration = mps_closed_loop_trajectory[-1].tracking_timestamp.total_seconds() - mps_closed_loop_trajectory[0].tracking_timestamp.total_seconds() print(&quot;MPS Closed Loop Trajectory duration: &quot; + f&quot;{mps_closed_loop_trajectory_duration:.2f}&quot;) mps_open_loop_trajectory = pilot_data_provider.get_mps_open_loop_trajectory() mps_open_loop_trajectory_duration = mps_open_loop_trajectory[-1].tracking_timestamp.total_seconds() - mps_open_loop_trajectory[0].tracking_timestamp.total_seconds() print(&quot;MPS Open Loop Trajectory duration: &quot; + f&quot;{mps_open_loop_trajectory_duration:.2f}&quot;) print(&quot;\\n&quot;) print(&quot;=== MPS - query pose by timestamp ===&quot;) query_timestamp_ns = int((mps_closed_loop_trajectory[0].tracking_timestamp.total_seconds()+1)*1e9) print(&quot;query timestamp ns:&quot;, query_timestamp_ns, '\\n') if mps_closed_loop_trajectory_duration &gt; 1: # If duration &gt; 1s nearest_mps_closed_loop_pose = pilot_data_provider.get_mps_closed_loop_pose(timestamp_ns=query_timestamp_ns, time_domain=TimeDomain.DEVICE_TIME) print(&quot;Nearest mps closed loop pose: &quot;, nearest_mps_closed_loop_pose, &quot;\\n&quot;) interpolated_mps_closed_loop_pose = pilot_data_provider.get_mps_interpolated_closed_loop_pose(timestamp_ns=query_timestamp_ns, time_domain=TimeDomain.DEVICE_TIME) print(&quot;Interpolated mps closed loop pose: &quot;, interpolated_mps_closed_loop_pose, &quot;\\n&quot;) if mps_open_loop_trajectory_duration &gt; 1: # If duration &gt; 1s query_timestamp_ns = int((mps_open_loop_trajectory[0].tracking_timestamp.total_seconds()+1)*1e9) nearest_mps_open_loop_pose = pilot_data_provider.get_mps_open_loop_pose(timestamp_ns=query_timestamp_ns, time_domain=TimeDomain.DEVICE_TIME) print(&quot;Nearest mps closed loop pose: &quot;, nearest_mps_open_loop_pose, &quot;\\n&quot;)   ","version":"Next","tagName":"h3"},{"title":"MPS - Hand Tracking‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - MPS Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/mps_loading#mps---hand-tracking","content":" The MPS Hand Tracking algorithm outputs 2 files related to hand-tracking: hand_tracking_results.csv and summary.json. The MPS Hand Tracking algorithm outputs 2 files related to hand-tracking: hand_tracking_results.csv and summary.json. See wiki page for data type definitions.  ","version":"Next","tagName":"h2"},{"title":"Hand Tracking Features‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - MPS Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/mps_loading#hand-tracking-features","content":" MPS Hand Tracking provides the following outputs for each detected hand:  21 landmarks: Detailed hand joint positionsWrist to device transformation: Spatial relationship between wrist and deviceWrist and palm positions: Key reference points for hand posePalm normals: Surface orientation for interaction analysis  ","version":"Next","tagName":"h3"},{"title":"Query Methods‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - MPS Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/mps_loading#query-methods","content":" The AriaGen2PilotDataProvider offers two query methods for hand tracking data:  get_mps_hand_tracking_result(): Returns the nearest hand tracking result for a given timestampget_mps_interpolated_hand_tracking_result(): Returns interpolated hand tracking result for smoother motion analysis  print(&quot;=== MPS - query whole hand tracking results ===&quot;) mps_hand_tracking_results = pilot_data_provider.get_mps_hand_tracking_result_list() mps_hand_tracking_results_duration = mps_hand_tracking_results[-1].tracking_timestamp.total_seconds() - mps_hand_tracking_results[0].tracking_timestamp.total_seconds() print(&quot;MPS hand tracking results duration: &quot; + f&quot;{mps_hand_tracking_results_duration:.2f}&quot;, &quot;\\n&quot;) print(&quot;=== MPS - query hand tracking result by timestamp ===&quot;) if mps_hand_tracking_results_duration &gt; 1: # If duration &gt; 1s query_timestamp_ns = int((mps_hand_tracking_results[0].tracking_timestamp.total_seconds()+1)*1e9) print(&quot;Query timestamp ns:&quot;, query_timestamp_ns, '\\n') nearest_mps_hand_tracking_result = pilot_data_provider.get_mps_hand_tracking_result(timestamp_ns=query_timestamp_ns, time_domain=TimeDomain.DEVICE_TIME) print(&quot;Nearest tracking result: &quot;, nearest_mps_hand_tracking_result, &quot;\\n&quot;) interpolated_mps_hand_tracking_result = pilot_data_provider.get_mps_interpolated_hand_tracking_result(timestamp_ns=query_timestamp_ns, time_domain=TimeDomain.DEVICE_TIME) print(&quot;Interpolated tracking result: &quot;, interpolated_mps_hand_tracking_result, &quot;\\n&quot;)   ","version":"Next","tagName":"h3"},{"title":"Visualization‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - MPS Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/mps_loading#visualization","content":" The following section shows how to plot:  Semi-dense point cloudClosed loop trajectoryHand pose results  ","version":"Next","tagName":"h2"},{"title":"Visualization Notes‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - MPS Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/mps_loading#visualization-notes","content":" Point Cloud Filtering: For better visualization performance, we filter the point cloud by confidence and limit the maximum point countTrajectory Caching: We accumulate trajectory points over time to visualize the full pathRerun Integration: We use Rerun for interactive 3D visualization with proper coordinate frames  import rerun as rr def plot_mps_semidense_point_cloud( point_cloud_data: list[mps.GlobalPointPosition] ) -&gt; None: if point_cloud_data == []: return points_array = np.array( [ point.position_world for point in point_cloud_data if hasattr(point, &quot;position_world&quot;) ] ) plot_style = get_plot_style(PlotEntity.SEMI_DENSE_POINT_CLOUD) rr.log( f&quot;world/{plot_style.label}&quot;, rr.Points3D( positions=points_array, colors=[] * len(points_array), radii=plot_style.plot_3d_size, ), static=True, ) def plot_closed_loop_pose( closed_loop_trajectory_pose: mps.ClosedLoopTrajectoryPose, ) -&gt; None: &quot;&quot;&quot;Plot MPS closed loop trajectory&quot;&quot;&quot; if not closed_loop_trajectory_pose: return # Get transform and add to trajectory cache T_world_device = closed_loop_trajectory_pose.transform_world_device closed_loop_trajectory_pose_cache.append(T_world_device.translation()[0]) # Plot device pose rr.log( &quot;world/device&quot;, ToTransform3D(T_world_device, axis_length=0.05), ) # Plot accumulated trajectory if len(closed_loop_trajectory_pose_cache) &gt; 1: plot_style = get_plot_style(PlotEntity.TRAJECTORY) rr.log( f&quot;world/{plot_style.label}&quot;, rr.LineStrips3D( [closed_loop_trajectory_pose_cache], colors=[plot_style.color], radii=plot_style.plot_3d_size, ), ) def _get_hand_plot_style(hand_label: str) -&gt; PlotStyle: if hand_label == &quot;left&quot;: landmarks_plot_entity = PlotEntity.HAND_TRACKING_LEFT_HAND_LANDMARKS skeleton_plot_entity = PlotEntity.HAND_TRACKING_LEFT_HAND_SKELETON else: landmarks_plot_entity = PlotEntity.HAND_TRACKING_RIGHT_HAND_LANDMARKS skeleton_plot_entity = PlotEntity.HAND_TRACKING_RIGHT_HAND_SKELETON return get_plot_style(landmarks_plot_entity), get_plot_style( skeleton_plot_entity ) def _plot_single_hand_3d( hand_joints_in_device: list[np.array], hand_label: str, ) -&gt; None: &quot;&quot;&quot; Plot single hand data in 3D and 2D camera views &quot;&quot;&quot; landmarks_style, skeleton_style = _get_hand_plot_style(hand_label=hand_label) if hand_joints_in_device is None: return # Plot 3D hand markers and skeleton hand_skeleton_3d = create_hand_skeleton_from_landmarks(hand_joints_in_device) rr.log( f&quot;world/device/hand-tracking/{hand_label}/{landmarks_style.label}&quot;, rr.Points3D( positions=hand_joints_in_device, colors=[landmarks_style.color], radii=landmarks_style.plot_3d_size, ), ) rr.log( f&quot;world/device/hand-tracking/{hand_label}/{skeleton_style.label}&quot;, rr.LineStrips3D( hand_skeleton_3d, colors=[skeleton_style.color], radii=skeleton_style.plot_3d_size, ), ) def plot_mps_hand_tracking_result_3d( hand_pose_data: mps.hand_tracking.HandTrackingResult, ) -&gt; None: &quot;&quot;&quot; Plot hand pose data within 3D world view &quot;&quot;&quot; rr.log( &quot;world/device/hand-tracking&quot;, rr.Clear.recursive(), ) if hand_pose_data is None: return if hand_pose_data.left_hand is not None: _plot_single_hand_3d( hand_joints_in_device=hand_pose_data.left_hand.landmark_positions_device, hand_label=&quot;left&quot;, ) if hand_pose_data.right_hand is not None: _plot_single_hand_3d( hand_joints_in_device=hand_pose_data.right_hand.landmark_positions_device, hand_label=&quot;right&quot;, ) closed_loop_trajectory_pose_cache=[] open_loop_trajectory_pose_cache=[] def plot_mps(): rr.init(&quot;rerun_viz_mps&quot;) # Create a Spatial3D view to display the points. blueprint = rrb.Blueprint( rrb.Spatial3DView( origin=&quot;/&quot;, name=&quot;3D Scene&quot;, # Set the background color to light blue. background=[0, 0, 0], ), collapse_panels=True, ) rr.notebook_show(blueprint=blueprint) # plot semi-dense point cloud plot_mps_semidense_point_cloud(semi_dense_point_cloud_filtered) for hand_tracking_result in mps_hand_tracking_results: closed_loop_pose = pilot_data_provider.get_mps_interpolated_closed_loop_pose(int(hand_tracking_result.tracking_timestamp.total_seconds()*1e9), TimeDomain.DEVICE_TIME) # plot hand tracking result plot_mps_hand_tracking_result_3d(hand_tracking_result) # plot closed loop pose plot_closed_loop_pose(closed_loop_pose) plot_mps()   ","version":"Next","tagName":"h3"},{"title":"Summary‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - MPS Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/mps_loading#summary","content":" This tutorial covered the essential aspects of working with MPS data in the Aria Gen2 Pilot Dataset:  ","version":"Next","tagName":"h2"},{"title":"Key Takeaways‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - MPS Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/mps_loading#key-takeaways","content":" MPS SLAM Trajectories: Understanding the difference between open loop (VIO-based, with drift) and closed loop (globally optimized, minimal drift) trajectoriesSemi-dense Point Cloud: Accessing high-quality 3D reconstructions with confidence-based filteringHand Tracking: Loading and visualizing detailed hand pose data with 21 landmarks per handData Querying: Using both nearest-neighbor and interpolated queries for smooth temporal access3D Visualization: Creating interactive visualizations with Rerun  ","version":"Next","tagName":"h3"},{"title":"MPS vs On-Device Data‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - MPS Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/mps_loading#mps-vs-on-device-data","content":" Aspect\tOn-Device (VIO/Tracking)\tMPS (Post-processing)Processing\tReal-time during recording\tCloud-based offline processing Accuracy\tGood for real-time use\tHigher accuracy with global optimization Latency\tImmediate availability\tRequires post-processing time Drift\tAccumulates over time\tMinimized with loop closure (SLAM) Point Cloud\tNot available\tDense semi-dense reconstructions Coordinate Frame\tOdometry/device frame\tGlobal world frame Use Cases\tLive feedback, real-time apps\tResearch, high-quality reconstruction, benchmarking ","version":"Next","tagName":"h3"},{"title":"Aria Gen2 Pilot Dataset Tutorial - Multi-Sequence Timestamp Alignment","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/multi_sequences_timestamp_alignment","content":"","keywords":"","version":"Next"},{"title":"What You'll Learn‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - Multi-Sequence Timestamp Alignment","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/multi_sequences_timestamp_alignment#what-youll-learn","content":" Understanding SubGHz time timestamp alignment between multiple Aria Gen2 devicesConverting timestamps between host and client devicesQuerying timestamp-aligned sensor data across multiple recordingsVisualizing timestamp-aligned RGB frames from multiple devicesPlotting trajectories and hand tracking from multiple devices in a shared world coordinate frame  ","version":"Next","tagName":"h2"},{"title":"Prerequisites‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - Multi-Sequence Timestamp Alignment","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/multi_sequences_timestamp_alignment#prerequisites","content":" Complete Tutorial 1 (VRS Data Loading) to understand basic data provider conceptsComplete Tutorial 2 (MPS Data Loading) to understand MPS trajectories and hand trackingDownload a multi-device sequence from the Aria Gen2 Pilot Dataset  ","version":"Next","tagName":"h2"},{"title":"SubGHz Timestamp Alignment Overview‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - Multi-Sequence Timestamp Alignment","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/multi_sequences_timestamp_alignment#subghz-timestamp-alignment-overview","content":" During multi-device recording, Aria Gen2 glasses use SubGHz radio signals for timestamp alignment:  Host Device: One device acts as the host, actively broadcasting SubGHz signals to a specified channelClient Device(s): Other devices act as clients, receiving SubGHz signals and recording a Time Domain Mapping data stream in their VRS file  Important Notes:  The time domain mapping stream only exists in client VRS files, not in the host VRSThis mapping enables converting timestamps: host DEVICE_TIME ‚Üî client DEVICE_TIMEMPS trajectories from timestamp-aligned recordings share the same world coordinate frame  ","version":"Next","tagName":"h2"},{"title":"Import Required Libraries‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - Multi-Sequence Timestamp Alignment","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/multi_sequences_timestamp_alignment#import-required-libraries","content":" The following libraries are required for this tutorial:  # Standard library imports import numpy as np import os from pathlib import Path # Project Aria Tools imports from projectaria_tools.core.stream_id import StreamId from projectaria_tools.core.sensor_data import TimeDomain, TimeQueryOptions, TimeSyncMode from projectaria_tools.core import mps from projectaria_tools.utils.rerun_helpers import ( create_hand_skeleton_from_landmarks, ToTransform3D ) # Aria Gen2 Pilot Dataset imports from aria_gen2_pilot_dataset import AriaGen2PilotDataProvider from aria_gen2_pilot_dataset.visualization.plot_style import get_plot_style, PlotEntity # Visualization library import rerun as rr import rerun.blueprint as rrb   ","version":"Next","tagName":"h2"},{"title":"Part 1: Single-Device Timestamp Alignment‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - Multi-Sequence Timestamp Alignment","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/multi_sequences_timestamp_alignment#part-1-single-device-timestamp-alignment","content":" Before diving into multi-device synchronization, let's understand how timestamp alignment works within a single Aria Gen2 device.  ","version":"Next","tagName":"h2"},{"title":"Understanding Time Domains‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - Multi-Sequence Timestamp Alignment","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/multi_sequences_timestamp_alignment#understanding-time-domains","content":" In projectaria_tools, every timestamp is linked to a specific TimeDomain, which represents the time reference or clock used to generate that timestamp. Timestamps from different TimeDomains are not directly comparable‚Äîonly timestamps within the same TimeDomain are consistent and can be accurately compared or aligned.  Supported Time Domains for Aria Gen2‚Äã  Important: Use DEVICE_TIME for single-device Aria data analysis  Time Domain\tDescription\tUsageDEVICE_TIME (Recommended)\tCapture time in device's time domain. Accurate and reliable. All sensors on the same Aria device share the same device time domain.\tUse this for single-device Aria data analysis RECORD_TIME\tTimestamps stored in the index of VRS files. For Aria glasses, these are equal to device timestamp converted to double-precision floating point.\tFast access, but use DEVICE_TIME for accuracy HOST_TIME\tTimestamps when sensor data is saved to the device (not when captured).\tShould not be needed for any purpose  For multi-device time alignment (covered in Part 2), we use:  SUBGHZ: Multi-device time alignment for Aria Gen2 using SubGHz signals  ","version":"Next","tagName":"h3"},{"title":"Load a Single Sequence‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - Multi-Sequence Timestamp Alignment","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/multi_sequences_timestamp_alignment#load-a-single-sequence","content":" Let's start by loading a single Aria Gen2 sequence to demonstrate timestamp-based queries:  ‚ö†Ô∏è Important: Update the path below to point to your downloaded sequence folder.  # TODO: Update this path to your dataset location sequence_path = &quot;path/to/your/sequence_folder&quot; # Initialize data provider print(&quot;Loading sequence data...&quot;) pilot_data_provider = AriaGen2PilotDataProvider(sequence_path) print(&quot;\\n&quot; + &quot;=&quot;*60) print(&quot;Data Loaded Successfully!&quot;) print(&quot;=&quot;*60)   ","version":"Next","tagName":"h3"},{"title":"Data API to Query by Timestamp‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - Multi-Sequence Timestamp Alignment","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/multi_sequences_timestamp_alignment#data-api-to-query-by-timestamp","content":" The data provider offers powerful timestamp-based data access through the get_vrs_$SENSOR_data_by_time_ns() API family. This is the recommended approach for temporal alignment across sensors and precise timestamp-based data retrieval.  For any sensor type, you can query data by timestamp using these functions:  get_vrs_image_data_by_time_ns() - Query image data (RGB, SLAM cameras)get_vrs_imu_data_by_time_ns() - Query IMU dataget_vrs_audio_data_by_time_ns() - Query audio dataAnd more...  TimeQueryOptions‚Äã  The TimeQueryOptions parameter controls how the system finds data when your query timestamp doesn't exactly match a recorded timestamp:  Option\tBehavior\tUse CaseBEFORE\tReturns the last valid data with timestamp ‚â§ query_time\tDefault and most common - Get the most recent data before or at the query time AFTER\tReturns the first valid data with timestamp ‚â• query_time\tGet the next available data after or at the query time CLOSEST\tReturns data with smallest `\ttimestamp - query_time  Boundary Behavior‚Äã  The API handles edge cases automatically:  Query Condition\tBEFORE\tAFTER\tCLOSESTquery_time &lt; first_timestamp\tReturns invalid data\tReturns first data\tReturns first data first_timestamp ‚â§ query_time ‚â§ last_timestamp\tReturns data with timestamp ‚â§ query_time\tReturns data with timestamp ‚â• query_time\tReturns temporally closest data query_time &gt; last_timestamp\tReturns last data\tReturns invalid data\tReturns last data  Let's demonstrate timestamp-based queries:  print(&quot;=&quot;*60) print(&quot;Single Device Timestamp-Based Query Example&quot;) print(&quot;=&quot;*60) # Select RGB stream ID rgb_stream_id = pilot_data_provider.get_vrs_stream_id_from_label(&quot;camera-rgb&quot;) # Get a timestamp within the recording (3 seconds after start) start_timestamp_ns = pilot_data_provider.vrs_data_provider.get_first_time_ns(rgb_stream_id, TimeDomain.DEVICE_TIME) selected_timestamp_ns = start_timestamp_ns + int(3e9) print(f&quot;\\nQuery timestamp: {selected_timestamp_ns} ns (3 seconds after start)&quot;) # Fetch the RGB frame that is CLOSEST to this selected timestamp_ns closest_rgb_data, closest_rgb_record = pilot_data_provider.get_vrs_image_data_by_time_ns( stream_id=rgb_stream_id, time_ns=selected_timestamp_ns, time_domain=TimeDomain.DEVICE_TIME, time_query_options=TimeQueryOptions.CLOSEST ) closest_timestamp_ns = closest_rgb_record.capture_timestamp_ns closest_frame_number = closest_rgb_record.frame_number print(f&quot;\\n‚úÖ CLOSEST frame to query timestamp:&quot;) print(f&quot; Frame #{closest_frame_number}&quot;) print(f&quot; Capture timestamp: {closest_timestamp_ns} ns&quot;) print(f&quot; Time difference: {abs(closest_timestamp_ns - selected_timestamp_ns) / 1e6:.2f} ms&quot;) # Fetch the frame BEFORE this frame prev_rgb_data, prev_rgb_record = pilot_data_provider.get_vrs_image_data_by_time_ns( stream_id=rgb_stream_id, time_ns=closest_timestamp_ns - 1, time_domain=TimeDomain.DEVICE_TIME, time_query_options=TimeQueryOptions.BEFORE ) prev_timestamp_ns = prev_rgb_record.capture_timestamp_ns prev_frame_number = prev_rgb_record.frame_number print(f&quot;\\n‚¨ÖÔ∏è BEFORE frame:&quot;) print(f&quot; Frame #{prev_frame_number}&quot;) print(f&quot; Capture timestamp: {prev_timestamp_ns} ns&quot;) # Fetch the frame AFTER this frame next_rgb_data, next_rgb_record = pilot_data_provider.get_vrs_image_data_by_time_ns( stream_id=rgb_stream_id, time_ns=closest_timestamp_ns + 1, time_domain=TimeDomain.DEVICE_TIME, time_query_options=TimeQueryOptions.AFTER ) next_timestamp_ns = next_rgb_record.capture_timestamp_ns next_frame_number = next_rgb_record.frame_number print(f&quot;\\n‚û°Ô∏è AFTER frame:&quot;) print(f&quot; Frame #{next_frame_number}&quot;) print(f&quot; Capture timestamp: {next_timestamp_ns} ns&quot;)   ","version":"Next","tagName":"h3"},{"title":"Visualizing Timestamp-Aligned Multi-Sensor Data‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - Multi-Sequence Timestamp Alignment","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/multi_sequences_timestamp_alignment#visualizing-timestamp-aligned-multi-sensor-data","content":" A common use case is to query and visualize data from multiple sensors at approximately the same timestamp. Let's demonstrate querying RGB and SLAM camera images at the same time:  Use Case: Get RGB + SLAM images at 5Hz to see timestamp-aligned multi-camera views  print(&quot;=&quot;*60) print(&quot;Multi-Sensor Synchronized Query Example&quot;) print(&quot;=&quot;*60) # Initialize a simple Rerun viewer for single-device visualization rr.init(&quot;single_device_sync_demo&quot;, spawn=False) # Get stream IDs for RGB and SLAM cameras all_labels = pilot_data_provider.vrs_data_provider.get_device_calibration().get_camera_labels() slam_labels = [label for label in all_labels if &quot;slam&quot; in label] slam_stream_ids = [pilot_data_provider.get_vrs_stream_id_from_label(label) for label in slam_labels] rgb_stream_id = pilot_data_provider.get_vrs_stream_id_from_label(&quot;camera-rgb&quot;) # Starting from +3 seconds into the recording, sample at 5Hz for 10 frames target_period_ns = int(2e8) # 200ms = 5 Hz start_timestamp_ns = pilot_data_provider.vrs_data_provider.get_first_time_ns(rgb_stream_id, TimeDomain.DEVICE_TIME) + int(3e9) print(f&quot;\\nQuerying RGB + SLAM images at 5Hz...&quot;) print(f&quot;Starting from +3 seconds, sampling 10 frames\\n&quot;) # Plot 10 samples current_timestamp_ns = start_timestamp_ns for frame_i in range(10): # Set time for Rerun rr.set_time_nanos(&quot;device_time&quot;, current_timestamp_ns) # Query and log RGB image rgb_image_data, rgb_image_record = pilot_data_provider.get_vrs_image_data_by_time_ns( stream_id=rgb_stream_id, time_ns=current_timestamp_ns, time_domain=TimeDomain.DEVICE_TIME, time_query_options=TimeQueryOptions.CLOSEST ) rr.log(&quot;single_device/rgb_image&quot;, rr.Image(rgb_image_data.to_numpy_array())) # Query and log SLAM images for slam_i, (slam_label, slam_stream_id) in enumerate(zip(slam_labels, slam_stream_ids)): slam_image_data, slam_image_record = pilot_data_provider.get_vrs_image_data_by_time_ns( stream_id=slam_stream_id, time_ns=current_timestamp_ns, time_domain=TimeDomain.DEVICE_TIME, time_query_options=TimeQueryOptions.CLOSEST ) rr.log(f&quot;single_device/{slam_label}&quot;, rr.Image(slam_image_data.to_numpy_array())) if frame_i == 0: print(f&quot;Frame {frame_i}: RGB timestamp {rgb_image_record.capture_timestamp_ns} ns&quot;) # Increment query timestamp current_timestamp_ns += target_period_ns rr.notebook_show() print(f&quot;\\n‚úÖ Successfully queried and logged 10 frames of synchronized multi-sensor data!&quot;)   ","version":"Next","tagName":"h3"},{"title":"Part 2: Multi-Device Time Alignment‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - Multi-Sequence Timestamp Alignment","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/multi_sequences_timestamp_alignment#part-2-multi-device-time-alignment","content":" Now that we understand single-device timestamp alignment, let's explore how to align timestamps across multiple Aria Gen2 devices using SubGHz signals.  ","version":"Next","tagName":"h2"},{"title":"Understanding Pilot Dataset Multi-Device Naming Convention‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - Multi-Sequence Timestamp Alignment","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/multi_sequences_timestamp_alignment#understanding-pilot-dataset-multi-device-naming-convention","content":" In the Aria Gen2 Pilot Dataset, timestamp-aligned multi-device sequences share the same base name with different numeric suffixes:  Host Device: Sequences ending with _0 (e.g., eat_0, play_0, walk_0)Client Devices: Sequences ending with _1, _2, _3, etc. (e.g., eat_1, eat_2, eat_3)  Example:  eat_0 ‚Üí Host device recordingeat_1 ‚Üí Client device 1 recordingeat_2 ‚Üí Client device 2 recordingeat_3 ‚Üí Client device 3 recording  All sequences with the same base name (e.g., all eat_*, play_*, walk_* sequences) share a SubGHz timestamp mapping and can be aligned using the time domain mapping.  ","version":"Next","tagName":"h3"},{"title":"Load Multiple Sequences‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - Multi-Sequence Timestamp Alignment","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/multi_sequences_timestamp_alignment#load-multiple-sequences","content":" We'll need data from both host and client devices. Make sure you have downloaded a multi-device sequence from the Aria Gen2 Pilot Dataset.  ‚ö†Ô∏è Important: Update the paths below to point to your downloaded host and client sequence folders.  # TODO: Update these paths to your multi-device dataset location host_sequence_path = &quot;/path/to/host_sequence&quot; client_sequence_path = &quot;/path/to/client_sequence&quot; # Initialize data providers for both devices print(&quot;Loading host device data...&quot;) host_data_provider = AriaGen2PilotDataProvider(host_sequence_path) print(&quot;Loading client device data...&quot;) client_data_provider = AriaGen2PilotDataProvider(client_sequence_path) print(&quot;\\n&quot; + &quot;=&quot;*60) print(&quot;Multi-Device Data Loaded Successfully!&quot;) print(&quot;=&quot;*60) print(f&quot;Host has MPS data: {'‚úÖ' if host_data_provider.has_mps_data() else '‚ùå'}&quot;) print(f&quot;Client has MPS data: {'‚úÖ' if client_data_provider.has_mps_data() else '‚ùå'}&quot;)   ","version":"Next","tagName":"h3"},{"title":"Understanding SubGHz Timestamp Conversion‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - Multi-Sequence Timestamp Alignment","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/multi_sequences_timestamp_alignment#understanding-subghz-timestamp-conversion","content":" The SubGHz synchronization creates a mapping between host and client device times. We can convert timestamps bidirectionally:  Host ‚Üí Client: client_vrs_provider.convert_from_synctime_to_device_time_ns(host_time, TimeSyncMode.SUBGHZ)Client ‚Üí Host: client_vrs_provider.convert_from_device_time_to_synctime_ns(client_time, TimeSyncMode.SUBGHZ)  Important: Both conversion functions are called on the client's VRS data provider, since only the client has the time domain mapping data.  Let's demonstrate timestamp conversion:  print(&quot;=&quot;*60) print(&quot;Multi-Device Timestamp Conversion Example&quot;) print(&quot;=&quot;*60) # Get RGB stream ID from host rgb_stream_id = host_data_provider.get_vrs_stream_id_from_label(&quot;camera-rgb&quot;) # Pick a timestamp in the middle of the host recording host_start_time = host_data_provider.vrs_data_provider.get_first_time_ns_all_streams(TimeDomain.DEVICE_TIME) host_end_time = host_data_provider.vrs_data_provider.get_last_time_ns_all_streams(TimeDomain.DEVICE_TIME) selected_host_timestamp_ns = (host_start_time + host_end_time) // 2 print(f&quot;\\nSelected host timestamp: {selected_host_timestamp_ns} ns&quot;) print(f&quot; (Host recording spans {(host_end_time - host_start_time) / 1e9:.2f} seconds)&quot;) # Convert from host time to client time converted_client_timestamp_ns = client_data_provider.vrs_data_provider.convert_from_synctime_to_device_time_ns( selected_host_timestamp_ns, TimeSyncMode.SUBGHZ ) print(f&quot;\\nConverted to client timestamp: {converted_client_timestamp_ns} ns&quot;) # Convert back from client time to host time (roundtrip) roundtrip_host_timestamp_ns = client_data_provider.vrs_data_provider.convert_from_device_time_to_synctime_ns( converted_client_timestamp_ns, TimeSyncMode.SUBGHZ ) print(f&quot;\\nRoundtrip back to host timestamp: {roundtrip_host_timestamp_ns} ns&quot;) # Calculate numerical difference roundtrip_error_ns = roundtrip_host_timestamp_ns - selected_host_timestamp_ns print(f&quot;\\nRoundtrip error: {roundtrip_error_ns} ns ({roundtrip_error_ns / 1e3:.3f} Œºs)&quot;) print(&quot; Note: Small numerical differences are expected due to interpolation&quot;)   ","version":"Next","tagName":"h3"},{"title":"Query APIs with TimeDomain.SUBGHZ‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - Multi-Sequence Timestamp Alignment","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/multi_sequences_timestamp_alignment#query-apis-with-timedomainsubghz","content":" Instead of manually converting timestamps, you can directly query client data using host timestamps by specifying time_domain=TimeDomain.SUBGHZ. This is more convenient when querying multiple data types.  The following example shows how to query client RGB images using host timestamps:  print(&quot;=&quot;*60) print(&quot;Query Client Data Using Host Timestamps&quot;) print(&quot;=&quot;*60) # Query host RGB image at the selected timestamp host_image_data, host_image_record = host_data_provider.get_vrs_image_data_by_time_ns( stream_id=rgb_stream_id, time_ns=selected_host_timestamp_ns, time_domain=TimeDomain.DEVICE_TIME, time_query_options=TimeQueryOptions.CLOSEST ) print(f&quot;\\nHost RGB frame:&quot;) print(f&quot; Query timestamp: {selected_host_timestamp_ns} ns&quot;) print(f&quot; Actual capture timestamp: {host_image_record.capture_timestamp_ns} ns&quot;) print(f&quot; Frame number: {host_image_record.frame_number}&quot;) print(f&quot; Image shape: {host_image_data.to_numpy_array().shape}&quot;) # Query client RGB image using the SAME host timestamp, but with TimeDomain.SUBGHZ client_image_data, client_image_record = client_data_provider.get_vrs_image_data_by_time_ns( stream_id=rgb_stream_id, time_ns=selected_host_timestamp_ns, # Using host timestamp! time_domain=TimeDomain.SUBGHZ, # Specify SUBGHZ domain time_query_options=TimeQueryOptions.CLOSEST ) print(f&quot;\\nClient RGB frame (queried with host timestamp):&quot;) print(f&quot; Query timestamp (host domain): {selected_host_timestamp_ns} ns&quot;) print(f&quot; Actual capture timestamp (client device time): {client_image_record.capture_timestamp_ns} ns&quot;) print(f&quot; Frame number: {client_image_record.frame_number}&quot;) print(f&quot; Image shape: {client_image_data.to_numpy_array().shape}&quot;) print(&quot;\\n‚úÖ Successfully queried synchronized frames from both devices!&quot;)   ","version":"Next","tagName":"h3"},{"title":"Part 3: Timestamp-Aligned Trajectory and Hand Tracking Visualization‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - Multi-Sequence Timestamp Alignment","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/multi_sequences_timestamp_alignment#part-3-timestamp-aligned-trajectory-and-hand-tracking-visualization","content":" Now we'll visualize timestamp-aligned data from both devices in 3D. Since MPS processes multi-device recordings together, the trajectories from both devices are already in the same world coordinate frame.  ","version":"Next","tagName":"h2"},{"title":"Load MPS Data‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - Multi-Sequence Timestamp Alignment","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/multi_sequences_timestamp_alignment#load-mps-data","content":" First, let's load the MPS trajectories and hand tracking data from both devices:  print(&quot;=&quot;*60) print(&quot;Loading MPS Data&quot;) print(&quot;=&quot;*60) # Load closed loop trajectories host_trajectory = host_data_provider.get_mps_closed_loop_trajectory() client_trajectory = client_data_provider.get_mps_closed_loop_trajectory() print(f&quot;\\nHost trajectory: {len(host_trajectory)} poses&quot;) print(f&quot; Duration: {(host_trajectory[-1].tracking_timestamp - host_trajectory[0].tracking_timestamp).total_seconds():.2f} seconds&quot;) print(f&quot;\\nClient trajectory: {len(client_trajectory)} poses&quot;) print(f&quot; Duration: {(client_trajectory[-1].tracking_timestamp - client_trajectory[0].tracking_timestamp).total_seconds():.2f} seconds&quot;) # Load hand tracking results host_hand_tracking = host_data_provider.get_mps_hand_tracking_result_list() client_hand_tracking = client_data_provider.get_mps_hand_tracking_result_list() print(f&quot;\\nHost hand tracking: {len(host_hand_tracking)} frames&quot;) print(f&quot;Client hand tracking: {len(client_hand_tracking)} frames&quot;) print(&quot;\\n‚úÖ MPS data loaded successfully!&quot;) print(&quot;\\n‚ö†Ô∏è Note: Both trajectories are in the same world coordinate frame&quot;)   ","version":"Next","tagName":"h3"},{"title":"Visualization Setup‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - Multi-Sequence Timestamp Alignment","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/multi_sequences_timestamp_alignment#visualization-setup","content":" We'll create a Rerun visualization with two views:  RGB View: Synchronized RGB frames from both host and client devices3D World View: Trajectories and hand tracking from both devices in the shared world coordinate frame  Let's define helper functions for plotting:  # Cache for accumulated trajectory points host_trajectory_cache = [] client_trajectory_cache = [] def plot_device_pose_and_trajectory(device_label: str, pose: mps.ClosedLoopTrajectoryPose, trajectory_cache: list, color: list): &quot;&quot;&quot; Plot device pose and accumulated trajectory in 3D world view. Args: device_label: Label for the device (e.g., &quot;host&quot; or &quot;client&quot;) pose: ClosedLoopTrajectoryPose object trajectory_cache: List to accumulate trajectory points color: RGB color for trajectory line &quot;&quot;&quot; if pose is None: return # Get transform and add to trajectory cache T_world_device = pose.transform_world_device trajectory_cache.append(T_world_device.translation()[0]) # Plot device pose rr.log( f&quot;world/{device_label}/device&quot;, ToTransform3D(T_world_device, axis_length=0.05), ) # Plot accumulated trajectory if len(trajectory_cache) &gt; 1: rr.log( f&quot;world/{device_label}/trajectory&quot;, rr.LineStrips3D( [trajectory_cache], colors=[color], radii=0.005, ), ) def plot_hand_tracking(device_label: str, hand_tracking_result: mps.hand_tracking.HandTrackingResult): &quot;&quot;&quot; Plot hand tracking landmarks and skeleton in 3D world view. Args: device_label: Label for the device (e.g., &quot;host&quot; or &quot;client&quot;) hand_tracking_result: HandTrackingResult object &quot;&quot;&quot; # Clear previous hand tracking data rr.log(f&quot;world/{device_label}/device/hand-tracking&quot;, rr.Clear.recursive()) if hand_tracking_result is None: return # Plot left hand if available if hand_tracking_result.left_hand is not None: landmarks = hand_tracking_result.left_hand.landmark_positions_device skeleton = create_hand_skeleton_from_landmarks(landmarks) landmarks_style = get_plot_style(PlotEntity.HAND_TRACKING_LEFT_HAND_LANDMARKS) skeleton_style = get_plot_style(PlotEntity.HAND_TRACKING_LEFT_HAND_SKELETON) rr.log( f&quot;world/{device_label}/device/hand-tracking/left/landmarks&quot;, rr.Points3D( positions=landmarks, colors=[landmarks_style.color], radii=landmarks_style.plot_3d_size, ), ) rr.log( f&quot;world/{device_label}/device/hand-tracking/left/skeleton&quot;, rr.LineStrips3D( skeleton, colors=[skeleton_style.color], radii=skeleton_style.plot_3d_size, ), ) # Plot right hand if available if hand_tracking_result.right_hand is not None: landmarks = hand_tracking_result.right_hand.landmark_positions_device skeleton = create_hand_skeleton_from_landmarks(landmarks) landmarks_style = get_plot_style(PlotEntity.HAND_TRACKING_RIGHT_HAND_LANDMARKS) skeleton_style = get_plot_style(PlotEntity.HAND_TRACKING_RIGHT_HAND_SKELETON) rr.log( f&quot;world/{device_label}/device/hand-tracking/right/landmarks&quot;, rr.Points3D( positions=landmarks, colors=[landmarks_style.color], radii=landmarks_style.plot_3d_size, ), ) rr.log( f&quot;world/{device_label}/device/hand-tracking/right/skeleton&quot;, rr.LineStrips3D( skeleton, colors=[skeleton_style.color], radii=skeleton_style.plot_3d_size, ), ) def plot_rgb_image(device_label: str, image_data, image_record): &quot;&quot;&quot; Plot RGB image in the RGB view. Args: device_label: Label for the device (e.g., &quot;host&quot; or &quot;client&quot;) image_data: ImageData object image_record: ImageDataRecord object &quot;&quot;&quot; if image_data is None: return rr.log( f&quot;{device_label}&quot;, rr.Image(image_data.to_numpy_array()) ) print(&quot;‚úÖ Helper functions defined!&quot;)   ","version":"Next","tagName":"h3"},{"title":"Visualization Part 1: Timestamp-Aligned RGB Frames‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - Multi-Sequence Timestamp Alignment","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/multi_sequences_timestamp_alignment#visualization-part-1-timestamp-aligned-rgb-frames","content":" Let's first visualize the timestamp-aligned RGB frames from both devices side-by-side.  # Initialize RGB Rerun viewer rr.init(&quot;rgb_viewer&quot;, spawn=False) print(&quot;‚úÖ RGB viewer initialized&quot;) # Get RGB stream ID rgb_stream_id = host_data_provider.get_vrs_stream_id_from_label(&quot;camera-rgb&quot;) # Define sampling parameters sampling_period_ns = int(2e8) # 200ms = 5 Hz (adjust for different frame rates) start_offset_ns = int(3e9) # Start 3 seconds into recording (skip initial setup) duration_ns = int(10e9) # Visualize 10 seconds (adjust as needed) # Get host recording time range host_start_time_ns = host_data_provider.vrs_data_provider.get_first_time_ns_all_streams(TimeDomain.DEVICE_TIME) host_end_time_ns = host_data_provider.vrs_data_provider.get_last_time_ns_all_streams(TimeDomain.DEVICE_TIME) # Calculate query range query_start_ns = host_start_time_ns + start_offset_ns query_end_ns = min(query_start_ns + duration_ns, host_end_time_ns) # Clear trajectory caches for fresh visualization host_trajectory_cache.clear() client_trajectory_cache.clear() print(&quot;Visualization Configuration:&quot;) print(&quot;=&quot; * 60) print(f&quot;Host recording duration: {(host_end_time_ns - host_start_time_ns) / 1e9:.2f} seconds&quot;) print(f&quot;Visualization start: {start_offset_ns / 1e9:.1f}s into recording&quot;) print(f&quot;Visualization duration: {(query_end_ns - query_start_ns) / 1e9:.2f} seconds&quot;) print(f&quot;Sampling rate: {1e9 / sampling_period_ns:.1f} Hz&quot;) print(f&quot;Expected frames: ~{int((query_end_ns - query_start_ns) / sampling_period_ns)}&quot;) print(&quot;=&quot; * 60) print(&quot;‚è≥ Logging RGB frames...\\n&quot;) current_timestamp_ns = query_start_ns frame_count = 0 while current_timestamp_ns &lt;= query_end_ns: # Query host RGB image host_image_data, host_image_record = host_data_provider.get_vrs_image_data_by_time_ns( stream_id=rgb_stream_id, time_ns=current_timestamp_ns, time_domain=TimeDomain.DEVICE_TIME, time_query_options=TimeQueryOptions.CLOSEST ) # Query client RGB image using host timestamp with SUBGHZ client_image_data, client_image_record = client_data_provider.get_vrs_image_data_by_time_ns( stream_id=rgb_stream_id, time_ns=current_timestamp_ns, # Host timestamp! time_domain=TimeDomain.SUBGHZ, # SUBGHZ domain for cross-device query time_query_options=TimeQueryOptions.CLOSEST ) # Set Rerun timestamp to the host's actual capture timestamp rr.set_time_nanos(&quot;device_time&quot;, host_image_record.capture_timestamp_ns) # Plot RGB images with the correct timestamp rr.log(&quot;rgb_image_in_host&quot;, rr.Image(host_image_data.to_numpy_array())) rr.log(&quot;rgb_image_in_client&quot;, rr.Image(client_image_data.to_numpy_array())) # Move to next timestamp current_timestamp_ns += sampling_period_ns frame_count += 1 # Print progress every 10 frames if frame_count % 10 == 0: print(f&quot; Processed {frame_count} RGB frames...&quot;) print(f&quot;\\n‚úÖ RGB data logging complete! Processed {frame_count} frames.&quot;) # Display RGB viewer rr.notebook_show() print(&quot;\\nüí° RGB Viewer - What to observe:&quot;) print(&quot; - Top: Host device RGB frames&quot;) print(&quot; - Bottom: Client device RGB frames&quot;) print(&quot; - Frames are synchronized using SubGHz time alignment&quot;) print(&quot; - May have slight timing differences (cameras not trigger-aligned)&quot;)   ","version":"Next","tagName":"h3"},{"title":"Visualization Part 2: 3D World View‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - Multi-Sequence Timestamp Alignment","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/multi_sequences_timestamp_alignment#visualization-part-2-3d-world-view","content":" Now let's visualize the 3D trajectories, hand tracking, and point cloud in a shared world coordinate frame.  # Initialize 3D World Rerun viewer rr.init(&quot;world_3d_viewer&quot;, spawn=False) world_blueprint = rrb.Blueprint( rrb.Spatial3DView( origin=&quot;world&quot;, name=&quot;3D World View&quot;, background=[0, 0, 0], ), collapse_panels=True, ) print(&quot;‚úÖ 3D world viewer initialized&quot;) # Load filtered point cloud from host print(&quot;Loading filtered point cloud from host...&quot;) host_point_cloud_filtered = host_data_provider.get_mps_semidense_point_cloud_filtered( filter_confidence=True, max_point_count=50000 # Limit to 50k points for performance ) # Convert to numpy array for plotting if host_point_cloud_filtered: points_array = np.array([point.position_world for point in host_point_cloud_filtered]) # Plot point cloud as static data plot_style = get_plot_style(PlotEntity.SEMI_DENSE_POINT_CLOUD) rr.log( f&quot;world/{plot_style.label}&quot;, rr.Points3D( positions=points_array, colors=[plot_style.color] * len(points_array), radii=plot_style.plot_3d_size, ), static=True, ) print(f&quot;‚úÖ Plotted {len(points_array)} filtered points from host&quot;) else: print(&quot;‚ö†Ô∏è No point cloud data available&quot;) # Clear trajectory caches for fresh visualization host_trajectory_cache.clear() client_trajectory_cache.clear() print(&quot;‚è≥ Logging 3D trajectories and hand tracking...\\n&quot;) current_timestamp_ns = query_start_ns frame_count = 0 while current_timestamp_ns &lt;= query_end_ns: # Query host MPS pose and hand tracking host_pose = host_data_provider.get_mps_interpolated_closed_loop_pose( timestamp_ns=current_timestamp_ns, time_domain=TimeDomain.DEVICE_TIME ) host_hand = host_data_provider.get_mps_interpolated_hand_tracking_result( timestamp_ns=current_timestamp_ns, time_domain=TimeDomain.DEVICE_TIME ) # Query client MPS pose and hand tracking using host timestamp with SUBGHZ client_pose = client_data_provider.get_mps_interpolated_closed_loop_pose( timestamp_ns=current_timestamp_ns, # Host timestamp! time_domain=TimeDomain.SUBGHZ # SUBGHZ domain for cross-device query ) client_hand = client_data_provider.get_mps_interpolated_hand_tracking_result( timestamp_ns=current_timestamp_ns, # Host timestamp! time_domain=TimeDomain.SUBGHZ # SUBGHZ domain for cross-device query ) # Set Rerun timestamp to the host's actual pose timestamp (if available) if host_pose is not None: rr.set_time_nanos(&quot;device_time&quot;, int(host_pose.tracking_timestamp.total_seconds() * 1e9)) else: rr.set_time_nanos(&quot;device_time&quot;, current_timestamp_ns) # Plot host trajectory and pose (blue color) with correct timestamp plot_device_pose_and_trajectory( &quot;host&quot;, host_pose, host_trajectory_cache, color=[0, 100, 255] ) plot_hand_tracking(&quot;host&quot;, host_hand) # Plot client trajectory and pose (red color) with the same timestamp plot_device_pose_and_trajectory( &quot;client&quot;, client_pose, client_trajectory_cache, color=[255, 100, 0] ) plot_hand_tracking(&quot;client&quot;, client_hand) # Move to next timestamp current_timestamp_ns += sampling_period_ns frame_count += 1 # Print progress every 10 frames if frame_count % 10 == 0: print(f&quot; Processed {frame_count} 3D frames...&quot;) print(f&quot;\\n‚úÖ 3D data logging complete! Processed {frame_count} frames.&quot;) print(f&quot;\\nüìä Trajectory Statistics:&quot;) print(f&quot; Host trajectory points: {len(host_trajectory_cache)}&quot;) print(f&quot; Client trajectory points: {len(client_trajectory_cache)}&quot;) # Display 3D world viewer rr.notebook_show(blueprint=world_blueprint) print(&quot;\\nüí° 3D World Viewer - What to observe:&quot;) print(&quot; - Gray points: Filtered semi-dense point cloud from host&quot;) print(&quot; - Blue trajectory: Host device path&quot;) print(&quot; - Red trajectory: Client device path&quot;) print(&quot; - Hand skeletons: Real-time hand tracking from both devices&quot;) print(&quot; - Both trajectories share the same world coordinate frame&quot;)   ","version":"Next","tagName":"h3"},{"title":"Summary‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - Multi-Sequence Timestamp Alignment","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/multi_sequences_timestamp_alignment#summary","content":" This tutorial covered multi-device timestamp alignment in the Aria Gen2 Pilot Dataset:  ","version":"Next","tagName":"h2"},{"title":"Key Takeaways‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - Multi-Sequence Timestamp Alignment","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/multi_sequences_timestamp_alignment#key-takeaways","content":" SubGHz timestamp alignment: Understanding the host/client model for multi-device time alignment Host broadcasts SubGHz signalsClient records time domain mapping (only in client VRS)Enables bidirectional timestamp conversion Timestamp Conversion APIs: convert_from_synctime_to_device_time_ns(): Host time ‚Üí Client device timeconvert_from_device_time_to_synctime_ns(): Client device time ‚Üí Host timeBoth functions called on client's VRS data provider Query APIs with TimeDomain.SUBGHZ: Query client data directly using host timestampsSpecify time_domain=TimeDomain.SUBGHZ in query functionsMore convenient than manual timestamp conversion Shared World Coordinate Frame: MPS trajectories from multi-device recordings are in the same world coordinate frameEnables direct comparison and multi-view analysisNo additional alignment needed Visualization Best Practices: Break large visualization code into manageable stepsUse different colors for different devicesShow timestamp-aligned RGB frames side-by-sideDisplay trajectories and hand tracking in unified 3D world view ","version":"Next","tagName":"h3"},{"title":"Advanced Installation From Source Code","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/research-tools/projectariatools/advanced-installation","content":"","keywords":"","version":"Next"},{"title":"Pre-requisites: install dependencies‚Äã","type":1,"pageTitle":"Advanced Installation From Source Code","url":"/projectaria_tools/gen2/research-tools/projectariatools/advanced-installation#prerequisites","content":" Before proceed to any of the advanced installation steps, please follow the command in the installer widget on Quick Start page to pull the correct branch from Github.  Then install the following third party dependency lib:  UbuntuFedoraMacOS # Deactivate CONDA / MAMBA virtual environment you may be in! This is IMPORTANT! conda deactivate # Install build essentials sudo apt install build-essential git cmake # Install VRS/Pangolin dependencies sudo apt install libgtest-dev libgmock-dev libgoogle-glog-dev libfmt-dev \\ liblz4-dev libzstd-dev libxxhash-dev libboost-all-dev libpng-dev \\ libjpeg-turbo8-dev libturbojpeg0-dev libglew-dev libgl1-mesa-dev libeigen3-dev \\ libepoxy-dev libopus-dev   Install FFmpeg: Aria Gen2 image data is encoded with H.265 codec.projectaria_tools relies on the decoding functions in the vrs library to decode H.265 image data. Here, we will build FFmpeg from source code with the exact same script from the vrs library:  # Set this to the root directory where you pulled the codebase SRC_DIR=${HOME}/Documents/projectaria_sandbox/projectaria_tools/ cd ${SRC_DIR} # Remove previously installed ffmpeg rm -rf ~/vrs_third_party_libs/ffmpeg/ # ffmpeg will be installed under ~/vrs_third_party_libs/ffmpeg/ ./build_third_party_libs/build_ffmpeg_linuxunix.sh   ","version":"Next","tagName":"h2"},{"title":"Build projectaria_tools lib using CMake, without visualization‚Äã","type":1,"pageTitle":"Advanced Installation From Source Code","url":"/projectaria_tools/gen2/research-tools/projectariatools/advanced-installation#cmake_no_viz","content":" cd ${SRC_DIR} cmake -B ./build/ -S ./ -DCMAKE_POLICY_VERSION_MINIMUM=3.5 cd build/ make -j8   ","version":"Next","tagName":"h2"},{"title":"Build projectaria_tools lib using CMake, with visualization‚Äã","type":1,"pageTitle":"Advanced Installation From Source Code","url":"/projectaria_tools/gen2/research-tools/projectariatools/advanced-installation#cmake_with_viz","content":" The C++ visualization binaries in projectaria_tools are built using the Pangolin library. Therefore, you need to install Pangolin as a system library first.  cd /tmp git clone --recursive https://github.com/stevenlovegrove/Pangolin.git mkdir -p Pangolin_Build &amp;&amp; cd Pangolin_Build cmake -DCMAKE_BUILD_TYPE=Release -DBUILD_TOOLS=OFF -DBUILD_PANGOLIN_PYTHON=OFF \\ -DBUILD_EXAMPLES=OFF ../Pangolin/ make -j8 sudo make install # For Linux systems, you may need to run the following command to update the dynamic linker cache so the Pangolin library can be found sudo ldconfig   Then you can build projectaria_tools in CMake:  cd ${SRC_DIR} cmake -B ./build/ -S ./ -DPROJECTARIA_TOOLS_BUILD_TOOLS=ON -DCMAKE_POLICY_VERSION_MINIMUM=3.5 cd build/ make -j8   ","version":"Next","tagName":"h2"},{"title":"Build projectaria_tools Python package from source code‚Äã","type":1,"pageTitle":"Advanced Installation From Source Code","url":"/projectaria_tools/gen2/research-tools/projectariatools/advanced-installation#python_from_source","content":" First, make sure you have installed the dependencies libraries as pre-requisites. You can build a local version of projectaria-tools Python package by doing the following. We strongly recommend you build this within a Python virtual environment.  # Go to your check-out source code directory cd ${SRC_DIR} python3 -m pip install --upgrade pip python3 -m pip install .   ","version":"Next","tagName":"h2"},{"title":"Build projectaria_tools Python package from source code with type hinting‚Äã","type":1,"pageTitle":"Advanced Installation From Source Code","url":"/projectaria_tools/gen2/research-tools/projectariatools/advanced-installation#python_from_source_type_hint","content":" Install pybind11-stubgen to generate the stub filesGenerate Python type hinting with generate_stubs.py script once projectaria_tools package is installedInstall type hinting package for projectaria_tools  # Install pybind11-stubgen to generate the stub files python3 -m pip install pybind11-stubgen==1.1 # Generate stubs cd $HOME/Documents/projectaria_sandbox/projectaria_tools python3 generate_stubs.py cp -r projectaria_tools-stubs/projectaria_tools . # Install projectaria-tools python package from source code. python3 -m pip install .  ","version":"Next","tagName":"h2"},{"title":"Build projectaria_tools in isolation with Pixi","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/research-tools/projectariatools/advanced-installation-pixi","content":"","keywords":"","version":"Next"},{"title":"Step 1: Install Pixi‚Äã","type":1,"pageTitle":"Build projectaria_tools in isolation with Pixi","url":"/projectaria_tools/gen2/research-tools/projectariatools/advanced-installation-pixi#step-1-install-pixi","content":" # Install Pixi curl -fsSL https://pixi.sh/install.sh | bash   ","version":"Next","tagName":"h3"},{"title":"Step 2: Compile and test the code‚Äã","type":1,"pageTitle":"Build projectaria_tools in isolation with Pixi","url":"/projectaria_tools/gen2/research-tools/projectariatools/advanced-installation-pixi#step-2-compile-and-test-the-code","content":" C++Python # When this command line first run, it will collect required dependencies, and then trigger the build and unit test pixi run run_c   ","version":"Next","tagName":"h3"},{"title":"Step 3: Access the compiled artifacts in C++‚Äã","type":1,"pageTitle":"Build projectaria_tools in isolation with Pixi","url":"/projectaria_tools/gen2/research-tools/projectariatools/advanced-installation-pixi#step-3-access-the-compiled-artifacts-in-c","content":" # Activate the environment pixi shell # You can now run and use things from the projects. # For example, with Python, you can import the projectaria_tools package and use it. python import projectaria_tools dir(projectaria_tools) # Will print &gt;&gt;&gt;['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'core']  ","version":"Next","tagName":"h3"},{"title":"Project Aria Tools","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/research-tools/projectariatools/overview","content":"","keywords":"","version":"Next"},{"title":"Getting Started‚Äã","type":1,"pageTitle":"Project Aria Tools","url":"/projectaria_tools/gen2/research-tools/projectariatools/overview#getting-started","content":" If you're new to projectaria-tools, start here:  Quick Start Installation Guide - Learn how to install projectaria-tools on your systemAdvanced Installation - Advanced installation options and configurationsPixi Installation - Install using Pixi package manager  ","version":"Next","tagName":"h2"},{"title":"Python Notebook Tutorials‚Äã","type":1,"pageTitle":"Project Aria Tools","url":"/projectaria_tools/gen2/research-tools/projectariatools/overview#python-notebook-tutorials","content":" Explore our comprehensive Python tutorials to learn how to work with Aria data:  Tutorial 1: VrsDataProvider Basics: how to perform basic operations in loading and access data in an Aria VRS file.Tutorial 2: Device Calibration: how to work with device calibration in Aria VRS.Tutorial 3: Queued Sensor Data: how to use the unified queued API to efficiently ‚Äústream‚Äù multi-sensor data.Tutorial 4: On-Device Eye Tracking and Hand Tracking: how to work with on-device-generated EyeGaze and Hand-tracking signals from Aria Gen2 glasses.Tutorial 5: On-Device VIO: how to work with on-device-generated VIO data from Aria Gen2 glasses.Tutorial 6: Timestamp Mapping in Single- and Multi-Device Recordings: understanding timestamp mapping in Aria data, and how to use timestamp mapping in multi-device recording.Tutorial 7: Machine Perception Services (MPS) Data Loading: how to load and visualize output data from Aria MP.  ","version":"Next","tagName":"h2"},{"title":"Visualization Tools‚Äã","type":1,"pageTitle":"Project Aria Tools","url":"/projectaria_tools/gen2/research-tools/projectariatools/overview#visualization-tools","content":" Tools for visualizing and exploring Aria data:  Python Visualization - Python-based visualizationC++ Visualization - C++ visualization toolVRS to MP4 - Convert VRS recordings to MP4 videoExport to CSV - Export sensor data to CSV format ","version":"Next","tagName":"h2"},{"title":"Installation","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/research-tools/projectariatools/installation","content":"","keywords":"","version":"Next"},{"title":"Installing projectaria-tools‚Äã","type":1,"pageTitle":"Installation","url":"/projectaria_tools/gen2/research-tools/projectariatools/installation#installing","content":" Select your preferences We‚Äôll generate the right install command Operating System Linux &amp; macOSWindows Package pipbuild from source (C++ &amp; Python) Channel StableDev Dev channel is unavailable when using pip. Install CommandCopy rm -rf $HOME/projectaria_gen2_python_env python3 -m venv $HOME/projectaria_gen2_python_env source $HOME/projectaria_gen2_python_env/bin/activate python3 -m pip install projectaria-tools'[all]'==2.0.0   Use the above table to get the exact command to install projectaria-tools library.  We strongly recommend install projectaria-tools via pip packages within a Python virtual environment.stable represents the most currently tested and supported version of PyTorch. This should be suitable for many users; develop is pushed continuously to the main branch on Github repo.For users who wants to build the library from source code (C++ or Python), please refer to instructions in the advanced installation page.  ","version":"Next","tagName":"h2"},{"title":"Supported Platforms‚Äã","type":1,"pageTitle":"Installation","url":"/projectaria_tools/gen2/research-tools/projectariatools/installation#supported-platforms","content":" OS / Platform\tOS / Distro Details\tAria Gen2 Support (projectaria-tools ‚â•2.0)\tSupported Python VersionsLinux (x64)\tFedora 40/41; Ubuntu 20.04 LTS (focal) / 22.04 LTS (jammy)\t‚úÖ Supported\t3.9 ‚Äì 3.12 macOS (Apple Silicon / ARM64)\tmacOS 14+ (Sonoma or newer) on M1/M2/M3\t‚úÖ Supported\t3.10 ‚Äì 3.12 macOS (Intel)\tmacOS 13+ (Ventura or newer)\t‚úÖ Supported\t3.9 ‚Äì 3.12 Windows (x64)\tMSVC 2019/2022\tüöß Planned\t3.10 ‚Äì 3.12 ","version":"Next","tagName":"h2"},{"title":"Tutorial 1: VrsDataProvider Basics","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/dataprovider","content":"","keywords":"","version":"Next"},{"title":"Introduction‚Äã","type":1,"pageTitle":"Tutorial 1: VrsDataProvider Basics","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/dataprovider#introduction","content":" Aria Gen2 glasses are Meta's dedicated research tool in an always-on glasses form factor. The data recorded by Aria Gen2 glasses are stored in VRS files, where each VRS file captures time-synchronized data streams from various sensors, such cameras, IMUs, audio, and more. The VrsDataProvider interface provides a unified way to access multimodal sensor data from these VRS files.  What you'll learn:  How to create a VrsDataProvider from a VRS file.Discover available sensor data streams, and check their configurationsRetrieve data using either sequential (index-based) or temporal (timestamp-based) access APIs.Learn about timing domains and time query options  Prerequisites:  Basic Python knowledge and a general understanding of multimodal sensor data.Download Aria Gen2 sample data from link  Note on VisualizationIf visualization window is not showing up, this is due to Rerun lib's caching issue. Just rerun the specific code cell.  ","version":"Next","tagName":"h2"},{"title":"Basic File Loading‚Äã","type":1,"pageTitle":"Tutorial 1: VrsDataProvider Basics","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/dataprovider#basic-file-loading","content":" The create_vrs_data_provider($FILE_PATH) factory function will create a vrs_data_provider object. This object is your entry point for accessing VRS data.  from projectaria_tools.core import data_provider # Load local VRS file vrs_file_path = &quot;path/to/your/recording.vrs&quot; vrs_data_provider = data_provider.create_vrs_data_provider(vrs_file_path)   ","version":"Next","tagName":"h2"},{"title":"Stream Discovery and Navigation‚Äã","type":1,"pageTitle":"Tutorial 1: VrsDataProvider Basics","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/dataprovider#stream-discovery-and-navigation","content":" ","version":"Next","tagName":"h2"},{"title":"Understanding Stream IDs‚Äã","type":1,"pageTitle":"Tutorial 1: VrsDataProvider Basics","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/dataprovider#understanding-stream-ids","content":" A VRS file contains multiple streams, each storing data from a specific sensor or on-device algorithm result.  Each VRS stream is identified by a unique StreamId (e.g. 1201-1), consisting RecordableTypeId (sensor type, e.g. 1201, standing for &quot;SLAM camera&quot;), and an instance_id (for multiple sensors of the same type, e.g. -1, standing for &quot;instance #1 of this sensor type&quot;). Below are some common RecordableTypeId in Aria recordings. Full definitions of all Recordable Types are given in this wiki page (TODO: Add website page), or refer to the StreamId.h file in the VRS repo.  RecordableTypeId\tDescription214\tRGB camera stream 1201\tSLAM camera stream 211\tEyeTracking camera stream 1202\tIMU sensor stream 231\tAudio sensor stream 373\tEyeGaze data stream from on-device EyeTracking algorithm.  ","version":"Next","tagName":"h3"},{"title":"Query StreamId By Label‚Äã","type":1,"pageTitle":"Tutorial 1: VrsDataProvider Basics","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/dataprovider#query-streamid-by-label","content":" # Get all available streams all_streams = vrs_data_provider.get_all_streams() print(f&quot;Found {len(all_streams)} streams in the VRS file:&quot;) # Print out each stream id, and their corresponding sensor label for stream_id in all_streams: label = vrs_data_provider.get_label_from_stream_id(stream_id) print(f&quot; --- Data stream {stream_id}'s label is: {label}&quot;)   # Find a specific stream's StreamId by sensor label print(&quot;Seeking RGB data stream...&quot;) rgb_stream_id = vrs_data_provider.get_stream_id_from_label(&quot;camera-rgb&quot;) if rgb_stream_id is not None: print(f&quot;Found camera-rgb stream in VRS file: {rgb_stream_id}&quot;) else: print(&quot;Cannot find camera-rgb stream in VRS file.&quot;)   ","version":"Next","tagName":"h3"},{"title":"Sensor Data Query APIs‚Äã","type":1,"pageTitle":"Tutorial 1: VrsDataProvider Basics","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/dataprovider#sensor-data-query-apis","content":" Query by index‚Äã  The query-by-index API allows you to retrieve the k-th data sample from a specific stream using the following syntax:  get_&lt;SENSOR&gt;_data_by_index(stream_id, index)   where &lt;SENSOR&gt; can be replaced by any sensor data type available in Aria VRS. See here for a full list of supported sensor types  This API is commonly used for sequential processing‚Äîsuch as iterating through all frames of a single stream‚Äîor when you know the exact frame number you want to query within a specific stream.  Important Note:The indices in each stream are independent and not correlated across different sensor streams. For example, the i-th RGB image does not necessarily correspond to the i-th SLAM image. This is because different sensors may operate at different frequencies or have missing frames, so their data streams are not synchronized by index.  # Visualize with Rerun import rerun as rr rr.init(&quot;rerun_viz_query_by_index&quot;) # Get number of samples in stream num_samples = vrs_data_provider.get_num_data(rgb_stream_id) print(f&quot;RGB stream has a total of {num_samples} frames\\n&quot;) # Access frames sequentially, and plot the first few frames first_few = min(10, num_samples) print(f&quot;Printing the capture timestamps from the first {first_few} frames&quot;) for i in range(first_few): # First 10 frames image_data, image_record = vrs_data_provider.get_image_data_by_index( rgb_stream_id, i ) # Access image properties timestamp_ns = image_record.capture_timestamp_ns print(f&quot;Frame {i}: timestamp = {timestamp_ns}&quot;) # Process image data if image_data.is_valid(): rr.set_time_nanos(&quot;device_time&quot;, timestamp_ns) rr.log(&quot;camera_rgb&quot;, rr.Image(image_data.to_numpy_array())) rr.notebook_show()   Query by Timestamp: TimeDomain and TimeQueryOptions‚Äã  A key feature of Aria devices is the ability to capture time-synchronized, multi-modal sensor data. To help you access this data with precise temporal control, projectaria_tools provides a comprehensive suite of time-based APIs.  The most commonly used is the timestamp-based query:  get_&lt;SENSOR&gt;_by_time_ns(stream_id, time_ns, time_domain=None, time_query_options=None)   where &lt;SENSOR&gt; can be replaced by any sensor data type available in Aria VRS. See here for a full list of supported sensor types  This API is often used to synchronize data across multiple sensor streams, fetch sensor data at specific timestamps, or perform temporal analysis.  TimeDomain and TimeQueryOptions‚Äã  When querying sensor data by timestamp, two important concepts are:  TimeDomain: Specifies the time reference for your query.TimeQueryOptions: Controls how the API selects data relative to your requested timestamp.  Below are all available options for each:  TimeDomain Options‚Äã  Name\tDescription\tTypical Use CaseRECORD_TIME\tTimestamp stored directly in the VRS index. Fast access, but time domain may vary.\tQuick access, not recommended for sync. DEVICE_TIME\tAccurate device capture time. All sensors on the same Aria device share this domain.\tRecommended for single-device data. HOST_TIME\tArrival time in the host computer's domain. May not be accurate.\tDebugging, host-side analysis. TIME_CODE\t[Aria-Gen1 only] TimeSync server's domain using external time-code devices, accurate across devices in multi-device capture.\tMulti-device synchronization. TIC_SYNC\t[Aria-Gen1 only] TimeSync server's domain using tic-sync, accurate for multi-device capture.\tMulti-device synchronization. SubGhz\t[Aria-Gen2 only] TimeSync server's domain using SubGhz signals, accurate for multi-device capture.\tMulti-device synchronization. Utc\tUTC time domain, only seconds-level accuracy.\tCoarse, global time reference.  TimeQueryOptions‚Äã  Name\tDescriptionBefore\tReturns the last valid data with timestamp &lt;= t_query. After\tReturns the first valid data with timestamp &gt;= t_query. Closest\tReturns the data sample closest to t_query. If two are equally close, returns the one before the query.  For detailed usage and best practices‚Äîespecially for time-sync across multiple devices‚Äîsee Tutorial_6.  from projectaria_tools.core.sensor_data import TimeDomain, TimeQueryOptions rr.init(&quot;rerun_viz_query_by_timestamp&quot;) # Get time bounds for RGB images first_timestamp_ns = vrs_data_provider.get_first_time_ns(rgb_stream_id, TimeDomain.DEVICE_TIME) last_timestamp_ns = vrs_data_provider.get_last_time_ns(rgb_stream_id, TimeDomain.DEVICE_TIME) # Query specific timestamp target_time_ns = first_timestamp_ns + int(1e9) # 1 second later image_data, image_record = vrs_data_provider.get_image_data_by_time_ns( rgb_stream_id, target_time_ns, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST ) actual_time_ns = image_record.capture_timestamp_ns print(f&quot;Requested RGB data that is closest to: {target_time_ns} ns, Got closest sample at: {actual_time_ns} ns&quot;) # Plot RGB and SLAM images at approx 1 hz camera_label_list = [&quot;camera-rgb&quot;, &quot;slam-front-left&quot;, &quot;slam-front-right&quot;, &quot;slam-side-left&quot;, &quot;slam-side-right&quot;] camera_stream_ids = [vrs_data_provider.get_stream_id_from_label(camera_label) for camera_label in camera_label_list] query_timestamp_ns = first_timestamp_ns for _ in range(10): for label, stream_id in zip(camera_label_list, camera_stream_ids): # Query each camera's data according to query timestamp image_data, image_record = vrs_data_provider.get_image_data_by_time_ns( stream_id, query_timestamp_ns, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST) # note that the actual timestamp of the image data is stored within image_record. It can be different from query_time. capture_time_ns = image_record.capture_timestamp_ns # Plot to Rerun if image_data.is_valid(): rr.set_time_nanos(&quot;device_time&quot;, capture_time_ns) rr.log(label, rr.Image(image_data.to_numpy_array())) query_timestamp_ns = query_timestamp_ns + int(1e9) # 1 second rr.notebook_show()  ","version":"Next","tagName":"h2"},{"title":"Aria Gen2 Pilot Dataset Tutorial - VRS Data Loading","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/vrs_loading","content":"","keywords":"","version":"Next"},{"title":"What You'll Learn‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - VRS Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/vrs_loading#what-youll-learn","content":" Initialize the AriaGen2PilotDataProvider.Discover available sensor data streams.Query sensor data by index and timestamp.Understand time domains and query options for temporal synchronization.Understand on device machine perception data stream: eye gaze, hand tracking, vio and vio high frequency.  ","version":"Next","tagName":"h2"},{"title":"Import Required Libraries‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - VRS Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/vrs_loading#import-required-libraries","content":" The following libraries are required for this tutorial:  # Standard library imports import numpy as np import os from pathlib import Path from datetime import timedelta # Project Aria Tools imports from projectaria_tools.core.stream_id import StreamId from projectaria_tools.core.sensor_data import TimeDomain, TimeQueryOptions, VioStatus, TrackingQuality from projectaria_tools.core.mps import get_unit_vector_from_yaw_pitch from projectaria_tools.utils.rerun_helpers import ( create_hand_skeleton_from_landmarks, AriaGlassesOutline, ToTransform3D ) # Aria Gen2 Pilot Dataset imports from aria_gen2_pilot_dataset import AriaGen2PilotDataProvider # Visualization library import rerun as rr   ","version":"Next","tagName":"h2"},{"title":"Initialize Data Provider‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - VRS Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/vrs_loading#initialize-data-provider","content":" The AriaGen2PilotDataProvider is the main interface for accessing data from the Aria Gen2 Pilot Dataset. It provides methods to query sensor data, discover available streams, and access device calibration information.  ‚ö†Ô∏è Important: Update the sequence_path below to point to your downloaded Aria Gen2 Pilot Dataset sequence folder.  Replace with the actual path to your downloaded sequence folder  sequence_path = &quot;path/to/your/sequence_folder&quot;  # Initialize the data provider pilot_data_provider = AriaGen2PilotDataProvider(sequence_path)   ","version":"Next","tagName":"h2"},{"title":"Check Available Data Modalities‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - VRS Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/vrs_loading#check-available-data-modalities","content":" Each Aria Gen2 Pilot dataset sequence contains the following data modalities:  ","version":"Next","tagName":"h2"},{"title":"Core Data Types‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - VRS Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/vrs_loading#core-data-types","content":" VRS - Raw sensor dataMPS (Machine Perception Service) - SLAM and hand tracking data.  ","version":"Next","tagName":"h3"},{"title":"Algorithm Outputs‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - VRS Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/vrs_loading#algorithm-outputs","content":" Foundation StereoHeart RateHand-Object InteractionDiarizationEgocentric Voxel Lifting  Please refer to the algorithm introduction here. For detailed instructions on loading and visualizing the algorithm output, see [tutorial_3_algorithm_data_loading.ipynb](TODO: add link).  Important Notes:  VRS data is mandatory for creating a pilot data providerAll other modalities are optional and depend on the specific sequenceThe availability of optional modalities varies between sequences  # Check what data types are available in this sequence print(&quot;Data Modalities Available in This Sequence:&quot;) print(&quot;=&quot; * 60) print(f&quot;Raw VRS Sensors: ‚úÖ (Mandatory)&quot;) print(f&quot;MPS Algorithms: {'‚úÖ' if pilot_data_provider.has_mps_data() else '‚ùå'}&quot;) print(f&quot;Foundation Stereo: {'‚úÖ' if pilot_data_provider.has_stereo_depth_data() else '‚ùå'}&quot;) print(f&quot;Heart Rate Monitoring: {'‚úÖ' if pilot_data_provider.has_heart_rate_data() else '‚ùå'}&quot;) print(f&quot;Hand-Object Interaction: {'‚úÖ' if pilot_data_provider.has_hand_object_interaction_data() else '‚ùå'}&quot;) print(f&quot;Diarization: {'‚úÖ' if pilot_data_provider.has_diarization_data() else '‚ùå'}&quot;) print(f&quot;Egocentric Voxel Lifting: {'‚úÖ' if pilot_data_provider.has_egocentric_voxel_lifting_data() else '‚ùå'}&quot;) print(&quot;=&quot; * 60)   ","version":"Next","tagName":"h3"},{"title":"VRS - Explore Available Streams‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - VRS Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/vrs_loading#vrs---explore-available-streams","content":" ","version":"Next","tagName":"h2"},{"title":"Stream Discovery and Navigation‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - VRS Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/vrs_loading#stream-discovery-and-navigation","content":" A VRS file contains multiple streams, each storing data from a specific sensor or on-device algorithm result.  Each VRS stream is identified by a unique StreamId (e.g. 1201-1), consisting of RecordableTypeId (sensor type, e.g. 1201, standing for ‚ÄúSLAM camera‚Äù), and an instance_id (for multiple sensors of the same type, e.g. -1, standing for ‚Äúinstance #1 of this sensor type‚Äù). Below are some common RecordableTypeId in Aria recordings. Full definitions of all Recordable Types are given in the Project Aria Tools documentation, or refer to the `StreamId.h` file in the VRS repo.  RecordableTypeId\tDescription214\tRGB camera stream 1201\tSLAM camera stream 211\tEyeTracking camera stream 1202\tIMU sensor stream 231\tAudio sensor stream 373\tEyeGaze data stream from on-device EyeTracking algorithm  # Get all available streams all_streams = pilot_data_provider.get_vrs_all_streams() print(f&quot;Found {len(all_streams)} streams in the VRS file:&quot;) # Print out each stream ID and their corresponding sensor label for stream_id in all_streams: label = pilot_data_provider.get_vrs_label_from_stream_id(stream_id) print(f&quot; --- Data stream {stream_id}'s label is: {label}&quot;)   # Find a specific stream's StreamId by sensor label print(&quot;Seeking RGB data stream...&quot;) rgb_stream_id = pilot_data_provider.get_vrs_stream_id_from_label(&quot;camera-rgb&quot;) if rgb_stream_id is not None: print(f&quot;Found camera-rgb stream in VRS file: {rgb_stream_id}&quot;) else: print(&quot;Cannot find camera-rgb stream in VRS file.&quot;)   ","version":"Next","tagName":"h3"},{"title":"VRS - Device Calibration‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - VRS Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/vrs_loading#vrs---device-calibration","content":" Device calibration information is essential for accurate sensor data processing and 3D reconstruction. The calibration data includes camera intrinsics, extrinsics, and sensor mounting information.  For detailed usage of device calibration, please refer to the Project Aria Tools documentation.  # Get device calibration information device_calibration = pilot_data_provider.get_vrs_device_calibration()   ","version":"Next","tagName":"h3"},{"title":"Sensor Data Query APIs‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - VRS Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/vrs_loading#sensor-data-query-apis","content":" The AriaGen2PilotDataProvider offers two main approaches for querying sensor data:  ","version":"Next","tagName":"h2"},{"title":"Query by Index‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - VRS Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/vrs_loading#query-by-index","content":" The query-by-index API allows you to retrieve the k-th data sample from a specific stream using the following syntax:  get_vrs_\\{SENSOR\\}_data_by_index(stream_id, index)   where \\{SENSOR\\} can be replaced by any sensor data type available in Aria VRS. See the full list of supported sensors.  This API is commonly used for:  Sequential processing (iterating through all frames of a single stream)When you know the exact frame number you want to query within a specific stream  Important Note:The indices in each stream are independent and not correlated across different sensor streams. For example, the i-th RGB image does not necessarily correspond to the i-th SLAM image. This is because different sensors may operate at different frequencies or have missing frames, so their data streams are not synchronized by index.  # Initialize Rerun for visualization rr.init(&quot;rerun_viz_query_by_index&quot;) # Get number of samples in the RGB stream num_samples = pilot_data_provider.get_vrs_num_data(rgb_stream_id) print(f&quot;RGB stream has a total of {num_samples} frames\\n&quot;) # Access frames sequentially and plot the first few frames first_few = min(10, num_samples) print(f&quot;Printing the capture timestamps from the first {first_few} frames&quot;) for i in range(first_few): # First 10 frames image_data, image_record = pilot_data_provider.get_vrs_image_data_by_index( rgb_stream_id, i ) # Access image properties print(f&quot;Frame {i}: timestamp = {image_record.capture_timestamp_ns}&quot;) # Process and visualize image data if image_data.is_valid(): rr.set_time_nanos(&quot;device_time&quot;, image_record.capture_timestamp_ns) rr.log(&quot;camera_rgb&quot;, rr.Image(image_data.to_numpy_array())) # Display the visualization rr.notebook_show()   ","version":"Next","tagName":"h3"},{"title":"Query by Timestamp: TimeDomain and TimeQueryOptions‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - VRS Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/vrs_loading#query-by-timestamp-timedomain-and-timequeryoptions","content":" A key feature of Aria devices is the ability to capture time-synchronized, multi-modal sensor data. To help you access this data with precise temporal control, projectaria_tools provides a comprehensive suite of time-based APIs.  The most commonly used is the timestamp-based query:  get_\\{SENSOR\\}_by_time_ns(stream_id, time_ns, time_domain=None, time_query_options=None)   where \\{SENSOR\\} can be replaced by any sensor data type available in Aria VRS. See here for a full list of supported {SENSOR}  This API is often used to:  Synchronize data across multiple sensor streamsFetch sensor data at specific timestampsPerform temporal analysis  TimeDomain and TimeQueryOptions‚Äã  When querying sensor data by timestamp, two important concepts are:  TimeDomain: Specifies the time reference for your query.TimeQueryOptions: Controls how the API selects data relative to your requested timestamp.  Below are all available options for each:  TimeDomain Options‚Äã  Name\tDescription\tTypical Use CaseRECORD_TIME\tTimestamp stored directly in the VRS index. Fast access, but time domain may vary.\tQuick access, not recommended for sync. DEVICE_TIME\tAccurate device capture time. All sensors on the same Aria device share this domain.\tRecommended for single-device data. HOST_TIME\tArrival time in the host computer‚Äôs domain. May not be accurate.\tDebugging, host-side analysis. SubGhz\tTimeSync server‚Äôs domain using SubGhz signals, accurate for multi-device capture.\tMulti-device synchronization. Utc\tUTC time domain, only seconds-level accuracy.\tCoarse, global time reference.  TimeQueryOptions‚Äã  Name\tDescriptionBefore\tReturns the last valid data with timestamp &lt;= t_query. After\tReturns the first valid data with timestamp &gt;= t_query. Closest\tReturns the data sample closest to t_query. If two are equally close, returns the one before the query.  from projectaria_tools.core.sensor_data import TimeDomain, TimeQueryOptions # Initialize Rerun for visualization rr.init(&quot;rerun_viz_query_by_timestamp&quot;) # Get time bounds for RGB images first_timestamp_ns = pilot_data_provider.get_vrs_timestamps_ns(rgb_stream_id, TimeDomain.DEVICE_TIME)[0] last_timestamp_ns = pilot_data_provider.get_vrs_timestamps_ns(rgb_stream_id, TimeDomain.DEVICE_TIME)[-1] # Query specific timestamp target_time_ns = first_timestamp_ns + 1000000000 # 1 second later image_data, image_record = pilot_data_provider.get_vrs_image_data_by_time_ns( rgb_stream_id, target_time_ns, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST ) actual_time_ns = image_record.capture_timestamp_ns print(f&quot;Requested RGB data that is closest to: {target_time_ns} ns, Got closest sample at: {actual_time_ns} ns&quot;) # Plot RGB and SLAM images at approximately 1 Hz camera_label_list = [&quot;camera-rgb&quot;, &quot;slam-front-left&quot;, &quot;slam-front-right&quot;, &quot;slam-side-left&quot;, &quot;slam-side-right&quot;] camera_stream_ids = [pilot_data_provider.get_vrs_stream_id_from_label(camera_label) for camera_label in camera_label_list] query_timestamp_ns = first_timestamp_ns for _ in range(10): for label, stream_id in zip(camera_label_list, camera_stream_ids): # Query each camera's data according to query timestamp image_data, image_record = pilot_data_provider.get_vrs_image_data_by_time_ns( stream_id, query_timestamp_ns, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST) # Note: the actual timestamp of the image data is stored within image_record. # It can be different from the query_time. capture_time_ns = image_record.capture_timestamp_ns # Visualize with Rerun if image_data.is_valid(): rr.set_time_nanos(&quot;device_time&quot;, capture_time_ns) rr.log(label, rr.Image(image_data.to_numpy_array())) query_timestamp_ns = query_timestamp_ns + int(1e9) # 1 second # Display the visualization rr.notebook_show()   ","version":"Next","tagName":"h3"},{"title":"On-Device Eye Tracking and Hand Tracking‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - VRS Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/vrs_loading#on-device-eye-tracking-and-hand-tracking","content":" Aria Gen2 glasses feature on-device machine perception algorithms that run during recording. This section covers how to access and visualize eye tracking and hand tracking data from VRS files.  ","version":"Next","tagName":"h2"},{"title":"Eye Tracking Data‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - VRS Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/vrs_loading#eye-tracking-data","content":" Eye tracking data provides information about where the user is looking, including gaze direction, depth estimation, and eye movement patterns.  EyeGaze Data Structure‚Äã  The EyeGaze data type represents on-device eye tracking results. Importantly, it directly reuses the EyeGaze data structure from MPS (Machine Perception Services), providing guaranteed compatibility across VRS and MPS.  Key EyeGaze fields:  Field Name\tDescriptionsession_uid\tUnique ID for the eyetracking session tracking_timestamp\tTimestamp of the eye tracking camera frame in device time domain, in us. vergence.t[x,y,z]_[left,right]_eye\tTranslation for each eye origin in CPF frame yaw,vergence.[left,right]_yaw\tEye gaze yaw angle (horizontal) in radians in CPF frame pitch,vergence.[left,right]_pitch(Gen2-only)\tEye gaze pitch angle (vertical) in radians in CPF frame. The left and right pitch are assumed to be the same in Aria-Gen1. depth\tDepth in meters of the 3D eye gaze point in CPF frame (0 = unavailable) yaw_low,yaw_high,pitch_low,pitch_high\tConfidence interval bounds for yaw and pitch angle Aria-Gen2 specific fields combined_gaze_origin_in_cpf\tCombined gaze origin in CPF frame (Gen2 only) spatial_gaze_point_in_cpf\t3D spatial gaze point in CPF frame vergence.[left,right]_entrance_pupil_position_meter\tEntrance pupil positions for each eye vergence.[left,right]_pupil_diameter_meter\tEntrance pupil diameter for each eye vergence.[left,right]_blink\tBlink detection for left and right eyes *_valid\tBoolean flags to indicating if the corresponding data field in EyeGaze is valid  EyeGaze API Reference‚Äã  In AriaGen2PilotDataProvider, EyeGaze is treated the same way as any other sensor data, and shares similar query APIs:  get_vrs_eye_gaze_data_by_index(stream_id, index): Query by index.get_vrs_eye_gaze_data_by_time_ns(stream_id, timestamp, time_domain, query_options): Query by timestamp.  # First, let's check if eye tracking data is available eyegaze_label = &quot;eyegaze&quot; eyegaze_stream_id = pilot_data_provider.get_vrs_stream_id_from_label(eyegaze_label) if eyegaze_stream_id is None: print(f&quot;‚ùå {eyegaze_label} data stream does not exist in this VRS file.&quot;) print(&quot;Please use a VRS file that contains valid eyegaze data for this tutorial.&quot;) else: print(f&quot;‚úÖ Found {eyegaze_label} data stream: {eyegaze_stream_id}&quot;) # Get number of eye tracking samples num_eyegaze_samples = pilot_data_provider.get_vrs_num_data(eyegaze_stream_id) print(f&quot;Total eye tracking samples: {num_eyegaze_samples}&quot;) # Sample a few eye tracking data points print(&quot;\\n=== EyeGaze Data Sample ===&quot;) selected_index = min(5, num_eyegaze_samples - 1) eyegaze_data = pilot_data_provider.get_vrs_eye_gaze_data_by_index(eyegaze_stream_id, selected_index) if eyegaze_data is not None: # Eyegaze timestamp is in format of datetime.deltatime in microseconds, convert it to integer from datetime import timedelta eyegaze_timestamp_ns = (eyegaze_data.tracking_timestamp // timedelta(microseconds=1)) * 1000 print(f&quot;Sample {selected_index}:&quot;) print(f&quot;\\tTracking timestamp: {eyegaze_timestamp_ns}&quot;) # Check if combined gaze is valid, if so, print out the gaze direction print(f&quot;\\tCombined gaze valid: {eyegaze_data.combined_gaze_valid}&quot;) if eyegaze_data.combined_gaze_valid: print(f&quot;\\tYaw: {eyegaze_data.yaw:.3f} rad&quot;) print(f&quot;\\tPitch: {eyegaze_data.pitch:.3f} rad&quot;) print(f&quot;\\tDepth: {eyegaze_data.depth:.3f} m&quot;) # Convert gaze direction to unit vector from projectaria_tools.core.mps import get_unit_vector_from_yaw_pitch gaze_direction_in_unit_vec = get_unit_vector_from_yaw_pitch(eyegaze_data.yaw, eyegaze_data.pitch) print(f&quot;\\tGaze direction in unit vec [xyz]: {gaze_direction_in_unit_vec}&quot;) # Check if spatial gaze point is valid print(f&quot;\\tSpatial gaze point valid: {eyegaze_data.spatial_gaze_point_valid}&quot;) if eyegaze_data.spatial_gaze_point_valid: print(f&quot;\\tSpatial gaze point in CPF: {eyegaze_data.spatial_gaze_point_in_cpf}&quot;) else: print(&quot;No eye tracking data available at the selected index.&quot;)   Eye Tracking Visualization‚Äã  To visualize eye tracking results in camera images, you need to project the gaze data into the camera coordinate system using device calibration.  Important Coordinate System Note:All eye tracking results in Aria are stored in a reference coordinate system called Central Pupil Frame (CPF), which is approximately the center of the user's two eye positions. This CPF frame is DIFFERENT from the Device frame in device calibration, where the latter is essentially the slam-front-left camera. To transform between CPF and Device, we use the device calibration API:  device_calibration.get_transform_device_cpf()   # Eye tracking visualization helper function def plot_eyegaze_in_camera(eyegaze_data, camera_label, camera_calib, T_device_cpf): &quot;&quot;&quot; A helper function to plot eyegaze's spatial gaze point into a camera image &quot;&quot;&quot; # Skip if eyegaze data is invalid if not (eyegaze_data.spatial_gaze_point_valid and eyegaze_data.combined_gaze_valid): return # First, transform spatial gaze point from CPF -&gt; Device -&gt; Camera frame spatial_gaze_point_in_cpf = eyegaze_data.spatial_gaze_point_in_cpf spatial_gaze_point_in_device = T_device_cpf @ spatial_gaze_point_in_cpf spatial_gaze_point_in_camera = ( camera_calib.get_transform_device_camera().inverse() @ spatial_gaze_point_in_device ) # Project into camera and plot 2D gaze location maybe_pixel = camera_calib.project(spatial_gaze_point_in_camera) if maybe_pixel is not None: rr.log( f&quot;{camera_label}&quot;, rr.Points2D( positions=[maybe_pixel], colors=[255, 64, 255], radii = [30.0] ), ) # Visualize eye tracking in camera images (if eye tracking data is available) if eyegaze_stream_id is not None: print(&quot;\\n=== Visualizing on-device eye tracking in camera images ===&quot;) # Get device calibration and CPF transform device_calib = pilot_data_provider.get_vrs_device_calibration() T_device_cpf = device_calib.get_transform_device_cpf() # Get RGB camera stream rgb_camera_label = &quot;camera-rgb&quot; rgb_stream_id = pilot_data_provider.get_vrs_stream_id_from_label(rgb_camera_label) rgb_camera_calib = device_calib.get_camera_calib(rgb_camera_label) # Initialize Rerun for visualization rr.init(&quot;rerun_viz_et_in_cameras&quot;) rr.notebook_show() # Get time bounds and sample data first_timestamp_ns = pilot_data_provider.get_vrs_timestamps_ns(rgb_stream_id, TimeDomain.DEVICE_TIME)[0] last_timestamp_ns = pilot_data_provider.get_vrs_timestamps_ns(rgb_stream_id, TimeDomain.DEVICE_TIME)[-1] # Sample a few frames for visualization sample_timestamps = [] for i in range(0, min(10, pilot_data_provider.get_vrs_num_data(rgb_stream_id)), 2): rgb_data, rgb_record = pilot_data_provider.get_vrs_image_data_by_index(rgb_stream_id, i) sample_timestamps.append(rgb_record.capture_timestamp_ns) # Visualize RGB images with eye tracking overlay for timestamp_ns in sample_timestamps: # Get RGB image rgb_data, rgb_record = pilot_data_provider.get_vrs_image_data_by_time_ns( rgb_stream_id, timestamp_ns, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST ) if rgb_data.is_valid(): # Visualize the RGB image rr.set_time_nanos(&quot;device_time&quot;, rgb_record.capture_timestamp_ns) rr.log(rgb_camera_label, rr.Image(rgb_data.to_numpy_array())) # Get eye tracking data for this timestamp eyegaze_data = pilot_data_provider.get_vrs_eye_gaze_data_by_time_ns( eyegaze_stream_id, timestamp_ns, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST ) if eyegaze_data is not None: # Plot eye tracking overlay plot_eyegaze_in_camera( eyegaze_data=eyegaze_data, camera_label=rgb_camera_label, camera_calib=rgb_camera_calib, T_device_cpf=T_device_cpf ) else: print(&quot;Skipping eye tracking visualization - no eye tracking data available.&quot;)   ","version":"Next","tagName":"h3"},{"title":"Hand Tracking Data‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - VRS Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/vrs_loading#hand-tracking-data","content":" Hand tracking data provides comprehensive 3D hand pose information including landmark positions, confidence scores, and hand gestures.  HandTracking Data Structure‚Äã  HandTracking data contains comprehensive 3D hand pose information. Importantly, it directly reuses the HandTrackingResults data structure from MPS (Machine Perception Services), providing guaranteed compatibility across VRS and MPS.  Key Fields in HandTrackingResults:  Field Name\tDescriptiontracking_timestamp\tTimestamp of the hand-tracking estimate in the device time domain. left_hand\tLeft-hand pose, or None if no valid pose is found for the timestamp. right_hand\tRight-hand pose, or None if no valid pose is found for the timestamp.  Single Hand fields (left or right):  Field Name\tDescriptionconfidence\tTracking confidence score for this hand. landmark_positions_device\tList of 21 hand-landmark positions in the device frame (3D points). See the wiki page for landmark definitions. transform_device_wrist\tFull SE3 transform of the wrist in the Device frame. wrist_and_palm_normal_device\tNormal vectors for the wrist and palm joints in the Device frame.  HandTracking Coordinate System‚Äã  All HandTracking results in Aria are stored in the Device coordinate frame, which is the same as device calibration. This makes it easier to work with compared to eye tracking data.  HandTracking API Reference‚Äã  In AriaGen2PilotDataProvider, HandTracking shares similar query APIs:  get_vrs_hand_pose_data_by_index(stream_id, index): Query by index.get_vrs_hand_pose_data_by_time_ns(stream_id, timestamp, time_domain, query_options): Query by timestamp.  # Check if hand tracking data is available handtracking_label = &quot;handtracking&quot; handtracking_stream_id = pilot_data_provider.get_vrs_stream_id_from_label(handtracking_label) if handtracking_stream_id is None: print(f&quot;‚ùå {handtracking_label} data stream does not exist in this VRS file.&quot;) print(&quot;Please use a VRS file that contains valid handtracking data for this tutorial.&quot;) else: print(f&quot;‚úÖ Found {handtracking_label} data stream: {handtracking_stream_id}&quot;) # Get number of hand tracking samples num_handtracking_samples = pilot_data_provider.get_vrs_num_data(handtracking_stream_id) print(f&quot;Total hand tracking samples: {num_handtracking_samples}&quot;) # Sample a few hand tracking data points print(&quot;\\n=== HandTracking Data Sample ===&quot;) selected_index = min(5, num_handtracking_samples - 1) hand_data = pilot_data_provider.get_vrs_hand_pose_data_by_index(handtracking_stream_id, selected_index) if hand_data is not None: print(f&quot;Sample {selected_index}:&quot;) print(f&quot;\\tTracking timestamp: {hand_data.tracking_timestamp}&quot;) # Print the content of left and right hand if valid if hand_data.left_hand is not None: print(&quot;\\tLeft hand detected&quot;) print(f&quot;\\t\\tConfidence: {hand_data.left_hand.confidence:.3f}&quot;) print(f&quot;\\t\\tLandmarks shape: {len(hand_data.left_hand.landmark_positions_device)}&quot;) print(f&quot;\\t\\tWrist location: {hand_data.left_hand.get_wrist_position_device()}&quot;) else: print(&quot;\\tLeft hand: Not detected&quot;) if hand_data.right_hand is not None: print(&quot;\\tRight hand detected&quot;) print(f&quot;\\t\\tConfidence: {hand_data.right_hand.confidence:.3f}&quot;) print(f&quot;\\t\\tLandmarks shape: {len(hand_data.right_hand.landmark_positions_device)}&quot;) print(f&quot;\\t\\tWrist location: {hand_data.right_hand.get_wrist_position_device()}&quot;) else: print(&quot;\\tRight hand: Not detected&quot;) else: print(&quot;No hand tracking data available at the selected index.&quot;)   Interpolated Hand Tracking Results‚Äã  Context:  In Aria-Gen2 glasses, the on-device hand-tracking data are calculated from the SLAM cameras, not RGB cameras. In the meantime, the SLAM cameras and RGB camera often run at different sampling frequencies, and their triggering are not aligned either. This causes the hand tracking result's timestamp to often not line up with that of RGB camera, causing additional challenges in accurately visualizing handtracking results in RGB images.  API to query interpolated handtracking results  To resolve this, AriaGen2PilotDataProvider enables a special query API for handtracking results:  get_vrs_interpolated_hand_pose_data(stream_id, timestamp_ns, time_domain)   which will return an interpolated hand tracking result, given any timestamp within valid timestamps of the VRS file.  Handtracking Interpolation Implementation  Find the 2 nearest hand-tracking results before and after the target timestamp.If the 2 hand-tracking results time delta is larger than 100 ms, interpolation is considered unreliable ‚Üí return None.Otherwise, interpolate each hand separately: a. For the left or right hand, perform interpolation only if both the &quot;before&quot; and &quot;after&quot; samples contain a valid result for that hand. b. If either sample is missing, the interpolated result for that hand will be None.Single-hand interpolation is calculated as: a. Apply linear interpolation on the 3D hand landmark positions. b. Apply SE3 interpolation on T_Device_Wrist 3D pose. c. Re-calculate the wrist and palm normal vectors. d. Take the min of confidence values.  # Demonstrate interpolated hand tracking (if hand tracking data is available) if handtracking_stream_id is not None: print(&quot;\\n=== Demonstrating query interpolated hand tracking results ===&quot;) # Get SLAM and RGB camera streams for comparison slam_stream_id = pilot_data_provider.get_vrs_stream_id_from_label(&quot;slam-front-left&quot;) rgb_stream_id = pilot_data_provider.get_vrs_stream_id_from_label(&quot;camera-rgb&quot;) if slam_stream_id is not None and rgb_stream_id is not None: # Retrieve a SLAM frame, use its timestamp as query slam_sample_index = min(10, pilot_data_provider.get_vrs_num_data(slam_stream_id) - 1) slam_data, slam_record = pilot_data_provider.get_vrs_image_data_by_index(slam_stream_id, slam_sample_index) slam_timestamp_ns = slam_record.capture_timestamp_ns # Retrieve the closest RGB frame rgb_data, rgb_record = pilot_data_provider.get_vrs_image_data_by_time_ns( rgb_stream_id, slam_timestamp_ns, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST ) rgb_timestamp_ns = rgb_record.capture_timestamp_ns # Retrieve the closest hand tracking data sample raw_ht_data = pilot_data_provider.get_vrs_hand_pose_data_by_time_ns( handtracking_stream_id, slam_timestamp_ns, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST ) if raw_ht_data is not None: from datetime import timedelta raw_ht_timestamp_ns = (raw_ht_data.tracking_timestamp // timedelta(microseconds=1)) * 1000 # Check if hand tracking aligns with RGB or SLAM data print(f&quot;SLAM timestamp: {slam_timestamp_ns}&quot;) print(f&quot;RGB timestamp: {rgb_timestamp_ns}&quot;) print(f&quot;Hand tracking timestamp: {raw_ht_timestamp_ns}&quot;) print(f&quot;Hand tracking-SLAM time diff: {abs(raw_ht_timestamp_ns - slam_timestamp_ns) / 1e6:.2f} ms&quot;) print(f&quot;Hand tracking-RGB time diff: {abs(raw_ht_timestamp_ns - rgb_timestamp_ns) / 1e6:.2f} ms&quot;) # Now, query interpolated hand tracking data sample using RGB timestamp interpolated_ht_data = pilot_data_provider.get_vrs_interpolated_hand_pose_data( handtracking_stream_id, rgb_timestamp_ns, TimeDomain.DEVICE_TIME ) # Check that interpolated hand tracking now aligns with RGB data if interpolated_ht_data is not None: interpolated_ht_timestamp_ns = (interpolated_ht_data.tracking_timestamp // timedelta(microseconds=1)) * 1000 print(f&quot;Interpolated hand tracking timestamp: {interpolated_ht_timestamp_ns}&quot;) print(f&quot;Interpolated hand tracking-RGB time diff: {abs(interpolated_ht_timestamp_ns - rgb_timestamp_ns) / 1e6:.2f} ms&quot;) else: print(&quot;Interpolated hand tracking data is None - interpolation failed&quot;) else: print(&quot;No raw hand tracking data found for comparison&quot;) else: print(&quot;Required camera streams not found for interpolation demonstration&quot;) else: print(&quot;Skipping hand tracking interpolation demonstration - no hand tracking data available.&quot;)   Hand Tracking Visualization‚Äã  To visualize hand tracking results in camera images, you need to project the hand landmarks and skeleton into the camera coordinate system using device calibration. Since hand tracking data is already in the Device frame, the transformation is more straightforward than eye tracking.  # Hand tracking visualization helper functions def plot_single_hand_in_camera(hand_joints_in_device, camera_label, camera_calib, hand_label): &quot;&quot;&quot; A helper function to plot a single hand data in 2D camera view &quot;&quot;&quot; # Setting different marker plot sizes for RGB and SLAM since they have different resolutions plot_ratio = 3.0 if camera_label == &quot;camera-rgb&quot; else 1.0 marker_color = [255,64,0] if hand_label == &quot;left&quot; else [255, 255, 0] # Project into camera frame, and also create line segments hand_joints_in_camera = [] for pt_in_device in hand_joints_in_device: pt_in_camera = ( camera_calib.get_transform_device_camera().inverse() @ pt_in_device ) pixel = camera_calib.project(pt_in_camera) hand_joints_in_camera.append(pixel) # Create hand skeleton in 2D image space from projectaria_tools.utils.rerun_helpers import create_hand_skeleton_from_landmarks hand_skeleton = create_hand_skeleton_from_landmarks(hand_joints_in_camera) # Remove &quot;None&quot; markers from hand joints in camera. This is intentionally done AFTER the hand skeleton creation hand_joints_in_camera = list( filter(lambda x: x is not None, hand_joints_in_camera) ) rr.log( f&quot;{camera_label}/{hand_label}/landmarks&quot;, rr.Points2D( positions=hand_joints_in_camera, colors= marker_color, radii= [3.0 * plot_ratio] ), ) rr.log( f&quot;{camera_label}/{hand_label}/skeleton&quot;, rr.LineStrips2D( hand_skeleton, colors=[0, 255, 0], radii= [0.5 * plot_ratio], ), ) def plot_handpose_in_camera(hand_pose, camera_label, camera_calib): &quot;&quot;&quot; A helper function to plot hand tracking results into a camera image &quot;&quot;&quot; # Plot both hands if hand_pose.left_hand is not None: plot_single_hand_in_camera( hand_joints_in_device=hand_pose.left_hand.landmark_positions_device, camera_label=camera_label, camera_calib = camera_calib, hand_label=&quot;left&quot;) if hand_pose.right_hand is not None: plot_single_hand_in_camera( hand_joints_in_device=hand_pose.right_hand.landmark_positions_device, camera_label=camera_label, camera_calib = camera_calib, hand_label=&quot;right&quot;) # Visualize hand tracking in camera images (if hand tracking data is available) if handtracking_stream_id is not None: print(&quot;\\n=== Visualizing on-device hand tracking in camera images ===&quot;) # Get device calibration device_calib = pilot_data_provider.get_vrs_device_calibration() # Get RGB camera stream rgb_camera_label = &quot;camera-rgb&quot; rgb_stream_id = pilot_data_provider.get_vrs_stream_id_from_label(rgb_camera_label) rgb_camera_calib = device_calib.get_camera_calib(rgb_camera_label) # Initialize Rerun for visualization rr.init(&quot;rerun_viz_ht_in_cameras&quot;) rr.notebook_show() # Get time bounds and sample data first_timestamp_ns = pilot_data_provider.get_vrs_timestamps_ns(rgb_stream_id, TimeDomain.DEVICE_TIME)[0] # Sample a few frames for visualization sample_timestamps = [] for i in range(0, min(10, pilot_data_provider.get_vrs_num_data(rgb_stream_id)), 2): rgb_data, rgb_record = pilot_data_provider.get_vrs_image_data_by_index(rgb_stream_id, i) sample_timestamps.append(rgb_record.capture_timestamp_ns) # Visualize RGB images with hand tracking overlay for timestamp_ns in sample_timestamps: # Get RGB image rgb_data, rgb_record = pilot_data_provider.get_vrs_image_data_by_time_ns( rgb_stream_id, timestamp_ns, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST ) if rgb_data.is_valid(): # Visualize the RGB image rr.set_time_nanos(&quot;device_time&quot;, rgb_record.capture_timestamp_ns) rr.log(rgb_camera_label, rr.Image(rgb_data.to_numpy_array())) # Query interpolated hand tracking data for this timestamp interpolated_hand_pose = pilot_data_provider.get_vrs_interpolated_hand_pose_data( handtracking_stream_id, timestamp_ns, TimeDomain.DEVICE_TIME ) if interpolated_hand_pose is not None: # Plot hand tracking overlay plot_handpose_in_camera( hand_pose=interpolated_hand_pose, camera_label=rgb_camera_label, camera_calib=rgb_camera_calib ) else: print(&quot;Skipping hand tracking visualization - no hand tracking data available.&quot;)   ","version":"Next","tagName":"h3"},{"title":"On-Device VIO (Visual Inertial Odometry)‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - VRS Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/vrs_loading#on-device-vio-visual-inertial-odometry","content":" VIO (Visual Inertial Odometry) combines camera images and IMU (Inertial Measurement Unit) data to estimate device pose and motion in real-time. VIO tracks the device's position, orientation, and velocity by performing visual tracking, IMU integration, sensor fusion, etc, making it the foundation for spatial tracking and understanding.  In Aria-Gen2 devices, the VIO algorithm runs on-device to produce 2 types of tracking results as part of the VRS file: VIO and VIO High Frequency.  ","version":"Next","tagName":"h2"},{"title":"VIO Data Structure‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - VRS Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/vrs_loading#vio-data-structure","content":" Data Type: FrontendOutput‚Äã  This is a new data type introduced to store the results from the VIO system, containing the following fields:  Field Name\tDescriptionfrontend_session_uid\tSession identifier (resets on VIO restart) frame_id\tFrame set identifier capture_timestamp_ns\tCenter capture time in nanoseconds unix_timestamp_ns\tUnix timestamp in nanoseconds status\tVIO status (VALID/INVALID) pose_quality\tPose quality (GOOD/BAD/UNKNOWN) visual_tracking_quality\tVisual-only tracking quality online_calib\tOnline calibration estimates for SLAM cameras and IMUs gravity_in_odometry\tGravity vector in odometry frame transform_odometry_bodyimu\tBody IMU's pose in odometry reference frame transform_bodyimu_device\tTransform from body IMU to device frame linear_velocity_in_odometry\tLinear velocity in odometry frame in m/s angular_velocity_in_bodyimu\tAngular velocity in body IMU frame in rad/s  Here, body IMU is the IMU that is picked as the reference for motion tracking. For Aria-Gen2's on-device VIO algorithm, this is often imu-left.  Important Note: Always check status == VioStatus.VALID and pose_quality == TrackingQuality.GOOD for VIO data validity!  ","version":"Next","tagName":"h3"},{"title":"VIO High Frequency Data Structure‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - VRS Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/vrs_loading#vio-high-frequency-data-structure","content":" VIO High Frequency results are generated directly from the on-device VIO results by performing IMU integration between VIO poses, hence providing a much higher data rate at approximately 800Hz.  Data Type: OpenLoopTrajectoryPose‚Äã  The VioHighFrequency stream re-uses the OpenLoopTrajectoryPose data structure defined in MPS.  Field Name\tDescriptiontracking_timestamp\tTimestamp in device time domain, in microseconds transform_odometry_device\tTransformation from device to odometry coordinate frame, represented as a SE3 instance. device_linear_velocity_odometry\tTranslational velocity of device in odometry frame, in m/s angular_velocity_device\tAngular velocity of device in device frame, in rad/s quality_score\tQuality of pose estimation (higher = better) gravity_odometry\tEarth gravity vector in odometry frame session_uid\tUnique identifier for VIO tracking session  Important Note: Due to the high frequency nature of this data (~800Hz), consider subsampling for visualization to maintain performance.  ","version":"Next","tagName":"h3"},{"title":"VIO API Reference‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - VRS Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/vrs_loading#vio-api-reference","content":" In AriaGen2PilotDataProvider, VIO data shares similar query APIs:  get_vrs_vio_data_by_index(stream_id, index): Query VIO data by index.get_vrs_vio_data_by_time_ns(stream_id, timestamp, time_domain, query_options): Query VIO data by timestamp.get_vrs_vio_high_freq_data_by_index(stream_id, index): Query VIO high frequency data by index.get_vrs_vio_high_freq_data_by_time_ns(stream_id, timestamp, time_domain, query_options): Query VIO high frequency data by timestamp.  # Check if VIO data is available vio_label = &quot;vio&quot; vio_stream_id = pilot_data_provider.get_vrs_stream_id_from_label(vio_label) vio_high_freq_label = &quot;vio_high_frequency&quot; vio_high_freq_stream_id = pilot_data_provider.get_vrs_stream_id_from_label(vio_high_freq_label) print(&quot;=== VIO Data Availability ===&quot;) if vio_stream_id is None: print(f&quot;‚ùå {vio_label} data stream does not exist in this VRS file.&quot;) else: print(f&quot;‚úÖ Found {vio_label} data stream: {vio_stream_id}&quot;) if vio_high_freq_stream_id is None: print(f&quot;‚ùå {vio_high_freq_label} data stream does not exist in this VRS file.&quot;) else: print(f&quot;‚úÖ Found {vio_high_freq_label} data stream: {vio_high_freq_stream_id}&quot;) # Sample VIO data (if available) if vio_stream_id is not None: print(&quot;\\n=== VIO Data Sample ===&quot;) # Find the first valid VIO data sample num_vio_samples = pilot_data_provider.get_vrs_num_data(vio_stream_id) first_valid_index = None for idx in range(100, 100 + min(10, num_vio_samples)): vio_data = pilot_data_provider.get_vrs_vio_data_by_index(vio_stream_id, idx) if vio_data is not None: # Check if VIO data is valid (we'll import the status enums) from projectaria_tools.core.sensor_data import VioStatus, TrackingQuality if (vio_data.status == VioStatus.VALID and vio_data.pose_quality == TrackingQuality.GOOD): first_valid_index = idx break if first_valid_index is not None: vio_data = pilot_data_provider.get_vrs_vio_data_by_index(vio_stream_id, first_valid_index) print(&quot;=&quot; * 50) print(f&quot;First VALID VIO Data Sample (Index: {first_valid_index})&quot;) print(&quot;=&quot; * 50) # Session Information print(f&quot;Session UID: {vio_data.frontend_session_uid}&quot;) print(f&quot;Frame ID: {vio_data.frame_id}&quot;) # Timestamps print(f&quot;Capture Time: {vio_data.capture_timestamp_ns} ns&quot;) print(f&quot;Unix Time: {vio_data.unix_timestamp_ns} ns&quot;) # Quality Status print(f&quot;Status: {vio_data.status}&quot;) print(f&quot;Pose Quality: {vio_data.pose_quality}&quot;) print(f&quot;Visual Quality: {vio_data.visual_tracking_quality}&quot;) # Transforms print(f&quot;Transform Odometry ‚Üí Body IMU:\\n{vio_data.transform_odometry_bodyimu.to_matrix()}&quot;) print(f&quot;Transform Body IMU ‚Üí Device:\\n{vio_data.transform_bodyimu_device.to_matrix()}&quot;) # Motion print(f&quot;Linear Velocity: {vio_data.linear_velocity_in_odometry}&quot;) print(f&quot;Angular Velocity: {vio_data.angular_velocity_in_bodyimu}&quot;) print(f&quot;Gravity Vector: {vio_data.gravity_in_odometry}&quot;) else: print(&quot;‚ö†Ô∏è No valid VIO sample found&quot;) # Sample VIO High Frequency data (if available) if vio_high_freq_stream_id is not None: print(&quot;\\n=== VIO High-Frequency Data Sample ===&quot;) # Find the first VIO high_frequency data sample with high quality value num_vio_high_freq_samples = pilot_data_provider.get_vrs_num_data(vio_high_freq_stream_id) first_valid_index = None for idx in range(min(10, num_vio_high_freq_samples)): vio_high_freq_data = pilot_data_provider.get_vrs_vio_high_freq_data_by_index(vio_high_freq_stream_id, idx) if vio_high_freq_data is not None and vio_high_freq_data.quality_score &gt; 0.5: first_valid_index = idx break if first_valid_index is not None: vio_high_freq_data = pilot_data_provider.get_vrs_vio_high_freq_data_by_index(vio_high_freq_stream_id, first_valid_index) print(&quot;=&quot; * 50) print(f&quot;First VIO High Freq Data Sample with good quality score (Index: {first_valid_index})&quot;) print(&quot;=&quot; * 50) # Timestamps, convert timedelta to nanoseconds capture_timestamp_ns = int(vio_high_freq_data.tracking_timestamp.total_seconds() * 1e9) # Session Information print(f&quot;Session UID: {vio_high_freq_data.session_uid}&quot;) # Timestamps print(f&quot;Tracking Time: {capture_timestamp_ns} ns&quot;) # Quality print(f&quot;Quality Score: {vio_high_freq_data.quality_score:.3f}&quot;) # Transform print(f&quot;Transform Odometry ‚Üí Device:\\n{vio_high_freq_data.transform_odometry_device.to_matrix()}&quot;) # Motion print(f&quot;Linear Velocity: {vio_high_freq_data.device_linear_velocity_odometry}&quot;) print(f&quot;Angular Velocity: {vio_high_freq_data.angular_velocity_device}&quot;) print(f&quot;Gravity Vector: {vio_high_freq_data.gravity_odometry}&quot;) else: print(&quot;‚ö†Ô∏è No valid VIO high frequency sample found&quot;)   ","version":"Next","tagName":"h3"},{"title":"VIO Trajectory Visualization‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - VRS Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/vrs_loading#vio-trajectory-visualization","content":" The following code demonstrates how to visualize a VIO trajectory in a 3D view, showing the device's movement through space over time.  # VIO trajectory visualization using sequential data access (matching Tutorial 5 approach) if vio_stream_id is not None: print(&quot;\\n=== Visualizing on-device VIO trajectory in 3D view ===&quot;) # Initialize Rerun for 3D visualization rr.init(&quot;rerun_viz_vio_trajectory&quot;) rr.notebook_show() # Get device calibration for glasses outline device_calib = pilot_data_provider.get_vrs_device_calibration() # Get time bounds for VIO data first_timestamp_ns = pilot_data_provider.get_vrs_timestamps_ns(vio_stream_id, TimeDomain.DEVICE_TIME)[0] last_timestamp_ns = pilot_data_provider.get_vrs_timestamps_ns(vio_stream_id, TimeDomain.DEVICE_TIME)[-1] # Play for only 3 seconds total_length_ns = last_timestamp_ns - first_timestamp_ns skip_begin_ns = int(15 * 1e9) # Skip 15 seconds duration_ns = int(3 * 1e9) # 3 seconds skip_end_ns = max(total_length_ns - skip_begin_ns - duration_ns, 0) start_time_ns = first_timestamp_ns + skip_begin_ns end_time_ns = start_time_ns + duration_ns print(f&quot;Visualizing VIO trajectory from {start_time_ns} to {end_time_ns} ns&quot;) # Plot VIO trajectory in 3D view using sequential approach # Need to keep a cache to store already-loaded trajectory vio_traj_cached_full = [] valid_vio_count = 0 # Get VIO timestamps in the time window all_vio_timestamps = pilot_data_provider.get_vrs_timestamps_ns(vio_stream_id, TimeDomain.DEVICE_TIME) window_timestamps = [ts for ts in all_vio_timestamps if start_time_ns &lt;= ts &lt;= end_time_ns] print(f&quot;Processing {len(window_timestamps)} VIO samples in time window&quot;) for timestamp_ns in window_timestamps: # Query VIO data by timestamp vio_data = pilot_data_provider.get_vrs_vio_data_by_time_ns( vio_stream_id, timestamp_ns, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST ) if vio_data is not None: # Check VIO data validity, only plot for valid data if (vio_data.status == VioStatus.VALID and vio_data.pose_quality == TrackingQuality.GOOD): # Set timestamp rr.set_time_nanos(&quot;device_time&quot;, vio_data.capture_timestamp_ns) # Set and plot the Device pose for the current timestamp, as a RGB axis T_World_Device = ( vio_data.transform_odometry_bodyimu @ vio_data.transform_bodyimu_device ) # Plot device pose as coordinate frame rr.log( &quot;world/device&quot;, ToTransform3D( T_World_Device, axis_length=0.05, ), ) # Also plot Aria glass outline for visualization aria_glasses_point_outline = AriaGlassesOutline( device_calib, use_cad_calib=True ) rr.log( &quot;world/device/glasses_outline&quot;, rr.LineStrips3D( aria_glasses_point_outline, colors=[200,200,200], radii=5e-4, ), ) # Plot gravity direction vector rr.log( &quot;world/vio_gravity&quot;, rr.Arrows3D( origins=[T_World_Device.translation()[0]], vectors=[ vio_data.gravity_in_odometry * 1e-2 ], # length converted from 9.8 meter -&gt; 10 cm colors=[101,67,33], radii=1.5e-3, ), static=False, ) # Plot VIO trajectory that are cached so far vio_traj_cached_full.append(T_World_Device.translation()[0]) rr.log( &quot;world/vio_trajectory&quot;, rr.LineStrips3D( vio_traj_cached_full, colors=[173, 216, 255], radii=1.5e-3, ), static=False, ) valid_vio_count += 1 else: print(f&quot;VIO data is invalid for timestamp {timestamp_ns}&quot;) print(f&quot;Visualized {valid_vio_count} valid VIO poses out of {len(window_timestamps)} samples&quot;) else: print(&quot;Skipping VIO trajectory visualization - no VIO data available.&quot;)   ","version":"Next","tagName":"h3"},{"title":"VIO High Frequency Visualization‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - VRS Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/vrs_loading#vio-high-frequency-visualization","content":" VIO High Frequency data provides much higher temporal resolution (~800Hz) compared to regular VIO data. This section demonstrates how to visualize the high-frequency trajectory data.  # VIO High Frequency trajectory visualization (if available) if vio_high_freq_stream_id is not None: print(&quot;\\n=== Visualizing VIO High Frequency trajectory ===&quot;) # Initialize Rerun for high frequency visualization rr.init(&quot;rerun_viz_vio_high_freq_trajectory&quot;) rr.notebook_show() # Get device calibration for glasses outline device_calib = pilot_data_provider.get_vrs_device_calibration() # Get time bounds for VIO high frequency data first_timestamp_ns = pilot_data_provider.get_vrs_timestamps_ns(vio_high_freq_stream_id, TimeDomain.DEVICE_TIME)[0] last_timestamp_ns = pilot_data_provider.get_vrs_timestamps_ns(vio_high_freq_stream_id, TimeDomain.DEVICE_TIME)[-1] # Play for only 2 seconds (shorter duration due to high frequency) total_length_ns = last_timestamp_ns - first_timestamp_ns skip_begin_ns = int(15 * 1e9) # Skip 15 seconds duration_ns = int(2 * 1e9) # 2 seconds skip_end_ns = max(total_length_ns - skip_begin_ns - duration_ns, 0) start_time_ns = first_timestamp_ns + skip_begin_ns end_time_ns = start_time_ns + duration_ns print(f&quot;Visualizing VIO High Frequency trajectory from {start_time_ns} to {end_time_ns} ns&quot;) # Get VIO high frequency timestamps in the time window all_vio_hf_timestamps = pilot_data_provider.get_vrs_timestamps_ns(vio_high_freq_stream_id, TimeDomain.DEVICE_TIME) window_timestamps = [ts for ts in all_vio_hf_timestamps if start_time_ns &lt;= ts &lt;= end_time_ns] # Subsample for performance (every 10th sample due to high frequency) subsampled_timestamps = window_timestamps[::10] print(f&quot;Processing {len(subsampled_timestamps)} VIO High Frequency samples (subsampled from {len(window_timestamps)})&quot;) # Plot VIO High Frequency trajectory in 3D view vio_hf_traj_cached_full = [] valid_vio_hf_count = 0 for timestamp_ns in subsampled_timestamps: # Query VIO high frequency data by timestamp vio_hf_data = pilot_data_provider.get_vrs_vio_high_freq_data_by_time_ns( vio_high_freq_stream_id, timestamp_ns, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST ) if vio_hf_data is not None and vio_hf_data.quality_score &gt; 0.5: # Set timestamp rr.set_time_nanos(&quot;device_time&quot;, int(vio_hf_data.tracking_timestamp.total_seconds() * 1e9)) # Plot device pose as coordinate frame T_World_Device = vio_hf_data.transform_odometry_device rr.log( &quot;world/device&quot;, ToTransform3D( T_World_Device, axis_length=0.03, # Smaller axis for high frequency data ), ) # Also plot Aria glass outline for visualization aria_glasses_point_outline = AriaGlassesOutline( device_calib, use_cad_calib=True ) rr.log( &quot;world/device/glasses_outline&quot;, rr.LineStrips3D( aria_glasses_point_outline, colors=[150,150,150], # Slightly different color for high frequency radii=3e-4, ), ) # Plot gravity direction vector rr.log( &quot;world/vio_hf_gravity&quot;, rr.Arrows3D( origins=[T_World_Device.translation()[0]], vectors=[ vio_hf_data.gravity_odometry * 1e-2 ], # length converted from 9.8 meter -&gt; 10 cm colors=[67,101,33], # Different color for high frequency radii=1e-3, ), static=False, ) # Plot VIO High Frequency trajectory that are cached so far vio_hf_traj_cached_full.append(T_World_Device.translation()[0]) rr.log( &quot;world/vio_hf_trajectory&quot;, rr.LineStrips3D( vio_hf_traj_cached_full, colors=[255, 173, 216], # Different color for high frequency radii=1e-3, ), static=False, ) valid_vio_hf_count += 1 print(f&quot;Visualized {valid_vio_hf_count} valid VIO High Frequency poses out of {len(subsampled_timestamps)} samples&quot;) else: print(&quot;Skipping VIO High Frequency trajectory visualization - no VIO High Frequency data available.&quot;)  ","version":"Next","tagName":"h3"},{"title":"Aria Gen2 Pilot Dataset Tutorial - Algorithm Data Loading","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/algorithm_loading","content":"","keywords":"","version":"Next"},{"title":"What You'll Learn‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - Algorithm Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/algorithm_loading#what-youll-learn","content":" Load and visualize Heart Rate monitoring dataAccess Diarization (speaker identification) resultsWork with Hand-Object Interaction segmentation dataExplore Egocentric Voxel Lifting 3D scene reconstructionProcess Foundation Stereo depth estimation dataUnderstand data structures and API patterns for algorithm outputs  ","version":"Next","tagName":"h2"},{"title":"Algorithm Data Overview‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - Algorithm Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/algorithm_loading#algorithm-data-overview","content":" The Aria Gen2 Pilot Dataset includes 5 types of algorithm outputs. Please find the introduction to algorithms here.  Heart Rate MonitoringDiarizationHand-Object InteractionEgocentric Voxel LiftingFoundation Stereo  Important Notes:  These are algorithm outputs (post-processed results), distinct from raw VRS sensor dataAlgorithm data availability varies by sequence - not all sequences contain all algorithm outputsEach algorithm has its own data structure and query patterns  ","version":"Next","tagName":"h2"},{"title":"Import Required Libraries‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - Algorithm Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/algorithm_loading#import-required-libraries","content":" The following libraries are required for this tutorial:  # Standard library imports import numpy as np import os from pathlib import Path from datetime import timedelta # Project Aria Tools imports from projectaria_tools.core.stream_id import StreamId from projectaria_tools.core.sensor_data import TimeDomain, TimeQueryOptions from projectaria_tools.core.calibration import DeviceCalibration from projectaria_tools.utils.rerun_helpers import ( create_hand_skeleton_from_landmarks, AriaGlassesOutline, ToTransform3D ) # Aria Gen2 Pilot Dataset imports from aria_gen2_pilot_dataset import AriaGen2PilotDataProvider from aria_gen2_pilot_dataset.data_provider.aria_gen2_pilot_dataset_data_types import ( HeartRateData, DiarizationData, HandObjectInteractionData, BoundingBox3D, BoundingBox2D, CameraIntrinsicsAndPose ) # Visualization library import rerun as rr   ","version":"Next","tagName":"h2"},{"title":"Initialize Data Provider‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - Algorithm Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/algorithm_loading#initialize-data-provider","content":" The AriaGen2PilotDataProvider is the main interface for accessing data from the Aria Gen2 Pilot Dataset. It provides methods to query algorithm data, check availability, and access device calibration information.  ‚ö†Ô∏è Important: Update the sequence_path below to point to your downloaded Aria Gen2 Pilot Dataset sequence folder.  # Replace with the actual path to your downloaded sequence folder sequence_path = &quot;path/to/your/sequence_folder&quot; # Initialize the data provider pilot_data_provider = AriaGen2PilotDataProvider(sequence_path)   ","version":"Next","tagName":"h2"},{"title":"Check Available Algorithm Data‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - Algorithm Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/algorithm_loading#check-available-algorithm-data","content":" Each Aria Gen2 Pilot dataset sequence may contain different algorithm outputs. Let's check what's available in this sequence.  # Check what algorithm data types are available in this sequence print(&quot;Algorithm Data Availability in This Sequence:&quot;) print(&quot;=&quot; * 60) print(f&quot;Heart Rate Monitoring: {'‚úÖ' if pilot_data_provider.has_heart_rate_data() else '‚ùå'}&quot;) print(f&quot;Diarization: {'‚úÖ' if pilot_data_provider.has_diarization_data() else '‚ùå'}&quot;) print(f&quot;Hand-Object Interaction: {'‚úÖ' if pilot_data_provider.has_hand_object_interaction_data() else '‚ùå'}&quot;) print(f&quot;Egocentric Voxel Lifting: {'‚úÖ' if pilot_data_provider.has_egocentric_voxel_lifting_data() else '‚ùå'}&quot;) print(f&quot;Foundation Stereo: {'‚úÖ' if pilot_data_provider.has_stereo_depth_data() else '‚ùå'}&quot;) print(&quot;=&quot; * 60) # Count available algorithms available_algorithms = [ pilot_data_provider.has_heart_rate_data(), pilot_data_provider.has_diarization_data(), pilot_data_provider.has_hand_object_interaction_data(), pilot_data_provider.has_egocentric_voxel_lifting_data(), pilot_data_provider.has_stereo_depth_data() ] available_count = sum(available_algorithms) print(f&quot;\\nTotal available algorithms: {available_count}/5&quot;)   ","version":"Next","tagName":"h2"},{"title":"Heart Rate Monitoring‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - Algorithm Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/algorithm_loading#heart-rate-monitoring","content":" Heart rate monitoring provides physiological data extracted from PPG (Photoplethysmography) sensors in the Aria glasses.  ","version":"Next","tagName":"h2"},{"title":"Heart Rate Data Structure‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - Algorithm Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/algorithm_loading#heart-rate-data-structure","content":" The HeartRateData class contains:  Field Name\tType\tDescriptiontimestamp_ns\tint\tTimestamp in device time domain (nanoseconds) heart_rate_bpm\tint\tHeart rate in beats per minute  ","version":"Next","tagName":"h3"},{"title":"Heart Rate API Reference‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - Algorithm Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/algorithm_loading#heart-rate-api-reference","content":" has_heart_rate_data(): Check if heart rate data is availableget_heart_rate_by_index(index): Get heart rate data by indexget_heart_rate_by_timestamp_ns(timestamp_ns, time_domain, time_query_options): Get heart rate data by timestampget_heart_rate_total_number(): Get total number of heart rate entries  # Heart Rate Data Loading and Analysis if pilot_data_provider.has_heart_rate_data(): print(&quot;‚úÖ Heart Rate data is available&quot;) # Get total number of heart rate entries total_heart_rate = pilot_data_provider.get_heart_rate_total_number() print(f&quot;Total heart rate entries: {total_heart_rate}&quot;) # Sample first few heart rate entries print(&quot;\\n=== Heart Rate Data Sample ===&quot;) sample_count = min(5, total_heart_rate) for i in range(sample_count): heart_rate_data = pilot_data_provider.get_heart_rate_by_index(i) if heart_rate_data is not None: print(f&quot;Entry {i}: timestamp={heart_rate_data.timestamp_ns} ns, heart_rate={heart_rate_data.heart_rate_bpm} bpm&quot;) # Query heart rate data by timestamp if total_heart_rate &gt; 0: # Get a sample timestamp from the middle of the sequence sample_heart_rate = pilot_data_provider.get_heart_rate_by_index(total_heart_rate // 2) if sample_heart_rate is not None: query_timestamp = sample_heart_rate.timestamp_ns # Query heart rate at this timestamp heart_rate_at_time = pilot_data_provider.get_heart_rate_by_timestamp_ns( query_timestamp, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST ) if heart_rate_at_time is not None: print(f&quot;\\nHeart rate at timestamp {query_timestamp}: {heart_rate_at_time.heart_rate_bpm} bpm&quot;) else: print(&quot;‚ùå Heart Rate data is not available in this sequence&quot;)   # Heart Rate Visualization if pilot_data_provider.has_heart_rate_data(): print(&quot;\\n=== Visualizing Heart Rate Data ===&quot;) # Initialize Rerun for visualization rr.init(&quot;rerun_viz_heart_rate&quot;) rr.notebook_show() # Get all heart rate data for time series visualization total_heart_rate = pilot_data_provider.get_heart_rate_total_number() # Sample heart rate data (every 10th entry for performance) sample_indices = range(0, total_heart_rate, max(1, total_heart_rate // 50)) for i in sample_indices: heart_rate_data = pilot_data_provider.get_heart_rate_by_index(i) if heart_rate_data is not None: # Convert timestamp to seconds for visualization timestamp_seconds = heart_rate_data.timestamp_ns / 1e9 # Set time and log heart rate as scalar (following visualizer pattern) rr.set_time_seconds(&quot;device_time&quot;, timestamp_seconds) rr.log(&quot;heart_rate_bpm&quot;, rr.Scalar(heart_rate_data.heart_rate_bpm)) else: print(&quot;Skipping heart rate visualization - no heart rate data available.&quot;)   ","version":"Next","tagName":"h3"},{"title":"Diarization‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - Algorithm Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/algorithm_loading#diarization","content":" Diarization provides speaker identification and voice activity detection from audio data.  ","version":"Next","tagName":"h2"},{"title":"Diarization Data Structure‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - Algorithm Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/algorithm_loading#diarization-data-structure","content":" The DiarizationData class contains:  Field Name\tType\tDescriptionstart_timestamp_ns\tint\tStart timestamp in device time domain (nanoseconds) end_timestamp_ns\tint\tEnd timestamp in device time domain (nanoseconds) speaker\tstr\tUnique identifier of the speaker content\tstr\tASR transcription text  ","version":"Next","tagName":"h3"},{"title":"Diarization API Reference‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - Algorithm Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/algorithm_loading#diarization-api-reference","content":" has_diarization_data(): Check if diarization data is availableget_diarization_data_by_index(index): Get diarization data by indexget_diarization_data_by_timestamp_ns(timestamp_ns, time_domain): Get diarization data containing timestamp (returns list)get_diarization_data_by_start_and_end_timestamps(start_ns, end_ns, time_domain): Get diarization data in time rangeget_diarization_data_total_number(): Get total number of diarization entries  # Diarization Data Loading and Analysis if pilot_data_provider.has_diarization_data(): print(&quot;‚úÖ Diarization data is available&quot;) # Get total number of diarization entries total_diarization = pilot_data_provider.get_diarization_data_total_number() print(f&quot;Total diarization entries: {total_diarization}&quot;) # Sample first few diarization entries print(&quot;\\n=== Diarization Data Sample ===&quot;) sample_count = min(3, total_diarization) for i in range(sample_count): diarization_data = pilot_data_provider.get_diarization_data_by_index(i) if diarization_data is not None: duration_ms = (diarization_data.end_timestamp_ns - diarization_data.start_timestamp_ns) / 1e6 print(f&quot;Entry {i}:&quot;) print(f&quot; Speaker: {diarization_data.speaker}&quot;) print(f&quot; Duration: {duration_ms:.1f} ms&quot;) print(f&quot; Content: {diarization_data.content[:100]}{'...' if len(diarization_data.content) &gt; 100 else ''}&quot;) print() # Query diarization data by timestamp if total_diarization &gt; 0: # Get a sample timestamp from the middle of the sequence sample_diarization = pilot_data_provider.get_diarization_data_by_index(total_diarization // 2) if sample_diarization is not None: query_timestamp = sample_diarization.start_timestamp_ns # Query diarization at this timestamp diarization_at_time = pilot_data_provider.get_diarization_data_by_timestamp_ns( query_timestamp, TimeDomain.DEVICE_TIME ) print(f&quot;Diarization entries at timestamp {query_timestamp}: {len(diarization_at_time)}&quot;) for entry in diarization_at_time[:2]: # Show first 2 entries print(f&quot; Speaker: {entry.speaker}, Content: {entry.content[:50]}...&quot;) else: print(&quot;‚ùå Diarization data is not available in this sequence&quot;)   # Diarization Visualization if pilot_data_provider.has_diarization_data(): print(&quot;\\n=== Visualizing Diarization Data ===&quot;) # Initialize Rerun for visualization rr.init(&quot;rerun_viz_diarization&quot;) rr.notebook_show() # Get RGB camera stream for overlay rgb_stream_id = pilot_data_provider.get_vrs_stream_id_from_label(&quot;camera-rgb&quot;) if rgb_stream_id is not None: # Get time bounds for RGB images first_timestamp_ns = pilot_data_provider.get_vrs_timestamps_ns(rgb_stream_id, TimeDomain.DEVICE_TIME)[0] # Sample a few RGB frames for visualization sample_timestamps = [] for i in range(50, min(100, pilot_data_provider.get_vrs_num_data(rgb_stream_id)), 2): rgb_data, rgb_record = pilot_data_provider.get_vrs_image_data_by_index(rgb_stream_id, i) sample_timestamps.append(rgb_record.capture_timestamp_ns) # Visualize RGB images with diarization overlay for timestamp_ns in sample_timestamps: # Get RGB image rgb_data, rgb_record = pilot_data_provider.get_vrs_image_data_by_time_ns( rgb_stream_id, timestamp_ns, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST ) if rgb_data.is_valid(): # Visualize the RGB image rr.set_time_nanos(&quot;device_time&quot;, rgb_record.capture_timestamp_ns) rr.log(&quot;camera_rgb&quot;, rr.Image(rgb_data.to_numpy_array())) # Get diarization data for this timestamp diarization_entries = pilot_data_provider.get_diarization_data_by_timestamp_ns( timestamp_ns, TimeDomain.DEVICE_TIME ) # Add diarization text overlay (following visualizer pattern) if diarization_entries: # Get image dimensions for positioning (following visualizer logic) width, height = rgb_data.get_width(), rgb_data.get_height() # Clear previous diarization overlays rr.log(&quot;camera_rgb/diarization&quot;, rr.Clear.recursive()) # Plot each diarization entry (following visualizer pattern exactly) for i, conv_data in enumerate(diarization_entries[:3]): # Show first 3 entries text_content = f&quot;{conv_data.speaker}: {conv_data.content}&quot; text_x = width // 2 # Center horizontally text_y = height - height // 15 - (i * 10 * 7) # Bottom positioning with vertical spacing rr.log( f&quot;camera_rgb/diarization/conversation_text_{i}&quot;, rr.Points2D( positions=[[text_x, text_y]], labels=[text_content], colors=[255, 255, 255], # White text from plot_style.py DIARIZATION_TEXT radii=10 # Text size from plot_style.py ) ) else: print(&quot;Skipping diarization visualization - no diarization data available.&quot;)   ","version":"Next","tagName":"h3"},{"title":"Hand-Object Interaction‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - Algorithm Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/algorithm_loading#hand-object-interaction","content":" Hand-Object Interaction provides segmentation masks for hands and interacting objects, enabling analysis of hand-object relationships.  ","version":"Next","tagName":"h2"},{"title":"Hand-Object Interaction Data Structure‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - Algorithm Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/algorithm_loading#hand-object-interaction-data-structure","content":" The HandObjectInteractionData class contains:  Field Name\tType\tDescriptiontimestamp_ns\tint\tTimestamp in device time domain (nanoseconds) category_id\tint\tCategory: 1=left_hand, 2=right_hand, 3=interacting_object masks\tList[np.ndarray]\tList of decoded binary masks (height, width) uint8 arrays bboxes\tList[List[float]]\tList of bounding boxes [x, y, width, height] for each mask scores\tList[float]\tList of confidence scores [0.0, 1.0] for each mask  ","version":"Next","tagName":"h3"},{"title":"Hand-Object Interaction API Reference‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - Algorithm Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/algorithm_loading#hand-object-interaction-api-reference","content":" has_hand_object_interaction_data(): Check if HOI data is availableget_hoi_data_by_timestamp_ns(timestamp_ns, time_domain, time_query_options): Get HOI data by timestamp (returns list)get_hoi_data_by_index(index): Get HOI data by indexget_hoi_total_number(): Get total number of HOI timestamps  # Hand-Object Interaction Data Loading and Analysis if pilot_data_provider.has_hand_object_interaction_data(): print(&quot;‚úÖ Hand-Object Interaction data is available&quot;) # Get total number of HOI entries total_hoi = pilot_data_provider.get_hoi_total_number() print(f&quot;Total HOI timestamps: {total_hoi}&quot;) # Sample first few HOI entries print(&quot;\\n=== Hand-Object Interaction Data Sample ===&quot;) sample_count = min(3, total_hoi) for i in range(sample_count): hoi_data_list = pilot_data_provider.get_hoi_data_by_index(i) if hoi_data_list is not None and len(hoi_data_list) &gt; 0: print(f&quot;Timestamp {i}: {len(hoi_data_list)} HOI entries&quot;) for j, hoi_data in enumerate(hoi_data_list[:2]): # Show first 2 entries category_names = {1: &quot;left_hand&quot;, 2: &quot;right_hand&quot;, 3: &quot;interacting_object&quot;} category_name = category_names.get(hoi_data.category_id, &quot;unknown&quot;) print(f&quot; Entry {j}: {category_name}, {len(hoi_data.masks)} masks, avg_score={np.mean(hoi_data.scores):.3f}&quot;) if len(hoi_data.masks) &gt; 0: print(f&quot; Mask shape: {hoi_data.masks[0].shape}&quot;) # Query HOI data by timestamp if total_hoi &gt; 0: # Get a sample timestamp from the middle of the sequence sample_hoi_list = pilot_data_provider.get_hoi_data_by_index(total_hoi // 2) if sample_hoi_list is not None and len(sample_hoi_list) &gt; 0: query_timestamp = sample_hoi_list[0].timestamp_ns # Query HOI at this timestamp hoi_at_time = pilot_data_provider.get_hoi_data_by_timestamp_ns( query_timestamp, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST ) if hoi_at_time is not None: print(f&quot;\\nHOI entries at timestamp {query_timestamp}: {len(hoi_at_time)}&quot;) for entry in hoi_at_time[:2]: # Show first 2 entries category_names = {1: &quot;left_hand&quot;, 2: &quot;right_hand&quot;, 3: &quot;interacting_object&quot;} category_name = category_names.get(entry.category_id, &quot;unknown&quot;) print(f&quot; {category_name}: {len(entry.masks)} masks, scores={[f'{s:.2f}' for s in entry.scores[:3]]}&quot;) else: print(&quot;‚ùå Hand-Object Interaction data is not available in this sequence&quot;)   # Hand-Object Interaction Visualization if pilot_data_provider.has_hand_object_interaction_data(): print(&quot;\\n=== Visualizing Hand-Object Interaction Data ===&quot;) # Initialize Rerun for visualization rr.init(&quot;rerun_viz_hoi&quot;) rr.notebook_show() # Get RGB camera stream for overlay rgb_stream_id = pilot_data_provider.get_vrs_stream_id_from_label(&quot;camera-rgb&quot;) if rgb_stream_id is not None: # Get time bounds for RGB images first_timestamp_ns = pilot_data_provider.get_vrs_timestamps_ns(rgb_stream_id, TimeDomain.DEVICE_TIME)[0] # Sample a few RGB frames for visualization sample_timestamps = [] for i in range(0, min(10, pilot_data_provider.get_vrs_num_data(rgb_stream_id)), 2): rgb_data, rgb_record = pilot_data_provider.get_vrs_image_data_by_index(rgb_stream_id, i) sample_timestamps.append(rgb_record.capture_timestamp_ns) # Visualize RGB images with HOI overlay for timestamp_ns in sample_timestamps: # Get RGB image rgb_data, rgb_record = pilot_data_provider.get_vrs_image_data_by_time_ns( rgb_stream_id, timestamp_ns, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST ) if rgb_data.is_valid(): # Visualize the RGB image rr.set_time_nanos(&quot;device_time&quot;, rgb_record.capture_timestamp_ns) rr.log(&quot;camera_rgb&quot;, rr.Image(rgb_data.to_numpy_array())) # Get HOI data for this timestamp hoi_entries = pilot_data_provider.get_hoi_data_by_timestamp_ns( timestamp_ns, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST ) # Visualize HOI masks as overlays (following visualizer pattern exactly) if hoi_entries: # Clear previous HOI overlays (following visualizer pattern) rr.log(&quot;camera_rgb/hoi_overlay&quot;, rr.Clear.recursive()) # Filter out HOI data too far away from the current frame (following visualizer logic) rgb_frame_interval_ns = 33_333_333 # ~30 FPS if abs(hoi_entries[0].timestamp_ns - timestamp_ns) &gt; rgb_frame_interval_ns / 2: continue # Color mapping from plot_style.py (following visualizer pattern) category_to_plot_style = { 1: [119, 172, 48, 128], # Green for left hand (HOI_LEFT_HAND) 2: [217, 83, 255, 128], # Purple for right hand (HOI_RIGHT_HAND) 3: [237, 177, 32, 128] # Orange for interacting object (HOI_INTERACTING_OBJECT) } # Determine mask shape from the first valid mask (following visualizer logic) mask_shape = next( ( mask.shape for hoi_data in hoi_entries for mask in hoi_data.masks if mask is not None and mask.size &gt; 0 ), None, ) if mask_shape is None: continue # Initialize combined RGBA overlay (following visualizer pattern) combined_rgba_overlay = np.zeros((*mask_shape, 4), dtype=np.uint8) # Overlay each category's mask with its color (following visualizer logic) for hoi_data in hoi_entries: category_id = hoi_data.category_id plot_style_color = category_to_plot_style.get(category_id, None) if not plot_style_color: continue for mask in hoi_data.masks: if mask is None or mask.size == 0: continue foreground_pixels = mask &gt; 0 combined_rgba_overlay[foreground_pixels] = plot_style_color # Log the combined segmentation overlay as an image (following visualizer pattern) rr.log( &quot;camera_rgb/hoi_overlay/combined&quot;, rr.Image(combined_rgba_overlay) ) else: print(&quot;Skipping HOI visualization - no HOI data available.&quot;)   ","version":"Next","tagName":"h3"},{"title":"Egocentric Voxel Lifting‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - Algorithm Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/algorithm_loading#egocentric-voxel-lifting","content":" Egocentric Voxel Lifting provides 3D scene reconstruction from egocentric view, including 3D bounding boxes and object instance information.  ","version":"Next","tagName":"h2"},{"title":"Egocentric Voxel Lifting Data Structure‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - Algorithm Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/algorithm_loading#egocentric-voxel-lifting-data-structure","content":" The EVL system provides two main data types:  BoundingBox3D (3D world coordinates):  Field Name\tType\tDescriptionstart_timestamp_ns\tint\tTimestamp in device time domain (nanoseconds) bbox3d\tBoundingBox3dData\t3D bounding box data (AABB, transform, etc.)  BoundingBox3dData structure:  Field Name\tType\tDescriptiontransform_scene_object\tSE3\tObject 6DoF pose in the scene (world), where: point_in_scene = T_Scene_Object * point_in_object aabb\tList[float]\tObject AABB (axes-aligned-bounding-box) in the object's local coordinate frame, represented as [xmin, xmax, ymin, ymax, zmin, zmax]  BoundingBox2D (2D camera projections):  Field Name\tType\tDescriptionstart_timestamp_ns\tint\tTimestamp in device time domain (nanoseconds) bbox2d\tBoundingBox2dData\t2D bounding box data  BoundingBox2dData structure:  Field Name\tType\tDescriptionbox_range\tList[float]\t2D bounding box range as [xmin, xmax, ymin, ymax] visibility_ratio\tfloat\tVisibility ratio calculated by occlusion between objects. visibility_ratio = 1: object is not occluded, visibility_ratio = 0: object is fully occluded  InstanceInfo (object metadata):  category: Object category namename: Specific object name  ","version":"Next","tagName":"h3"},{"title":"Egocentric Voxel Lifting API Reference‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - Algorithm Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/algorithm_loading#egocentric-voxel-lifting-api-reference","content":" has_egocentric_voxel_lifting_data(): Check if EVL data is availableget_evl_3d_bounding_boxes_by_timestamp_ns(timestamp_ns, time_domain, time_query_options): Get 3D bounding boxes (returns Dict[int, BoundingBox3D])get_evl_2d_bounding_boxes_by_timestamp_ns(timestamp_ns, time_domain, camera_label): Get 2D bounding boxes for specific cameraget_evl_instance_info_by_id(instance_id): Get object category/name information  # Egocentric Voxel Lifting Data Loading and Analysis if pilot_data_provider.has_egocentric_voxel_lifting_data(): print(&quot;‚úÖ Egocentric Voxel Lifting data is available&quot;) # Get RGB camera stream for 2D projection rgb_stream_id = pilot_data_provider.get_vrs_stream_id_from_label(&quot;camera-rgb&quot;) if rgb_stream_id is not None: # Get a sample timestamp from RGB stream first_timestamp_ns = pilot_data_provider.get_vrs_timestamps_ns(rgb_stream_id, TimeDomain.DEVICE_TIME)[0] sample_timestamp = first_timestamp_ns + int(5e9) # 5 seconds into sequence # Query 3D bounding boxes evl_3d_bboxes = pilot_data_provider.get_evl_3d_bounding_boxes_by_timestamp_ns( sample_timestamp, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST ) if evl_3d_bboxes is not None: print(f&quot;\\n=== EVL 3D Bounding Boxes at timestamp {sample_timestamp} ===&quot;) print(f&quot;Found {len(evl_3d_bboxes)} 3D bounding boxes&quot;) for instance_id, bbox_3d in list(evl_3d_bboxes.items())[:3]: # Show first 3 # Get instance info instance_info = pilot_data_provider.get_evl_instance_info_by_id(instance_id) if instance_info is not None: print(f&quot;Instance {instance_id}: {instance_info.category} - {instance_info.name}&quot;) print(f&quot; AABB: {bbox_3d.bbox3d.aabb}&quot;) print(f&quot; Transform: {bbox_3d.bbox3d.transform_scene_object.to_matrix()[:3, 3]}&quot;) # Query 2D bounding boxes for RGB camera evl_2d_bboxes = pilot_data_provider.get_evl_2d_bounding_boxes_by_timestamp_ns( sample_timestamp, TimeDomain.DEVICE_TIME, &quot;camera-rgb&quot; ) if evl_2d_bboxes is not None: print(f&quot;\\n=== EVL 2D Bounding Boxes for RGB camera ===&quot;) print(f&quot;Found {len(evl_2d_bboxes)} 2D bounding boxes&quot;) for instance_id, bbox_2d in list(evl_2d_bboxes.items())[:3]: # Show first 3 print(f&quot;Instance {instance_id}: 2D bbox {bbox_2d.bbox2d.box_range}&quot;) else: print(&quot;‚ùå Egocentric Voxel Lifting data is not available in this sequence&quot;)   from aria_gen2_pilot_dataset.visualization.plot_utils import extract_bbox_projection_data, project_3d_bbox_to_2d_camera # Egocentric Voxel Lifting Visualization if pilot_data_provider.has_egocentric_voxel_lifting_data(): print(&quot;\\n=== Visualizing Egocentric Voxel Lifting Data ===&quot;) # Initialize Rerun for visualization rr.init(&quot;rerun_viz_evl&quot;) rr.notebook_show() # Get RGB camera stream for 2D projection rgb_stream_id = pilot_data_provider.get_vrs_stream_id_from_label(&quot;camera-rgb&quot;) if rgb_stream_id is not None: # Get time bounds for RGB images first_timestamp_ns = pilot_data_provider.get_vrs_timestamps_ns(rgb_stream_id, TimeDomain.DEVICE_TIME)[0] # Sample a few RGB frames for visualization sample_timestamps = [] for i in range(0, min(10, pilot_data_provider.get_vrs_num_data(rgb_stream_id)), 2): rgb_data, rgb_record = pilot_data_provider.get_vrs_image_data_by_index(rgb_stream_id, i) sample_timestamps.append(rgb_record.capture_timestamp_ns) # Visualize RGB images with EVL 2D and 3D bounding boxes for timestamp_ns in sample_timestamps: # Get RGB image rgb_data, rgb_record = pilot_data_provider.get_vrs_image_data_by_time_ns( rgb_stream_id, timestamp_ns, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST ) if rgb_data.is_valid(): # Visualize the RGB image rr.set_time_nanos(&quot;device_time&quot;, rgb_record.capture_timestamp_ns) rr.log(&quot;camera_rgb&quot;, rr.Image(rgb_data.to_numpy_array())) # Get EVL 3D bounding boxes for this timestamp evl_3d_bboxes = pilot_data_provider.get_evl_3d_bounding_boxes_by_timestamp_ns( timestamp_ns, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST ) # Visualize projected 3D bounding boxes (following visualizer pattern) if evl_3d_bboxes is not None: # Clear previous EVL overlays (following visualizer pattern) rr.log(&quot;camera_rgb/evl_3d_bboxes_projected&quot;, rr.Clear.recursive()) # Get trajectory pose from MPS data (following visualizer pattern exactly) trajectory_pose = pilot_data_provider.get_mps_closed_loop_pose( timestamp_ns, TimeDomain.DEVICE_TIME ) if trajectory_pose is not None: # Get RGB camera calibration for projection device_calibration = pilot_data_provider.get_vrs_device_calibration() rgb_camera_calibration = device_calibration.get_camera_calib(&quot;camera-rgb&quot;) if rgb_camera_calibration is not None: # Get transforms and image dimensions (following visualizer pattern exactly) T_world_device = trajectory_pose.transform_world_device T_device_camera = rgb_camera_calibration.get_transform_device_camera() T_world_camera = T_world_device @ T_device_camera # Get image dimensions image_width, image_height = rgb_camera_calibration.get_image_size() # Extract bbox data for projection using utility function (following visualizer pattern) projection_data = extract_bbox_projection_data(pilot_data_provider, evl_3d_bboxes) # Collect all projection results for batching all_projected_lines = [] all_line_colors = [] label_positions = [] label_texts = [] label_colors = [] # Project each bounding box using utility function (following visualizer pattern) for data in projection_data: projection_result = project_3d_bbox_to_2d_camera( corners_in_world=data[&quot;corners_world&quot;], T_world_camera=T_world_camera, camera_calibration=rgb_camera_calibration, image_width=image_width, image_height=image_height, label=data[&quot;label&quot;], ) if projection_result: projected_lines, line_colors, label_position = projection_result # Collect projection data for batching if projected_lines: all_projected_lines.extend(projected_lines) if line_colors and len(line_colors) &gt;= len(projected_lines): all_line_colors.extend(line_colors[:len(projected_lines)]) else: all_line_colors.extend([0, 255, 0] * len(projected_lines)) # Green color if label_position and data[&quot;label&quot;]: label_positions.append(label_position) label_texts.append(data[&quot;label&quot;]) label_colors.append([0, 255, 0]) # Green text # Log all projected lines in batch (following visualizer pattern) if all_projected_lines: rr.log( &quot;camera_rgb/evl_3d_bboxes_projected/wireframes&quot;, rr.LineStrips2D( all_projected_lines, colors=all_line_colors, radii=1.5 # Match plot_style.py EVL line thickness ) ) # Log all labels in batch (following visualizer pattern) if label_positions: rr.log( &quot;camera_rgb/evl_3d_bboxes_projected/labels&quot;, rr.Points2D( positions=label_positions, labels=label_texts, colors=label_colors, radii=10 # Text size from plot_style.py ) ) # Visualize 3D bounding boxes in world coordinates (following visualizer pattern exactly) if evl_3d_bboxes is not None: # Clear previous 3D bounding boxes (following visualizer pattern) rr.log(&quot;world/evl_3d_bboxes&quot;, rr.Clear.recursive()) bb3d_sizes = [] bb3d_centers = [] bb3d_quats_xyzw = [] bb3d_labels = [] for instance_id, boundingBox3d in evl_3d_bboxes.items(): # Extract BoundingBox3dData from our BoundingBox3D wrapper (following visualizer logic) bbox3d_data = boundingBox3d.bbox3d # Get AABB in object's local coordinates: [xmin, xmax, ymin, ymax, zmin, zmax] aabb = bbox3d_data.aabb # Calculate dimensions (following visualizer logic) object_dimensions = np.array([ aabb[1] - aabb[0], # width (xmax - xmin) aabb[3] - aabb[2], # height (ymax - ymin) aabb[5] - aabb[4], # depth (zmax - zmin) ]) # Get world center and rotation from transform_scene_object (following visualizer logic) T_scene_object = bbox3d_data.transform_scene_object quat_and_translation = np.squeeze(T_scene_object.to_quat_and_translation()) quaternion_wxyz = quat_and_translation[0:4] # [w, x, y, z] world_center = quat_and_translation[4:7] # [x, y, z] # Convert quaternion to ReRun format [x, y, z, w] (following visualizer logic) quat_xyzw = [ quaternion_wxyz[1], quaternion_wxyz[2], quaternion_wxyz[3], quaternion_wxyz[0], ] # Get label (following visualizer logic) label = f&quot;instance_{instance_id}&quot; instance_info = pilot_data_provider.get_evl_instance_info_by_id(instance_id) if instance_info: if hasattr(instance_info, &quot;category&quot;) and instance_info.category: label = instance_info.category elif hasattr(instance_info, &quot;name&quot;) and instance_info.name: label = instance_info.name # Add to lists (following visualizer pattern) bb3d_centers.append(world_center) bb3d_sizes.append(object_dimensions) bb3d_quats_xyzw.append(quat_xyzw) bb3d_labels.append(label) # Visualize using ReRun Boxes3D with plot style (following visualizer pattern exactly) if bb3d_sizes: # Split into batches of 20 (ReRun limitation, following visualizer logic) MAX_BOXES_PER_BATCH = 20 batch_id = 0 while batch_id * MAX_BOXES_PER_BATCH &lt; len(bb3d_sizes): start_idx = batch_id * MAX_BOXES_PER_BATCH end_idx = min(len(bb3d_sizes), start_idx + MAX_BOXES_PER_BATCH) rr.log( f&quot;world/evl_3d_bboxes/batch_{batch_id}&quot;, rr.Boxes3D( sizes=bb3d_sizes[start_idx:end_idx], centers=bb3d_centers[start_idx:end_idx], rotations=bb3d_quats_xyzw[start_idx:end_idx], labels=bb3d_labels[start_idx:end_idx], colors=[0, 255, 0, 70], # Green with alpha from plot_style.py EVL_BBOX_3D radii=0.005, # From plot_style.py EVL_BBOX_3D plot_3d_size show_labels=False, ) ) batch_id += 1 else: print(&quot;Skipping EVL visualization - no EVL data available.&quot;)   ","version":"Next","tagName":"h3"},{"title":"Foundation Stereo Depth‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - Algorithm Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/algorithm_loading#foundation-stereo-depth","content":" Foundation Stereo provides depth estimation from stereo camera pairs, including depth maps and rectified images.  ","version":"Next","tagName":"h2"},{"title":"Foundation Stereo Data Structure‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - Algorithm Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/algorithm_loading#foundation-stereo-data-structure","content":" The CameraIntrinsicsAndPose class contains:  Field Name\tType\tDescriptiontimestamp_ns\tint\tTimestamp in device time domain (nanoseconds) camera_projection\tCameraProjection\tCamera intrinsics and model information transform_world_camera\tSE3\tCamera pose in world coordinates  Depth Map Format:  Rectified depth maps of slam-front-left camera, 512 x 512, 16-bit grayscale PNG(1 unit = 1mm).  Rectified SLAM Image:  Matching rectified slam-front-left camera images, 8-bit grayscale PNG.  ","version":"Next","tagName":"h3"},{"title":"Foundation Stereo API Reference‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - Algorithm Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/algorithm_loading#foundation-stereo-api-reference","content":" has_stereo_depth_data(): Check if stereo depth data is availableget_stereo_depth_depth_map_by_index(index): Get depth map by indexget_stereo_depth_depth_map_by_timestamp_ns(timestamp_ns, time_domain, time_query_option): Get depth map by timestampget_stereo_depth_rectified_slam_front_left_by_index(index): Get rectified image by indexget_stereo_depth_rectified_slam_front_left_by_timestamp_ns(timestamp_ns, time_domain, time_query_option): Get rectified image by timestampget_stereo_depth_camera_intrinsics_and_pose_by_index(index): Get camera info by indexget_stereo_depth_camera_intrinsics_and_pose_by_timestamp_ns(timestamp_ns, time_domain, time_query_option): Get camera info by timestampget_stereo_depth_data_total_number(): Get total number of depth entries  # Foundation Stereo Depth Data Loading and Analysis if pilot_data_provider.has_stereo_depth_data(): print(&quot;‚úÖ Foundation Stereo data is available&quot;) # Get total number of stereo depth entries total_stereo = pilot_data_provider.get_stereo_depth_data_total_number() print(f&quot;Total stereo depth entries: {total_stereo}&quot;) # Sample first few stereo depth entries print(&quot;\\n=== Foundation Stereo Data Sample ===&quot;) sample_count = min(3, total_stereo) for i in range(sample_count): # Get depth map depth_map = pilot_data_provider.get_stereo_depth_depth_map_by_index(i) # Get rectified image rectified_image = pilot_data_provider.get_stereo_depth_rectified_slam_front_left_by_index(i) # Get camera info camera_info = pilot_data_provider.get_stereo_depth_camera_intrinsics_and_pose_by_index(i) if depth_map is not None: print(f&quot;Entry {i}:&quot;) print(f&quot; Depth map shape: {depth_map.shape}, dtype: {depth_map.dtype}&quot;) print(f&quot; Depth range: {depth_map[depth_map &gt; 0].min()}-{depth_map[depth_map &gt; 0].max()} mm&quot;) print(f&quot; Valid pixels: {np.sum(depth_map &gt; 0)}/{depth_map.size} ({100*np.sum(depth_map &gt; 0)/depth_map.size:.1f}%)&quot;) if rectified_image is not None: print(f&quot; Rectified image shape: {rectified_image.shape}&quot;) if camera_info is not None: print(f&quot; Camera model: {camera_info.camera_projection.model_name()}&quot;) print(f&quot; Focal lengths: {camera_info.camera_projection.get_focal_lengths()}&quot;) print(f&quot; Principal point: {camera_info.camera_projection.get_principal_point()}&quot;) print(f&quot; Projection params: {camera_info.camera_projection.projection_params()}&quot;) # Query stereo depth data by timestamp if total_stereo &gt; 0: slam_front_left_stream_id = pilot_data_provider.get_vrs_stream_id_from_label(&quot;slam-front-left&quot;) sample_timestamps = [] for i in range(50, min(100, pilot_data_provider.get_vrs_num_data(slam_front_left_stream_id)), 2): rgb_data, rgb_record = pilot_data_provider.get_vrs_image_data_by_index(rgb_stream_id, i) sample_timestamps.append(rgb_record.capture_timestamp_ns) if sample_timestamp is not None: # Query depth map at this timestamp depth_at_time = pilot_data_provider.get_stereo_depth_depth_map_by_timestamp_ns( sample_timestamp, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST ) if depth_at_time is not None: print(f&quot;\\nDepth map at timestamp {sample_timestamp}:&quot;) print(f&quot; Shape: {depth_at_time.shape}&quot;) print(f&quot; Valid depth range: {depth_at_time[depth_at_time &gt; 0].min()}-{depth_at_time[depth_at_time &gt; 0].max()} mm&quot;) else: print(&quot;‚ùå Foundation Stereo data is not available in this sequence&quot;)   # Foundation Stereo Depth Visualization if pilot_data_provider.has_stereo_depth_data(): print(&quot;\\n=== Visualizing Foundation Stereo Depth Data ===&quot;) # Initialize Rerun for visualization rr.init(&quot;rerun_viz_stereo_depth&quot;) rr.notebook_show() # Get total number of stereo depth entries total_stereo = pilot_data_provider.get_stereo_depth_data_total_number() slam_front_left_stream_id = pilot_data_provider.get_vrs_stream_id_from_label(&quot;slam-front-left&quot;) sample_timestamps = [] for i in range(50, min(100, pilot_data_provider.get_vrs_num_data(slam_front_left_stream_id)), 2): rgb_data, rgb_record = pilot_data_provider.get_vrs_image_data_by_index(rgb_stream_id, i) sample_timestamps.append(rgb_record.capture_timestamp_ns) for query_timestamp_ns in sample_timestamps: # Get depth map depth_map = pilot_data_provider.get_stereo_depth_depth_map_by_timestamp_ns(timestamp_ns=query_timestamp_ns, time_domain=TimeDomain.DEVICE_TIME, time_query_option=TimeQueryOptions.CLOSEST) # Get rectified image rectified_image = pilot_data_provider.get_stereo_depth_rectified_slam_front_left_by_timestamp_ns(timestamp_ns=query_timestamp_ns, time_domain=TimeDomain.DEVICE_TIME, time_query_option=TimeQueryOptions.CLOSEST) # Get camera info camera_info = pilot_data_provider.get_stereo_depth_camera_intrinsics_and_pose_by_timestamp_ns(timestamp_ns=query_timestamp_ns, time_domain=TimeDomain.DEVICE_TIME, time_query_option=TimeQueryOptions.CLOSEST) if depth_map is not None and rectified_image is not None and camera_info is not None: # Set timestamp rr.set_time_nanos(&quot;device_time&quot;, camera_info.timestamp_ns) # Clear previous depth visualizations rr.log(&quot;depth_image&quot;, rr.Clear.recursive()) rr.log(&quot;rectified_slam_front_left&quot;, rr.Clear.recursive()) rr.log(&quot;world/stereo_depth_depth_camera&quot;, rr.Clear.recursive()) # Visualize rectified SLAM image (following visualizer pattern) if rectified_image is not None: rr.log(&quot;rectified_slam_front_left&quot;, rr.Image(rectified_image)) # Visualize depth as 3D point cloud # Get original camera intrinsics original_fx, original_fy = camera_info.camera_projection.get_focal_lengths() original_ux, original_uy = camera_info.camera_projection.get_principal_point() # Apply downsampling factor (following visualizer logic) factor = 4 # depth_image_downsample_factor scaled_fx = original_fx / factor scaled_fy = original_fy / factor scaled_ux = original_ux / factor scaled_uy = original_uy / factor # Resize depth map (following visualizer pattern) subsampled_depth_map = depth_map[::factor, ::factor] if factor &gt; 1 else depth_map # Set up depth camera in world coordinate system (following visualizer pattern) rr.log( &quot;world/stereo_depth&quot;, rr.Pinhole( resolution=[subsampled_depth_map.shape[1], subsampled_depth_map.shape[0]], focal_length=[scaled_fx, scaled_fy], principal_point=[scaled_ux, scaled_uy], ), static=True, ) # Log camera transform (following visualizer pattern) rr.log( &quot;world/stereo_depth&quot;, ToTransform3D(camera_info.transform_world_camera, axis_length=0.02) ) # Log depth image with proper scaling (following visualizer pattern exactly) DEPTH_IMAGE_SCALING = 1000 # mm to meters rr.log( &quot;world/stereo_depth&quot;, rr.DepthImage( subsampled_depth_map, meter=DEPTH_IMAGE_SCALING, colormap=&quot;Magma&quot;, point_fill_ratio=0.3 ) ) else: print(&quot;Skipping stereo depth visualization - no stereo depth data available.&quot;)   ","version":"Next","tagName":"h3"},{"title":"Summary‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - Algorithm Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/algorithm_loading#summary","content":" This tutorial has demonstrated how to use the AriaGen2PilotDataProvider to access and visualize algorithm output data from the Aria Gen2 Pilot Dataset:  ","version":"Next","tagName":"h2"},{"title":"Key Concepts Covered‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - Algorithm Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/algorithm_loading#key-concepts-covered","content":" Heart Rate Monitoring - Physiological data from PPG sensors with time series visualizationDiarization - Speaker identification and voice activity detection with text overlayHand-Object Interaction - Segmentation masks for hands and objects with colored overlaysEgocentric Voxel Lifting - 3D scene reconstruction with 2D/3D bounding box visualizationFoundation Stereo - Depth estimation with 2D depth maps and 3D point clouds  ","version":"Next","tagName":"h3"},{"title":"Important Notes‚Äã","type":1,"pageTitle":"Aria Gen2 Pilot Dataset Tutorial - Algorithm Data Loading","url":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/algorithm_loading#important-notes","content":" Data Availability: Algorithm data availability varies by sequence - always check availability before processingData Structures: Each algorithm has its own data structure with specific fields and formatsQuery Patterns: Use index-based queries for sequential processing, timestamp-based queries for synchronizationVisualization: Use appropriate visualization methods for each data type (scalars, images, bounding box, etc.)Performance: Consider subsampling for large datasets and high-frequency data ","version":"Next","tagName":"h3"},{"title":"Tutorial 2: Device calibration","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/calibration","content":"","keywords":"","version":"Next"},{"title":"Introduction‚Äã","type":1,"pageTitle":"Tutorial 2: Device calibration","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/calibration#introduction","content":" Most sensors in Aria glasses are calibrated both extrinsically and intrinsically, allowing you to rectify sensor measurements to real-world quantities. Calibration is performed per device, and the information is stored in the VRS file. This tutorial demonstrates how to work with device calibration in Project Aria using projectaria_tools.  ","version":"Next","tagName":"h2"},{"title":"What You'll Learn‚Äã","type":1,"pageTitle":"Tutorial 2: Device calibration","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/calibration#what-youll-learn","content":" How to obtain each sensor's calibration dataCamera calibration: projection and unprojection, and how to post-process images according to calibration (distort).IMU calibration: measurement rectification.Multi-sensor coordination and sensor poses, and the concept of the &quot;Device&quot; frame.  Pre-requisite:  Familiarity with VRS basics from Tutorial_1_vrs_Data_provider_basics.ipynb.Download Aria Gen2 sample data from link  Note on VisualizationIf visualization window is not showing up, this is due to Rerun lib's caching issue. Just rerun the specific code cell.  ","version":"Next","tagName":"h3"},{"title":"Obtaining Device Calibration Content‚Äã","type":1,"pageTitle":"Tutorial 2: Device calibration","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/calibration#obtaining-device-calibration-content","content":" Each VRS file's device calibration can be accessed as a DeviceCalibration instance via the VrsDataProvider API.  from projectaria_tools.core import data_provider # Load local VRS file vrs_file_path = &quot;path/to/your/recording.vrs&quot; vrs_data_provider = data_provider.create_vrs_data_provider(vrs_file_path) # Obtain device calibration device_calib = vrs_data_provider.get_device_calibration() if device_calib is None: raise RuntimeError( &quot;device calibration does not exist! Please use a VRS that contains valid device calibration for this tutorial. &quot; ) # You can obtain device version (Aria Gen1 vs Gen2), or device subtype (DVT with small/large frame width + short/long temple arms, etc) information from calibration if device_calib is not None: device_version = device_calib.get_device_version() device_subtype = device_calib.get_device_subtype() print(f&quot;Aria Device Version: {device_version}&quot;) print(f&quot;Device Subtype: {device_subtype}&quot;)   ","version":"Next","tagName":"h3"},{"title":"Accessing Individual Sensor Calibration‚Äã","type":1,"pageTitle":"Tutorial 2: Device calibration","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/calibration#accessing-individual-sensor-calibration","content":" DeviceCalibration provides APIs to query the intrinsics and extrinsics of each calibrated sensor.  1. Camera calibration content‚Äã  # Get sensor labels within device calibration all_labels = device_calib.get_all_labels() print(f&quot;All sensors within device calibration: {all_labels}&quot;) print(f&quot;Cameras: {device_calib.get_camera_labels()}&quot;) # Query a specific camera's calibration rgb_camera_label = &quot;camera-rgb&quot; camera_calib = device_calib.get_camera_calib(rgb_camera_label) if camera_calib is None: raise RuntimeError( &quot;camera-rgb calibration does not exist! Please use a VRS that contains valid RGB camera calibration for this tutorial. &quot; ) print(f&quot;-------------- camera calibration for {rgb_camera_label} ----------------&quot;) print(f&quot;Image Size: {camera_calib.get_image_size()}&quot;) print(f&quot;Camera Model Type: {camera_calib.get_model_name()}&quot;) print( f&quot;Camera Intrinsics Params: {camera_calib.get_projection_params()}, \\n&quot; f&quot;where focal is {camera_calib.get_focal_lengths()}, &quot; f&quot;and principal point is {camera_calib.get_principal_point()}\\n&quot; ) # Get extrinsics (device to camera transformation) T_device_camera = camera_calib.get_transform_device_camera() print(f&quot;Camera Extrinsics T_Device_Camera:\\n{T_device_camera.to_matrix()}&quot;)   2. IMU calibration content‚Äã  imu_label = &quot;imu-right&quot; imu_calib = device_calib.get_imu_calib(imu_label) if imu_calib is None: raise RuntimeError( &quot;imu-right calibration does not exist! Please use a VRS that contains valid IMU calibration for this tutorial. &quot; ) print(f&quot;-------------- IMU calibration for {imu_label} ----------------&quot;) # Get IMU intrinsics parameters accel_bias = imu_calib.get_accel_model().get_bias() accel_rectification_matrix = imu_calib.get_accel_model().get_rectification() gyro_bias = imu_calib.get_gyro_model().get_bias() gyro_rectification_matrix = imu_calib.get_gyro_model().get_rectification() print(f&quot;Accelerometer Intrinsics:&quot;) print(f&quot; Bias: {accel_bias}&quot;) print(f&quot; Rectification Matrix:\\n{accel_rectification_matrix}&quot;) print(f&quot;Gyroscope Intrinsics:&quot;) print(f&quot; Bias: {gyro_bias}&quot;) print(f&quot; Rectification Matrix:\\n{gyro_rectification_matrix}&quot;) # Get extrinsics (device to IMU transformation) T_device_imu = imu_calib.get_transform_device_imu() print(f&quot;IMU Extrinsics T_Device_IMU:\\n{T_device_imu.to_matrix()}&quot;) print(f&quot; \\n ------IMPORTANT----- \\n &quot; f&quot;Please use .raw_to_rectified_[accel,gyro]() and .rectified_to_raw_[accel,gyro]() APIs to apply IMU calibration! \\n&quot;)   3. Magnetometer, barometer, and microphone calibration content‚Äã  print(f&quot;Magnetometers: {device_calib.get_magnetometer_labels()}&quot;) print(f&quot;Barometers: {device_calib.get_barometer_labels()}&quot;) print(f&quot;Microphones: {device_calib.get_microphone_labels()}&quot;) # ---------------- # Magnetometer calibration # ---------------- magnetometer_label = &quot;mag0&quot; magnetometer_calib = device_calib.get_magnetometer_calib(magnetometer_label) if magnetometer_calib is None: raise RuntimeError( f&quot;{magnetometer_label} calibration does not exist! Please use a VRS that contains valid magnetometer calibration for this tutorial.&quot; ) # Get magnetometer intrinsics parameters mag_bias = magnetometer_calib.get_model().get_bias() mag_rectification_matrix = magnetometer_calib.get_model().get_rectification() print(f&quot;Magnetometer calibration for {magnetometer_label} only have intrinsics:&quot;) print(f&quot; Bias: {mag_bias}&quot;) print(f&quot; Rectification Matrix:\\n{mag_rectification_matrix}&quot;) # ---------------- # Barometer calibration # ---------------- baro_label = &quot;baro0&quot; baro_calib = device_calib.get_barometer_calib(baro_label) if baro_calib is None: raise RuntimeError( f&quot;{baro_label} calibration does not exist! Please use a VRS that contains valid barometer calibration for this tutorial.&quot; ) print(f&quot;Barometer calibration for {baro_label} only have intrinsics:&quot;) print(f&quot; Slope: {baro_calib.get_slope()}&quot;) print(f&quot; Offset in Pascal:\\n{baro_calib.get_offset_pa()}&quot;) # ---------------- # Microphone calibration, containing both mic and speaker calibrations. # ---------------- microphone_labels = device_calib.get_microphone_labels() speaker_labels = device_calib.get_speaker_labels() audio_sensor_labels = device_calib.get_audio_labels() print(f&quot;Both mic and speakers are calibrated. \\n&quot; f&quot;List of mics that are calibrated: {microphone_labels} \\n&quot; f&quot;List of speakers that are calibrated: {speaker_labels}&quot;) for audio_label in audio_sensor_labels: audio_calib = device_calib.get_microphone_calib(audio_label) if audio_calib is None: print(f&quot;Audio sensor calibration for {audio_label} is not available.&quot;) continue print(f&quot;Audio sensor calibration for {audio_label} only has intrinsics:&quot;) print(f&quot; sensitivity delta: {audio_calib.get_d_sensitivity_1k_dbv()}&quot;) print(f&quot; \\n ------IMPORTANT----- \\n &quot; f&quot;Please use .raw_to_rectified() and .rectified_to_raw() APIs to apply magnetometer, barometer, and microphone calibration!\\n&quot;)   ","version":"Next","tagName":"h3"},{"title":"Camera Intrinsics: Project and Unproject‚Äã","type":1,"pageTitle":"Tutorial 2: Device calibration","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/calibration#camera-intrinsics-project-and-unproject","content":" A camera intrinsic model maps between a 3D point in the camera coordinate system and its corresponding 2D pixel on the sensor. This supports:  Projection: 3D point ‚Üí 2D pixelUnprojection: 2D pixel ‚Üí 3D ray  import numpy as np # Project 3D point to pixel point_3d = np.array([0.1, 0.05, 1.0]) # Point in camera frame (meters) pixel = camera_calib.project(point_3d) if pixel is not None: print(f&quot;3D point {point_3d} projected to -&gt; pixel {pixel}&quot;) else: print(f&quot;3D point {point_3d} projected out of camera sensor plane&quot;) # Unproject pixel to 3D ray. test_pixel = np.array([400, 300]) ray_3d = camera_calib.unproject(test_pixel) print(f&quot;Pixel {test_pixel} unprojected to -&gt; 3D ray {ray_3d}&quot;)   ","version":"Next","tagName":"h3"},{"title":"Camera Intrinsics: undistortion‚Äã","type":1,"pageTitle":"Tutorial 2: Device calibration","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/calibration#camera-intrinsics-undistortion","content":" Camera calibration enables post-processing of Aria images, such as undistorting images from a fisheye to a linear camera model. Steps:  Use vrs_data_provider to access the camera image and calibration.Create a linear camera model using get_linear_camera_calibration function.Apply distort_by_calibration to distort or undistort the image from the actual Fisheye camera model to linear camera model.  import rerun as rr from projectaria_tools.core import calibration rr.init(&quot;rerun_viz_image_undistortion&quot;) # We already obtained RGB camera calibration as `camera_calib`. # Now, create a linear camera model that is similar to camera_calib linear_camera_model = calibration.get_linear_camera_calibration( image_width=camera_calib.get_image_size()[0], image_height=camera_calib.get_image_size()[1], focal_length=camera_calib.get_focal_lengths()[0], label=&quot;test_linear_camera&quot;, ) rgb_stream_id = vrs_data_provider.get_stream_id_from_label(&quot;camera-rgb&quot;) num_samples = vrs_data_provider.get_num_data(rgb_stream_id) # Plot a few frames from RGB camera, and also plot the undistorted images first_few = min(10, num_samples) for i in range(first_few): # Query RGB images image_data, image_record = vrs_data_provider.get_image_data_by_index( rgb_stream_id, i ) if not image_data.is_valid(): continue # Plot original RGB image timestamp_ns = image_record.capture_timestamp_ns rr.set_time_nanos(&quot;device_time&quot;, timestamp_ns) rr.log(&quot;camera_rgb&quot;, rr.Image(image_data.to_numpy_array())) # Undistort RGB image to a linear camera model undistorted_image = calibration.distort_by_calibration( arraySrc=image_data.to_numpy_array(), dstCalib=linear_camera_model, srcCalib=camera_calib, ) rr.log(&quot;undistorted_camera_rgb&quot;, rr.Image(undistorted_image)) rr.notebook_show()   ","version":"Next","tagName":"h3"},{"title":"IMU Intrinsics: Measurement Rectification‚Äã","type":1,"pageTitle":"Tutorial 2: Device calibration","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/calibration#imu-intrinsics-measurement-rectification","content":" IMU intrinsics are represented by an affine model. The raw sensor readout (value_raw) is compensated to obtain the real acceleration or angular velocity (value_compensated):  value_compensated = M^-1 * (value_raw - bias)   M is an upper triangular matrix (no global rotation between IMU body and accelerometer frame).  To simulate sensor readout from real values:  value_raw = M * value_compensated + bias   Note that in the following example, the difference between raw reading and compensated IMU signals are pretty close, therefore the plotting may look similar.  def _set_imu_plot_colors(rerun_plot_label): &quot;&quot;&quot; A helper function to set colors for the IMU plots in rerun &quot;&quot;&quot; rr.log( f&quot;{rerun_plot_label}/accl/x[m/sec2]&quot;, rr.SeriesLine(color=[230, 25, 75], name=&quot;accel/x[m/sec2]&quot;), static=True, ) # Red rr.log( f&quot;{rerun_plot_label}/accl/y[m/sec2]&quot;, rr.SeriesLine(color=[60, 180, 75], name=&quot;accel/y[m/sec2]&quot;), static=True, ) # Green rr.log( f&quot;{rerun_plot_label}/accl/z[m/sec2]&quot;, rr.SeriesLine(color=[0, 130, 200], name=&quot;accel/z[m/sec2]&quot;), static=True, ) # Blue rr.log( f&quot;{rerun_plot_label}/gyro/x[rad/sec2]&quot;, rr.SeriesLine(color=[245, 130, 48], name=&quot;gyro/x[rad/sec2]&quot;), static=True, ) # Orange rr.log( f&quot;{rerun_plot_label}/gyro/y[rad/sec2]&quot;, rr.SeriesLine(color=[145, 30, 180], name=&quot;gyro/y[rad/sec2]&quot;), static=True, ) # Purple rr.log( f&quot;{rerun_plot_label}/gyro/z[rad/sec2]&quot;, rr.SeriesLine(color=[70, 240, 240], name=&quot;gyro/z[rad/sec2]&quot;), static=True, ) # Cyan def _plot_imu_signals(accel_data, gyro_data, rerun_plot_label): &quot;&quot;&quot; This is a helper function to plot IMU signals in Rerun 1D plot &quot;&quot;&quot; rr.log( f&quot;{rerun_plot_label}/accl/x[m/sec2]&quot;, rr.Scalar(accel_data[0]), ) rr.log( f&quot;{rerun_plot_label}/accl/y[m/sec2]&quot;, rr.Scalar(accel_data[1]), ) rr.log( f&quot;{rerun_plot_label}/accl/z[m/sec2]&quot;, rr.Scalar(accel_data[2]), ) rr.log( f&quot;{rerun_plot_label}/gyro/x[rad/sec2]&quot;, rr.Scalar(gyro_data[0]), ) rr.log( f&quot;{rerun_plot_label}/gyro/y[rad/sec2]&quot;, rr.Scalar(gyro_data[1]), ) rr.log( f&quot;{rerun_plot_label}/gyro/z[rad/sec2]&quot;, rr.Scalar(gyro_data[2]), ) rr.init(&quot;rerun_viz_imu_rectification&quot;) imu_label = &quot;imu-right&quot; imu_calib = device_calib.get_imu_calib(imu_label) imu_stream_id = vrs_data_provider.get_stream_id_from_label(imu_label) if imu_calib is None or imu_stream_id is None: raise RuntimeError( &quot;imu-right calibration or stream data does not exist! Please use a VRS that contains valid IMU calibration and data for this tutorial. &quot; ) num_samples = vrs_data_provider.get_num_data(imu_stream_id) first_few = min(5000, num_samples) # Set same colors for both plots _set_imu_plot_colors(&quot;imu_right&quot;) _set_imu_plot_colors(&quot;imu_right_compensated&quot;) for i in range(0, first_few, 50): # Query IMU data imu_data = vrs_data_provider.get_imu_data_by_index(imu_stream_id, i) # Plot raw IMU readings rr.set_time_nanos(&quot;device_time&quot;, imu_data.capture_timestamp_ns) # Get compensated imu data compensated_accel = imu_calib.raw_to_rectified_accel(imu_data.accel_msec2) compensated_gyro = imu_calib.raw_to_rectified_gyro(imu_data.gyro_radsec) # print one sample content if i == 0: print( f&quot;IMU compensation: raw accel {imu_data.accel_msec2} , compensated accel {compensated_accel}&quot; ) print( f&quot;IMU compensation: raw gyro {imu_data.gyro_radsec} , compensated gyro {compensated_gyro}&quot; ) # Plot raw IMU readings _plot_imu_signals(imu_data.accel_msec2, imu_data.gyro_radsec, &quot;imu_right&quot;) # Plot compensated IMU readings in a separate plot _plot_imu_signals(compensated_accel, compensated_gyro, &quot;imu_right_compensated&quot;) rr.notebook_show()   ","version":"Next","tagName":"h3"},{"title":"5. Accessing Sensor Extrinsics‚Äã","type":1,"pageTitle":"Tutorial 2: Device calibration","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/calibration#5-accessing-sensor-extrinsics","content":" The core API to query sensor extrinsics is:  get_transform_device_sensor(label = sensor_label, use_cad_calib = False)   This API returns the extrinsics of the sensor, represented as a Sophus::SE3 (translation + rotation). in the reference coordinate frame of Device.  The Device frame is the reference coordinate system for all sensors.For Aria-Gen2, the &quot;Device&quot; frame is the left front-facing SLAM camera (slam-front-left).All sensor extrinsics are defined relative to this frame.  The optional parameter use_cad_calib controls the &quot;source&quot; of the sensor extrinsics.  use_cad_calib=False (default): this will return the sensor extrinsics from factory calibration, if the sensor's extrinsics is factory-calibrated. This include: CamerasIMUs use_cad_calib=True: this will return the sensor's extrinsics in their designed location in CAD. This is useful for sensors without factory-calibrated extrinsics, including: MagnetometerBarometerMicrophones  from projectaria_tools.utils.rerun_helpers import ( AriaGlassesOutline, ToTransform3D, ToBox3D, ) rr.init(&quot;rerun_viz_sensor_extrinsics&quot;) # Obtain a glass outline for visualization. This outline uses factory calibration extrinsics if possible, uses CAD extrinsics if factory calibration is not available. glass_outline = AriaGlassesOutline(device_calib, use_cad_calib=False) rr.log(&quot;device/glasses_outline&quot;, rr.LineStrips3D([glass_outline]), static=True) # Plot all the sensor locations from either factory calibration (if available) or CAD sensor_labels = device_calib.get_all_labels() camera_labels = device_calib.get_camera_labels() for sensor in sensor_labels: # Query for sensor extrinsics from factory calibration if possible. Fall back to CAD values if unavailable. if (&quot;camera&quot; in sensor) or (&quot;imu&quot; in sensor): T_device_sensor = device_calib.get_transform_device_sensor(label = sensor, get_cad_value = False) else: T_device_sensor = device_calib.get_transform_device_sensor(label = sensor, get_cad_value = True) # Skip if extrinsics cannot be obtained if T_device_sensor is None: print(f&quot;Warning: sensor {sensor} does not have extrinsics from neither factory calibration nor CAD, skipping the plotting.&quot;) continue # Plot sensor labels rr.log(f&quot;device/{sensor}&quot;, ToTransform3D(T_device_sensor), static=True) rr.log( f&quot;device/{sensor}/text&quot;, ToBox3D(sensor, [1e-5, 1e-5, 1e-5]), static=True, ) # For cameras, also plot camera frustum if sensor in camera_labels: camera_calibration = device_calib.get_camera_calib(sensor) rr.log(f&quot;device/{sensor}_frustum&quot;, ToTransform3D(T_device_sensor), static=True) rr.log( f&quot;device/{sensor}_frustum&quot;, rr.Pinhole( resolution=[ camera_calibration.get_image_size()[0], camera_calibration.get_image_size()[1], ], focal_length=float(camera_calibration.get_focal_lengths()[0]), ), static=True, ) rr.notebook_show()  ","version":"Next","tagName":"h3"},{"title":"Tutorial 4: Using On-Device Eye-tracking and Hand-tracking","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/eyetracking-handtracking","content":"","keywords":"","version":"Next"},{"title":"Introduction‚Äã","type":1,"pageTitle":"Tutorial 4: Using On-Device Eye-tracking and Hand-tracking","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/eyetracking-handtracking#introduction","content":" In Aria-Gen2 glasses, one of the key upgrade from Aria-Gen1 is the capability to run Machine Perception (MP) algorithms on the device during streaming / recording. Currently supported on-device MP algorithms include Eye-tracking, Hand-tracking, and VIO. These algorithm results are stored as separate data streams in the VRS file.  This tutorial focuses on demonstration of how to use the Eye-tracking and Hand-tracking results.  What you'll learn:  How to access on-device EyeGaze and HandTracking data from VRS filesUnderstanding the concept of interpolated hand tracking and why interpolation is neededHow to visualize EyeGaze and HandTracking data projected onto 2D camera images using DeviceCalibrationHow to match MP data with camera frames using timestamps  Prerequisites  Complete Tutorial 1 (VrsDataProvider Basics) to understand basic data provider conceptsComplete Tutorial 2 (Device Calibration) to understand how to properly use calibration in Aria data.Download Aria Gen2 sample data from link  Note on VisualizationIf visualization window is not showing up, this is due to Rerun lib's caching issue. Just rerun the specific code cell, or restart the python kernel.  from projectaria_tools.core import data_provider # Load VRS file vrs_file_path = &quot;path/to/your/recording.vrs&quot; vrs_data_provider = data_provider.create_vrs_data_provider(vrs_file_path) # Access device calibration device_calib = vrs_data_provider.get_device_calibration()   # Query EyeGaze data streams eyegaze_label = &quot;eyegaze&quot; eyegaze_stream_id = vrs_data_provider.get_stream_id_from_label(eyegaze_label) if eyegaze_stream_id is None: raise RuntimeError( f&quot;{eyegaze_label} data stream does not exist! Please use a VRS that contains valid eyegaze data for this tutorial.&quot; ) # Query HandTracking data streams handtracking_label = &quot;handtracking&quot; handtracking_stream_id = vrs_data_provider.get_stream_id_from_label(handtracking_label) if handtracking_stream_id is None: raise RuntimeError( f&quot;{handtracking_label} data stream does not exist! Please use a VRS that contains valid handtracking data for this tutorial.&quot; )   ","version":"Next","tagName":"h2"},{"title":"On-Device Eye-tracking results‚Äã","type":1,"pageTitle":"Tutorial 4: Using On-Device Eye-tracking and Hand-tracking","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/eyetracking-handtracking#on-device-eye-tracking-results","content":" ","version":"Next","tagName":"h2"},{"title":"EyeGaze Data Structure‚Äã","type":1,"pageTitle":"Tutorial 4: Using On-Device Eye-tracking and Hand-tracking","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/eyetracking-handtracking#eyegaze-data-structure","content":" The EyeGaze data type represents on-device eye tracking results.Importantly, it directly reuses the EyeGaze data structurefrom MPS (Machine Perception Services), providing guaranteed compatibility across VRS and MPS.  Key EyeGaze fields  Field Name\tDescriptionsession_uid\tUnique ID for the eyetracking session tracking_timestamp\tTimestamp of the eye tracking camera frame in device time domain, in us. yaw\tGaze direction in yaw (horizontal) in radians pitch\tGaze direction in pitch (vertical) in radians depth\tEstimated gaze depth distance, in meters combined_gaze_origin_in_cpf\tCombined gaze origin in CPF frame (Gen2 only) spatial_gaze_point_in_cpf\t3D spatial gaze point in CPF frame vergence.[left,right]_entrance_pupil_position_meter\tEntrance pupil positions for each eye vergence.[left,right]_pupil_diameter_meter\tEntrance pupil diameter for each eye vergence.[left,right]_blink\tBlink detection for left and right eyes *_valid\tBoolean flags to indicating if the corresponding data field in EyeGaze is valid  ","version":"Next","tagName":"h3"},{"title":"EyeGaze API Reference‚Äã","type":1,"pageTitle":"Tutorial 4: Using On-Device Eye-tracking and Hand-tracking","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/eyetracking-handtracking#eyegaze-api-reference","content":" In vrs_data_provider, EyeGaze is treated the same way as any other sensor data, and share similar query APIs covered in Tutorial_1_vrs_data_provider_basics:  vrs_data_provider.get_eye_gaze_data_by_index(stream_id, index): Query by index.vrs_data_provider.get_eye_gaze_data_by_time_ns(stream_id, timestamp, time_domain, query_options): Query by timestamp.  from projectaria_tools.core.mps import get_unit_vector_from_yaw_pitch from datetime import timedelta print(&quot;=== EyeGaze Data Sample ===&quot;) num_eyegaze_samples = vrs_data_provider.get_num_data(eyegaze_stream_id) selected_index = min(5, num_eyegaze_samples) print(f&quot;Sample {selected_index}:&quot;) eyegaze_data = vrs_data_provider.get_eye_gaze_data_by_index(eyegaze_stream_id, selected_index) # Eyegaze timestamp is in format of datetime.deltatime in microseconds, convert it to integer eyegaze_timestamp_ns = (eyegaze_data.tracking_timestamp // timedelta(microseconds=1)) * 1000 print(f&quot;\\tTracking timestamp: {eyegaze_timestamp_ns}&quot;) # check if combined gaze is valid, if so, print out the gaze direction print(f&quot;\\tCombined gaze valid: {eyegaze_data.combined_gaze_valid}&quot;) if eyegaze_data.combined_gaze_valid: print(f&quot;\\tYaw: {eyegaze_data.yaw:.3f} rad&quot;) print(f&quot;\\tPitch: {eyegaze_data.pitch:.3f} rad&quot;) print(f&quot;\\tDepth: {eyegaze_data.depth:.3f} m&quot;) # Can also print gaze direction in unit vector gaze_direction_in_unit_vec = get_unit_vector_from_yaw_pitch(eyegaze_data.yaw, eyegaze_data.pitch) print(f&quot;\\tGaze direction in unit vec [xyz]: {gaze_direction_in_unit_vec}&quot;) # Check if spatial gaze point is valid, if so, print out the spatial gaze point print( f&quot;\\tSpatial gaze point valid: {eyegaze_data.spatial_gaze_point_valid}&quot; ) if eyegaze_data.spatial_gaze_point_valid: print( f&quot;\\tSpatial gaze point in CPF: {eyegaze_data.spatial_gaze_point_in_cpf}&quot; )   ","version":"Next","tagName":"h3"},{"title":"EyeGaze visualization in camera images‚Äã","type":1,"pageTitle":"Tutorial 4: Using On-Device Eye-tracking and Hand-tracking","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/eyetracking-handtracking#eyegaze-visualization-in-camera-images","content":" To visualize EyeGaze in camera images, you just need to project eye tracking results into the camera images using the camera's calibration. But please note the coordinate frame difference, entailed below.  EyeGaze Coordinate System - Central Pupil Frame (CPF)  All Eyetracking results in Aria are stored in a reference coordinates system called Central Pupil Frame (CPF), which is approximately the center of user's two eye positions. Note that this CPF frame is DIFFERENT from the Device frame in device calibration, where the latter is essentially the slam-front-left (for Gen2) or camera-slam-left (for Gen1) camera. To transform between CPF and Device, we provide the following API to query their relative pose, and see the following code cell for usage:  device_calibration.get_transform_device_cpf()   ","version":"Next","tagName":"h3"},{"title":"Visualizing Eye-tracking Data‚Äã","type":1,"pageTitle":"Tutorial 4: Using On-Device Eye-tracking and Hand-tracking","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/eyetracking-handtracking#visualizing-eye-tracking-data","content":" import rerun as rr import numpy as np from projectaria_tools.core.sensor_data import TimeDomain, TimeQueryOptions def visualize_eyegaze_in_camera(camera_label, eyegaze_data, camera_calib, device_calib): &quot;&quot;&quot; Project eye-tracking data onto camera image &quot;&quot;&quot; # Convert gaze direction to 3D vector yaw, pitch = eyegaze_data.yaw, eyegaze_data.pitch gaze_vector_device = np.array([ np.cos(pitch) * np.sin(yaw), np.sin(pitch), np.cos(pitch) * np.cos(yaw) ]) # Transform to camera coordinate system T_device_camera = camera_calib.get_transform_device_camera() gaze_vector_camera = T_device_camera.inverse().rotationMatrix() @ gaze_vector_device # Project to image coordinates (assuming gaze origin at camera center) gaze_distance = 2.0 # meters gaze_point_camera = gaze_vector_camera * gaze_distance gaze_pixel = camera_calib.project(gaze_point_camera) # Visualize gaze point on image if camera_calib.is_valid_projection(gaze_pixel): rr.log( f&quot;{camera_label}/eyegaze&quot;, rr.Points2D( positions=[gaze_pixel], colors=[255, 0, 0], # Red color radii=[5.0] ) ) # Example usage in visualization loop rr.init(&quot;rerun_viz_eyegaze&quot;) if eyegaze_stream_id is not None: rgb_stream_id = vrs_data_provider.get_stream_id_from_label(&quot;camera-rgb&quot;) rgb_camera_calib = device_calib.get_camera_calib(&quot;camera-rgb&quot;) # Visualize first few frames with eye-tracking data for i in range(min(10, num_eyegaze_samples)): eyegaze_data = vrs_data_provider.get_eyegaze_data_by_index(eyegaze_stream_id, i) # Find closest RGB frame eyegaze_timestamp_ns = int(eyegaze_data.tracking_timestamp.total_seconds() * 1e9) rgb_data, rgb_record = vrs_data_provider.get_image_data_by_time_ns( rgb_stream_id, eyegaze_timestamp_ns, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST ) if rgb_data.is_valid(): rr.set_time_nanos(&quot;device_time&quot;, rgb_record.capture_timestamp_ns) rr.log(&quot;camera-rgb&quot;, rr.Image(rgb_data.to_numpy_array())) # Overlay eye-tracking data visualize_eyegaze_in_camera(&quot;camera-rgb&quot;, eyegaze_data, rgb_camera_calib, device_calib) rr.notebook_show()   ","version":"Next","tagName":"h3"},{"title":"Accessing Hand-tracking Data‚Äã","type":1,"pageTitle":"Tutorial 4: Using On-Device Eye-tracking and Hand-tracking","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/eyetracking-handtracking#accessing-hand-tracking-data","content":" ","version":"Next","tagName":"h2"},{"title":"Basic Hand-tracking Data Access‚Äã","type":1,"pageTitle":"Tutorial 4: Using On-Device Eye-tracking and Hand-tracking","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/eyetracking-handtracking#basic-hand-tracking-data-access","content":" Hand-tracking provides 3D pose estimation for both hands, including joint positions and hand poses.  # Query HandTracking stream handtracking_stream_id = vrs_data_provider.get_stream_id_from_label(&quot;handtracking&quot;) if handtracking_stream_id is None: print(&quot;This VRS file does not contain on-device hand-tracking data.&quot;) else: print(f&quot;Found hand-tracking stream: {handtracking_stream_id}&quot;) # Get total number of hand-tracking samples num_handtracking_samples = vrs_data_provider.get_num_data(handtracking_stream_id) print(f&quot;Total hand-tracking samples: {num_handtracking_samples}&quot;) # Access hand-tracking data print(&quot;\\nFirst few hand-tracking samples:&quot;) for i in range(min(3, num_handtracking_samples)): hand_pose_data = vrs_data_provider.get_hand_pose_data_by_index(handtracking_stream_id, i) print(f&quot;\\nSample {i}:&quot;) print(f&quot;\\tTimestamp: {hand_pose_data.tracking_timestamp}&quot;) # Check left hand if hand_pose_data.left_hand is not None: print(f&quot;\\tLeft hand detected:&quot;) print(f&quot;\\tConfidence: {hand_pose_data.left_hand.confidence}&quot;) print(f&quot;\\tNumber of landmarks: {len(hand_pose_data.left_hand.landmark_positions_device)}&quot;) else: print(f&quot;\\tLeft hand: Not detected&quot;) # Check right hand if hand_pose_data.right_hand is not None: print(f&quot;\\tRight hand detected:&quot;) print(f&quot;\\tConfidence: {hand_pose_data.right_hand.confidence}&quot;) print(f&quot;\\tNumber of landmarks: {len(hand_pose_data.right_hand.landmark_positions_device)}&quot;) else: print(f&quot;\\tRight hand: Not detected&quot;)   ","version":"Next","tagName":"h3"},{"title":"Hand-tracking Data Structure‚Äã","type":1,"pageTitle":"Tutorial 4: Using On-Device Eye-tracking and Hand-tracking","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/eyetracking-handtracking#hand-tracking-data-structure","content":" Hand-tracking data contains:  Tracking Timestamp: When the hand-tracking measurement was takenLeft/Right Hand Data: Each hand (when detected) includes: Confidence: Detection confidence scoreLandmark Positions: 3D positions of hand joints in device coordinate systemWrist Transform: 6DOF pose of the wristPalm Normal: Normal vector of the palm  ","version":"Next","tagName":"h3"},{"title":"Interpolated Hand-tracking Data‚Äã","type":1,"pageTitle":"Tutorial 4: Using On-Device Eye-tracking and Hand-tracking","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/eyetracking-handtracking#interpolated-hand-tracking-data","content":" Since hand-tracking and camera data may not be perfectly synchronized, Aria provides interpolated hand-tracking data that can be queried at arbitrary timestamps.  from projectaria_tools.core.sensor_data import SensorDataType, TimeDomain, TimeQueryOptions from datetime import timedelta print(&quot;\\n=== Demonstrating query interpolated hand tracking results ===&quot;) # Demonstrate how to query interpolated handtracking results slam_stream_id = vrs_data_provider.get_stream_id_from_label(&quot;slam-front-left&quot;) rgb_stream_id = vrs_data_provider.get_stream_id_from_label(&quot;camera-rgb&quot;) # Retrieve a SLAM frame, use its timestamp as query slam_sample_index = min(10, vrs_data_provider.get_num_data(slam_stream_id) - 1) slam_data_and_record = vrs_data_provider.get_image_data_by_index(slam_stream_id, slam_sample_index) slam_timestamp_ns = slam_data_and_record[1].capture_timestamp_ns # Retrieve the closest RGB frame rgb_data_and_record = vrs_data_provider.get_image_data_by_time_ns( rgb_stream_id, slam_timestamp_ns, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST ) rgb_timestamp_ns = rgb_data_and_record[1].capture_timestamp_ns # Retrieve the closest hand tracking data sample raw_ht_data = vrs_data_provider.get_hand_pose_data_by_time_ns( handtracking_stream_id, slam_timestamp_ns, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST ) raw_ht_timestamp_ns = (raw_ht_data.tracking_timestamp // timedelta(microseconds=1)) * 1000 # Check if hand tracking aligns with RGB or SLAM data print(f&quot;SLAM timestamp: {slam_timestamp_ns}&quot;) print(f&quot;RGB timestamp: {rgb_timestamp_ns}&quot;) print(f&quot;hand tracking timestamp: {raw_ht_timestamp_ns}&quot;) print(f&quot;hand tracking-SLAM time diff: {abs(raw_ht_timestamp_ns - slam_timestamp_ns) / 1e6:.2f} ms&quot;) print(f&quot;hand tracking- RGB time diff: {abs(raw_ht_timestamp_ns - rgb_timestamp_ns) / 1e6:.2f} ms&quot;) # Now, query interpolated hand tracking data sample using RGB timestamp. interpolated_ht_data = vrs_data_provider.get_interpolated_hand_pose_data( handtracking_stream_id, rgb_timestamp_ns ) # Check that interpolated hand tracking now aligns with RGB data if interpolated_ht_data is not None: interpolated_ht_timestamp_ns = (interpolated_ht_data.tracking_timestamp// timedelta(microseconds=1)) * 1000 print(f&quot;Interpolated hand tracking timestamp: {interpolated_ht_timestamp_ns}&quot;) print(f&quot;Interpolated hand tracking-RGB time diff: {abs(interpolated_ht_timestamp_ns - rgb_timestamp_ns) / 1e6:.2f} ms&quot;) else: print(&quot;Interpolated hand tracking data is None - interpolation failed&quot;)   ","version":"Next","tagName":"h3"},{"title":"Visualizing Hand-tracking Results in Cameras‚Äã","type":1,"pageTitle":"Tutorial 4: Using On-Device Eye-tracking and Hand-tracking","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/eyetracking-handtracking#visualizing-hand-tracking-results-in-cameras","content":" import rerun as rr from projectaria_tools.utils.rerun_helpers import create_hand_skeleton_from_landmarks def plot_single_hand_in_camera(hand_joints_in_device, camera_label, camera_calib, hand_label): &quot;&quot;&quot; A helper function to plot a single hand data in 2D camera view &quot;&quot;&quot; # Setting different marker plot sizes for RGB and SLAM since they have different resolutions plot_ratio = 3.0 if camera_label == &quot;camera-rgb&quot; else 1.0 marker_color = [255,64,0] if hand_label == &quot;left&quot; else [255, 255, 0] # project into camera frame, and also create line segments hand_joints_in_camera = [] for pt_in_device in hand_joints_in_device: pt_in_camera = ( camera_calib.get_transform_device_camera().inverse() @ pt_in_device ) pixel = camera_calib.project(pt_in_camera) hand_joints_in_camera.append(pixel) # Create hand skeleton in 2D image space hand_skeleton = create_hand_skeleton_from_landmarks(hand_joints_in_camera) # Remove &quot;None&quot; markers from hand joints in camera. This is intentionally done AFTER the hand skeleton creation hand_joints_in_camera = list( filter(lambda x: x is not None, hand_joints_in_camera) ) rr.log( f&quot;{camera_label}/{hand_label}/landmarks&quot;, rr.Points2D( positions=hand_joints_in_camera, colors= marker_color, radii= [3.0 * plot_ratio] ), ) rr.log( f&quot;{camera_label}/{hand_label}/skeleton&quot;, rr.LineStrips2D( hand_skeleton, colors=[0, 255, 0], radii= [0.5 * plot_ratio], ), ) def plot_handpose_in_camera(hand_pose, camera_label, camera_calib): &quot;&quot;&quot; A helper function to plot hand tracking results into a camera image &quot;&quot;&quot; # Plot both hands if hand_pose.left_hand is not None: plot_single_hand_in_camera( hand_joints_in_device=hand_pose.left_hand.landmark_positions_device, camera_label=camera_label, camera_calib = camera_calib, hand_label=&quot;left&quot;) if hand_pose.right_hand is not None: plot_single_hand_in_camera( hand_joints_in_device=hand_pose.right_hand.landmark_positions_device, camera_label=camera_label, camera_calib = camera_calib, hand_label=&quot;right&quot;) print(&quot;\\n=== Visualizing on-device hand tracking in camera images ===&quot;) # First, query the RGB camera stream id device_calib = vrs_data_provider.get_device_calibration() rgb_camera_label = &quot;camera-rgb&quot; slam_camera_labels = [&quot;slam-front-left&quot;, &quot;slam-front-right&quot;, &quot;slam-side-left&quot;, &quot;slam-side-right&quot;] rgb_stream_id = vrs_data_provider.get_stream_id_from_label(rgb_camera_label) slam_stream_ids = [vrs_data_provider.get_stream_id_from_label(label) for label in slam_camera_labels] rr.init(&quot;rerun_viz_ht_in_cameras&quot;) # Set up a sensor queue with only RGB images. # Handtracking data will be queried with interpolated API. deliver_options = vrs_data_provider.get_default_deliver_queued_options() deliver_options.deactivate_stream_all() for stream_id in slam_stream_ids + [rgb_stream_id]: deliver_options.activate_stream(stream_id) # Play for only 3 seconds total_length_ns = vrs_data_provider.get_last_time_ns_all_streams(TimeDomain.DEVICE_TIME) - vrs_data_provider.get_first_time_ns_all_streams(TimeDomain.DEVICE_TIME) skip_begin_ns = int(15 * 1e9) # Skip 15 seconds duration_ns = int(3 * 1e9) # 3 seconds skip_end_ns = max(total_length_ns - skip_begin_ns - duration_ns, 0) deliver_options.set_truncate_first_device_time_ns(skip_begin_ns) deliver_options.set_truncate_last_device_time_ns(skip_end_ns) # Plot image data, and overlay hand tracking data for sensor_data in vrs_data_provider.deliver_queued_sensor_data(deliver_options): # -- # Only image data will be obtained. # -- device_time_ns = sensor_data.get_time_ns(TimeDomain.DEVICE_TIME) image_data_and_record = sensor_data.image_data_and_record() stream_id = sensor_data.stream_id() camera_label = vrs_data_provider.get_label_from_stream_id(stream_id) camera_calib = device_calib.get_camera_calib(camera_label) # Visualize the RGB images. rr.set_time_nanos(&quot;device_time&quot;, device_time_ns) rr.log(f&quot;{camera_label}&quot;, rr.Image(image_data_and_record[0].to_numpy_array())) # Query and plot interpolated hand tracking result interpolated_hand_pose = vrs_data_provider.get_interpolated_hand_pose_data(handtracking_stream_id, device_time_ns, TimeDomain.DEVICE_TIME) if interpolated_hand_pose is not None: plot_handpose_in_camera(hand_pose = interpolated_hand_pose, camera_label = camera_label, camera_calib = camera_calib) # Wait for rerun to buffer 1 second of data import time time.sleep(1) rr.notebook_show()   ","version":"Next","tagName":"h3"},{"title":"Understanding Interpolation‚Äã","type":1,"pageTitle":"Tutorial 4: Using On-Device Eye-tracking and Hand-tracking","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/eyetracking-handtracking#understanding-interpolation","content":" Hand-tracking interpolation is crucial for synchronizing hand data with camera frames:  Why Interpolation is Needed: Hand-tracking algorithms may run at different frequencies than cameras, leading to temporal misalignment. Interpolation Algorithm: The system uses linear interpolation for 3D positions and SE3 interpolation for poses. Interpolation Rules: Both hands must be valid in both before/after samples for interpolation to workIf either hand is missing in either sample, the interpolated result for that hand will be NoneSingle-hand interpolation includes: Linear interpolation on 3D hand landmark positionsSE3 interpolation on wrist 3D poseRe-calculated wrist and palm normal vectorsMinimum confidence values  ","version":"Next","tagName":"h2"},{"title":"Summary‚Äã","type":1,"pageTitle":"Tutorial 4: Using On-Device Eye-tracking and Hand-tracking","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/eyetracking-handtracking#summary","content":" This tutorial covered accessing and visualizing on-device eye-tracking and hand-tracking data:  Eye-tracking Data: Access gaze direction information and project onto camera imagesHand-tracking Data: Access 3D hand pose data including joint positions and confidence scoresInterpolated Data: Use interpolated hand-tracking for better temporal alignment with camera dataVisualization: Project MP data onto 2D camera images for analysis and debugging  These on-device MP algorithms provide real-time insights into user behavior and can be combined with other sensor data for comprehensive analysis of user interactions and movements. ","version":"Next","tagName":"h2"},{"title":"Tutorial 3: Sequential Accessing Multi-sensor Data","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/queue","content":"","keywords":"","version":"Next"},{"title":"Introduction‚Äã","type":1,"pageTitle":"Tutorial 3: Sequential Accessing Multi-sensor Data","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/queue#introduction","content":" This tutorial shows how to use the unified queued API in vrs_data_provider to efficiently stream multi-sensor data from Aria VRS files.  We will learn how to use the unified SensorData interface, access time-ordered sensor data queues, and customize stream control and time windowing for efficient processing.  In this tutorial, we will learn:  Use the basic queued API to iterate through all sensor data.Explore the unified SensorData interfaceCustomize stream selection and time windowing in this queued API.Apply frame rate subsampling for efficient processing  Prerequisites  Complete Tutorial 1 (VrsDataProvider Basics) to understand basic data provider conceptsDownload Aria Gen2 sample data from link  Note on VisualizationIf visualization window is not showing up, this is due to Rerun lib's caching issue. Just rerun the specific code cell.  from projectaria_tools.core import data_provider # Load local VRS file vrs_file_path = &quot;path/to/your/recording.vrs&quot; vrs_data_provider = data_provider.create_vrs_data_provider(vrs_file_path)   ","version":"Next","tagName":"h2"},{"title":"Basic Sequential Data Access API‚Äã","type":1,"pageTitle":"Tutorial 3: Sequential Accessing Multi-sensor Data","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/queue#basic-sequential-data-access-api","content":" The deliver_queued_sensor_data() method in vrs_data_provider provides a unified way to iterate through all sensor data in timestamp order. This is the primary API for sequential access to multi-sensor data.  Key features of the queued API:  Returns data from ALL streams in the VRS fileOrders data by device timestamp (chronological order)Customizable via stream selection, sub-sampling each stream, etc.The returned data can be further converted to each sensor type via a unified interface.  Here is a simple example to query the first K data samples from the VRS, and inspect each data sample's properties:  print(&quot;\\n=== Basic Sequential Data Access ===&quot;) print(&quot;Processing all sensor data in timestamp order...&quot;) # Variables to store how many data samples has arrived for each sensor stream data_count = 0 per_stream_data_counts = {} total_num_samples = 5000 # Call deliver queued sensor data API to obtain a &quot;streamed&quot; data, in sorted timestamp order # The iterator would return a unified SensorData instance print(f&quot;Start inspecting the first {total_num_samples} data samples in the VRS&quot;) for sensor_data in vrs_data_provider.deliver_queued_sensor_data(): # Which stream does this sensor data belong to stream_id = sensor_data.stream_id() stream_label = vrs_data_provider.get_label_from_stream_id(stream_id) # Aggregate data count for this stream data_count += 1 if stream_label not in per_stream_data_counts: per_stream_data_counts[stream_label] = 0 per_stream_data_counts[stream_label] += 1 # Limit output for demonstration if data_count &gt;= total_num_samples: print(&quot;Stopping after 5000 samples for demonstration...&quot;) break # Print data counts for each sensor stream print(f&quot;\\nTotal processed: {data_count} sensor data samples&quot;) print(&quot;Data count per stream:&quot;) for stream_label, count in per_stream_data_counts.items(): print(f&quot;\\t{stream_label}: {count}&quot;)   ","version":"Next","tagName":"h2"},{"title":"Understanding the Unified SensorData Interface‚Äã","type":1,"pageTitle":"Tutorial 3: Sequential Accessing Multi-sensor Data","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/queue#understanding-the-unified-sensordata-interface","content":" The queued API returns data using a unified SensorData interface. This allows you to handle different types of sensor data (images, IMU, audio, etc.) in a consistent way, regardless of the sensor type.  Each SensorData object provides:  Stream ID and stream label for identificationSensor data type (IMAGE, IMU, AUDIO, etc.)Timestamps in different time domainsAccess to the actual sensor data (images, IMU, audio, etc.)  from projectaria_tools.core.sensor_data import SensorDataType, TimeDomain, TimeQueryOptions print(&quot;\\n=== Exploring the SensorData Interface ===&quot;) # Get a few samples to examine their properties data_count = 0 for sensor_data in vrs_data_provider.deliver_queued_sensor_data(): if data_count &gt;= 5: break # Inspect where this sensor data come from, and what is its data type stream_id = sensor_data.stream_id() stream_label = vrs_data_provider.get_label_from_stream_id(stream_id) data_type = sensor_data.sensor_data_type() # Inspect the device timestamp of this sensor data device_time = sensor_data.get_time_ns(TimeDomain.DEVICE_TIME) print(f&quot;\\nSample {data_count + 1}:&quot;) print(f&quot; Stream: {stream_label} (Stream ID: {stream_id})&quot;) print(f&quot; Type: {data_type}&quot;) print(f&quot; Device Time: {device_time/1e9:.6f}s&quot;) # Map sensor data to its specific type, and inspect its actual data content. # Here we use image and IMU as an example if data_type == SensorDataType.IMAGE: image_data = sensor_data.image_data_and_record()[0] print(f&quot; Image size: {image_data.get_width()} x {image_data.get_height()}&quot;) print(f&quot; Pixel format: {image_data.get_pixel_format()}&quot;) elif data_type == SensorDataType.IMU: imu_data = sensor_data.imu_data() accel = imu_data.accel_msec2 gyro = imu_data.gyro_radsec print(f&quot; IMU Accel: [{accel[0]:.3f}, {accel[1]:.3f}, {accel[2]:.3f}] m/s¬≤&quot;) print(f&quot; IMU Gyro: [{gyro[0]:.3f}, {gyro[1]:.3f}, {gyro[2]:.3f}] rad/s&quot;) data_count += 1   ","version":"Next","tagName":"h2"},{"title":"Customizing Data Access with DeliverQueuedOptions‚Äã","type":1,"pageTitle":"Tutorial 3: Sequential Accessing Multi-sensor Data","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/queue#customizing-data-access-with-deliverqueuedoptions","content":" The real power of the queued API comes from customization options. The DeliverQueuedOptions class allows you to:  Apply time windowing - Process only specific time ranges.Select specific streams - Choose which sensors to include.Subsample data - Reduce frame rates for specific streams.  These all provide flexible ways to control the sensor queue, to focus on specific time periods or sensor modalities, or customize data rates for different analysis needs.  import rerun as rr print(&quot;\\n=== Customizing Data Access with DeliverQueuedOptions ===&quot;) customized_deliver_options = vrs_data_provider.get_default_deliver_queued_options() # ----------------- # 1. Stream selection feature - only select RGB, 1 SLAM camera, and 1 ET camera data. # ----------------- rgb_to_select = vrs_data_provider.get_stream_id_from_label(&quot;camera-rgb&quot;) slam_to_select = vrs_data_provider.get_stream_id_from_label(&quot;slam-front-right&quot;) et_to_select = vrs_data_provider.get_stream_id_from_label(&quot;camera-et-right&quot;) # First deactivate all streams, then just add back selected streams customized_deliver_options.deactivate_stream_all() for selected_stream_id in [rgb_to_select,slam_to_select,et_to_select]: customized_deliver_options.activate_stream(selected_stream_id) # ----------------- # 2. Time windowing feature - Skip first 2 seconds, and play for 3 seconds, if possible # ----------------- total_length_ns = vrs_data_provider.get_last_time_ns_all_streams(TimeDomain.DEVICE_TIME) - vrs_data_provider.get_first_time_ns_all_streams(TimeDomain.DEVICE_TIME) skip_begin_ns = int(2 * 1e9) # 2 seconds duration_ns = int(3 * 1e9) # 3 seconds skip_end_ns = max(total_length_ns - skip_begin_ns - duration_ns, 0) customized_deliver_options.set_truncate_first_device_time_ns(skip_begin_ns) customized_deliver_options.set_truncate_last_device_time_ns(skip_end_ns) # ----------------- # 3. Per-stream sub-sampling feature - subsample slam camera at rate of 3 # ----------------- slam_subsample_rate = 3 customized_deliver_options.set_subsample_rate(stream_id = slam_to_select, rate = slam_subsample_rate) # ----------------- # 4. Deliver customized data queue, and visualize # ----------------- print(f&quot;Start visualizing customized sensor data queue&quot;) rr.init(&quot;rerun_viz_customized_sensor_data_queue&quot;) for sensor_data in vrs_data_provider.deliver_queued_sensor_data(customized_deliver_options): stream_id = sensor_data.stream_id() stream_label = vrs_data_provider.get_label_from_stream_id(stream_id) device_time_ns = sensor_data.get_time_ns(TimeDomain.DEVICE_TIME) image_data_and_record = sensor_data.image_data_and_record() # Visualize rr.set_time_nanos(&quot;device_time&quot;, device_time_ns) rr.log(stream_label, rr.Image(image_data_and_record[0].to_numpy_array())) rr.notebook_show()  ","version":"Next","tagName":"h2"},{"title":"Tutorial 7: Loading and Visualizing MPS Output Data","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/mps","content":"","keywords":"","version":"Next"},{"title":"Introduction‚Äã","type":1,"pageTitle":"Tutorial 7: Loading and Visualizing MPS Output Data","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/mps#introduction","content":" This tutorial demonstrates how to access and visualize Machine Perception Services (MPS) results. MPS provides cloud-based processing of Aria data to generate high-quality 3D reconstruction, SLAM trajectories, and other perception outputs.  What you'll learn:  How to load and access MPS SLAM trajectory data (open loop and closed loop)How to load and visualize MPS semi-dense point clouds and observationsHow to create 3D visualizations of MPS SLAM results  Prerequisites  Complete Tutorial 1 (VrsDataProvider Basics) to understand basic data provider conceptsComplete Tutorial 2 (Device Calibration) to understand how to properly use calibration in Aria data.Download Aria Gen2 sample data: VRS and MPS output zip file.  Note on VisualizationIf visualization window is not showing up, this is due to Rerun lib's caching issue. Just rerun the specific code cell.  import os from projectaria_tools.core import mps # Set up paths to your MPS data mps_folder_path = &quot;path/to/your/mps/folder/&quot; vrs_file_path = &quot;path/to/your/recording.vrs&quot; # Load VRS data provider for additional context vrs_data_provider = data_provider.create_vrs_data_provider(vrs_file_path)   ","version":"Next","tagName":"h2"},{"title":"MPS SLAM Trajectories‚Äã","type":1,"pageTitle":"Tutorial 7: Loading and Visualizing MPS Output Data","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/mps#mps-slam-trajectories","content":" ","version":"Next","tagName":"h2"},{"title":"Understanding Open Loop vs Closed Loop Trajectories‚Äã","type":1,"pageTitle":"Tutorial 7: Loading and Visualizing MPS Output Data","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/mps#understanding-open-loop-vs-closed-loop-trajectories","content":" MPS SLAM algorithm outputs 2 trajectory files (see wiki page for data type definitions):  Open loop trajectory: High-frequency (1kHz) odometry from visual-inertial odometry (VIO), accurate over short periods but drifts over time and distance.Closed loop trajectory: High-frequency (1kHz) pose from mapping with loop closure corrections, reducing drift but possibly less accurate locally over short spans.  ","version":"Next","tagName":"h3"},{"title":"Loading Closed Loop Trajectory‚Äã","type":1,"pageTitle":"Tutorial 7: Loading and Visualizing MPS Output Data","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/mps#loading-closed-loop-trajectory","content":" from projectaria_tools.core.mps.utils import ( filter_points_from_confidence, get_nearest_pose, ) print(&quot;=== MPS - Closed loop trajectory ===&quot;) # Load MPS closed-loop trajectory data closed_loop_trajectory_file = os.path.join( mps_folder_path, &quot;slam&quot;, &quot;closed_loop_trajectory.csv&quot; ) closed_loop_trajectory = mps.read_closed_loop_trajectory(closed_loop_trajectory_file) # Print out the content of the first sample in closed_loop_trajectory if closed_loop_trajectory: sample = closed_loop_trajectory[0] print(&quot;ClosedLoopTrajectoryPose sample:&quot;) print(f&quot; tracking_timestamp: {int(sample.tracking_timestamp.total_seconds() * 1e6)} us&quot;) print(f&quot; utc_timestamp: {int(sample.utc_timestamp.total_seconds() * 1e6)} us&quot;) print(f&quot; transform_world_device:\\n{sample.transform_world_device}&quot;) print(f&quot; device_linear_velocity_device: {sample.device_linear_velocity_device}&quot;) print(f&quot; angular_velocity_device: {sample.angular_velocity_device}&quot;) print(f&quot; quality_score: {sample.quality_score}&quot;) print(f&quot; gravity_world: {sample.gravity_world}&quot;) print(f&quot; graph_uid: {sample.graph_uid}&quot;) else: print(&quot;closed_loop_trajectory is empty.&quot;)   ","version":"Next","tagName":"h3"},{"title":"Loading Open Loop Trajectory‚Äã","type":1,"pageTitle":"Tutorial 7: Loading and Visualizing MPS Output Data","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/mps#loading-open-loop-trajectory","content":" print(&quot;=== MPS - Open loop trajectory ===&quot;) # Load MPS open-loop trajectory data open_loop_trajectory_file = os.path.join( mps_folder_path, &quot;slam&quot;, &quot;open_loop_trajectory.csv&quot; ) open_loop_trajectory = mps.read_open_loop_trajectory(open_loop_trajectory_file) # Print out the content of the first sample in open_loop_trajectory if open_loop_trajectory: sample = open_loop_trajectory[0] print(&quot;OpenLoopTrajectoryPose sample:&quot;) print(f&quot; tracking_timestamp: {int(sample.tracking_timestamp.total_seconds() * 1e6)} us&quot;) print(f&quot; utc_timestamp: {int(sample.utc_timestamp.total_seconds() * 1e6)} us&quot;) print(f&quot; transform_odometry_device:\\n{sample.transform_odometry_device}&quot;) print(f&quot; device_linear_velocity_odometry: {sample.device_linear_velocity_odometry}&quot;) print(f&quot; angular_velocity_device: {sample.angular_velocity_device}&quot;) print(f&quot; quality_score: {sample.quality_score}&quot;) print(f&quot; gravity_odometry: {sample.gravity_odometry}&quot;) print(f&quot; session_uid: {sample.session_uid}&quot;) else: print(&quot;open_loop_trajectory is empty.&quot;)   ","version":"Next","tagName":"h3"},{"title":"MPS Semi-dense Point Cloud and Observations‚Äã","type":1,"pageTitle":"Tutorial 7: Loading and Visualizing MPS Output Data","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/mps#mps-semi-dense-point-cloud-and-observations","content":" ","version":"Next","tagName":"h2"},{"title":"Understanding Point Cloud Data‚Äã","type":1,"pageTitle":"Tutorial 7: Loading and Visualizing MPS Output Data","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/mps#understanding-point-cloud-data","content":" MPS SLAM algorithm outputs 2 files related to semi-dense point cloud (see wiki page for data type definitions):  semidense_points.csv.gz: Global points in the world coordinate frame.semidense_observations.csv.gz: Point observations for each camera, at each timestamp.  Note that semidense point files are normally large, therefore loading them may take some time.  ","version":"Next","tagName":"h3"},{"title":"Loading Semi-dense Point Cloud‚Äã","type":1,"pageTitle":"Tutorial 7: Loading and Visualizing MPS Output Data","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/mps#loading-semi-dense-point-cloud","content":" print(&quot;=== MPS - Semi-dense Point Cloud ===&quot;) # Load MPS semi-dense point cloud data semidense_points_file = os.path.join( mps_folder_path, &quot;slam&quot;, &quot;semidense_points.csv.gz&quot; ) semidense_points = mps.read_global_point_cloud(semidense_points_file) # Print out the content of the first sample in semidense_points if semidense_points: sample = semidense_points[0] print(&quot;GlobalPointPosition sample:&quot;) print(f&quot; uid: {sample.uid}&quot;) print(f&quot; graph_uid: {sample.graph_uid}&quot;) print(f&quot; position_world: {sample.position_world}&quot;) print(f&quot; inverse_distance_std: {sample.inverse_distance_std}&quot;) print(f&quot; distance_std: {sample.distance_std}&quot;) print(f&quot;Total number of semi-dense points: {len(semidense_points)}&quot;) else: print(&quot;semidense_points is empty.&quot;) # Filter semidense points by inv_dep or depth. # The filter will KEEP points with (inv_dep or depth &lt; threshold) filtered_semidense_points = filter_points_from_confidence(raw_points = semidense_points, threshold_invdep = 1e-3, threshold_dep = 5e-2) print(f&quot;Filtering semidense points from a total of {len(semidense_points)} points down to {len(filtered_semidense_points)}&quot;)   ","version":"Next","tagName":"h3"},{"title":"Loading Point Observations‚Äã","type":1,"pageTitle":"Tutorial 7: Loading and Visualizing MPS Output Data","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/mps#loading-point-observations","content":" print(&quot;=== MPS - Semi-dense Point Observations ===&quot;) # Load MPS semi-dense point observations data semidense_observations_file = os.path.join( mps_folder_path, &quot;slam&quot;, &quot;semidense_observations.csv.gz&quot; ) semidense_observations = mps.read_point_observations(semidense_observations_file) # Print out the content of the first sample in semidense_observations if semidense_observations: sample = semidense_observations[0] print(&quot;PointObservation sample:&quot;) print(f&quot; point_uid: {sample.point_uid}&quot;) print(f&quot; frame_capture_timestamp: {int(sample.frame_capture_timestamp.total_seconds() * 1e6)} us&quot;) print(f&quot; camera_serial: {sample.camera_serial}&quot;) print(f&quot; uv: {sample.uv}&quot;) print(f&quot;Total number of point observations: {len(semidense_observations)}&quot;) else: print(&quot;semidense_observations is empty.&quot;)   ","version":"Next","tagName":"h3"},{"title":"Visualizing MPS SLAM Results‚Äã","type":1,"pageTitle":"Tutorial 7: Loading and Visualizing MPS Output Data","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/mps#visualizing-mps-slam-results","content":" In the following code snippet, we demonstrate how to visualize the MPS SLAM results in a 3D view.  We first prepare a short trajectory segment, then extract the semidense points position, along with timestamp-mapped observations for visualization purpose. Finally we plot everything in Rerun.  ","version":"Next","tagName":"h2"},{"title":"Color Mapping Helper Function‚Äã","type":1,"pageTitle":"Tutorial 7: Loading and Visualizing MPS Output Data","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/mps#color-mapping-helper-function","content":" from collections import defaultdict import numpy as np # A helper coloring function def color_from_zdepth(z_depth_m: float) -&gt; np.ndarray: &quot;&quot;&quot; Map z-depth (meters, along the camera's forward axis) to a bright Viridis-like RGB color. - If z_depth_m &lt;= 0 (point is behind the camera), return black [0, 0, 0]. - Near (0.2 m) -&gt; yellow, Far (5.0 m) -&gt; purple. Returns an array of shape (3,) with dtype=uint8. &quot;&quot;&quot; if not np.isfinite(z_depth_m) or z_depth_m &lt;= 0.0: return np.array([0, 0, 0], dtype=np.uint8) NEAR_METERS, FAR_METERS = 0.2, 5.0 # Normalize to [0,1], then flip so near ‚Üí bright (yellow), far ‚Üí dark (purple) clamped = min(max(float(z_depth_m), NEAR_METERS), FAR_METERS) normalized_position = (clamped - NEAR_METERS) / (FAR_METERS - NEAR_METERS + 1e-12) gradient_position = 1.0 - normalized_position # Viridis-like anchor colors: purple ‚Üí blue ‚Üí teal ‚Üí green ‚Üí yellow color_stops = [ (68, 1, 84), (59, 82, 139), (33, 145, 140), (94, 201, 98), (253, 231, 37), ] # Locate segment and blend between its endpoints segment_count = len(color_stops) - 1 continuous_index = gradient_position * segment_count lower_segment_index = int(continuous_index) if lower_segment_index &gt;= segment_count: red, green, blue = color_stops[-1] else: segment_fraction = continuous_index - lower_segment_index r0, g0, b0 = color_stops[lower_segment_index] r1, g1, b1 = color_stops[lower_segment_index + 1] red = r0 + segment_fraction * (r1 - r0) green = g0 + segment_fraction * (g1 - g0) blue = b0 + segment_fraction * (b1 - b0) return np.array([int(red), int(green), int(blue)], dtype=np.uint8)   ","version":"Next","tagName":"h3"},{"title":"Preparing Data for Visualization‚Äã","type":1,"pageTitle":"Tutorial 7: Loading and Visualizing MPS Output Data","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/mps#preparing-data-for-visualization","content":" print(&quot;=== Preparing MPS SLAM results for visualization ===&quot;) # Check if we have valid SLAM data to visualize if not closed_loop_trajectory or not semidense_points: raise RuntimeError(&quot;Warning: This tutorial requires valid MPS SLAM data to run.&quot;) # -- # Prepare Trajectory data # -- # Select a short segment of trajectory (e.g., first 5000 samples, subsampled by 50) segment_length = min(50000, len(closed_loop_trajectory)) trajectory_segment = closed_loop_trajectory[:segment_length:50] timestamp_to_pose = { pose.tracking_timestamp: pose for pose in trajectory_segment } print(f&quot;Finished preparing a trajectory of length {len(trajectory_segment)}... &quot;) # ----------- # Prepare Semidense point data # ----------- # Filter the semidense point cloud by confidence and limit max point count, and extract the point positions filtered_semidense_point_cloud_data = filter_points_from_confidence(semidense_points) points_positions = np.array( [ point.position_world for point in filtered_semidense_point_cloud_data ] ) print(f&quot;Finished preparing filtered semidense points cloud of {len(filtered_semidense_point_cloud_data)} points... &quot;) # ----------- # Prepare Semidense observation data # ----------- # Based on RGB observations, create a per-timestamp point position list, and color them according to its distance from RGB camera point_uid_to_position = { point.uid: np.array(point.position_world) for point in filtered_semidense_point_cloud_data } # A helper function that creates a easier-to-query mapping to obtain observations according to timestamps slam_1_serial = vrs_data_provider.get_device_calibration().get_camera_calib(&quot;slam-front-left&quot;).get_serial_number() timestamp_to_point_positions = defaultdict(list) # t_ns -&gt; [position, position, ...] timestamp_to_point_colors = defaultdict(list) # t_ns -&gt; [color, color, ...] for obs in semidense_observations: # Only add observations for SLAM_1 camera, and if the timestamp is in the chosen trajectory segment if ( obs.camera_serial == slam_1_serial and obs.frame_capture_timestamp in timestamp_to_pose and obs.point_uid in point_uid_to_position): # Insert point position obs_timestamp = obs.frame_capture_timestamp point_position = point_uid_to_position[obs.point_uid] timestamp_to_point_positions[obs_timestamp].append(point_position) # Insert point color T_world_device = timestamp_to_pose[obs_timestamp].transform_world_device point_in_device = T_world_device.inverse() @ point_position point_z_depth = point_in_device.squeeze()[2] point_color = color_from_zdepth(point_z_depth) timestamp_to_point_colors[obs_timestamp].append(point_color) from itertools import islice print(f&quot;Finished preparing semidense points observations: &quot;) for timestamp, points in islice(timestamp_to_point_positions.items(), 5): print(f&quot;\\t timestamp {int(timestamp.total_seconds() * 1e9)} ns has {len(points)} observed points in slam-front-left view. &quot;) print(f&quot;\\t ...&quot;)   ","version":"Next","tagName":"h3"},{"title":"3D Visualization with Rerun‚Äã","type":1,"pageTitle":"Tutorial 7: Loading and Visualizing MPS Output Data","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/mps#3d-visualization-with-rerun","content":" import rerun as rr import numpy as np from projectaria_tools.utils.rerun_helpers import ( AriaGlassesOutline, ToTransform3D, ToBox3D, ) from projectaria_tools.core.mps.utils import ( filter_points_from_confidence, get_nearest_pose, ) print(&quot;=== Visualizing MPS SLAM Results in 3D ===&quot;) # Initialize Rerun rr.init(&quot;MPS SLAM Visualization&quot;) # Set up the 3D scene rr.log(&quot;world&quot;, rr.ViewCoordinates.RIGHT_HAND_Z_UP, static=True) # Log point cloud rr.log( &quot;world/semidense_points&quot;, rr.Points3D( positions=points_positions, colors=[255, 255, 255, 125], radii=0.001 ), static=True ) # Aria glass outline for visualization purpose device_calib = vrs_data_provider.get_device_calibration() aria_glasses_point_outline = AriaGlassesOutline( device_calib, use_cad_calib=True ) # Plot Closed loop trajectory closed_loop_traj_cached_full = [] observation_points_cached = None observation_colors_cached = None for closed_loop_pose in trajectory_segment: capture_timestamp_ns = int(closed_loop_pose.tracking_timestamp.total_seconds() * 1e9) rr.set_time_nanos(&quot;device_time&quot;, capture_timestamp_ns) T_world_device = closed_loop_pose.transform_world_device # Log device pose as a coordinate frame rr.log( &quot;world/device&quot;, ToTransform3D( T_world_device, axis_length=0.05, ), ) # Plot Aria glass outline rr.log( &quot;world/device/glasses_outline&quot;, rr.LineStrips3D( aria_glasses_point_outline, colors=[150,200,40], radii=5e-3, ), ) # Plot gravity direction vector rr.log( &quot;world/vio_gravity&quot;, rr.Arrows3D( origins=[T_world_device.translation()[0]], vectors=[ closed_loop_pose.gravity_world * 1e-2 ], # length converted from 9.8 meter -&gt; 10 cm colors=[101,67,33], radii=5e-3, ), static=False, ) # Update cached results for observations. Cache is needed because observation has a much lower freq than high-freq trajectory. if closed_loop_pose.tracking_timestamp in timestamp_to_point_positions.keys(): observation_points_cached = timestamp_to_point_positions[closed_loop_pose.tracking_timestamp] observation_colors_cached = timestamp_to_point_colors[closed_loop_pose.tracking_timestamp] if observation_points_cached is not None: rr.log( &quot;world/semidense_observations&quot;, rr.Points3D( positions = observation_points_cached, colors = observation_colors_cached, radii=0.01 ), static = False ) # Plot the entire VIO trajectory that are cached so far closed_loop_traj_cached_full.append(T_world_device.translation()[0]) rr.log( &quot;world/vio_trajectory&quot;, rr.LineStrips3D( closed_loop_traj_cached_full, colors=[173, 216, 255], radii=5e-3, ), static=False, ) rr.notebook_show()   ","version":"Next","tagName":"h3"},{"title":"Understanding MPS Data Structures‚Äã","type":1,"pageTitle":"Tutorial 7: Loading and Visualizing MPS Output Data","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/mps#understanding-mps-data-structures","content":" ","version":"Next","tagName":"h2"},{"title":"Trajectory Data Types‚Äã","type":1,"pageTitle":"Tutorial 7: Loading and Visualizing MPS Output Data","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/mps#trajectory-data-types","content":" ClosedLoopTrajectoryPose‚Äã  tracking_timestamp: Device timestamp when pose was computedutc_timestamp: UTC timestamptransform_world_device: 6DOF pose in world coordinate framedevice_linear_velocity_device: Linear velocity in device frameangular_velocity_device: Angular velocity in device framequality_score: Pose estimation quality (higher = better)gravity_world: Gravity vector in world framegraph_uid: Unique identifier for the pose graph  OpenLoopTrajectoryPose‚Äã  tracking_timestamp: Device timestamp when pose was computedutc_timestamp: UTC timestamptransform_odometry_device: 6DOF pose in odometry coordinate framedevice_linear_velocity_odometry: Linear velocity in odometry frameangular_velocity_device: Angular velocity in device framequality_score: Pose estimation quality (higher = better)gravity_odometry: Gravity vector in odometry framesession_uid: Unique identifier for the session  ","version":"Next","tagName":"h3"},{"title":"Point Cloud Data Types‚Äã","type":1,"pageTitle":"Tutorial 7: Loading and Visualizing MPS Output Data","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/mps#point-cloud-data-types","content":" GlobalPointPosition‚Äã  uid: Unique identifier for the 3D pointgraph_uid: Identifier linking point to pose graphposition_world: 3D position in world coordinate frameinverse_distance_std: Inverse distance standard deviation (quality metric)distance_std: Distance standard deviation (quality metric)  PointObservation‚Äã  point_uid: Links observation to 3D pointframe_capture_timestamp: When the observation was capturedcamera_serial: Serial number of the observing camerauv: 2D pixel coordinates of the observation  ","version":"Next","tagName":"h3"},{"title":"MPS vs On-Device Comparisons‚Äã","type":1,"pageTitle":"Tutorial 7: Loading and Visualizing MPS Output Data","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/mps#mps-vs-on-device-comparisons","content":" ","version":"Next","tagName":"h2"},{"title":"Key Differences‚Äã","type":1,"pageTitle":"Tutorial 7: Loading and Visualizing MPS Output Data","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/mps#key-differences","content":" Aspect\tOn-Device (VIO/SLAM)\tMPS SLAMProcessing\tReal-time during recording\tCloud-based post-processing Accuracy\tGood for real-time use\tHigher accuracy with global optimization Frequency\t20Hz (VIO), 800Hz (high-freq)\t1kHz (both open/closed loop) Drift\tAccumulates over time\tMinimized with loop closure Point Cloud\tNot available\tDense semi-dense reconstructions Coordinate Frame\tOdometry frame\tGlobal world frame  ","version":"Next","tagName":"h3"},{"title":"Use Cases‚Äã","type":1,"pageTitle":"Tutorial 7: Loading and Visualizing MPS Output Data","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/mps#use-cases","content":" On-Device Data: Real-time applications, live feedback, immediate processingMPS Data: High-quality reconstruction, research analysis, detailed mapping  ","version":"Next","tagName":"h3"},{"title":"Summary‚Äã","type":1,"pageTitle":"Tutorial 7: Loading and Visualizing MPS Output Data","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/mps#summary","content":" This tutorial covered the essential aspects of working with MPS data:  Trajectory Access: Loading both open loop and closed loop trajectoriesPoint Cloud Data: Accessing semi-dense 3D reconstructions and observationsData Filtering: Using confidence thresholds to improve point cloud quality3D Visualization: Creating comprehensive visualizations with trajectories and point cloudsData Structures: Understanding the comprehensive MPS data formats  MPS provides high-quality, globally consistent 3D reconstructions that are ideal for:  Research Applications: Detailed spatial analysis and mapping3D Reconstruction: High-fidelity environmental modelingMotion Analysis: Accurate trajectory analysis without driftMulti-modal Studies: Combining precise 3D data with sensor informationBenchmarking: Comparing against ground truth for algorithm development ","version":"Next","tagName":"h2"},{"title":"C++ Visualization Tools","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/cppviz","content":"","keywords":"","version":"Next"},{"title":"Prerequisites‚Äã","type":1,"pageTitle":"C++ Visualization Tools","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/cppviz#prerequisites","content":" To use the C++ visualization tools, you need to build them from source using CMake. These tools are not included in the standard Python package installation.  Required build step: Follow the &quot;build using CMake from source code&quot; instructions in the Advanced Installation guide to compile the C++ visualization tools.  The C++ tools require the Pangolin library for visualization. If Pangolin is not found during compilation, the visualization tools will not be built.  ","version":"Next","tagName":"h2"},{"title":"aria_viewer‚Äã","type":1,"pageTitle":"C++ Visualization Tools","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/cppviz#aria_viewer","content":" The aria_viewer is a C++ binary that provides real-time visualization of Aria VRS (Video Recording and Sensor) data using the Pangolin framework. It offers native performance for visualizing multi-modal sensor data from Aria devices.     ","version":"Next","tagName":"h2"},{"title":"Basic Usage‚Äã","type":1,"pageTitle":"C++ Visualization Tools","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/cppviz#basic-usage","content":" First, navigate to the built directory:  cd ${SRC}/build/tools/visualization/   Then run the viewer command:  LinuxmacOS aria_viewer --vrs path/to/your/file.vrs   ","version":"Next","tagName":"h3"},{"title":"Command Line Options‚Äã","type":1,"pageTitle":"C++ Visualization Tools","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/cppviz#command-line-options","content":" Parameter\tType\tRequired\tDescription--vrs\tstring\tYes\tPath to the VRS file you want to visualize  ","version":"Next","tagName":"h3"},{"title":"What You'll See‚Äã","type":1,"pageTitle":"C++ Visualization Tools","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/cppviz#what-youll-see","content":" The C++ viewer provides an interactive visualization environment with multiple components:  RGB &amp; SLAM Camera Streams: Real-time display of RGB and SLAM camera images with overlaid eye gaze and hand tracking results1D Sensor Data: Time series plots of: IMU signals (accelerometer and gyroscope data)Audio signals (microphone waveforms)Barometer signals (pressure and temperature readings) 3D Visualization: Interactive 3D scene showing: VIO trajectory (Visual-Inertial Odometry path)Eye gaze vectors in 3D spaceHand tracking results and posesDevice pose and orientation  ","version":"Next","tagName":"h3"},{"title":"Interactive Controls‚Äã","type":1,"pageTitle":"C++ Visualization Tools","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/cppviz#interactive-controls","content":" The viewer includes real-time controls for:  Play/Pause: Start and stop data playbackSpeed Control: Adjust playback speed using a sliderTimestamp Navigation: Jump to specific timestamps by dragging the timestamp sliderStream Toggles: Enable/disable individual data streams for focused analysis3D Camera Controls: Navigate and explore the 3D visualization  ","version":"Next","tagName":"h3"},{"title":"Known Issues and Workarounds‚Äã","type":1,"pageTitle":"C++ Visualization Tools","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/cppviz#known-issues-and-workarounds","content":" High-frequency sequence playback issue: There is a known issue when playing high-frequency Aria-Gen2 data at real speed, where camera views may update asynchronously. This is due to CPU-based image decoding speed limitations.  Workarounds:  Use Python viewer: Fall back to the aria_rerun_viewer for smooth high-frequency playbackReduce playback speed: Use the speed control slider to play at slower speedsRandom access: Dragging the timestamp slider for random access always works correctly  The development team is actively working on a solution for this issue. ","version":"Next","tagName":"h3"},{"title":"Tutorial 6: Device Time Alignment in Aria Gen2","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/time-sync","content":"","keywords":"","version":"Next"},{"title":"Introduction‚Äã","type":1,"pageTitle":"Tutorial 6: Device Time Alignment in Aria Gen2","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/time-sync#introduction","content":" In Project Aria glasses, one of the key features is that it provides multi-sensor data that are temporally aligned to a shared, device-time domain for each single device, and also provide multi-device Time Alignment using SubGHz signals (Aria Gen2), TICSync (Aria Gen1), or TimeCode signals (Aria Gen1). In this tutorial, we will demonstrate how to use such temporal aligned data from Aria Gen2 recordings.  What you'll learn:  How to access temporally aligned sensor data on a single VRS recording.How to access temporally aligned sensor data across multiple recordings using SubGHz signals.  Prerequisites  Complete Tutorial 1 (VrsDataProvider Basics) to understand basic data provider conceptsComplete Tutorial 3 (Sequential Access multi-sensor data) to understand how to create a queue of sensor data from VRS file.Download Aria Gen2 sample data: host recording and client recording  Note on visualization:If visualization window is not showing up, this is due to Rerun lib's caching issue. Just rerun the specific code cell, or restart the Python kernel.  from projectaria_tools.core import data_provider # Load local VRS file vrs_file_path = &quot;path/to/your/recording.vrs&quot; vrs_data_provider = data_provider.create_vrs_data_provider(vrs_file_path)   ","version":"Next","tagName":"h2"},{"title":"Single-Device Timestamp alignment‚Äã","type":1,"pageTitle":"Tutorial 6: Device Time Alignment in Aria Gen2","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/time-sync#single-device-timestamp-alignment","content":" In projectaria_tools, every timestamp is linked to a specific TimeDomain, which represents the time reference or clock used to generate that timestamp. Timestamps from different TimeDomains are not directly comparable‚Äîonly timestamps within the same TimeDomain are consistent and can be accurately compared or aligned.  ","version":"Next","tagName":"h2"},{"title":"Supported Time Domains‚Äã","type":1,"pageTitle":"Tutorial 6: Device Time Alignment in Aria Gen2","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/time-sync#supported-time-domains","content":" Important: Use DEVICE_TIME for single-device Aria data analysis  The following table shows all supported time domains in projectaria_tools. For single-device Aria data analysis, use DEVICE_TIME for accurate temporal alignment between sensors.  Time Domain\tDescription\tUsageDEVICE_TIME (Recommended)\tCapture time in device's time domain. Accurate and reliable. All sensors on the same Aria device share the same device time domain.\tUse this for single-device Aria data analysis RECORD_TIME\tTimestamps stored in the index of VRS files. For Aria glasses, these are equal to device timestamp converted to double-precision floating point.\tFast access, but use DEVICE_TIME for accuracy HOST_TIME\tTimestamps when sensor data is saved to the device (not when captured).\tShould not be needed for any purpose --- Time Domains for Multi-device time alignment --- SUBGHZ\tMulti-device time alignment option for Aria Gen2\tSee next part in this tutorial UTC\tMulti-device time alignment option\tSee next part in this tutorial TIME_CODE\tMulti-device time alignment option for Aria Gen1\tSee Gen1 multi-device tutorial TIC_SYNC\tMulti-device time alignment option for Aria Gen1\tSee Gen1 multi-device tutorial  ","version":"Next","tagName":"h3"},{"title":"Data API to query by timestamp‚Äã","type":1,"pageTitle":"Tutorial 6: Device Time Alignment in Aria Gen2","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/time-sync#data-api-to-query-by-timestamp","content":" The VRS data provider offers powerful timestamp-based data access through the get_&lt;SENSOR&gt;_data_by_time_ns() API family. This is the recommended approach for temporal alignment across sensors and precise timestamp-based data retrieval.  For any sensor type, you can query data by timestamp using the get_&lt;SENSOR&gt;_data_by_time_ns() function, where &lt;SENSOR&gt; can be replaced by any sensor data type available in Aria VRS. See the VrsDataProvider.h for a complete list of supported sensor types.  TimeQueryOptions  This TimeQueryOptions parameter controls how the system finds data when your query timestamp doesn't exactly match a recorded timestamp:  Option\tBehavior\tUse CaseBEFORE\tReturns the last valid data with timestamp ‚â§ query_time\tDefault and most common - Get the most recent data before or at the query time AFTER\tReturns the first valid data with timestamp ‚â• query_time\tGet the next available data after or at the query time CLOSEST\tReturns data with smallest `\ttimestamp - query_time  ","version":"Next","tagName":"h3"},{"title":"Boundary Behavior‚Äã","type":1,"pageTitle":"Tutorial 6: Device Time Alignment in Aria Gen2","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/time-sync#boundary-behavior","content":" The API handles edge cases automatically:  Query Condition\tBEFORE\tAFTER\tCLOSESTquery_time &lt; first_timestamp\tReturns invalid data\tReturns first data\tReturns first data first_timestamp ‚â§ query_time ‚â§ last_timestamp\tReturns data with timestamp ‚â§ query_time\tReturns data with timestamp ‚â• query_time\tReturns temporally closest data query_time &gt; last_timestamp\tReturns last data\tReturns invalid data\tReturns last data  ","version":"Next","tagName":"h3"},{"title":"Single VRS Timestamp-Based Query Example‚Äã","type":1,"pageTitle":"Tutorial 6: Device Time Alignment in Aria Gen2","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/time-sync#single-vrs-timestamp-based-query-example","content":" from projectaria_tools.core.sensor_data import SensorDataType, TimeDomain, TimeQueryOptions print(&quot;=== Single VRS timestamp based query ===&quot;) # Select RGB stream ID rgb_stream_id = vrs_data_provider.get_stream_id_from_label(&quot;camera-rgb&quot;) # Get a timestamp within the recording (3 seconds after start) start_timestamp_ns = vrs_data_provider.get_first_time_ns(rgb_stream_id, TimeDomain.DEVICE_TIME) selected_timestamp_ns = start_timestamp_ns + int(3e9) # Fetch the RGB frame that is CLOSEST to this selected timestamp_ns closest_rgb_data_and_record = vrs_data_provider.get_image_data_by_time_ns( stream_id = rgb_stream_id, time_ns = selected_timestamp_ns, time_domain = TimeDomain.DEVICE_TIME, time_query_options = TimeQueryOptions.CLOSEST ) closest_timestamp_ns = closest_rgb_data_and_record[1].capture_timestamp_ns closest_frame_number = closest_rgb_data_and_record[1].frame_number print(f&quot; The closest RGB frame to query timestamp {selected_timestamp_ns} is the {closest_frame_number}-th frame, with capture timestamp of {closest_timestamp_ns}&quot;) # Fetch the frame BEFORE this frame prev_rgb_data_and_record = vrs_data_provider.get_image_data_by_time_ns( stream_id = rgb_stream_id, time_ns = closest_timestamp_ns - 1, time_domain = TimeDomain.DEVICE_TIME, time_query_options = TimeQueryOptions.BEFORE ) prev_timestamp_ns = prev_rgb_data_and_record[1].capture_timestamp_ns prev_frame_number = prev_rgb_data_and_record[1].frame_number print(f&quot; The previous RGB frame is the {prev_frame_number}-th frame, with capture timestamp of {prev_timestamp_ns}&quot;) # Fetch the frame AFTER this frame next_rgb_data_and_record = vrs_data_provider.get_image_data_by_time_ns( stream_id = rgb_stream_id, time_ns = closest_timestamp_ns + 1, time_domain = TimeDomain.DEVICE_TIME, time_query_options = TimeQueryOptions.AFTER ) next_timestamp_ns = next_rgb_data_and_record[1].capture_timestamp_ns next_frame_number = next_rgb_data_and_record[1].frame_number print(f&quot; The next RGB frame is the {next_frame_number}-th frame, with capture timestamp of {next_timestamp_ns}&quot;)   ","version":"Next","tagName":"h3"},{"title":"Visualizing Synchronized Multi-sensor Data‚Äã","type":1,"pageTitle":"Tutorial 6: Device Time Alignment in Aria Gen2","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/time-sync#visualizing-synchronized-multi-sensor-data","content":" In this additional example, we demonstrate how to query and visualize &quot;groups&quot; of RGB + SLAM images approximately at the same timestamp.  import rerun as rr print(&quot;=== Single VRS timestamp-based query visualization examples ===&quot;) rr.init(&quot;rerun_viz_single_vrs_timestamp_based_query&quot;) # Select RGB and SLAM stream IDs to visualize all_labels = vrs_data_provider.get_device_calibration().get_camera_labels() slam_labels = [label for label in all_labels if &quot;slam&quot; in label ] slam_stream_ids = [vrs_data_provider.get_stream_id_from_label(label) for label in slam_labels] rgb_stream_id = vrs_data_provider.get_stream_id_from_label(&quot;camera-rgb&quot;) # Starting from +3 seconds into the recording, and at 5Hz frequency target_period_ns = int(2e8) start_timestamp_ns = vrs_data_provider.get_first_time_ns(rgb_stream_id, TimeDomain.DEVICE_TIME) + int(3e9) # Plot 20 samples current_timestamp_ns = start_timestamp_ns for frame_i in range(20): # Query and plot RGB image rgb_image_data, rgb_image_record = vrs_data_provider.get_image_data_by_time_ns( stream_id = rgb_stream_id, time_ns = current_timestamp_ns, time_domain = TimeDomain.DEVICE_TIME, time_query_options = TimeQueryOptions.CLOSEST) rr.set_time_nanos(&quot;device_time&quot;, rgb_image_record.capture_timestamp_ns) rr.log(&quot;rgb_image&quot;, rr.Image(rgb_image_data.to_numpy_array())) # Query and plot SLAM images for slam_i in range(len(slam_labels)): single_slam_label = slam_labels[slam_i] single_slam_stream_id = slam_stream_ids[slam_i] slam_image_data, slam_image_record = vrs_data_provider.get_image_data_by_time_ns( stream_id = single_slam_stream_id, time_ns = current_timestamp_ns, time_domain = TimeDomain.DEVICE_TIME, time_query_options = TimeQueryOptions.CLOSEST) rr.set_time_nanos(&quot;device_time&quot;, slam_image_record.capture_timestamp_ns) rr.log(single_slam_label, rr.Image(slam_image_data.to_numpy_array())) # Increment query timestamp current_timestamp_ns += target_period_ns rr.notebook_show()   ","version":"Next","tagName":"h3"},{"title":"Multi-Device Timestamp alignment‚Äã","type":1,"pageTitle":"Tutorial 6: Device Time Alignment in Aria Gen2","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/time-sync#multi-device-timestamp-alignment","content":" While recording, multiple Aria-Gen2 glasses can enable a feature that allows their timestamps to be mapped across devices using SubGHz signals. Please refer to the multi-device recording wiki page from ARK (TODO: add link) to learn how to record with this feature.  Basically, one pair of glasses acts as the host device, that actively broadcasts SubGHz signals to a specified channel; all other glasses act as client devices, that receives the SubGHz signals, and record a new Time Domain Mapping data streams in their VRS file. It is essentially a timestamp hash mapping from host DEVICE_TIME -&gt; client DEVICE_TIME. Therefore this mapping data stream only exists in client VRS, but not host VRS.  In projectaria_tools, we provide 2 types of APIs to easily perform timestamp-based query across multi-device recordings:  Converter APIs provides direct convert functions that maps timestamps between any 2 TimeDomain.Query APIs that allows users to specifies time_domain = TimeDomain.SUBGHZ in a client VRS, to query &quot;from timestamp of the host&quot;.  The following code shows examples of using each type of API. Note that in the visualization example, the host and client windows will play intermittently. This is expected and correct, because the host and client devices' RGB cameras are NOT trigger aligned by nature.  ","version":"Next","tagName":"h2"},{"title":"Timestamp Converter APIs‚Äã","type":1,"pageTitle":"Tutorial 6: Device Time Alignment in Aria Gen2","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/time-sync#timestamp-converter-apis","content":" import rerun as rr from projectaria_tools.core.sensor_data import ( SensorData, ImageData, TimeDomain, TimeQueryOptions, TimeSyncMode, ) # Create data providers for both host and client recordings host_recording = &quot;path/to/host.vrs&quot; host_data_provider = data_provider.create_vrs_data_provider(host_recording) client_recording = &quot;path/to/client.vrs&quot; client_data_provider = data_provider.create_vrs_data_provider(client_recording) print(&quot;======= Multi-VRS time mapping example: Timestamp converter APIs ======&quot;) # Because host and client recordings may start at different times, # we manually pick a timestamp in the middle of the host recording. # Note that for host, we always use DEVICE_TIME domain. selected_timestamp_host = (host_data_provider.get_first_time_ns_all_streams(time_domain = TimeDomain.DEVICE_TIME) + host_data_provider.get_last_time_ns_all_streams(time_domain = TimeDomain.DEVICE_TIME)) // 2 # Convert from host time to client time selected_timestamp_client = client_data_provider.convert_from_synctime_to_device_time_ns(selected_timestamp_host, TimeSyncMode.SUBGHZ) # Convert from client time back to host time. Note that there could be some small numerical differences compared selected_timestamp_host_roundtrip = client_data_provider.convert_from_device_time_to_synctime_ns(selected_timestamp_client, TimeSyncMode.SUBGHZ) print(f&quot; Selected host timestamp is {selected_timestamp_host}; &quot;) print(f&quot; Converted to client timestamp is {selected_timestamp_client}; &quot;) print(f&quot; Then roundtrip convert back to host:{selected_timestamp_host_roundtrip}, &quot; f&quot; And delta value from original host timestamp is {selected_timestamp_host_roundtrip - selected_timestamp_host}. This is mainly due to numerical errors. &quot;)   ","version":"Next","tagName":"h3"},{"title":"Multi-Device Query APIs‚Äã","type":1,"pageTitle":"Tutorial 6: Device Time Alignment in Aria Gen2","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/time-sync#multi-device-query-apis","content":" print(&quot;======= Multi-VRS time mapping example: Query APIs ======&quot;) rr.init(&quot;rerun_viz_multi_vrs_time_mapping&quot;) # Set up sensor queue options in host VRS, only turn on RGB stream host_deliver_options = host_data_provider.get_default_deliver_queued_options() host_deliver_options.deactivate_stream_all() rgb_stream_id = host_data_provider.get_stream_id_from_label(&quot;camera-rgb&quot;) host_deliver_options.activate_stream(rgb_stream_id) # Select only a segment to plot host_vrs_start_timestamp = host_data_provider.get_first_time_ns_all_streams(time_domain = TimeDomain.DEVICE_TIME) host_segment_start = host_vrs_start_timestamp + int(20e9) # 20 seconds after start host_segment_duration = int(5e9) host_segment_end = host_segment_start + host_segment_duration host_vrs_end_timestamp = host_data_provider.get_last_time_ns_all_streams(time_domain = TimeDomain.DEVICE_TIME) host_deliver_options.set_truncate_first_device_time_ns(host_segment_start - host_vrs_start_timestamp) host_deliver_options.set_truncate_last_device_time_ns(host_vrs_end_timestamp - host_segment_end) # Plot RGB image data from both host and client for sensor_data in host_data_provider.deliver_queued_sensor_data(host_deliver_options): # --------- # Plotting in host. # Everything is done in DEVICE_TIME domain. # --------- host_image_data, host_image_record = sensor_data.image_data_and_record() # Set timestamps directly from host image record host_timestamp_ns = host_image_record.capture_timestamp_ns rr.set_time_nanos(&quot;device_time&quot;, host_timestamp_ns) rr.log(&quot;rgb_image_in_host&quot;, rr.Image(host_image_data.to_numpy_array())) # --------- # Plotting in client. # All the query APIs are done in SUBGHZ domain. # --------- # Query the closest RGB image from client VRS client_image_data, client_image_record = client_data_provider.get_image_data_by_time_ns( stream_id = rgb_stream_id, time_ns = host_timestamp_ns, time_domain = TimeDomain.SUBGHZ, time_query_options = TimeQueryOptions.CLOSEST) # Still need to convert client's device time back to host's time, # because we want to log this image data on host's timeline in Rerun client_timestamp_ns = client_image_record.capture_timestamp_ns converted_client_timestamp_ns = client_data_provider.convert_from_device_time_to_synctime_ns(client_timestamp_ns, TimeSyncMode.SUBGHZ) rr.set_time_nanos(&quot;device_time&quot;, converted_client_timestamp_ns) # Plot client image rr.log(&quot;rgb_image_in_client&quot;, rr.Image(client_image_data.to_numpy_array())) rr.notebook_show()  ","version":"Next","tagName":"h3"},{"title":"Export Gen2 On-device Machine Perception data to CSV","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/exportcsv","content":"","keywords":"","version":"Next"},{"title":"Basic Usage‚Äã","type":1,"pageTitle":"Export Gen2 On-device Machine Perception data to CSV","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/exportcsv#basic-usage","content":" gen2_mp_csv_exporter --vrs-path path/to/your/gen2_file.vrs \\ --output-folder ./exported_mp_data/ \\ --vio-high-freq-subsample-rate 80   ","version":"Next","tagName":"h2"},{"title":"Command Line Options‚Äã","type":1,"pageTitle":"Export Gen2 On-device Machine Perception data to CSV","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/exportcsv#command-line-options","content":" Parameter\tType\tRequired\tDescription--vrs-path\tstring\tYes\tPath to the Gen2 VRS file containing on-device MP data --output-folder\tstring\tNo\tFolder to output MP CSV files. Default: current directory --vio-high-freq-subsample-rate\tint\tNo\tSubsample rate for VIO high frequency data. Default: 1 (no subsampling)  ","version":"Next","tagName":"h2"},{"title":"Data Types Exported‚Äã","type":1,"pageTitle":"Export Gen2 On-device Machine Perception data to CSV","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/exportcsv#data-types-exported","content":" After running the tool, your output folder will contain the following files, with csv / JSONL formats that are compatible with MPS data formats:  output_folder/ ‚îú‚îÄ‚îÄ slam/ ‚îÇ ‚îú‚îÄ‚îÄ open_loop_trajectory.csv # VIO high-frequency poses ‚îÇ ‚îî‚îÄ‚îÄ online_calibration.jsonl # Online calibration updates ‚îú‚îÄ‚îÄ eye_gaze/ ‚îÇ ‚îî‚îÄ‚îÄ generalized_eye_gaze.csv # Eye gaze data ‚îî‚îÄ‚îÄ hand_tracking/ ‚îî‚îÄ‚îÄ hand_tracking_results.csv # Hand tracking results   Data Type\tSource VRS Data Stream\tOutput File\tContent\tNotesVIO High-Frequency Trajectory\tvio_high_frequency\t${OUTPUT_FOLDER}/slam/open_loop_trajectory.csv\tHigh-frequency device trajectory poses from Visual-Inertial Odometry\tCompatible with MPS open-loop trajectory format. Subsampling configurable via --vio-high-freq-subsample-rate parameter Eye Gaze Data\teyegaze\t${OUTPUT_FOLDER}/eye_gaze/generalized_eye_gaze.csv\tEye gaze directions and fixation points\tCompatible with MPS eye gaze format Hand Tracking Data\thandtracking\t${OUTPUT_FOLDER}/hand_tracking/hand_tracking_results.csv\tHand pose estimates, joint positions, and hand landmarks\tCompatible with MPS hand tracking format Online Calibration Data\tvio\t${OUTPUT_FOLDER}/slam/online_calibration.jsonl\tOnline SLAM camera and IMU calibration updates computed on-device\tCompatible with MPS online calibration format  ","version":"Next","tagName":"h2"},{"title":"Important Notes‚Äã","type":1,"pageTitle":"Export Gen2 On-device Machine Perception data to CSV","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/exportcsv#important-notes","content":" Gen2 specific: This tool only works with Gen2 Aria VRS files that contain on-device MP dataStream availability: Not all Gen2 recordings may contain all MP data streamsData consistency: The tool preserves original timestamps and coordinate systems from the on-device processingCalibration updates: Online calibration data represents dynamic updates computed during recording, which may differ from factory calibration ","version":"Next","tagName":"h2"},{"title":"Tutorial 5: On-Device VIO data streams","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/vio","content":"","keywords":"","version":"Next"},{"title":"Introduction‚Äã","type":1,"pageTitle":"Tutorial 5: On-Device VIO data streams","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/vio#introduction","content":" In Aria-Gen2 glasses, one of the key upgrade from Aria-Gen1 is the capability to run Machine Perception (MP) algorithms on the device during streaming / recording. Currently supported on-device MP algorithms include Eye-tracking, Hand-tracking, and VIO. These algorithm results are stored as separate data streams in the VRS file.  VIO (Visual Inertial Odometry) combines camera images and IMU (Inertial Measurement Unit) data to estimate device pose and motion in real-time. VIO tracks the device's position, orientation, and velocity by performing visual tracking, IMU integration, sensor fusion, etc, making it the foundation for spatial tracking and understanding.  In Aria-Gen2 devices, the VIO algorithm are run on device to produce 2 types of tracking results as part of the VRS file: VIO and VIO High Frequency. This tutorial focuses on demonstration of how to use the on-device VIO and VIO_high_frequency results.  What you'll learn:  How to access on-device VIO and VIO_high_frequency data from VRS filesHow to visualize 3D trajectory from on-device VIO data.  Prerequisites  Complete Tutorial 1 (VrsDataProvider Basics) to understand basic data provider conceptsComplete Tutorial 2 (Device Calibration) to understand how to properly use calibration in Aria data.Download Aria Gen2 sample data from link  Note on visualization:If visualization window is not showing up, this is due to Rerun lib's caching issue. Just rerun the specific code cell.  from projectaria_tools.core import data_provider # Load local VRS file vrs_file_path = &quot;path/to/your/recording.vrs&quot; vrs_data_provider = data_provider.create_vrs_data_provider(vrs_file_path) # Query VIO data streams vio_label = &quot;vio&quot; vio_stream_id = vrs_data_provider.get_stream_id_from_label(vio_label) if vio_stream_id is None: raise RuntimeError( f&quot;{vio_label} data stream does not exist! Please use a VRS that contains valid VIO data for this tutorial.&quot; ) # Query VIO_high_frequency data streams vio_high_freq_label = &quot;vio_high_frequency&quot; vio_high_freq_stream_id = vrs_data_provider.get_stream_id_from_label(vio_high_freq_label) if vio_high_freq_stream_id is None: raise RuntimeError( f&quot;{vio_high_freq_label} data stream does not exist! Please use a VRS that contains valid VIO high frequency data for this tutorial.&quot; )   ","version":"Next","tagName":"h2"},{"title":"On-Device VIO Data Stream‚Äã","type":1,"pageTitle":"Tutorial 5: On-Device VIO data streams","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/vio#on-device-vio-data-stream","content":" ","version":"Next","tagName":"h2"},{"title":"Data Type: FrontendOutput‚Äã","type":1,"pageTitle":"Tutorial 5: On-Device VIO data streams","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/vio#data-type-frontendoutput","content":" This a new data type introduced to store the results from the VIO system, containing the following fields:  Field Name\tDescriptionfrontend_session_uid\tSession identifier (resets on VIO restart) frame_id\tFrame set identifier capture_timestamp_ns\tCenter capture time in nanoseconds unix_timestamp_ns\tUnix timestamp in nanoseconds status\tVIO status (VALID/INVALID) pose_quality\tPose quality (GOOD/BAD/UNKNOWN) visual_tracking_quality\tVisual-only tracking quality online_calib\tOnline calibration estimates for SLAM cameras and IMUs gravity_in_odometry\tGravity vector in odometry frame transform_odometry_bodyimu\tBody IMU's pose in odometry reference frame transform_bodyimu_device\tTransform from body IMU to device frame linear_velocity_in_odometry\tLinear velocity in odometry frame in m/s angular_velocity_in_bodyimu\tAngular velocity in body IMU frame in rad/s  Here, body IMU is the IMU that is picked as the reference for motion tracking. For Aria-Gen2' on-device VIO algorithm, this is often imu-left.  Important Note: Always check status == VioStatus.VALID andpose_quality == TrackingQuality.GOOD for VIO data validity!  ","version":"Next","tagName":"h3"},{"title":"Data Access API‚Äã","type":1,"pageTitle":"Tutorial 5: On-Device VIO data streams","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/vio#data-access-api","content":" from projectaria_tools.core.sensor_data import VioStatus, TrackingQuality print(&quot;=== VIO Data Sample ===&quot;) # Find the first valid VIO data sample num_vio_samples = vrs_data_provider.get_num_data(vio_stream_id) first_valid_index = None for idx in range(num_vio_samples): vio_data = vrs_data_provider.get_vio_data_by_index(vio_stream_id, idx) if ( vio_data.status == VioStatus.VALID and vio_data.pose_quality == TrackingQuality.GOOD ): first_valid_index = idx break if first_valid_index is not None: print(&quot;=&quot; * 50) print(f&quot;First VALID VIO Data Sample (Index: {first_valid_index})&quot;) print(&quot;=&quot; * 50) # Session Information print(f&quot;Session UID: {vio_data.frontend_session_uid}&quot;) print(f&quot;Frame ID: {vio_data.frame_id}&quot;) # Timestamps print(f&quot;Capture Time: {vio_data.capture_timestamp_ns} ns&quot;) print(f&quot;Unix Time: {vio_data.unix_timestamp_ns} ns&quot;) # Quality Status print(f&quot;Status: {vio_data.status}&quot;) print(f&quot;Pose Quality: {vio_data.pose_quality}&quot;) print(f&quot;Visual Quality: {vio_data.visual_tracking_quality}&quot;) # Transforms print(f&quot;Transform Odometry ‚Üí Body IMU:\\n{vio_data.transform_odometry_bodyimu.to_matrix()}&quot;) print(f&quot;Transform Body IMU ‚Üí Device:\\n{vio_data.transform_bodyimu_device.to_matrix()}&quot;) # Motion print(f&quot;Linear Velocity: {vio_data.linear_velocity_in_odometry}&quot;) print(f&quot;Angular Velocity: {vio_data.angular_velocity_in_bodyimu}&quot;) print(f&quot;Gravity Vector: {vio_data.gravity_in_odometry}&quot;) else: print(&quot;‚ö†Ô∏è No valid VIO sample found&quot;)   ","version":"Next","tagName":"h3"},{"title":"On-Device VIO High Frequency Data Stream‚Äã","type":1,"pageTitle":"Tutorial 5: On-Device VIO data streams","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/vio#on-device-vio-high-frequency-data-stream","content":" VIO High Frequency results are generated directly from the on-device VIO results by performing IMU integration between VIO poses, hence provides a much higher data rate at approximately 800Hz.  ","version":"Next","tagName":"h2"},{"title":"Data Type: OpenLoopTrajectoryPose‚Äã","type":1,"pageTitle":"Tutorial 5: On-Device VIO data streams","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/vio#data-type-openlooptrajectorypose","content":" The VioHighFrequency stream re-uses the OpenLoopTrajectoryPose data structure defined in MPS.  Field Name\tDescriptiontracking_timestamp\tTimestamp in device time domain, in microseconds transform_odometry_device\tTransformation from device to odometry coordinate frame, represented as a SE3 instance. device_linear_velocity_odometry\tTranslational velocity of device in odometry frame, in m/s angular_velocity_device\tAngular velocity of device in device frame, in rad/s quality_score\tQuality of pose estimation (higher = better) gravity_odometry\tEarth gravity vector in odometry frame session_uid\tUnique identifier for VIO tracking session  Important Note: Due to the high frequency nature of this data (~800Hz), consider subsampling for visualization to maintain performance.  print(&quot;=== VIO High-Frequency Data Sample ===&quot;) # Find the first VIO high_frequency data sample with high quality value num_vio_high_freq_samples = vrs_data_provider.get_num_data(vio_high_freq_stream_id) first_valid_index = None for idx in range(num_vio_samples): vio_high_freq_data = vrs_data_provider.get_vio_high_freq_data_by_index(vio_high_freq_stream_id, idx) if ( vio_high_freq_data.quality_score &gt; 0.5 ): first_valid_index = idx break if first_valid_index is not None: print(&quot;=&quot; * 50) print(f&quot;First VIO High Freq Data Sample with good quality score (Index: {first_valid_index})&quot;) print(&quot;=&quot; * 50) # Timestamps, convert timedelta to nanoseconds capture_timestamp_ns = int(vio_high_freq_data.tracking_timestamp.total_seconds() * 1e9) # Session Information print(f&quot;Session UID: {vio_high_freq_data.session_uid}&quot;) # Timestamps print(f&quot;Tracking Time: {capture_timestamp_ns} ns&quot;) # Quality print(f&quot;Quality Score: {vio_high_freq_data.quality_score:.3f}&quot;) # Transform print(f&quot;Transform Odometry ‚Üí Device:\\n{vio_high_freq_data.transform_odometry_device.to_matrix()}&quot;) # Motion print(f&quot;Linear Velocity: {vio_high_freq_data.device_linear_velocity_odometry}&quot;) print(f&quot;Angular Velocity: {vio_high_freq_data.angular_velocity_device}&quot;) print(f&quot;Gravity Vector: {vio_high_freq_data.gravity_odometry}&quot;)   ","version":"Next","tagName":"h3"},{"title":"Visualizing On-Device VIO trajectory‚Äã","type":1,"pageTitle":"Tutorial 5: On-Device VIO data streams","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/vio#visualizing-on-device-vio-trajectory","content":" The following code snippets demonstrate how to visualize a VIO trajectory, along with glass frame + hand tracking results, in a 3D view.  def plot_single_hand_3d( hand_joints_in_device, hand_label ): &quot;&quot;&quot; A helper function to plot single hand data in 3D view &quot;&quot;&quot; marker_color = [255,64,0] if hand_label == &quot;left&quot; else [255, 255, 0] hand_skeleton_3d = create_hand_skeleton_from_landmarks(hand_joints_in_device) rr.log( f&quot;world/device/handtracking/{hand_label}/landmarks&quot;, rr.Points3D( positions=hand_joints_in_device, colors= marker_color, radii=5e-3, ), ) rr.log( f&quot;world/device/handtracking/{hand_label}/hand_skeleton&quot;, rr.LineStrips3D( hand_skeleton_3d, colors=[0, 255, 0], radii=3e-3, ), ) def plot_hand_pose_data_3d(hand_pose_data): &quot;&quot;&quot; A helper function to plot hand pose data in 3D world view &quot;&quot;&quot; # Clear the canvas (only if hand_tracking_label exists for this device version) rr.log( f&quot;world/device/handtracking&quot;, rr.Clear.recursive(), ) # Plot both hands if hand_pose_data.left_hand is not None: plot_single_hand_3d( hand_joints_in_device=hand_pose_data.left_hand.landmark_positions_device, hand_label=&quot;left&quot;, ) if hand_pose_data.right_hand is not None: plot_single_hand_3d( hand_joints_in_device=hand_pose_data.right_hand.landmark_positions_device, hand_label=&quot;right&quot;, )   ","version":"Next","tagName":"h2"},{"title":"3D Trajectory Visualization‚Äã","type":1,"pageTitle":"Tutorial 5: On-Device VIO data streams","url":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/vio#3d-trajectory-visualization","content":" import rerun as rr from projectaria_tools.core.sensor_data import SensorDataType, TimeDomain, TimeQueryOptions from projectaria_tools.utils.rerun_helpers import ( create_hand_skeleton_from_landmarks, AriaGlassesOutline, ToTransform3D ) print(&quot;\\n=== Visualizing on-device VIO trajectory + HandTracking in 3D view ===&quot;) rr.init(&quot;rerun_viz_vio_trajectory&quot;) device_calib = vrs_data_provider.get_device_calibration() handtracking_stream_id = vrs_data_provider.get_stream_id_from_label(&quot;handtracking&quot;) # Set up a data queue deliver_options = vrs_data_provider.get_default_deliver_queued_options() deliver_options.deactivate_stream_all() deliver_options.activate_stream(vio_stream_id) # Play for only 3 seconds total_length_ns = vrs_data_provider.get_last_time_ns_all_streams(TimeDomain.DEVICE_TIME) - vrs_data_provider.get_first_time_ns_all_streams(TimeDomain.DEVICE_TIME) skip_begin_ns = int(15 * 1e9) # Skip 15 seconds duration_ns = int(3 * 1e9) # 3 seconds skip_end_ns = max(total_length_ns - skip_begin_ns - duration_ns, 0) deliver_options.set_truncate_first_device_time_ns(skip_begin_ns) deliver_options.set_truncate_last_device_time_ns(skip_end_ns) # Plot VIO trajectory in 3D view. # Need to keep a cache to store already-loaded trajectory vio_traj_cached_full = [] for sensor_data in vrs_data_provider.deliver_queued_sensor_data(deliver_options): # Convert sensor data to VIO data vio_data = sensor_data.vio_data() # Check VIO data validity, only plot for valid data if ( vio_data.status != VioStatus.VALID or vio_data.pose_quality != TrackingQuality.GOOD): print(f&quot;VIO data is invalid for timestamp {sensor_data.get_time_ns(TimeDomain.DEVICE_TIME)}&quot;) continue # Set timestamp rr.set_time_nanos(&quot;device_time&quot;, vio_data.capture_timestamp_ns) # Set and plot the Device pose for the current timestamp, as a RGB axis T_World_Device = ( vio_data.transform_odometry_bodyimu @ vio_data.transform_bodyimu_device ) rr.log( &quot;world/device&quot;, ToTransform3D( T_World_Device, axis_length=0.05, ), ) # Also plot Aria glass outline for visualization aria_glasses_point_outline = AriaGlassesOutline( device_calib, use_cad_calib=True ) rr.log( &quot;world/device/glasses_outline&quot;, rr.LineStrips3D( aria_glasses_point_outline, colors=[200,200,200], radii=5e-4, ), ) # Plot gravity direction vector rr.log( &quot;world/vio_gravity&quot;, rr.Arrows3D( origins=[T_World_Device.translation()[0]], vectors=[ vio_data.gravity_in_odometry * 1e-2 ], # length converted from 9.8 meter -&gt; 10 cm colors=[101,67,33], radii=1.5e-3, ), static=False, ) # Plot VIO trajectory that are cached so far vio_traj_cached_full.append(T_World_Device.translation()[0]) rr.log( &quot;world/vio_trajectory&quot;, rr.LineStrips3D( vio_traj_cached_full, colors=[173, 216, 255], radii=1.5e-3, ), static=False, ) # For visualization purpose, also plot the hand tracking results interpolated_hand_pose = vrs_data_provider.get_interpolated_hand_pose_data(handtracking_stream_id, vio_data.capture_timestamp_ns, TimeDomain.DEVICE_TIME) if interpolated_hand_pose is not None: plot_hand_pose_data_3d(hand_pose_data = interpolated_hand_pose) rr.notebook_show()  ","version":"Next","tagName":"h3"},{"title":"Technical Specifications Reference","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/technical-specs","content":"","keywords":"","version":"Next"},{"title":"Specifications by Category‚Äã","type":1,"pageTitle":"Technical Specifications Reference","url":"/projectaria_tools/gen2/technical-specs#specifications-by-category","content":" Device - Hardware specs, profiles, calibration, CAD modelsVRS Format - File format, streams, toolsMPS - MPS output formats and data structuresClient SDK - API and CLI referenceCoordinate Systems - 3D/2D frames, transformations, projections    ","version":"Next","tagName":"h2"},{"title":"Quick Reference Tables‚Äã","type":1,"pageTitle":"Technical Specifications Reference","url":"/projectaria_tools/gen2/technical-specs#quick-reference-tables","content":" ","version":"Next","tagName":"h2"},{"title":"Recording Profiles‚Äã","type":1,"pageTitle":"Technical Specifications Reference","url":"/projectaria_tools/gen2/technical-specs#recording-profiles","content":" Profile\tRGB\tCV Cameras\tET\tHT\tVIO\tGPSprofile8\t10Hz, 2560√ó1920\t30Hz\t30Hz\t30Hz\t10Hz\t‚úì profile9\t5Hz, 2560√ó1920\t10Hz\t30Hz\t30Hz\t10Hz\t‚úó profile10\t30Hz, 2016√ó1512\t30Hz\t30Hz\t30Hz\t10Hz\t‚úì  Full profile specs ‚Üí  ","version":"Next","tagName":"h3"},{"title":"Sensor Specifications‚Äã","type":1,"pageTitle":"Technical Specifications Reference","url":"/projectaria_tools/gen2/technical-specs#sensor-specifications","content":" Sensor\tQuantity\tRate\tResolution/RangeCV Cameras\t4\t30 Hz\t512√ó512 RGB Camera\t1\t10-30 Hz\t2016√ó1512 - 2560√ó1920 ET Cameras\t2\t5 Hz\t200√ó200 IMU\t2\t800 Hz\t- Microphones\t7+1\t16 kHz\t- GPS\t1\t1 Hz\t- Magnetometer\t1\t100 Hz\t- Barometer\t1\t50 Hz\t- PPG\t1\t128 Hz\t- ALS\t1\t~9.4 Hz\t-  Full hardware specs ‚Üí  ","version":"Next","tagName":"h3"},{"title":"Common Stream IDs‚Äã","type":1,"pageTitle":"Technical Specifications Reference","url":"/projectaria_tools/gen2/technical-specs#common-stream-ids","content":" Stream Label\tStreamId\tTypergb\t214-1\tCamera slam-left\t1201-1\tCamera slam-right\t1201-2\tCamera imu-left\t1202-1\tIMU imu-right\t1202-2\tIMU eyetracking\t231-1\tMP Output handtracking\t232-1\tMP Output  Full stream ID mapper ‚Üí  ","version":"Next","tagName":"h3"},{"title":"Coordinate Conventions‚Äã","type":1,"pageTitle":"Technical Specifications Reference","url":"/projectaria_tools/gen2/technical-specs#coordinate-conventions","content":" Convention\tValueCoordinate system\tRight-handed Quaternion order\t(x, y, z, w) Rotation matrix\tRow-major Transform notation\tT_dest_source  Coordinate system details ‚Üí    ","version":"Next","tagName":"h3"},{"title":"Common Abbreviations‚Äã","type":1,"pageTitle":"Technical Specifications Reference","url":"/projectaria_tools/gen2/technical-specs#common-abbreviations","content":" Term\tMeaningVRS\tVideo Recording Storage MPS\tMachine Perception Services VIO\tVisual Inertial Odometry SLAM\tSimultaneous Localization and Mapping CV\tComputer Vision (cameras) ET\tEye Tracking HT\tHand Tracking IMU\tInertial Measurement Unit PPG\tPhotoplethysmography ALS\tAmbient Light Sensor CPF\tCentral Pupil Frame ","version":"Next","tagName":"h2"},{"title":"Python Visualization Tools","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/pythonviz","content":"","keywords":"","version":"Next"},{"title":"aria_rerun_viewer‚Äã","type":1,"pageTitle":"Python Visualization Tools","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/pythonviz#aria_rerun_viewer","content":" The aria_rerun_viewer is a Python tool that visualizes Aria VRS (Video Recording and Sensor) files using the Rerun visualization framework. It supports both Aria Gen1 and Gen2 devices and can display multiple sensor streams including cameras, IMU, audio, eye gaze, hand tracking, and more.     ","version":"Next","tagName":"h2"},{"title":"Basic Usage‚Äã","type":1,"pageTitle":"Python Visualization Tools","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/pythonviz#basic-usage","content":" aria_rerun_viewer --vrs path/to/your/file.vrs   ","version":"Next","tagName":"h3"},{"title":"Command Line Options‚Äã","type":1,"pageTitle":"Python Visualization Tools","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/pythonviz#command-line-options","content":" Parameter\tType\tRequired\tDescription--vrs\tstring\tYes\tPath to the VRS file you want to visualize --skip-begin-sec\tfloat\tNo\tNumber of seconds to skip at the beginning of the VRS file --skip-end-sec\tfloat\tNo\tNumber of seconds to skip at the end of the VRS file --enabled-streams\tstring list\tNo\tEnable specific streams by their labels (space-separated). Available streams include: camera-rgb, slam-front-left, slam-front-right, slam-side-left, slam-side-right, camera-et-left, camera-et-right, imu-left, imu-right, mic, baro0, mag0, gps, handtracking, eyegaze, vio, vio_high_frequency. Default: all available streams --subsample-rates\tstring list\tNo\tSpecify subsampling rates for streams in the format stream=rate (space-separated pairs). Example: camera-rgb=2 eyegaze=5. Default: vio_high_frequency=10  ","version":"Next","tagName":"h3"},{"title":"Examples‚Äã","type":1,"pageTitle":"Python Visualization Tools","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/pythonviz#examples","content":" Basic Visualization‚Äã  aria_rerun_viewer --vrs recording.vrs   Visualize Only RGB Camera and Eye Gaze‚Äã  aria_rerun_viewer \\ --vrs recording.vrs \\ --enabled-streams camera-rgb eyegaze   Skip Beginning and End of Recording‚Äã  aria_rerun_viewer \\ --vrs recording.vrs \\ --skip-begin-sec 10 \\ --skip-end-sec 5   Apply Custom Subsampling‚Äã  aria_rerun_viewer \\ --vrs recording.vrs \\ --subsample-rates camera-rgb=3 vio_high_frequency=20   Complex Example with Multiple Options‚Äã  aria_rerun_viewer \\ --vrs recording.vrs \\ --enabled-streams camera-rgb slam-front-left slam-front-right eyegaze handtracking \\ --subsample-rates camera-rgb=2 handtracking=5 \\ --skip-begin-sec 30 \\ --skip-end-sec 10   ","version":"Next","tagName":"h3"},{"title":"What You'll See‚Äã","type":1,"pageTitle":"Python Visualization Tools","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/pythonviz#what-youll-see","content":" The viewer displays data in an interactive 3D environment using Rerun:  RGB &amp; SLAM Camera stream: RGB and SLAM camera images, with overlaid eye gaze and hand tracking results.1D Sensor Data: IMU, audio, magnetometer, and barometer data plotted as 1D time series.3D World View: 3D visualization of the VIO trajectory, eye gaze, and hand tracking results.Device calibration: 3D representation of the sensor locations on Aria device.  ","version":"Next","tagName":"h3"},{"title":"Important Notes‚Äã","type":1,"pageTitle":"Python Visualization Tools","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/pythonviz#important-notes","content":" VIO High Frequency Subsampling: The vio_high_frequency stream runs at 800Hz by default, which is automatically subsampled to 80Hz (subsample rate of 10) to improve visualization performance. You can adjust this using --subsample-rates vio_high_frequency=&lt;rate&gt;. Image Decoding Performance: Image decoding is currently performed on CPU on Linux, so plotting speed might be slow depending on CPU load. To see smooth visualization, wait until Rerun caches some data then click play again, or use subsample options like --subsample-rates camera-rgb=2 to reduce the frame rate.  ","version":"Next","tagName":"h3"},{"title":"viewer_mps‚Äã","type":1,"pageTitle":"Python Visualization Tools","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/pythonviz#viewer_mps","content":" The viewer_mps tool visualizes Aria data along with Machine Perception Services (MPS) outputs like SLAM trajectories, point clouds, eye gaze, and hand tracking results.     ","version":"Next","tagName":"h2"},{"title":"Basic Usage‚Äã","type":1,"pageTitle":"Python Visualization Tools","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/pythonviz#basic-usage-1","content":" viewer_mps --vrs path/to/recording.vrs   ","version":"Next","tagName":"h3"},{"title":"Command Line Options‚Äã","type":1,"pageTitle":"Python Visualization Tools","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/pythonviz#command-line-options-1","content":" Input Files‚Äã  --vrs: Path to VRS file--trajectory: Path(s) to MPS trajectory files (supports multiple files)--points: Path(s) to MPS global point cloud files (supports multiple files)--eyegaze: Path to MPS eye gaze file--hands: Path to MPS wrist and palm poses file--hands_all: Path to MPS full hand tracking results file--mps_folder: Path to MPS folder (overrides default &lt;vrs_file&gt;/mps location)  Visualization Options‚Äã  --no_rectify_image: Show raw fisheye RGB images without undistortion--web: Run viewer in web browser instead of desktop app  ","version":"Next","tagName":"h3"},{"title":"Examples‚Äã","type":1,"pageTitle":"Python Visualization Tools","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/pythonviz#examples-1","content":" Auto-detect MPS Data‚Äã  # Automatically finds MPS data in &lt;vrs_file&gt;/mps folder viewer_mps --vrs recording.vrs   Specify Individual MPS Files‚Äã  viewer_mps \\ --vrs recording.vrs \\ --trajectory trajectory/closed_loop_trajectory.csv \\ --points global_points/global_points.csv.gz \\ --eyegaze eye_gaze/general_eye_gaze.csv   Web Browser Mode‚Äã  viewer_mps \\ --vrs recording.vrs \\ --web   Multiple Trajectories and Point Clouds‚Äã  viewer_mps \\ --trajectory trajectory1.csv trajectory2.csv \\ --points points1.csv points2.csv   ","version":"Next","tagName":"h3"},{"title":"What You'll See‚Äã","type":1,"pageTitle":"Python Visualization Tools","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/pythonviz#what-youll-see-1","content":" The MPS viewer provides:  3D Scene: SLAM trajectory, point clouds, and device poses in 3D spaceCamera Views: RGB camera feeds with overlaid eye gaze and hand tracking projectionsHand Tracking: 3D hand landmarks, skeleton connections, and wrist/palm posesEye Gaze: 3D gaze vectors and their projections onto camera images  This tool is particularly useful for validating MPS processing results and understanding the spatial relationships between different data modalities. ","version":"Next","tagName":"h3"},{"title":"VRS to MP4 Converter","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/vrstomp4","content":"","keywords":"","version":"Next"},{"title":"Basic Usage‚Äã","type":1,"pageTitle":"VRS to MP4 Converter","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/vrstomp4#basic-usage","content":" vrs_to_mp4 --vrs input.vrs --output_video output.mp4   ","version":"Next","tagName":"h2"},{"title":"Command Line Options‚Äã","type":1,"pageTitle":"VRS to MP4 Converter","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/vrstomp4#command-line-options","content":" Parameter\tType\tRequired\tDescription--vrs\tstring\tYes\tPath to the VRS file to be converted to a video --output_video\tstring\tYes\tPath to the MP4 video file you want to create --stream_id\tstring\tNo\tStream ID to convert to video. Options: 214-1 (RGB), 1201-1, 1201-2, 1201-3, 1201-4 (SLAM cameras). Default: 214-1 --log_folder\tstring\tNo\tFolder to store logs: mp4_to_vrs_time_map.csv, vrs_to_mp4_log.json, audio files --downsample\tinteger\tNo\tDownsampling factor for VRS images (must be ‚â•1). Default: 1 --audio_channels\tinteger list\tNo\tAudio channel indices to include in the MP4. Default: [0, 2] (the 2 mics on the lower frame of the glasses)  ","version":"Next","tagName":"h2"},{"title":"Stream IDs Explained‚Äã","type":1,"pageTitle":"VRS to MP4 Converter","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/vrstomp4#stream-ids-explained","content":" 214-1: RGB camera stream (default choice)1201-1, 1201-2, 1201-3, 1201-4: SLAM camera streams (grayscale)  The RGB stream (214-1) is typically what you want for creating viewable videos, while SLAM streams are useful for computer vision applications.  ","version":"Next","tagName":"h3"},{"title":"Examples‚Äã","type":1,"pageTitle":"VRS to MP4 Converter","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/vrstomp4#examples","content":" ","version":"Next","tagName":"h2"},{"title":"Basic RGB Video Conversion‚Äã","type":1,"pageTitle":"VRS to MP4 Converter","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/vrstomp4#basic-rgb-video-conversion","content":" vrs_to_mp4 --vrs recording.vrs --output_video output.mp4   ","version":"Next","tagName":"h3"},{"title":"Select Specific Audio Channels‚Äã","type":1,"pageTitle":"VRS to MP4 Converter","url":"/projectaria_tools/gen2/research-tools/projectariatools/tools/vrstomp4#select-specific-audio-channels","content":" vrs_to_mp4 \\ --vrs recording.vrs \\ --output_video custom_audio.mp4 \\ --audio_channels 0 1 3  ","version":"Next","tagName":"h3"},{"title":"3D Coordinate Frame Conventions for Project Aria Glasses","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/technical-specs/coordinate/3d-coor","content":"","keywords":"","version":"Next"},{"title":"SE(3) Lie groups‚Äã","type":1,"pageTitle":"3D Coordinate Frame Conventions for Project Aria Glasses","url":"/projectaria_tools/gen2/technical-specs/coordinate/3d-coor#se3-lie-groups","content":" Extrinsics in calibration refer to the 6-DoF pose among the sensors. These 6-DoF poses are represented by SE(3) Lie group. The quaternion part of SE(3) uses Hamilton convention following the Eigen library, in which the exact formula to convert a quaternion to a rotation matrix of the SE(3) can be found in the Eigen code repository.  We use the SE3d class in the Sophus Library to represent SE(3) Lie groups, and provide a minimal pybind for the class.  ","version":"Next","tagName":"h2"},{"title":"A note on sensor naming and motivation‚Äã","type":1,"pageTitle":"3D Coordinate Frame Conventions for Project Aria Glasses","url":"/projectaria_tools/gen2/technical-specs/coordinate/3d-coor#a-note-on-sensor-naming-and-motivation","content":" T_sensor1_sensor2 represents a relative SE(3) transformation from sensor2 frame to sensor1 frame. An easy mnemonic is the chaining principle is: T_sensor1_sensor2 * T_sensor2_sensor3 * p_sensor3 = p_sensor1 (where p_sensor is a 3D point measured from sensor).  ","version":"Next","tagName":"h3"},{"title":"Code‚Äã","type":1,"pageTitle":"3D Coordinate Frame Conventions for Project Aria Glasses","url":"/projectaria_tools/gen2/technical-specs/coordinate/3d-coor#code","content":" PythonC++ transform_a_b represents a SE(3) rigid transformation from b coordinate frame to a coordinate frame. p_a represents an R^3 point (or vector) in the coordinate system of a. Easy mnemonics of the chaining principle (a, b, c are coordinate frames): transform_a_c = transform_a_b @ transform_b_c; p_a = transform_a_b @ p_b If you want to get quaternion from the SE3d, please notice the order is consistent to numpy quaternion_a_b = transform_a_b.to_quat() # order is w, x, y, z   3D Coordinate frame conventions  Every sensor on Aria glasses has their own local coordinate system. We represent the 6DoF pose of each sensor as the relative pose (rotation and translation) with regard to the ‚ÄúDevice frame&quot;. The device frame is by-default the local frame of the left front-facing SLAM camera (slam-front-left).  ","version":"Next","tagName":"h3"},{"title":"Camera coordinate system convention‚Äã","type":1,"pageTitle":"3D Coordinate Frame Conventions for Project Aria Glasses","url":"/projectaria_tools/gen2/technical-specs/coordinate/3d-coor#camera-coordinate-system-convention","content":" A camera's local frame has its origin at the camera's optical center. Coarsely, when the camera is placed up-right, the camera coordinate frame's axes points to left, up and forward.  More rigorously, we define a camera's local frame based on the optical axis and the entrance pupil of its lens. Both are uniquely defined for each camera according to the camera's lens prescription. The origin of a camera's local frame is at center of the camera's entrance pupil. The frame's Z axis is aligned with the optical axis. The camera's X axis are aligned with the projection of the image plane's X axis on the entrance pupil plane. The cross-product of the X and Z axis defines the system's Y axis.    Figure 1. Camera 3D Coordinate System  ","version":"Next","tagName":"h2"},{"title":"Non-visual sensor coordinate system‚Äã","type":1,"pageTitle":"3D Coordinate Frame Conventions for Project Aria Glasses","url":"/projectaria_tools/gen2/technical-specs/coordinate/3d-coor#non-visual-sensor-coordinate-system","content":" We choose the IMU coordinate systems to have their origins at the position of the accelerometer, oriented along the direction of the accelerometer sensitive axis, eventually orthogonalized to compensate for sensor orthogonalities error. We use a similar arrangement for the magnetometer.    Figure 2. IMU 3D Coordinate System  ","version":"Next","tagName":"h2"},{"title":"The nominal Central Pupil Frame (CPF)‚Äã","type":1,"pageTitle":"3D Coordinate Frame Conventions for Project Aria Glasses","url":"/projectaria_tools/gen2/technical-specs/coordinate/3d-coor#the-nominal-central-pupil-frame-cpf","content":" The CPF frame is a commonly used coordinate frame in eye tracking or AR / VR applications. Conventionally, the CPF frame is placed at the midpoint between the eye boxes of the left and right eye, with X-axis points left, Y-axis points up, and the Z-axis points forward, from the person's perspective. In Aria glasses, the CPF frame is only a rough estimation obtained from its CAD design, therefore using CPF-frame-based poses are generally not recommend. Please refer to this page to learn the details.    Figure 3. IMU 3D coordinate frame ","version":"Next","tagName":"h2"},{"title":"Client SDK Python API Reference","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/technical-specs/client-sdk/api","content":"","keywords":"","version":"Next"},{"title":"Core Classes‚Äã","type":1,"pageTitle":"Client SDK Python API Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/api#core-classes","content":" ","version":"Next","tagName":"h2"},{"title":"DeviceClient‚Äã","type":1,"pageTitle":"Client SDK Python API Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/api#deviceclient","content":" The DeviceClient class manages connections to Aria devices and handles device discovery.  Constructor‚Äã  DeviceClient()   Creates a new DeviceClient instance.  Methods‚Äã  authenticate()‚Äã  Authenticate with the device using credentials.  Returns: Certificate hash pushed to the device (str)  Raises: RuntimeError if authentication fails  connect()‚Äã  Establish a connection to an Aria device.  Returns: Device object representing the connected device  Raises: RuntimeError if connection fails  is_connected(device: Device) -&gt; bool‚Äã  Check if a device is currently connected.  Parameters:  device (Device): The device to check connection status for  Returns: bool - True if connected, False otherwise  usb_network_devices()‚Äã  Get a list of devices available via USB network connection.  Returns: List of available USB network devices  Raises: RuntimeError if unable to query devices  active_connections() -&gt; List[Device]‚Äã  Get a list of all currently active device connections.  Returns: List of Device objects  disconnect(device: Device) -&gt; None‚Äã  Disconnect from a specific device.  Parameters:  device (Device): The device to disconnect from  disconnect_all() -&gt; None‚Äã  Disconnect from all connected devices.  set_client_config(config: DeviceClientConfig) -&gt; None‚Äã  Configure the device client settings.  Parameters:  config (DeviceClientConfig): Configuration object for the client    ","version":"Next","tagName":"h3"},{"title":"Device‚Äã","type":1,"pageTitle":"Client SDK Python API Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/api#device","content":" The Device class represents a connected Aria device and provides methods to control it.  Methods‚Äã  connection_id() -&gt; str‚Äã  Get the unique connection identifier for this device.  Returns: String connection ID  status()‚Äã  Get current device status.  Returns: Device status object  Raises: RuntimeError if unable to retrieve device status  serial()‚Äã  Get the device serial number.  Returns: String serial number  Raises: RuntimeError if unable to retrieve serial number  Streaming Methods‚Äã  start_streaming()‚Äã  Start streaming data from the device.  Returns: None  Raises: RuntimeError if unable to start streaming  stop_streaming()‚Äã  Stop streaming data from the device.  Returns: None  Raises: RuntimeError if unable to stop streaming  set_streaming_config(streaming_config: HttpStreamingConfig) -&gt; None‚Äã  Configure streaming settings.  Parameters:  streaming_config (HttpStreamingConfig): Streaming configuration object  install_streaming_certs()‚Äã  Install default streaming certificates on the device.  Returns: None  Raises: RuntimeError if certificate installation fails  install_streaming_certs(streaming_certificates: StreamingCertsPem)‚Äã  Install custom streaming certificates on the device.  Parameters:  streaming_certificates (StreamingCertsPem): Custom certificates in PEM format  Returns: None  Raises: RuntimeError if certificate installation fails  uninstall_streaming_certs()‚Äã  Remove streaming certificates from the device.  Returns: None  Raises: RuntimeError if certificate removal fails  Recording Methods‚Äã  set_recording_config(recording_config: RecordingConfig) -&gt; None‚Äã  Configure recording settings.  Parameters:  recording_config (RecordingConfig): Recording configuration object  start_recording()‚Äã  Start recording on the device.  Returns: UUID of the recording (str)  Raises: RuntimeError if unable to start recording  stop_recording()‚Äã  Stop the current recording.  Returns: None  Raises: RuntimeError if unable to stop recording  list_recordings()‚Äã  Print a list of all recordings on the device.  Returns: None  Raises: RuntimeError if unable to list recordings  recording_info(uuid: str)‚Äã  Print detailed information about a specific recording.  Parameters:  uuid (str): Recording UUID  Returns: None  Raises: RuntimeError if recording not found or unable to retrieve info  delete_recording(uuid: str)‚Äã  Delete a specific recording from the device.  Parameters:  uuid (str): Recording UUID to delete  Returns: None  Raises: RuntimeError if unable to delete recording  download_recording(uuid: str, output_path: str = &quot;&quot;)‚Äã  Download a specific recording from the device.  Parameters:  uuid (str): Recording UUID to downloadoutput_path (str, optional): Local path to save the recording. Defaults to current directory.  Returns: None  Raises: RuntimeError if download fails  download_all_recordings(output_path: str = &quot;&quot;)‚Äã  Download all recordings from the device.  Parameters:  output_path (str, optional): Local directory to save recordings. Defaults to current directory.  Returns: None  Raises: RuntimeError if download fails  delete_all_recordings()‚Äã  Delete all recordings from the device.  Returns: None  Raises: RuntimeError if unable to delete recordings  Text-to-Speech Methods‚Äã  render_tts(text: str)‚Äã  Render text-to-speech on the device.  Parameters:  text (str): Text to convert to speech  Returns: None  Raises: RuntimeError if TTS rendering fails  stop_tts()‚Äã  Stop current text-to-speech playback.  Returns: None  Raises: RuntimeError if unable to stop TTS    ","version":"Next","tagName":"h3"},{"title":"StreamDataInterface‚Äã","type":1,"pageTitle":"Client SDK Python API Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/api#streamdatainterface","content":" The StreamDataInterface class handles receiving and processing streaming data from the device.  Constructor‚Äã  StreamDataInterface(enable_image_decoding: bool, enable_raw_stream: bool)   Parameters:  enable_image_decoding (bool): Enable automatic image decodingenable_raw_stream (bool): Enable raw stream data access  Methods‚Äã  record_to_vrs(vrs_path: str) -&gt; None‚Äã  Save streaming data to a VRS file.  Parameters:  vrs_path (str): Path where VRS file from received streaming data will be saved  Callback Registration Methods‚Äã  The following methods register callbacks for different data types:  register_imu_callback(callback: Callable)‚Äã  Register callback for IMU (Inertial Measurement Unit) data.  Parameters:  callback (Callable): Function callback(motion_data: projectaria_tools.core.sensor_data.MotionData, sensor_label: str) called for each IMU sample  register_imu_batch_callback(callback: Callable)‚Äã  Register callback for batched IMU data.  Parameters:  callback (Callable): Function callback(motion_data_batch: List[projectaria_tools.core.sensor_data.MotionData], sensor_label: str) called for IMU batches  register_eye_gaze_callback(callback: Callable)‚Äã  Register callback for eye gaze tracking data.  Parameters:  callback (Callable): Function callback(eye_gaze_data: projectaria_tools.core.mps.EyeGaze) called for eye gaze samples  register_hand_pose_callback(callback: Callable)‚Äã  Register callback for hand pose tracking data.  Parameters:  callback (Callable): Function callback(hand_pose_data: projectaria_tools.core.mps.hand_tracking) called for hand pose samples  register_audio_callback(callback: Callable)‚Äã  Register callback for audio data.  Parameters:  callback (Callable): Function callback(audio_data: projectaria_tools.core.sensor_data.AudioData, audio_record: projectaria_tools.core.sensor_data.AudioDataRecord, num_channels: int) called for audio samples  register_rgb_callback(callback: Callable)‚Äã  Register callback for RGB camera data.  Parameters:  callback (Callable): Function callback(image_data: projectaria_tools.core.sensor_data.ImageData, image_record: projectaria_tools.core.sensor_data.ImageDataRecord) called for RGB images  register_slam_callback(callback: Callable)‚Äã  Register callback for SLAM camera data.  Parameters:  callback (Callable): Function callback(image_data: projectaria_tools.core.sensor_data.ImageData, image_record: projectaria_tools.core.sensor_data.ImageDataRecord) called for SLAM images  register_et_callback(callback: Callable)‚Äã  Register callback for eye tracking camera data.  Parameters:  callback (Callable): Function callback(image_data: projectaria_tools.core.sensor_data.ImageData, image_record: projectaria_tools.core.sensor_data.ImageDataRecord) called for ET images  register_barometer_callback(callback: Callable)‚Äã  Register callback for barometer data.  Parameters:  callback (Callable): Function callback(barometer_data: projectaria_tools.core.sensor_data.BarometerData) called for barometer samples  register_magnetometer_callback(callback: Callable)‚Äã  Register callback for magnetometer data.  Parameters:  callback (Callable): Function callback(motion_data: projectaria_tools.core.sensor_data.MotionData, sensor_label: str) called for magnetometer samples  register_vio_callback(callback: Callable)‚Äã  Register callback for Visual-Inertial Odometry (VIO) data.  Parameters:  callback (Callable): Function callback(frontend_output: projectaria_tools.core.sensor_data.FrontendOutput) called for VIO updates  register_vio_high_frequency_callback(callback: Callable)‚Äã  Register callback for high-frequency VIO data.  Parameters:  callback (Callable): Function callback(vio_data: projectaria_tools.core.mps.OpenLoopTrajectoryPose) called for high-frequency VIO updates  register_device_calib_callback(callback: Callable)‚Äã  Register callback for device calibration updates.  Parameters:  callback (Callable): Function callback(device_calibration: projectaria_tools.core.calibration.DeviceCalibration) called when calibration is received  Queue Size Configuration Methods‚Äã  The following methods configure queue sizes for different data streams:  set_rgb_queue_size(size: int) -&gt; None‚Äã  Set the queue size for RGB image data.  Parameters:  size (int): Queue size  set_slam_queue_size(size: int) -&gt; None‚Äã  Set the queue size for SLAM image data.  Parameters:  size (int): Queue size  set_et_queue_size(size: int) -&gt; None‚Äã  Set the queue size for eye tracking image data.  Parameters:  size (int): Queue size  set_imu_queue_size(size: int) -&gt; None‚Äã  Set the queue size for IMU data.  Parameters:  size (int): Queue size  set_imu_batch_queue_size(size: int) -&gt; None‚Äã  Set the queue size for batched IMU data.  Parameters:  size (int): Queue size  set_vio_high_freq_queue_size(size: int) -&gt; None‚Äã  Set the queue size for high-frequency VIO data.  Parameters:  size (int): Queue size  set_vio_high_freq_batch_queue_size(size: int) -&gt; None‚Äã  Set the queue size for batched high-frequency VIO data.  Parameters:  size (int): Queue size  set_eye_gaze_queue_size(size: int) -&gt; None‚Äã  Set the queue size for eye gaze data.  Parameters:  size (int): Queue size  set_hand_pose_queue_size(size: int) -&gt; None‚Äã  Set the queue size for hand pose data.  Parameters:  size (int): Queue size  set_vio_queue_size(size: int) -&gt; None‚Äã  Set the queue size for VIO data.  Parameters:  size (int): Queue size  Queue Size Getter Methods‚Äã  The following methods retrieve queue sizes for different data streams:  get_rgb_queue_size() -&gt; int‚Äã  Get the queue size for RGB image data.  Returns: Queue size (int)  get_slam_queue_size() -&gt; int‚Äã  Get the queue size for SLAM image data.  Returns: Queue size (int)  get_imu_queue_size() -&gt; int‚Äã  Get the queue size for IMU data.  Returns: Queue size (int)  get_imu_batch_queue_size() -&gt; int‚Äã  Get the queue size for batched IMU data.  Returns: Queue size (int)  get_et_queue_size() -&gt; int‚Äã  Get the queue size for eye tracking image data.  Returns: Queue size (int)  get_vio_queue_size() -&gt; int‚Äã  Get the queue size for VIO data.  Returns: Queue size (int)  get_vio_high_freq_queue_size() -&gt; int‚Äã  Get the queue size for high-frequency VIO data.  Returns: Queue size (int)  get_vio_high_freq_batch_queue_size() -&gt; int‚Äã  Get the queue size for batched high-frequency VIO data.  Returns: Queue size (int)  get_eye_gaze_queue_size() -&gt; int‚Äã  Get the queue size for eye gaze data.  Returns: Queue size (int)  get_hand_pose_queue_size() -&gt; int‚Äã  Get the queue size for hand pose data.  Returns: Queue size (int)    ","version":"Next","tagName":"h3"},{"title":"Configuration Classes‚Äã","type":1,"pageTitle":"Client SDK Python API Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/api#configuration-classes","content":" ","version":"Next","tagName":"h2"},{"title":"HttpStreamingConfig‚Äã","type":1,"pageTitle":"Client SDK Python API Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/api#httpstreamingconfig","content":" Configuration for HTTP-based streaming.  Attributes:  profile_name (str): Name of the streaming profile to usestreaming_cert_name (str): Name of the streaming certificatestreaming_interface (StreamingInterface): Network interface to use for streamingsecurity_options (StreamingSecurityOptions): Security settings for streamingadvanced_config (HttpStreamerConfig): Advanced streaming configurationkeep_streaming_on_disconnection (bool): Continue streaming if disconnected  ","version":"Next","tagName":"h3"},{"title":"RecordingConfig‚Äã","type":1,"pageTitle":"Client SDK Python API Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/api#recordingconfig","content":" Configuration for on-device recording.  Attributes:  profile_name (str): Name of the recording profile to usecustom_profile (str): Custom recording profile in JSON formatrecording_name (str): Name for the recording  ","version":"Next","tagName":"h3"},{"title":"StreamingCertsPem‚Äã","type":1,"pageTitle":"Client SDK Python API Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/api#streamingcertspem","content":" Streaming certificates in PEM format.  Attributes:  root_ca_cert (str): Root CA certificate in PEM formatpublisher_cert (str): Publisher certificate in PEM formatpublisher_key (str): Publisher private key in PEM formatkey_password (str): Password for the private keycert_name (str): Name for this certificate set  ","version":"Next","tagName":"h3"},{"title":"HttpStreamerConfig‚Äã","type":1,"pageTitle":"Client SDK Python API Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/api#httpstreamerconfig","content":" Advanced HTTP streamer configuration.  Attributes:  endpoint (Endpoint): Streaming endpoint configuration  ","version":"Next","tagName":"h3"},{"title":"Endpoint‚Äã","type":1,"pageTitle":"Client SDK Python API Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/api#endpoint","content":" HTTP endpoint configuration.  Attributes:  url (str): Endpoint URLverify_server_certificates (bool): Whether to verify server certificatesauth (SslAuthentication): SSL authentication credentials    ","version":"Next","tagName":"h3"},{"title":"HTTP Server Classes‚Äã","type":1,"pageTitle":"Client SDK Python API Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/api#http-server-classes","content":" ","version":"Next","tagName":"h2"},{"title":"AriaGen2HttpServer‚Äã","type":1,"pageTitle":"Client SDK Python API Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/api#ariagen2httpserver","content":" HTTP server for receiving streaming data from devices.  Constructor‚Äã  AriaGen2HttpServer(config: HttpServerConfig, handler_factory: AriaGen2HandlerFactory)   Parameters:  config (HttpServerConfig): Server configurationhandler_factory (AriaGen2HandlerFactory): Factory for creating stream handlers  Methods‚Äã  stop() -&gt; None‚Äã  Stop the HTTP server.  join() -&gt; None‚Äã  Wait for the server to finish.  ","version":"Next","tagName":"h3"},{"title":"HttpServerConfig‚Äã","type":1,"pageTitle":"Client SDK Python API Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/api#httpserverconfig","content":" Configuration for the HTTP server.  Attributes:  address (str): Server bind addressport (int): Server portuse_ssl (bool): Enable SSL/TLSmdns_hostname (str): mDNS hostname for discoverycertificate (Certificate): Server SSL certificateca_root (str): CA root certificatethreads (int): Number of server threadsidle_timeout_sec (int): Idle timeout in seconds  ","version":"Next","tagName":"h3"},{"title":"Certificate‚Äã","type":1,"pageTitle":"Client SDK Python API Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/api#certificate","content":" Server SSL certificate configuration.  Attributes:  cert (str): Certificate in PEM formatkey (str): Private key in PEM formatkey_password (str): Password for the private key  ","version":"Next","tagName":"h3"},{"title":"AriaGen2HandlerFactory‚Äã","type":1,"pageTitle":"Client SDK Python API Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/api#ariagen2handlerfactory","content":" Factory for creating stream data handlers.  Methods‚Äã  create_factory_handler(handler_object: StreamDataInterface) -&gt; AriaGen2HandlerFactory‚Äã  Create a handler factory from a StreamDataInterface object.  Parameters:  handler_object (StreamDataInterface): Stream data interface instance  Returns: AriaGen2HandlerFactory instance    ","version":"Next","tagName":"h3"},{"title":"Enumerations‚Äã","type":1,"pageTitle":"Client SDK Python API Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/api#enumerations","content":" ","version":"Next","tagName":"h2"},{"title":"StreamingInterface‚Äã","type":1,"pageTitle":"Client SDK Python API Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/api#streaminginterface","content":" Available network interfaces for streaming.  Values:  WIFI_STA - WiFi Station modeUSB_RNDIS - USB RNDIS (Windows)USB_NCM - USB NCM (Linux/Mac)WIFI_SAP - WiFi Soft Access Point mode  ","version":"Next","tagName":"h3"},{"title":"RecordingType‚Äã","type":1,"pageTitle":"Client SDK Python API Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/api#recordingtype","content":" Types of recordings.  Values:  RECORDING_TYPE_PROTOTYPE - Prototype recording    ","version":"Next","tagName":"h3"},{"title":"Message Types‚Äã","type":1,"pageTitle":"Client SDK Python API Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/api#message-types","content":" ","version":"Next","tagName":"h2"},{"title":"SharedMessage‚Äã","type":1,"pageTitle":"Client SDK Python API Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/api#sharedmessage","content":" Low-level message data structure.  Attributes:  id (int, read-only): Message IDpayload (IBuffer, read-only): Message payload  Methods:  size() -&gt; int: Get message size in bytes  ","version":"Next","tagName":"h3"},{"title":"IBuffer‚Äã","type":1,"pageTitle":"Client SDK Python API Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/api#ibuffer","content":" Buffer interface for binary data.  Methods:  data() -&gt; bytes: Get buffer data as bytesas_memoryview() -&gt; memoryview: Get buffer as Python memoryview ","version":"Next","tagName":"h3"},{"title":"Aria Gen2 CLI Reference","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/technical-specs/client-sdk/cli","content":"","keywords":"","version":"Next"},{"title":"Global Options‚Äã","type":1,"pageTitle":"Aria Gen2 CLI Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/cli#global-options","content":" These options apply to all commands:  ","version":"Next","tagName":"h2"},{"title":"--serial <serial_number>‚Äã","type":1,"pageTitle":"Aria Gen2 CLI Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/cli#--serial-serial_number","content":" Connect to a specific device by serial number. Mandatory when multiple devices are connected via USB.  aria_gen2 --serial IM0XXXXXXX device status     ","version":"Next","tagName":"h3"},{"title":"Authentication Commands‚Äã","type":1,"pageTitle":"Aria Gen2 CLI Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/cli#authentication-commands","content":" Manage device authentication and pairing.  ","version":"Next","tagName":"h2"},{"title":"auth pair‚Äã","type":1,"pageTitle":"Aria Gen2 CLI Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/cli#auth-pair","content":" Pair the SDK on this host with a device connected over USB.  aria_gen2 auth pair   ","version":"Next","tagName":"h3"},{"title":"auth check‚Äã","type":1,"pageTitle":"Aria Gen2 CLI Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/cli#auth-check","content":" Check if the SDK is authenticated with the device.  aria_gen2 auth check   ","version":"Next","tagName":"h3"},{"title":"auth unpair‚Äã","type":1,"pageTitle":"Aria Gen2 CLI Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/cli#auth-unpair","content":" Unpair the SDK from the device.  aria_gen2 auth unpair   ","version":"Next","tagName":"h3"},{"title":"auth remove-certs‚Äã","type":1,"pageTitle":"Aria Gen2 CLI Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/cli#auth-remove-certs","content":" Remove locally stored SDK authentication certificates (does not require a connected device).  aria_gen2 auth remove-certs     ","version":"Next","tagName":"h3"},{"title":"Device Commands‚Äã","type":1,"pageTitle":"Aria Gen2 CLI Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/cli#device-commands","content":" Control and query device information.  ","version":"Next","tagName":"h2"},{"title":"device list‚Äã","type":1,"pageTitle":"Aria Gen2 CLI Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/cli#device-list","content":" List all connected USB devices.  aria_gen2 device list   ","version":"Next","tagName":"h3"},{"title":"device status‚Äã","type":1,"pageTitle":"Aria Gen2 CLI Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/cli#device-status","content":" Get current device status.  aria_gen2 device status   ","version":"Next","tagName":"h3"},{"title":"Device Profile Commands‚Äã","type":1,"pageTitle":"Aria Gen2 CLI Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/cli#device-profile-commands","content":" device profile list‚Äã  List available device profiles.  aria_gen2 device profile list   ","version":"Next","tagName":"h3"},{"title":"Text-to-Speech Commands‚Äã","type":1,"pageTitle":"Aria Gen2 CLI Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/cli#text-to-speech-commands","content":" device tts start --tts-text &lt;text&gt;‚Äã  Start text-to-speech rendering on the device.  Options:  --tts-text (required): Text to convert to speech  aria_gen2 device tts start --tts-text &quot;Hello, this is a test&quot;   device tts stop‚Äã  Stop text-to-speech rendering.  aria_gen2 device tts stop   ","version":"Next","tagName":"h3"},{"title":"WiFi Commands‚Äã","type":1,"pageTitle":"Aria Gen2 CLI Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/cli#wifi-commands","content":" device wifi scan‚Äã  Scan for nearby WiFi networks.  aria_gen2 device wifi scan   device wifi connect‚Äã  Connect to a WiFi network.  Options:  --ssid &lt;ssid&gt; (required): WiFi network SSID--password &lt;password&gt;: WiFi network password--auth &lt;type&gt;: Authentication type (default: wpa) Values: none, eap, wpa, wep --hidden: Connect to hidden network--username &lt;username&gt;: Username for enterprise mode (EAP)--disable-other: Disable other WiFi networks--skip-internet-check: Skip internet connectivity check  # Connect to WPA network aria_gen2 device wifi connect --ssid MyNetwork --password mypassword # Connect to open network aria_gen2 device wifi connect --ssid OpenNetwork --auth none # Connect to enterprise network aria_gen2 device wifi connect --ssid CorpNetwork --auth eap \\ --username user@example.com --password mypassword   device wifi forget --ssid &lt;ssid&gt;‚Äã  Forget a WiFi network.  aria_gen2 device wifi forget --ssid MyNetwork   device wifi keep-on --set &lt;0|1&gt;‚Äã  Configure WiFi keep-on setting.  Options:  --set (required): 0 to disable, 1 to enable  aria_gen2 device wifi keep-on --set 1   ","version":"Next","tagName":"h3"},{"title":"WiFi Hotspot Commands‚Äã","type":1,"pageTitle":"Aria Gen2 CLI Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/cli#wifi-hotspot-commands","content":" device hotspot start‚Äã  Start WiFi hotspot on the device.  Options:  --wifi-band &lt;band&gt;: WiFi frequency band (default: BAND_5GHZ) Values: BAND_2GHZ, BAND_5GHZ, BAND_6GHZ --channel-bandwidth &lt;bandwidth&gt;: Channel bandwidth (default: BW_40MHZ) Values: BW_20MHZ, BW_40MHZ, BW_80MHZ, BW_160MHZ  aria_gen2 device hotspot start --wifi-band BAND_5GHZ --channel-bandwidth BW_80MHZ   device hotspot stop‚Äã  Stop WiFi hotspot.  aria_gen2 device hotspot stop   device hotspot status‚Äã  Get WiFi hotspot status.  aria_gen2 device hotspot status     ","version":"Next","tagName":"h3"},{"title":"Recording Commands‚Äã","type":1,"pageTitle":"Aria Gen2 CLI Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/cli#recording-commands","content":" Control on-device recording and manage recordings.  ","version":"Next","tagName":"h2"},{"title":"recording start‚Äã","type":1,"pageTitle":"Aria Gen2 CLI Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/cli#recording-start","content":" Start recording on the device.  Options:  --profile &lt;profile_name&gt;: Recording profile name (default: profile8)--json-profile &lt;custom profile json&gt;: Custom profile json file--recording-name &lt;name&gt;: Name for the recording file  # Start recording with default profile aria_gen2 recording start # Start recording with a different profile aria_gen2 recording start --profile profile10 # Start recording with a custom profile aria_gen2 recording start --json-profile custom_profile.json # Start recording with custom name aria_gen2 recording start --recording-name my_recording_001   ","version":"Next","tagName":"h3"},{"title":"recording stop‚Äã","type":1,"pageTitle":"Aria Gen2 CLI Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/cli#recording-stop","content":" Stop the current recording.  aria_gen2 recording stop   ","version":"Next","tagName":"h3"},{"title":"recording list‚Äã","type":1,"pageTitle":"Aria Gen2 CLI Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/cli#recording-list","content":" List all recordings on the device.  aria_gen2 recording list   ","version":"Next","tagName":"h3"},{"title":"recording info --uuid <uuid>‚Äã","type":1,"pageTitle":"Aria Gen2 CLI Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/cli#recording-info---uuid-uuid","content":" Get information about a specific recording.  Options:  --uuid, -u (required): Recording UUID  aria_gen2 recording info --uuid 123e4567-e89b-12d3-a456-426614174000   ","version":"Next","tagName":"h3"},{"title":"recording delete --uuid <uuid>‚Äã","type":1,"pageTitle":"Aria Gen2 CLI Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/cli#recording-delete---uuid-uuid","content":" Delete a specific recording.  Options:  --uuid, -u (required): Recording UUID to delete  aria_gen2 recording delete --uuid 123e4567-e89b-12d3-a456-426614174000   ","version":"Next","tagName":"h3"},{"title":"recording download --uuid <uuid>‚Äã","type":1,"pageTitle":"Aria Gen2 CLI Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/cli#recording-download---uuid-uuid","content":" Download a specific recording.  Options:  --uuid, -u (required): Recording UUID to download--output, -o: Output path for the recording file  # Download to current directory aria_gen2 recording download --uuid 123e4567-e89b-12d3-a456-426614174000 # Download to specific path aria_gen2 recording download --uuid 123e4567-e89b-12d3-a456-426614174000 \\ --output /path/to/recordings/   ","version":"Next","tagName":"h3"},{"title":"recording download-all‚Äã","type":1,"pageTitle":"Aria Gen2 CLI Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/cli#recording-download-all","content":" Download all recordings from the device.  Options:  --output, -o: Output directory for recording files  aria_gen2 recording download-all --output /path/to/recordings/   ","version":"Next","tagName":"h3"},{"title":"recording delete-all‚Äã","type":1,"pageTitle":"Aria Gen2 CLI Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/cli#recording-delete-all","content":" Delete all recordings from the device.  aria_gen2 recording delete-all     ","version":"Next","tagName":"h3"},{"title":"Streaming Commands‚Äã","type":1,"pageTitle":"Aria Gen2 CLI Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/cli#streaming-commands","content":" Control device streaming and manage streaming certificates.  ","version":"Next","tagName":"h2"},{"title":"streaming start‚Äã","type":1,"pageTitle":"Aria Gen2 CLI Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/cli#streaming-start","content":" Start streaming from the device.  Options:  --url &lt;url&gt;: Streaming endpoint URL (default: https://oatmeal_server.local:6768)--profile &lt;profile_name&gt;: Streaming profile name (default: mp_streaming_demo)--streaming-cert-name &lt;name&gt;: Pre-installed streaming certificate name--verify-server-certs / --no-verify-server-certs: Verify server certificates (default: no verification)--use-ephemeral-certs: Use ephemeral streaming certificates--local-certs-dir &lt;path&gt;: Local streaming certificates directory--keep-streaming-on-disconnection: Continue streaming after WiFi disconnection  # Start streaming over USB aria_gen2 streaming start # Start streaming with custom URL aria_gen2 streaming start --url https://192.168.1.100:8080   ","version":"Next","tagName":"h3"},{"title":"streaming stop‚Äã","type":1,"pageTitle":"Aria Gen2 CLI Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/cli#streaming-stop","content":" Stop streaming.  Options:  --remove-ephemeral-certs: Remove ephemeral streaming certificates from local host--local-certs-dir &lt;path&gt;: Local streaming certificates directory  aria_gen2 streaming stop aria_gen2 streaming stop --remove-ephemeral-certs   ","version":"Next","tagName":"h3"},{"title":"Streaming Certificate Commands‚Äã","type":1,"pageTitle":"Aria Gen2 CLI Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/cli#streaming-certificate-commands","content":" streaming install-certs‚Äã  Install streaming certificates on both device and local host.  Options:  --use-ephemeral-certs: Generate and install ephemeral certificates--local-certs-dir &lt;path&gt;: Local streaming certificates directory  aria_gen2 streaming install-certs --use-ephemeral-certs   streaming install-certs user-defined-certs‚Äã  Install user-defined streaming certificates.  Options:  --cert &lt;file&gt;: Certificate file (PEM format)--key &lt;file&gt;: Private key file (PEM format)--key-password &lt;file&gt;: Private key password file--ca-root &lt;file&gt;: CA root certificate file--cert-name &lt;name&gt;: Certificate name  aria_gen2 streaming install-certs user-defined-certs \\ --cert publisher.crt \\ --key publisher.key \\ --key-password key_password.txt \\ --ca-root ca_root.crt \\ --cert-name my_streaming_cert   streaming uninstall-certs‚Äã  Remove streaming certificates from both device and local host.  Options:  --local-certs-dir &lt;path&gt;: Local streaming certificates directory  aria_gen2 streaming uninstall-certs     ","version":"Next","tagName":"h3"},{"title":"Diagnostic and Utility Tools‚Äã","type":1,"pageTitle":"Aria Gen2 CLI Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/cli#diagnostic-and-utility-tools","content":" The following standalone CLI tools help with device setup, troubleshooting, and data visualization.  ","version":"Next","tagName":"h2"},{"title":"aria_doctor‚Äã","type":1,"pageTitle":"Aria Gen2 CLI Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/cli#aria_doctor","content":" Resolve internet connectivity issues and configure open ports when first connecting to the device.  This tool helps troubleshoot and fix common networking problems when setting up a new device connection.  aria_doctor   ","version":"Next","tagName":"h3"},{"title":"aria_diagnostics‚Äã","type":1,"pageTitle":"Aria Gen2 CLI Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/cli#aria_diagnostics","content":" Collect error logs and diagnostic information from the device and save them to diagnostics.zip.  This tool is useful for troubleshooting device issues and gathering logs for support.  aria_diagnostics   The diagnostic archive will be saved as diagnostics.zip in the current directory.  Linux Users If aria_diagnostics fails to run, you may need to install net-tools: sudo apt install net-tools   ","version":"Next","tagName":"h3"},{"title":"aria_streaming_viewer‚Äã","type":1,"pageTitle":"Aria Gen2 CLI Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/cli#aria_streaming_viewer","content":" Visualize real-time streaming data from the device.  This tool provides a live view of sensor data streams, including images, IMU data, and other sensor outputs.  Options:  --real-time: Enable real-time streaming mode with reduced queue sizes to minimize latency (default: disabled)--interpolate: Interpolate hand pose data for smoother visualization (default: use closest data point)--jpeg-quality &lt;quality&gt;: Set JPEG quality for image visualization, range 1-100 (default: 50)  # Basic usage aria_streaming_viewer # Enable real-time mode with minimal latency aria_streaming_viewer --real-time # Enable hand pose interpolation aria_streaming_viewer --interpolate # Set custom JPEG quality aria_streaming_viewer --jpeg-quality 80 # Combine options aria_streaming_viewer --real-time --interpolate --jpeg-quality 75   info The device must be streaming data before launching the viewer. Use aria_gen2 streaming start to begin streaming. If you encounter issues, see the Viewer Not Showing Data troubleshooting guide.    ","version":"Next","tagName":"h3"},{"title":"Common Usage Examples‚Äã","type":1,"pageTitle":"Aria Gen2 CLI Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/cli#common-usage-examples","content":" ","version":"Next","tagName":"h2"},{"title":"Quick Start: Stream from Device‚Äã","type":1,"pageTitle":"Aria Gen2 CLI Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/cli#quick-start-stream-from-device","content":" # 1. Authenticate aria_gen2 auth pair # 2. Start streaming aria_gen2 streaming start # 3. Stop streaming aria_gen2 streaming stop   ","version":"Next","tagName":"h3"},{"title":"Record Data on Device‚Äã","type":1,"pageTitle":"Aria Gen2 CLI Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/cli#record-data-on-device","content":" # Start recording aria_gen2 recording start --recording-name experiment_001 # ... wait for recording to complete ... # Stop recording aria_gen2 recording stop # List recordings aria_gen2 recording list # Download recording aria_gen2 recording download -u &lt;uuid&gt; -o ./recordings/   ","version":"Next","tagName":"h3"},{"title":"Connect Device to WiFi‚Äã","type":1,"pageTitle":"Aria Gen2 CLI Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/cli#connect-device-to-wifi","content":" # Scan for networks aria_gen2 device wifi scan # Connect to WiFi aria_gen2 device wifi connect --ssid MyNetwork --password mypassword   ","version":"Next","tagName":"h3"},{"title":"Work with Multiple Devices‚Äã","type":1,"pageTitle":"Aria Gen2 CLI Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/cli#work-with-multiple-devices","content":" # List devices aria_gen2 device list # Connect to specific device by serial aria_gen2 --serial 1M0XXXXXXX device status   ","version":"Next","tagName":"h3"},{"title":"Debug Issues‚Äã","type":1,"pageTitle":"Aria Gen2 CLI Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/cli#debug-issues","content":" # Check authentication aria_gen2 auth check # Get device status aria_gen2 device status     ","version":"Next","tagName":"h3"},{"title":"Additional Resources‚Äã","type":1,"pageTitle":"Aria Gen2 CLI Reference","url":"/projectaria_tools/gen2/technical-specs/client-sdk/cli#additional-resources","content":" For troubleshooting common issues with device connection, authentication, recording, or streaming, please refer to the Troubleshooting Guide.  Key points:  Most commands require a connected device (via USB)Authentication (auth pair) must be performed before first useRecording and streaming cannot run simultaneouslyIf you encounter connection issues, run aria_doctor to resolve networking problemsFor detailed troubleshooting steps, see the Troubleshooting Guide ","version":"Next","tagName":"h2"},{"title":"Project Aria Glasses 2D Image Coordinate System Conventions","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/technical-specs/coordinate/2d-coor","content":"Project Aria Glasses 2D Image Coordinate System Conventions For any provided camera intrinsic calibration value, we use the convention that the color value of a pixel with integer coordinates (u,v)(u,v)(u,v) is the average color of the square spanning from (u‚àí0.5,v‚àí0.5)(u-0.5,v-0.5)(u‚àí0.5,v‚àí0.5) to (u+0.5,v+0.5)(u+0.5,v+0.5)(u+0.5,v+0.5) in continuous coordinates. This is visualized in the Figure 1, and has the following important consequences: Checking in bound: A pixel (u,v)(u,v)(u,v) is considered to be in bound if ‚àí0.5‚â§u&lt;W‚àí0.5-0.5\\leq u&lt;W-0.5‚àí0.5‚â§u&lt;W‚àí0.5 and ‚àí0.5‚â§v&lt;H‚àí0.5-0.5\\leq v&lt;H-0.5‚àí0.5‚â§v&lt;H‚àí0.5.Interpolation: In bilinear interpolation, a point (u,v) can be interpolated of all four neighboring integer-valued pixel coordinates are in-bound. That requires 0‚â§u‚â§W‚àí10 \\leq u \\leq W-10‚â§u‚â§W‚àí1 and 0‚â§v‚â§H‚àí10 \\leq v \\leq H-10‚â§v‚â§H‚àí1.Image down-sampling: When downsampling images by a factor of sss, every s√óss \\times ss√ós pixel are squeezed into a single pixel. For example, the intensity at pixel s√óss \\times ss√ós in the scaled image accounts for all the photons collected in the area [‚àí0.5,s‚àí0.5]√ó[‚àí0.5,s‚àí0.5][-0.5,s-0.5]\\times[-0.5,s-0.5][‚àí0.5,s‚àí0.5]√ó[‚àí0.5,s‚àí0.5] (i.e. column 000 to s‚àí1s - 1s‚àí1, and row 000 to s‚àí1s-1s‚àí1 in the discrete coordinate) in the original image. In order to keep this assumption valid, the re-scaled point pscaledp_\\text{scaled}pscaled‚Äã not only needs to scale from the corresponding point in the original image poriginalp_\\text{original}poriginal‚Äã but also accounts for the (0.5,0.5)(0.5,0.5)(0.5,0.5) translation accordingly by pscaled=s(poriginal+0.5)‚àí0.5p_\\text{scaled} =s (p_\\text{original}+0.5)-0.5pscaled‚Äã=s(poriginal‚Äã+0.5)‚àí0.5 Figure 1: 2D Image Coordinate System Conventions","keywords":"","version":"Next"},{"title":"Central Pupil Frame (CPF) in Aria and `projectaria-tools`","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/technical-specs/coordinate/cpf-coor","content":"","keywords":"","version":"Next"},{"title":"What is Central Pupil Frame?‚Äã","type":1,"pageTitle":"Central Pupil Frame (CPF) in Aria and `projectaria-tools`","url":"/projectaria_tools/gen2/technical-specs/coordinate/cpf-coor#what-is-central-pupil-frame","content":" The central pupil frame (CPF) is defined as the middle-point interpolation of the left and right nominal pupil coordinate frame in a glass. This concept has caused a lot of confusion, particularly in the ET gaze ML pipeline and other Aria calibration use cases.  This document provides recommendations about using CPF from a calibration perspective, specifically for Aria calibration, though the concepts can extend to other devices.  ","version":"Next","tagName":"h2"},{"title":"Recommendations‚Äã","type":1,"pageTitle":"Central Pupil Frame (CPF) in Aria and `projectaria-tools`","url":"/projectaria_tools/gen2/technical-specs/coordinate/cpf-coor#recommendations","content":" In projectaria-tools, you can use get_T*/get_transform*() functions to retrieve sensor extrinsics/poses. You can use the parameter getCadValue to switch between factory calibration value (=false, default) or CAD value (=true). Using CPF-based poses is generally not recommended because: When using T_CPF_sensor(..., getCadValue=false), this transform from CPF (in the CAD space) to a calibrated sensor (in the calibration space) is ill-defined, and the code definition depends on implementation.When using T_CPF_sensor(getCadValue=true), this gives a well-defined pose in CAD space. However, the error in CAD space is generally much larger than calibration error if the pose is applied to real data. We recommend using relative poses between physical sensors, e.g., pose from sensor frame 1 to sensor frame 2 (T_sensor1_sensor2), as the sensor-frame-based pose definition does not have the same nuances CPF-based poses have: T_sensor1_sensor2 = T_Device_sensor1.inverse() * T_Device_sensor2   ","version":"Next","tagName":"h2"},{"title":"Coordinate Frames and Poses‚Äã","type":1,"pageTitle":"Central Pupil Frame (CPF) in Aria and `projectaria-tools`","url":"/projectaria_tools/gen2/technical-specs/coordinate/cpf-coor#coordinate-frames-and-poses","content":" Coordinate frames are the position and orientation of cameras and sensors. Their conventions are defined in 3D Coordinate System Conventions. These are absolute concepts, such as &quot;origins being the optical center of the cameras&quot; and &quot;z-axis being the optical axis of the camera&quot;.  In comparison, poses are relative: they are the transforms from one coordinate frame to another.  ","version":"Next","tagName":"h2"},{"title":"Nuance 1: Coordinate frames as SE(3) are actually poses from an arbitrary reference (\"device\") frame‚Äã","type":1,"pageTitle":"Central Pupil Frame (CPF) in Aria and `projectaria-tools`","url":"/projectaria_tools/gen2/technical-specs/coordinate/cpf-coor#nuance-1-coordinate-frames-as-se3-are-actually-poses-from-an-arbitrary-reference-device-frame","content":" As illustrated in Figure 1, you cannot well define a coordinate frame without defining the coordinate frame where these points and vectors live. This coordinate frame is called the reference frame. So when we say &quot;pose&quot; of sensor1, we explicitly assume it is sensor1's pose in the reference frame.    Figure 1. Coordinate frame and pose. Coordinate frames (red) are &quot;absolute&quot; concepts; they need to be defined in a reference frame (frame1 in the diagram). Relative poses between frames are what is observable. When we say pose of frame X we are actually referring to T_frame1_frameX where frame1 being the reference frame.  In projectaria-tools, we define a device frame as the reference frame (and default to the left SLAM camera). So when we talk about the coordinate frame of RGB, we are actually talking about the pose of RGB relative to the device (the left SLAM camera) coordinate frame.  ","version":"Next","tagName":"h3"},{"title":"CAD Model and Factory Calibration‚Äã","type":1,"pageTitle":"Central Pupil Frame (CPF) in Aria and `projectaria-tools`","url":"/projectaria_tools/gen2/technical-specs/coordinate/cpf-coor#cad-model-and-factory-calibration","content":" CAD models are constant, theoretical values defined by the design of the device hardware. Devices are fabricated according to the design, but with errors. A device's factory calibration is a one-time measurement of the position of each component in the device, with the goal that:  Calibration error MUCH SMALLER THAN CAD error  Otherwise there's no point doing calibration. This leads to our recommendation that, every time you use a CAD model with real data, consider why you don't want to take advantage of the reduced error in factory calibration.  ","version":"Next","tagName":"h2"},{"title":"Nuance 2: T_CPF_sensor(calibration) is not well defined‚Äã","type":1,"pageTitle":"Central Pupil Frame (CPF) in Aria and `projectaria-tools`","url":"/projectaria_tools/gen2/technical-specs/coordinate/cpf-coor#nuance-2-t_cpf_sensorcalibration-is-not-well-defined","content":" Since nominal pupil frames are the designed coordinate frame of the eyes, they only live in the CAD model. CPF, defined as the middle-interpolation of the two nominal pupil frames, therefore, also only lives in the CAD model. However, these entities are not observable during Aria calibration.  Now, if we need to report T_CPF_sensor(calibration), this pose is a transform from the CAD model space to the calibration model space. The two model spaces are independent (just like two Aria units), and there's no standard convention on how this alignment should be done. Therefore this pose is mathematically undefined.  In projectaria-tools, we choose to align Aria's CAD and device calibration according to the device frame (as shown in Figure 2), therefore:  T_frame1(CAD)_frame2(calibration) = T_frame1(CAD)_device(CAD) * T_device(calibration)_frame2(calibration)   We often call this &quot;anchoring frame1 and frame2 to the device frame&quot;.  And T_CPF_sensor depends on the device frame:  T_CPF_sensor(calibration) = T_CPF_device(CAD) * T_device_sensor(calibration)   ()  Figure 2. Poses between the CAD model space and the calibration model space are not well defined mathematically. Implicitly, we need to align them via device frame (according to projectaria tools implementation).  ","version":"Next","tagName":"h3"},{"title":"Nuance 3: In the projectaria tool definition, if switching to a different device frame, the value of T_CPF_sensor in the factory calibration model changes‚Äã","type":1,"pageTitle":"Central Pupil Frame (CPF) in Aria and `projectaria-tools`","url":"/projectaria_tools/gen2/technical-specs/coordinate/cpf-coor#nuance-3-in-the-projectaria-tool-definition-if-switching-to-a-different-device-frame-the-value-of-t_cpf_sensor-in-the-factory-calibration-model-changes","content":" Let's say we have two definitions of T_CPF_sensor based on device frame1 and device frame2:  T_CPF_sensor (device1) = T_CPF_device1 (CAD) * T_device1_sensor (calibration) T_CPF_sensor (device2) = T_CPF_device2 (CAD) * T_device2_sensor (calibration)   As illustrated in Figure 3, the above two generally would not be the same, because generally:  T_device1_device2 (CAD) != T_device1_device2 (calibration)   Thus changing the device frame is generally accounting for the CAD model error between the two device frames. This means, if you would like to use a T_CPF_sensor with a sensor frame in the calibration space, you really need to be aware of what CPF means here, and it is generally a proxy of the device frame. A better choice would be staying away from CPF-based pose conventions. Rather we recommend using pose among sensor frames to be able to safely switch between CAD space and calibration space.    Figure 3. When switching device frame convention, since the pose between the two frames differ between the CAD model and the calibration model, the value T_CPF_sensor(calib) also differs. ","version":"Next","tagName":"h3"},{"title":"CAD File Downloads","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/technical-specs/device/cad","content":"","keywords":"","version":"Next"},{"title":"Glasses CAD:‚Äã","type":1,"pageTitle":"CAD File Downloads","url":"/projectaria_tools/gen2/technical-specs/device/cad#glasses-cad","content":" \tFrame Width\tTemple Arm Length\tNosepad\tCAD file1\tNarrow\tShort\tHigh\tOpen Hinge, Closed Hinge 2\tNarrow\tShort\tLow\tOpen Hinge, Closed Hinge 3\tNarrow\tLong\tHigh\tOpen Hinge, Closed Hinge 4\tNarrow\tLong\tLow\tOpen Hinge, Closed Hinge 5\tWide\tShort\tHigh\tOpen Hinge, Closed Hinge 6\tWide\tShort\tLow\tOpen Hinge, Closed Hinge 7\tWide\tLong\tHigh\tOpen Hinge, Closed Hinge 8\tWide\tLong\tLow\tOpen Hinge, Closed Hinge  ","version":"Next","tagName":"h2"},{"title":"Glasses Mount Reference Design:‚Äã","type":1,"pageTitle":"CAD File Downloads","url":"/projectaria_tools/gen2/technical-specs/device/cad#glasses-mount-reference-design","content":" The Aria Gen 2 glasses can be mounted using the following design files as a reference. The two reference CAD files differ by frame width. Additionally, the designs can accommodate either an open or close hinge application. In these files, they are designed to be compatible with a standard camera mount, ¬º‚Äù-20 insert (McMaster PN: 93365A160). A 16mm diameter o-ring is used to retain the glasses to the mount (McMaster PN: 1174N407).  Frame Width\tArms\tCAD fileNarrow\tUnfolded\tNEBULA TRIPOD MOUNT - NARROW - UNFOLDED.zip Narrow\tFolded\tNEBULA TRIPOD MOUNT - NARROW - FOLDED.zip Wide\tUnfolded\tNEBULA TRIPOD MOUNT - WIDE - UNFOLDED.zip Wide\tFolded\tNEBULA TRIPOD MOUNT - WIDE - FOLDED.zip ","version":"Next","tagName":"h2"},{"title":"Aria Gen2 Device Calibration","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/technical-specs/device/calibration","content":"","keywords":"","version":"Next"},{"title":"Overview‚Äã","type":1,"pageTitle":"Aria Gen2 Device Calibration","url":"/projectaria_tools/gen2/technical-specs/device/calibration#overview","content":" Aria Gen2 devices contain multiple sensors with factory calibration data. Depending on the sensor type, calibration may include:  Intrinsic calibration: Corrects for sensor-specific distortions and biases (e.g., camera lens distortion, IMU bias, microphone sensitivity)Extrinsic calibration: Defines spatial relationships (position and orientation) between sensors in the device coordinate frame (available for cameras, IMUs, and LED rings)  The calibration data is stored in a standardized JSON format and is accessible as Python DeviceCalibration objects through Client SDK or from Project Aria Tools.    ","version":"Next","tagName":"h2"},{"title":"Sensor Calibration Availability‚Äã","type":1,"pageTitle":"Aria Gen2 Device Calibration","url":"/projectaria_tools/gen2/technical-specs/device/calibration#sensor-calibration-availability","content":" The following table shows which calibration data is available for each sensor type:  Sensor Type\tSensor Label(s)\tIntrinsics\tExtrinsicsRGB Camera\tcamera-rgb\t‚úÖ FisheyeRadTanThinPrism\t‚úÖ T_Device_Camera SLAM Cameras\tslam-front-left slam-front-right slam-side-left slam-side-right\t‚úÖ FisheyeRadTanThinPrism\t‚úÖ T_Device_Camera Eye Tracking Cameras\tcamera-et-left camera-et-right\t‚úÖ KannalaBrandtK3\t‚úÖ T_Device_Camera IMU\timu-left imu-right\t‚úÖ Accelerometer + Gyroscope (Bias, Rectification)\t‚úÖ T_Device_Imu Magnetometer\tmag0\t‚úÖ Bias + Rectification\t‚ùå Barometer\tbaro0\t‚úÖ Offset + Slope\t‚ùå Microphones\tmic0 - mic5, mic6 (contact mic)\t‚úÖ Sensitivity (dBV @ 1kHz)\t‚ùå Speakers\tLSPK, RSPK\t‚úÖ Sensitivity (dBV)\t‚ùå  ","version":"Next","tagName":"h2"},{"title":"Camera Projection Models‚Äã","type":1,"pageTitle":"Aria Gen2 Device Calibration","url":"/projectaria_tools/gen2/technical-specs/device/calibration#camera-projection-models","content":" Details of the camera intrinsics model can be found in calibration insight  FisheyeRadTanThinPrism: Used for RGB and SLAM cameras Parameters: focal length (f), principal point (cx, cy), radial distortion (k0-k5), tangential distortion (p0, p1), thin prism (s0-s2) KannalaBrandtK3: Used for eye tracking cameras Parameters: focal lengths (fx, fy), principal point (cx, cy), Kannala-Brandt distortion coefficients (kb0-kb3)  ","version":"Next","tagName":"h3"},{"title":"IMU Calibration Components‚Äã","type":1,"pageTitle":"Aria Gen2 Device Calibration","url":"/projectaria_tools/gen2/technical-specs/device/calibration#imu-calibration-components","content":" Details of the IMU models can be found in calibration insight  Accelerometer: Bias offset (m/s¬≤) and 3x3 rectification matrix for scale/cross-axis correctionsGyroscope: Bias offset (rad/s), rectification matrix, and G-sensitivity matrix    ","version":"Next","tagName":"h3"},{"title":"Device Coordinate System‚Äã","type":1,"pageTitle":"Aria Gen2 Device Calibration","url":"/projectaria_tools/gen2/technical-specs/device/calibration#device-coordinate-system","content":" ","version":"Next","tagName":"h2"},{"title":"Origin Specification‚Äã","type":1,"pageTitle":"Aria Gen2 Device Calibration","url":"/projectaria_tools/gen2/technical-specs/device/calibration#origin-specification","content":" By default, the origin is located at the slam-front-left camera. All sensor transformations (T_Device_Sensor) defines device (i.e. origin) to be slam-front-left.  ","version":"Next","tagName":"h3"},{"title":"Coordinate Frame Convention‚Äã","type":1,"pageTitle":"Aria Gen2 Device Calibration","url":"/projectaria_tools/gen2/technical-specs/device/calibration#coordinate-frame-convention","content":" Aria Gen2 uses a right-handed coordinate system:  X-axis: Points to the right when viewing from behind the glassesY-axis: Points downwardZ-axis: Points forward (out from the front of the glasses)  For more details on coordinate frame convention, please visit Coordinate Convention.    ","version":"Next","tagName":"h3"},{"title":"Additional Resources‚Äã","type":1,"pageTitle":"Aria Gen2 Device Calibration","url":"/projectaria_tools/gen2/technical-specs/device/calibration#additional-resources","content":" Coordinate Frame Conventions - 3D Coordinate Frame Conventions for Project Aria GlassesClient SDK API Reference - Complete API documentationCalibration Tutorial - Working with calibration dataDevice Profiles - Sensor configurations and profiles ","version":"Next","tagName":"h2"},{"title":"Hardware Specification","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/technical-specs/device/hardware","content":"","keywords":"","version":"Next"},{"title":"RGB camera‚Äã","type":1,"pageTitle":"Hardware Specification","url":"/projectaria_tools/gen2/technical-specs/device/hardware#rgb-camera","content":" The Aria Gen 2 features a high-resolution RGB camera with rolling shutter, integrating the Sony IMX681 sensor. This module provides a 133¬∞ horizontal and 99¬∞ vertical field of view, with a 12 megapixel, 1.0 Œºm pixel sensor capable of full-resolution (4032 √ó 3024) capture at 24 fps. The RGB pipeline supports functions such as binning, downscaling, and cropping, enabling higher frame rates and diverse video formats and output resolutions. A dedicated on-device color ISP is included within the RGB pipeline, offering features like auto-exposure and auto-white balance.     Figure: RGB Camera FOV  ","version":"Next","tagName":"h2"},{"title":"Computer vision (CV) cameras‚Äã","type":1,"pageTitle":"Hardware Specification","url":"/projectaria_tools/gen2/technical-specs/device/hardware#computer-vision-cv-cameras","content":" Each of the CV cameras provides a field-of-view of 119¬∞x119¬∞ and contains a custom-designed [ref1, ref2], stacked digital pixel sensor with a global shutter. This sensor features 4.6‚ÄØ¬µm pixels in a 512 √ó 512 array and employs an overlapped triple quantization scheme to enable single-frame, single-exposure high dynamic range (HDR) imaging, with dynamic range capability exceeding 110‚ÄØdB. Monochrome image data from each sensor is processed by a dedicated image signal processor (ISP), which performs noise reduction‚Äîincluding fixed pattern and defective pixel correction‚Äîand implements a tuneable HDR tonemapping scheme to map the high dynamic range output to standard 8-bit formats. The front-facing CV cameras form a stereo pair with significant overlap that enables off-device depth reconstruction of the scene.     Fig: FOV of the CV cameras on Aria Gen 2     Fig: Comparison of the dynamic range of the CV camera and RGB camera to showcase the HDR capability of the CV camera  ","version":"Next","tagName":"h2"},{"title":"Spatial microphones‚Äã","type":1,"pageTitle":"Hardware Specification","url":"/projectaria_tools/gen2/technical-specs/device/hardware#spatial-microphones","content":" The audio capture capabilities support 7 acoustic microphones at sampling rate of up to 48 KHz for each individual microphone. This high sampling rate ensures detailed and accurate audio recording, capable of capturing a broad spectrum of sounds with exceptional fidelity.  ","version":"Next","tagName":"h2"},{"title":"Contact microphone‚Äã","type":1,"pageTitle":"Hardware Specification","url":"/projectaria_tools/gen2/technical-specs/device/hardware#contact-microphone","content":" The Aria Gen 2 device incorporates a high-fidelity contact microphone, the Knowles V2S200D, embedded within the glasses' nosepad. This placement is critical for optimal audio capture, minimizing external noise interference and maximizing the clarity of the user's voice. The microphone samples audio at up to 48 KHz, enabling clear capture of the wearer's voice even in noisy, windy conditions.     Figure demonstrates the amplitude of wearer speech in a wind tunnel test. The top image, from a contact microphone, clearly shows the wearer's voice waveform isolated and ambient wind noise suppressed. In contrast, the corresponding response from the acoustic microphone, shown in the bottom image, indicates that it picks up the ambient noise.  ","version":"Next","tagName":"h2"},{"title":"Eye tracking system‚Äã","type":1,"pageTitle":"Hardware Specification","url":"/projectaria_tools/gen2/technical-specs/device/hardware#eye-tracking-system","content":" The eye tracking system for each eye consists of a 400x400 pixel resolution infrared (IR) camera and 8 IR LEDs. The data from these cameras is processed on-device, yielding real-time, high quality gaze estimations at up to 90 Hz framerate. Alternatively, the data from the eye tracking cameras can be recorded and processed off-device.  ","version":"Next","tagName":"h2"},{"title":"PPG‚Äã","type":1,"pageTitle":"Hardware Specification","url":"/projectaria_tools/gen2/technical-specs/device/hardware#ppg","content":" A photoplethysmography (PPG) sensor is integrated into the nosepad of the spectacles and enables recording the wearer‚Äôs heart rate. The PPG sampling rate is typically 128 Hz, with a maximum of 512 Hz.  ","version":"Next","tagName":"h2"},{"title":"Inertial measurement units (IMUs)‚Äã","type":1,"pageTitle":"Hardware Specification","url":"/projectaria_tools/gen2/technical-specs/device/hardware#inertial-measurement-units-imus","content":" Aria Gen 2 features dual 6-axis gyroscopes and accelerometers with a maximum sampling rate of 1600 Hz and a typical sampling rate of 800 Hz. The data is consumed by the on-device VIO system and can also be recorded.  ","version":"Next","tagName":"h2"},{"title":"Barometer‚Äã","type":1,"pageTitle":"Hardware Specification","url":"/projectaria_tools/gen2/technical-specs/device/hardware#barometer","content":" The barometer on Aria Gen 2 is a low noise sensor for atmospheric pressure with a maximum sampling rate of 240 Hz max and a typical sampling rate of 100 Hz.  ","version":"Next","tagName":"h2"},{"title":"Magnetometer‚Äã","type":1,"pageTitle":"Hardware Specification","url":"/projectaria_tools/gen2/technical-specs/device/hardware#magnetometer","content":" The magnetometer on Aria Gen 2 is an ultra-low noise sensor with a maximum sampling rate of 400 Hz and a typical sampling rate of 100 Hz.  ","version":"Next","tagName":"h2"},{"title":"Proximity Sensors‚Äã","type":1,"pageTitle":"Hardware Specification","url":"/projectaria_tools/gen2/technical-specs/device/hardware#proximity-sensors","content":" A proximity sensor detects when the glasses are worn. This sensor uses a threshold-crossing interrupt mechanism to communicate with the integrated computing system.  ","version":"Next","tagName":"h2"},{"title":"GNSS‚Äã","type":1,"pageTitle":"Hardware Specification","url":"/projectaria_tools/gen2/technical-specs/device/hardware#gnss","content":" Global Navigation Satellite System (GNSS) support across bands such as L1, L1 + L5, E1, and E1 + E5. The device supports L1 + L5 (GPS) and E1 + E5 (Galileo) dual-frequency signals, which significantly improve acquisition speeds and positioning precision.  ","version":"Next","tagName":"h2"},{"title":"Ambient Light Sensor (ALS)‚Äã","type":1,"pageTitle":"Hardware Specification","url":"/projectaria_tools/gen2/technical-specs/device/hardware#ambient-light-sensor-als","content":" The VD6281 sensor provides correlated color temperature (CCT) and lux measurements. It features 5 channels (red, green, blue, IR, UVA), with an additional clear channel for flicker detection. We provide an example of ALS data below, captured during a recording as the wearer moves through an office environment and onto an outdoor balcony. The data highlights three frames, demonstrating that the UV and IR channels of the ALS sensor exhibit distinct changes when transitioning to outdoor environments. These signals can be leveraged for classification of indoor/outdoor environments. We provide an example of ALS data below, captured during a recording as the wearer moves from an indoor office environment and onto an outdoor balcony. The data highlights three frames, demonstrating that the UV and IR channels of the ALS sensor exhibit distinct changes when transitioning to outdoor environments and can be used for classification of indoor/outdoor environments.     ","version":"Next","tagName":"h2"},{"title":"On-device hardware accelerators‚Äã","type":1,"pageTitle":"Hardware Specification","url":"/projectaria_tools/gen2/technical-specs/device/hardware#on-device-hardware-accelerators","content":" The device‚Äôs coprocessor supports on-device compression for image/video using H265 HEVC and audio data using OPUS encoders. It also features hardware-accelerated support for machine perception, including 3D articulated hand tracking, eye-tracking with gaze per eye output, and advanced signals such as pupil diameter and blink detection, as well as 6DoF localization.  ","version":"Next","tagName":"h2"},{"title":"Multi-device time alignment‚Äã","type":1,"pageTitle":"Hardware Specification","url":"/projectaria_tools/gen2/technical-specs/device/hardware#multi-device-time-alignment","content":" A SubGHz radio facilitates the transmission of device timestamps between devices. One broadcaster device transmits its time, while other devices receive this broadcast and subsequently compute time alignment (difference) between the broadcaster and receiver. (Note that this does not synchronize the devices.) This approach circumvents the need for round-trip communication typically associated with TicSync. The system operates under the assumption of negligible time-of-flight within its 30m indoor and 100m outdoor operating ranges. The measured time offset error has been observed to be less than 10 Œºs. Methodologies for integrating Aria Gen2's time alignment capabilities with other systems may be made available in the future.     Fig: The figure illustrates a pair of Aria Gen2 devices observing a common timing panel and the level of alignment in the camera frames from the two devices Top: The test setup consists of two Aria Gen 2 devices co-observing a timing board. Both Aria Gen 2 devices are time-aligned using a SubGHz radio. Bottom: These images were captured from the front left CV camera on each device. Based on the timing board content as observed on each camera, the real-world timing difference between the two images (and therefore the trigger times of their respective CV cameras) is 57.325 ms. The timing inaccuracy between the two time-aligned devices as reported by the timestamps is measured to be 2.18 Œºs, with a measurement precision of ¬±5 Œºs. ","version":"Next","tagName":"h2"},{"title":"Sensor Measurement Models in Project Aria Devices","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/technical-specs/device/calibration_insights/sensor_measurement_model","content":"","keywords":"","version":"Next"},{"title":"IMUs‚Äã","type":1,"pageTitle":"Sensor Measurement Models in Project Aria Devices","url":"/projectaria_tools/gen2/technical-specs/device/calibration_insights/sensor_measurement_model#imus","content":" For IMUs, we employ an affine model where the value from the readout of accelerometer sas_asa‚Äã or gyroscope sgs_gsg‚Äã, is compensated to obtain a &quot;real&quot; acceleration aaa and angular velocity œâ\\omegaœâ by  a=Ma‚àí1(sa‚àíba)œâ=Mg‚àí1(sg‚àíbg)a = M_a^{-1}(s_a - b_a) \\qquad \\omega = M_g^{-1}(s_g - b_g)a=Ma‚àí1‚Äã(sa‚Äã‚àíba‚Äã)œâ=Mg‚àí1‚Äã(sg‚Äã‚àíbg‚Äã)  MaM_aMa‚Äã and MgM_gMg‚Äã are assumed to be upper triangular so that there is no global rotation from the imu body frame to the accelerometer frame.  Inversely, we can simulate the sensor read-out from acceleration or angular velocity by  sa=Maa+basg=Mgœâ+bgs_a = M_a a + b_a \\qquad s_g = M_g \\omega + b_gsa‚Äã=Ma‚Äãa+ba‚Äãsg‚Äã=Mg‚Äãœâ+bg‚Äã  When the read-out signal exceeds a threshold, the signal saturates. Saturation limits are sensor dependent and referenced in the following table for accelerometer and gyrometers.  \taccel-left\taccel-right\tgyro-left\tgyro-rightsaturation\t8g\t16g\t2000\t2000  ","version":"Next","tagName":"h2"},{"title":"Magnetometer, barometer and audio‚Äã","type":1,"pageTitle":"Sensor Measurement Models in Project Aria Devices","url":"/projectaria_tools/gen2/technical-specs/device/calibration_insights/sensor_measurement_model#magnetometer-barometer-and-audio","content":" Similar to the IMU rectification model, the sensor readouts for magnetometer, barometer, and audio data are modeled as linear to the real rrr (magnetic field, air pressure and sound intensity).  Audio specifically is bias only. ","version":"Next","tagName":"h2"},{"title":"Project Aria Gen 2 Profiles","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/technical-specs/device/profile","content":"","keywords":"","version":"Next"},{"title":"Profile Usage Recommendations‚Äã","type":1,"pageTitle":"Project Aria Gen 2 Profiles","url":"/projectaria_tools/gen2/technical-specs/device/profile#profile-usage-recommendations","content":" ","version":"Next","tagName":"h2"},{"title":"When to Use Pre-defined Profiles‚Äã","type":1,"pageTitle":"Project Aria Gen 2 Profiles","url":"/projectaria_tools/gen2/technical-specs/device/profile#when-to-use-pre-defined-profiles","content":" Pre-defined profiles (profile8, profile9, profile10, mp_streaming_demo) are:  ‚úÖ Thoroughly tested and validated‚úÖ Optimized for device stability and thermal performance‚úÖ Fully supported by all downstream tools‚úÖ Guaranteed to work reliably‚úÖ Recommended for production use and research studies  Recommended for:  Production deploymentsLarge-scale data collectionResearch studies requiring reliable dataUsers who want maximum compatibilityFirst-time Aria users  ","version":"Next","tagName":"h3"},{"title":"When to Consider Custom Profiles‚Äã","type":1,"pageTitle":"Project Aria Gen 2 Profiles","url":"/projectaria_tools/gen2/technical-specs/device/profile#when-to-consider-custom-profiles","content":" Custom profiles may be appropriate for:  Advanced research with specific sensor requirementsPrototyping new applicationsExperimental configurationsSpecialized use cases not covered by pre-defined profiles  ‚ö†Ô∏è Important: Custom profiles should only be used by advanced users who understand the risks and limitations.    ","version":"Next","tagName":"h3"},{"title":"Custom Profiles‚Äã","type":1,"pageTitle":"Project Aria Gen 2 Profiles","url":"/projectaria_tools/gen2/technical-specs/device/profile#custom_profile_example","content":" ","version":"Next","tagName":"h2"},{"title":"Overview‚Äã","type":1,"pageTitle":"Project Aria Gen 2 Profiles","url":"/projectaria_tools/gen2/technical-specs/device/profile#overview","content":" Custom profiles allow advanced users to define their own sensor configurations for specialized research needs. However, custom profiles come with significant risks and limitations and should be used with caution.  ","version":"Next","tagName":"h3"},{"title":"‚ö†Ô∏è Important Considerations and Risks‚Äã","type":1,"pageTitle":"Project Aria Gen 2 Profiles","url":"/projectaria_tools/gen2/technical-specs/device/profile#Ô∏è-important-considerations-and-risks","content":" Before creating a custom profile, understand these critical limitations:  1. Device Stability and Data Quality‚Äã  Thermal and Power Issues:  Custom profiles that push sensor settings (e.g., high frame rates, high resolutions, enabling many sensors simultaneously) can cause the device to overheat or drain the battery rapidlyThis can lead to unexpected shutdowns or reduced recording/streaming timesSome combinations may trigger warnings in the Aria Companion App, but not all edge cases are covered automaticallyUsing extreme values or not basing your custom profile on a tested template significantly increases these risks  Data Quality Risks:  Unusual sensor configurations may result in: Poor data qualitySynchronization issues between sensorsMissing data streamsInconsistent sensor performance These issues may not be immediately apparent and could compromise your research data  Fragmented Datasets:  Custom profiles can lead to fragmented datasets, making it harder to: Compare data across different studiesAggregate data from multiple usersReproduce research results This reduces the overall utility of the data for broader research purposes  2. Compatibility and Downstream Tooling‚Äã  Tooling Incompatibility:  Downstream tools (e.g., Project Aria Tools, MPS, analytics pipelines) may not be fully compatible with all custom profile configurationsData recorded with custom profiles may: Not be processed correctly by standard toolsRequire additional engineering work to support new formatsBe incompatible with existing analysis pipelines  Lack of Standardization:  Custom profiles are not as thoroughly tested or supported as pre-defined profilesThis can lead to: Unexpected issues when using standard toolsDifficulties sharing data with collaboratorsLimited community support  3. Known Bugs and Open Issues‚Äã  Sensor Data Retrieval:  There have been reports of issues retrieving certain sensor data (e.g., GPS) when using custom profilesThese issues may not be present with recommended pre-defined profiles  Parameter Guardrails:  While the Companion App includes some guardrails, not all problematic combinations are blockedUsers may still encounter hardware or data quality issues when defining profiles via the SDK  ‚ö†Ô∏è Use at Your Own Risk:Custom profiles are provided as-is for advanced users. Meta does not guarantee stability, data quality, or tool compatibility for custom profile configurations.    ","version":"Next","tagName":"h3"},{"title":"Custom Profile Template‚Äã","type":1,"pageTitle":"Project Aria Gen 2 Profiles","url":"/projectaria_tools/gen2/technical-specs/device/profile#custom-profile-template","content":" Below is a template for creating custom profiles. It is strongly recommended to base your custom profile on one of the pre-defined profiles (e.g., profile8) rather than starting from scratch.  This example is based on profile8 with the following modifications:  RGB frame rate: Reduced from 10Hz to 5HzSLAM camera frame rate: Reduced from 30Hz to 10HzHand tracking: Reduced from 30Hz to 10Hz  { &quot;name&quot;: &quot;custom_profile&quot;, &quot;description&quot;: &quot;Custom profile based on profile8: Reduced RGB frame rate from 10Hz to 5Hz. Reduced SLAM frame rate from 30Hz to 10Hz. Reduced HT from 30Hz to 10Hz&quot;, &quot;imus&quot;: { &quot;rate_hz&quot;: 800 }, &quot;magnetometer&quot;: { &quot;rate_hz&quot;: 100 }, &quot;barometer&quot;: { &quot;rate_hz&quot;: 50 }, &quot;audio&quot;: { &quot;sample_rate&quot;: &quot;R_16KHZ&quot;, &quot;frame_period&quot;: &quot;P_20MS&quot;, &quot;sample_format&quot;: &quot;S16&quot;, &quot;encoding_format&quot;: &quot;OPUS&quot;, &quot;opus_config&quot;: { &quot;complexity&quot;: 10, &quot;bitrate&quot;: 256000 } }, &quot;gps&quot;: { &quot;rate_hz&quot;: 1 }, &quot;ble&quot;: { &quot;period_ms&quot;: 30000, &quot;duration_ms&quot;: 2000 }, &quot;wifi&quot;: { &quot;period_ms&quot;: 30000, &quot;scan_mode&quot;: &quot;ACTIVE&quot; }, &quot;slam_cameras&quot;: { &quot;rate_hz&quot;: 10, &quot;auto_exposure&quot;: {}, &quot;encoding&quot;: &quot;H265&quot;, &quot;video_config&quot;: { &quot;cqp&quot;: { &quot;qp&quot;: 22 } } }, &quot;et_cameras&quot;: { &quot;rate_hz&quot;: 5, &quot;auto_exposure&quot;: {}, &quot;encoding&quot;: &quot;H265&quot;, &quot;video_config&quot;: { &quot;cqp&quot;: { &quot;qp&quot;: 22 } }, &quot;resolution&quot;: &quot;R_200x200&quot;, &quot;ir_led&quot;: &quot;TRUE&quot; }, &quot;rgb_camera&quot;: { &quot;rate_hz&quot;: 5, &quot;auto_exposure&quot;: {}, &quot;encoding&quot;: &quot;H265&quot;, &quot;video_config&quot;: { &quot;cqp&quot;: { &quot;qp&quot;: 22 } }, &quot;resolution&quot;: &quot;R_4032x3024&quot;, &quot;width&quot;: 2560, &quot;height&quot;: 1920 }, &quot;et&quot;: { &quot;rate_hz&quot;: 30 }, &quot;ht&quot;: { &quot;rate_hz&quot;: 10 }, &quot;vio&quot;: { &quot;rate_hz&quot;: 10 }, &quot;device_info&quot;: { &quot;period_ms&quot;: 30000 }, &quot;ppg&quot;: { &quot;rate_hz&quot;: 128 }, &quot;temperature&quot;: { &quot;rate_hz&quot;: 1 }, &quot;als&quot;: { &quot;rate_hz&quot;: 9.434, &quot;exposure_time_us&quot;: 3200 }, &quot;vio_high_frequency_pose&quot;: { &quot;rate_hz&quot;: 800 }, &quot;tags&quot;: [ &quot;&quot; ] }   ","version":"Next","tagName":"h3"},{"title":"Template Notes‚Äã","type":1,"pageTitle":"Project Aria Gen 2 Profiles","url":"/projectaria_tools/gen2/technical-specs/device/profile#template-notes","content":" This example demonstrates a conservative approach to customization: making minimal changes to a tested profile (profile8) to reduce power consumption while maintaining core functionality.    ","version":"Next","tagName":"h3"},{"title":"Best Practices for Custom Profiles‚Äã","type":1,"pageTitle":"Project Aria Gen 2 Profiles","url":"/projectaria_tools/gen2/technical-specs/device/profile#best-practices-for-custom-profiles","content":" If you must use a custom profile, follow these guidelines:  ","version":"Next","tagName":"h2"},{"title":"1. Start from a Pre-defined Profile‚Äã","type":1,"pageTitle":"Project Aria Gen 2 Profiles","url":"/projectaria_tools/gen2/technical-specs/device/profile#1-start-from-a-pre-defined-profile","content":" ‚úÖ Base your custom profile on profile8, profile9, or profile10‚úÖ Make incremental changes rather than dramatic modifications‚ùå Avoid starting with a blank profile or extreme parameter values  ","version":"Next","tagName":"h3"},{"title":"2. Test Thoroughly‚Äã","type":1,"pageTitle":"Project Aria Gen 2 Profiles","url":"/projectaria_tools/gen2/technical-specs/device/profile#2-test-thoroughly","content":" Test the profile with short recordings firstMonitor device temperature during useCheck battery consumption ratesVerify all expected sensor streams are presentValidate data quality before large-scale data collection  ","version":"Next","tagName":"h3"},{"title":"3. Monitor Device Health‚Äã","type":1,"pageTitle":"Project Aria Gen 2 Profiles","url":"/projectaria_tools/gen2/technical-specs/device/profile#3-monitor-device-health","content":" Watch for thermal warnings in the Companion AppNote any unexpected shutdowns or errorsTrack recording duration vs. battery lifeMonitor data completeness and quality  ","version":"Next","tagName":"h3"},{"title":"4. Conservative Parameter Selection‚Äã","type":1,"pageTitle":"Project Aria Gen 2 Profiles","url":"/projectaria_tools/gen2/technical-specs/device/profile#4-conservative-parameter-selection","content":" Frame rates: Lower rates reduce power and thermal loadResolution: Higher resolutions increase data size and processingEncoding quality: Lower QP values (higher quality) increase bitrateSensor combinations: Enabling all sensors simultaneously increases load  ","version":"Next","tagName":"h3"},{"title":"5. Validate Data Compatibility‚Äã","type":1,"pageTitle":"Project Aria Gen 2 Profiles","url":"/projectaria_tools/gen2/technical-specs/device/profile#5-validate-data-compatibility","content":" Test with Project Aria Tools to ensure data can be loadedVerify compatibility with your analysis pipelineCheck that all required sensor streams are accessibleConsider data sharing and reproducibility needs  ","version":"Next","tagName":"h3"},{"title":"6. Document Your Configuration‚Äã","type":1,"pageTitle":"Project Aria Gen 2 Profiles","url":"/projectaria_tools/gen2/technical-specs/device/profile#6-document-your-configuration","content":" Keep detailed records of custom profile parametersDocument the rationale for each modificationNote any issues or limitations encounteredShare configuration details with collaborators    ","version":"Next","tagName":"h3"},{"title":"Parameter Guidelines‚Äã","type":1,"pageTitle":"Project Aria Gen 2 Profiles","url":"/projectaria_tools/gen2/technical-specs/device/profile#parameter-guidelines","content":" When modifying parameters, consider these general guidelines:  Parameter Type\tConsiderationsFrame Rates\tHigher rates = more data, higher power consumption, thermal load Resolution\tHigher resolution = larger files, more processing, better quality Encoding Quality (QP)\tLower QP (e.g., 18) = higher quality, larger files; Higher QP (e.g., 30) = lower quality, smaller files Sensor Combinations\tMore active sensors = higher power draw, more thermal stress Bitrate\tFixed bitrate (CBR) for consistent file sizes; Variable rate (CQP) for consistent quality    ","version":"Next","tagName":"h2"},{"title":"Using Custom Profiles‚Äã","type":1,"pageTitle":"Project Aria Gen 2 Profiles","url":"/projectaria_tools/gen2/technical-specs/device/profile#using-custom-profiles","content":" Custom profiles can be loaded via the Client SDK. See the Client SDK documentation for detailed API usage and examples.    ","version":"Next","tagName":"h2"},{"title":"Support and Troubleshooting‚Äã","type":1,"pageTitle":"Project Aria Gen 2 Profiles","url":"/projectaria_tools/gen2/technical-specs/device/profile#support-and-troubleshooting","content":" ","version":"Next","tagName":"h2"},{"title":"For Pre-defined Profiles‚Äã","type":1,"pageTitle":"Project Aria Gen 2 Profiles","url":"/projectaria_tools/gen2/technical-specs/device/profile#for-pre-defined-profiles","content":" Full support available through standard channelsIssues will be investigated and resolvedDocumentation is comprehensive and maintained  ","version":"Next","tagName":"h3"},{"title":"For Custom Profiles‚Äã","type":1,"pageTitle":"Project Aria Gen 2 Profiles","url":"/projectaria_tools/gen2/technical-specs/device/profile#for-custom-profiles","content":" ‚ö†Ô∏è Limited support availableCommunity forum may provide assistanceUsers are responsible for validating their configurationsNo guarantee of compatibility with tools or services  ","version":"Next","tagName":"h3"},{"title":"Getting Help‚Äã","type":1,"pageTitle":"Project Aria Gen 2 Profiles","url":"/projectaria_tools/gen2/technical-specs/device/profile#getting-help","content":" For questions about profiles:  Pre-defined profiles: Contact support via support channelsCustom profiles: Check the community forum and known issues ","version":"Next","tagName":"h3"},{"title":"MPS Output - SLAM","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/technical-specs/mps/data_formats","content":"","keywords":"","version":"Next"},{"title":"Overview‚Äã","type":1,"pageTitle":"MPS Output - SLAM","url":"/projectaria_tools/gen2/technical-specs/mps/data_formats#overview","content":" This section covers SLAM Machine Perception Services (MPS). SLAM and Multi-SLAM outputs may be part of Open Dataset Releases or Project Aria Partners can request MPS services on their own data.  SLAM outputs are available for all recordings made with CV cameras and IMU enabled. Partner data is not made available to Meta researchers or Meta's affiliates. Go to MPS Data Lifecycle for more details about how partner data is processed and stored.  The following outputs are generated if you request SLAM data using the MPS CLI:  6DoF TrajectorySemi-Dense Point CloudOnline Sensor Calibration ","version":"Next","tagName":"h2"},{"title":"Basics","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/technical-specs/mps/data_formats/basics","content":"","keywords":"","version":"Next"},{"title":"MPS File Structure‚Äã","type":1,"pageTitle":"Basics","url":"/projectaria_tools/gen2/technical-specs/mps/data_formats/basics#mps-file-structure","content":" MPS outputs use the following structure, in this example recording1.vrs was used to generate MPS.   ‚îî‚îÄ‚îÄ Example folder ‚îú‚îÄ‚îÄ mps_recording1_vrs ‚îÇ ‚îú‚îÄ‚îÄ slam ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ closed_loop_trajectory.csv ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ online_calibration.jsonl ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ open_loop_trajectory.csv ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ semidense_observations.csv.gz ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ semidense_points.csv.gz ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ summary.json ‚îÇ ‚îú‚îÄ‚îÄ vrs_health_check.json ‚îî‚îÄ‚îÄ recording1.vrs   mps_[name of VRS file]_vrs Sibling directory where all the intermediate data and MPS output is saved vrs_health_check.json Output of the health check performed on your computer before data is uploadedContains information about data drops in all the sensor streams slam folder Contains outputs after running SLAM (Trajectory and Semi-Dense Point Cloud data)  ","version":"Next","tagName":"h2"},{"title":"Common terminologies‚Äã","type":1,"pageTitle":"Basics","url":"/projectaria_tools/gen2/technical-specs/mps/data_formats/basics#common-terminologies","content":" ","version":"Next","tagName":"h2"},{"title":"graph_uid‚Äã","type":1,"pageTitle":"Basics","url":"/projectaria_tools/gen2/technical-specs/mps/data_formats/basics#graph_uid","content":" graph_uid is a unique identifier for the world coordinate frame. For all the 3D geometric instances like pose and points in the world frames (having _world in the suffix), when they have the same graph_uid, they are in the same coordinate frame.  ","version":"Next","tagName":"h3"},{"title":"tracking_timestamp_us‚Äã","type":1,"pageTitle":"Basics","url":"/projectaria_tools/gen2/technical-specs/mps/data_formats/basics#tracking_timestamp_us","content":" tracking_timestamp_us's values are shaped by whether it is real world or synthetic data.  For real world data, tracking_timestamp_us provides the Device timestamps from your Aria glasses.  In simulation datasets, this will be the timestamp in the simulator.  This clock has arbitrary starting points, which are not synchronized between recording sessions or devices.This clock is strictly monotonic, has stable clock speed, and is accurate in duration If you want to compute the time duration between two timestamps (especially when touching dynamics, e.g. integrating acceleration to velocity over time), you should use this timestamp.  ","version":"Next","tagName":"h3"},{"title":"utc_timestamp_ns‚Äã","type":1,"pageTitle":"Basics","url":"/projectaria_tools/gen2/technical-specs/mps/data_formats/basics#utc_timestamp_ns","content":" utc_timestamp_ns is the timestamp from Aria real-time clock (RTC). This time is synchronized to the cell phone time via the Aria Mobile Companion app to get UTC time at the beginning of the recording which is a rough estimate of the external standard clock.  This clock is not available in the simulation datasets.This clock provides rough synchronization between sessions and devices.This clock is not guaranteed to be monotonic, or have stable clock speed, due to synchronization with NTP. So do not compute duration between two UTC timestamps.  ","version":"Next","tagName":"h3"},{"title":"Operator summary‚Äã","type":1,"pageTitle":"Basics","url":"/projectaria_tools/gen2/technical-specs/mps/data_formats/basics#operator-summary","content":" The operator summary includes individual operator‚Äôs status and whether the operation was successful. There are three possible status flags:  SUCCESS: the operator successfully finished, without known issues.WARNING: The operator finished, but internally it detected problem(s) that may affect results quality. The operator still outputs the results, but we don‚Äôt have confidence in the quality of the results, so consume the results with caution.ERROR: the operator did not finish, finished with major error, or the quality of the results are too bad to be consumed. Results may or may not be generated, and any results should not be consumed.  The summary also provides information about processes as well as any warning or error messages available  Summary JSON output example:   &quot;SLAM&quot;: { &quot;status&quot;: &quot;SUCCESS&quot;, &quot;info&quot;: [ &quot;Recording total time: 1104.00s; Trajectory total length: 155.42m&quot;, &quot;Total Vision Translational Correction (mm): p50: 0.048; p99: 0.451&quot;, &quot;Rotational Correction (deg): p50: 0.001; p99: 0.007&quot; ], &quot;warnings&quot;: [], &quot;errors&quot;: [] }, ...  ","version":"Next","tagName":"h2"},{"title":"MPS Output - Semi-Dense Point Cloud","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/technical-specs/mps/data_formats/mps_pointcloud","content":"","keywords":"","version":"Next"},{"title":"What are semi-dense points?‚Äã","type":1,"pageTitle":"MPS Output - Semi-Dense Point Cloud","url":"/projectaria_tools/gen2/technical-specs/mps/data_formats/mps_pointcloud#what-are-semi-dense-points","content":" Semi-dense points are the 3D points associated with tracks from our semi-dense tracking pipeline. Semi-dense tracks are continually created in pixel locations of input frames that lie in regions of high image gradient, and are then successively tracked in the following frames. Each track is associated with a 3D point, parameterized as an inverse distance along a ray originating from the track's first initial observation, as well as its uncertainty in inverse distance and distance. These points are transformed from their original camera coordinate spaces to the same coordinate frame associated with the closed loop trajectory of the sequence.  ","version":"Next","tagName":"h2"},{"title":"User needs to define how to enforce quality‚Äã","type":1,"pageTitle":"MPS Output - Semi-Dense Point Cloud","url":"/projectaria_tools/gen2/technical-specs/mps/data_formats/mps_pointcloud#user-needs-to-define-how-to-enforce-quality","content":" To support user flexibility the tool outputs the associated points of all tracks regardless of quality. This means the data will contain a number of points whose positions have high uncertainty and are geometrically less accurate.  Users will either need to threshold the point cloud by setting a maximum allowed inverse distance / distance certainty or correctly weight points by their certainty when using them in downstream tasks.  Nominal threshold values are a maximum inv_dist_std of 0.005 and a maximum dist_std of 0.01.  ","version":"Next","tagName":"h3"},{"title":"Points in the world coordinate frame‚Äã","type":1,"pageTitle":"MPS Output - Semi-Dense Point Cloud","url":"/projectaria_tools/gen2/technical-specs/mps/data_formats/mps_pointcloud#points-in-the-world-coordinate-frame","content":" This file is the gzip compressed semi-dense points in the world coordinate system. The world coordinate frame is the same frame of the closed loop trajectory.  Column\tType\tDescriptionuid\tint\tA unique identifier of this point within this map graph_uid\tstring\tUnique identifier of the world coordinate frame. Associated with an equivalent graph_uid found in close_loop_trajectory.csv, depending on the frame this point was first observed in p{x,y,z}_world\tfloat\tPoint location in the world coordinate frame p_world inv_dist_std\tfloat\tStandard deviation of the inverse distance estimate, in meter^-1. Could be used for determining the quality of the 3D point position estimate dist_std\tfloat\tStandard deviation of the distance estimate, in meters. Could be used for determining the quality of the 3D point position estimate  ","version":"Next","tagName":"h2"},{"title":"Point observations‚Äã","type":1,"pageTitle":"MPS Output - Semi-Dense Point Cloud","url":"/projectaria_tools/gen2/technical-specs/mps/data_formats/mps_pointcloud#point-observations","content":" The observation file is the gzip compressed semi-dense 2D observations, described in image pixel 2D coordinate frame.  Column\tType\tDescriptionuid\tint\tA unique identifier integer of this point within this map frame_tracking_timestamp_us\tint\tAria device timestamp of the host frame‚Äôs center of exposure, in microsecond camera_serial\tstring\tThe serial number of the camera which observes this point u\tfloat\tThe sub-pixel-accuracy observed measurement of the point in pixels, in the observing frame‚Äôs camera v\tfloat\tThe sub-pixel-accuracy observed measurement of the point in pixels, in the observing frame‚Äôs camera ","version":"Next","tagName":"h2"},{"title":"MPS Output - Calibration","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/technical-specs/mps/data_formats/mps_calibration","content":"","keywords":"","version":"Next"},{"title":"Online calibration‚Äã","type":1,"pageTitle":"MPS Output - Calibration","url":"/projectaria_tools/gen2/technical-specs/mps/data_formats/mps_calibration#online-calibration","content":" online_calibration.jsonl contains one json online calibration record per line. Each record is a json dict object that contains timestamp metadata and the result of online calibration for the cameras and IMUs.  The calibration parameters contain intrinsics and extrinsics parameters for each sensor as well as a time offsets which best temporally align their data. ","version":"Next","tagName":"h2"},{"title":"MPS output - Trajectory","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/technical-specs/mps/data_formats/mps_trajectory","content":"","keywords":"","version":"Next"},{"title":"Open loop trajectory‚Äã","type":1,"pageTitle":"MPS output - Trajectory","url":"/projectaria_tools/gen2/technical-specs/mps/data_formats/mps_trajectory#open-loop-trajectory","content":" Open loop trajectory is the high frequency (IMU rate, which is 1kHz) odometry estimation output by the visual-inertial odometry (VIO), in an arbitrary odometry coordinate frame. The estimation includes pose and dynamics (translational and angular velocities).  The open loop trajectory has good ‚Äúrelative‚Äù and ‚Äúlocal‚Äù accuracy: the relative transformation between two poses is accurate when the time span between two frames is short (within a few minutes). However, the open loop trajectory has increased drift error accumulated over time spent and travel distance. Consider using closed loop trajectory if you are looking for trajectory without drift error.  For the utility function to load the open loop trajectory in Python, please check the python notebook tutorial  Column\tType\tDescriptiontracking_timestamp_us\tint\tAria device timestamp in microseconds utc_timestamp_ns\tint\tWall clock UTC time in nanoseconds. If not available, the value will be -1 session_uid\tstring\tUnique identifier of the odometry coordinate frame. When the session_uid is the same, poses and velocities are defined in the same coordinate frame {tx,ty,tz,qx,qy,qz,qw}_odometry_device\tfloat\tPose of the device coordinate frame in odometry frame T_odometry_device, include translation (tx, ty, tz) in meters and rotation quaternion (qx, qy, qz, qw) device_linear_velocity_{x,y,z}_odometry\tfloat\tVelocity of device coordinate frame in odometry frame, (x, y, z) in meter/s angular_velocity_{x,y,z}_device\tfloat\tAngular velocity of device coordinate frame in device frame, (x, y, z) in rad/s gravity_{x,y,z}_odometry\tfloat\tEarth gravity vector in odometry frame, (x, y, z) in meter/s^2. This vector is pointing toward the ground, and includes gravitation and centrifugal forces from earth rotation quality_score\tfloat\tA quality score between 0.0 to 1.0. The larger the score is, the higher confidence the estimation has higher quality  ","version":"Next","tagName":"h2"},{"title":"Closed loop trajectory‚Äã","type":1,"pageTitle":"MPS output - Trajectory","url":"/projectaria_tools/gen2/technical-specs/mps/data_formats/mps_trajectory#closed-loop-trajectory","content":" Closed loop trajectory is the high frequency (IMU rate, which is 1kHz) pose estimation output by our mapping process, in an arbitrary gravity aligned world coordinate frame. The estimation includes pose and dynamics (translational and angular velocities).  Closed loop trajectories are fully bundle adjusted with detected loop closures, reducing the VIO drift which is present in the open loop trajectories. However, due to the loop closure correction, the ‚Äúrelative‚Äù and ‚Äúlocal‚Äù trajectory accuracy within a short time span (i.e. seconds) might be worse compared to open loop trajectories.  In some open datasets we also share and use this format for trajectory pose ground truth from simulation or Optitrack, and the files will be called in a different file name aria_gt_trajectory.csv.  For the utility function to load the closed loop trajectory in Python and C++, please check the code examples  Column\tType\tDescriptiongraph_uid\tstring\tUnique identifier of the world coordinate frame tracking_timestamp_us\tint\tAria device timestamp in microsecond utc_timestamp_ns\tint\tWall clock UTC time in nanosecond. If not available, the value will be -1 {tx,ty,tz,qx,qy,qz,qw}_world_device\tfloat\tPose of the device coordinate frame in world frame T_world_device, translation (tx, ty, tz) in meters and rotation quaternion (qx, qy, qz, qw) device_linear_velocity_{x,y,z}_device\tfloat\tVelocity of device coordinate frame in device frame, (x, y, z) in meter/s angular_velocity_{x,y,z}_device\tfloat\tAngular velocity of device coordinate frame in device frame, (x, y, z) in rad/s gravity_{x,y,z}_world\tfloat\tGravity vector (x, y, z) in the world frame, in meter/s^2. MPS output will all have fixed value` [0, 0, -9.81]‚Äô, while other source (e.g. simulation or Optitrack ground truth) may give different values quality_score\tfloat\tA quality score between 0.0 to 1.0. The larger the score is, the higher confidence the estimation has higher quality` ","version":"Next","tagName":"h2"},{"title":"Project Aria VRS Data Format","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/technical-specs/vrs/data-format","content":"","keywords":"","version":"Next"},{"title":"Aria data streams‚Äã","type":1,"pageTitle":"Project Aria VRS Data Format","url":"/projectaria_tools/gen2/technical-specs/vrs/data-format#aria-data-streams","content":" In VRS, data is organized by streams, each storing data measured by a specific sensor, or calculated from an on-device machine perception algorithm.  The VRS streams are uniquely identified by their StreamId, each consisting of a RecordableTypeId to categorize the type of the stream data, and an InstanceId for identifying the specific instance of the sensor. For example, the first SLAM camera is identified with StreamId = 1201-1, where 1201 is the numerical ID for SLAM camera data type, and -1 identifies the first of all the SLAM cameras.  For convenience, we also provide a short label for each stream within our library projectaria-tools, which provides a human-readable way to access the data. See this page to learn more.  ","version":"Next","tagName":"h2"},{"title":"Aria sensor data and configuration‚Äã","type":1,"pageTitle":"Project Aria VRS Data Format","url":"/projectaria_tools/gen2/technical-specs/vrs/data-format#aria-sensor-data-and-configuration","content":" Sensor data includes:  Sensor readoutTimestampsAcquisition parameters (exposure and gain settings)Conditions (e.g. temperature) during data collection  Most sensor data of a single stream and at a specific timestamp is stored as a single piece, except for image and audio.  ","version":"Next","tagName":"h2"},{"title":"How data is stored for image recordings‚Äã","type":1,"pageTitle":"Project Aria VRS Data Format","url":"/projectaria_tools/gen2/technical-specs/vrs/data-format#how-data-is-stored-for-image-recordings","content":" Each camera stores a single image frame at a time.The image frame contains two parts, the image itself and the image record. The image record stores timestamps, frame id, and acquisition parameters, such as exposure and gain. This avoids having to read image data to get the information in the record.  ","version":"Next","tagName":"h3"},{"title":"How data is stored for audio recordings‚Äã","type":1,"pageTitle":"Project Aria VRS Data Format","url":"/projectaria_tools/gen2/technical-specs/vrs/data-format#how-data-is-stored-for-audio-recordings","content":" The audio data is grouped into data chunks of 4096 audio samples from all microphones.Each chunk contains two parts, the data part for the audio signal, and the report part for the timestamps of each audio signal.  ","version":"Next","tagName":"h3"},{"title":"Sensor configuration blob‚Äã","type":1,"pageTitle":"Project Aria VRS Data Format","url":"/projectaria_tools/gen2/technical-specs/vrs/data-format#sensor-configuration-blob","content":" The sensor configuration blob stores the static information of a stream. Common sensor configuration stores information, such as sensor model, sensor serial (if available) as well as frame rate. Stream-specific information, such as image resolution, is also stored in configurations.  Go to this python tutorial to learn how to access and use the sensor data using Python data utilities. Go to the source code for the detailed implementation of sensor data and configurations. ","version":"Next","tagName":"h3"},{"title":"Useful VRS Tools","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/technical-specs/vrs/useful-tools","content":"Useful VRS Tools The most intuitive way to access Aria VRS files is via the tools (and APIs) shipped in projectaria-tools library, which includes tools to visualize Aria VRS, convert to MP4, and export to other data format.Users can inspect the data quality of Aria VRS file using VrsHealthCheck, which is a tool that we specially tailored for Aria VRS recordings.Users can also choose to use the native tool provided by the VRS library to perform some simple actions on the VRS file, including inspecting basic data information, extracting to images or audio files, etc. But please note that the VRS library needs to be built with H.265 decoding support in order to properly handle Aria Gen2 VRS files.","keywords":"","version":"Next"},{"title":"VRS Stream ID to Label Mapping in Aria Data","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/technical-specs/vrs/streamid-label-mapper","content":"VRS Stream ID to Label Mapping in Aria Data In Aria VRS files, each data stream is uniquely identified by a VRS StreamId. This page provides the mapping information between all the VRS StreamIds, and the corresponding readable sensor labels used in device calibration and projectaria_tools APIs. Table 1: StreamID to label mapping for Aria Gen2 Sensor Stream ID Recordable Type ID label ET camera left 211-1 EyeCameraRecordableClass camera-et-left ET camera right 211-2 EyeCameraRecordableClass camera-et-right RGB camera 214-1 RgbCameraRecordableClass camera-rgb All microphones 231-1 StereoAudioRecordableClass mic Temperature 246-1 TemperatureRecordableClass temperature Barometer 247-1 BarometerRecordableClass baro0 Photoplethysmography sensor 248-1 PpgRecordableClass ppg GPS (sensor) 281-2 GpsRecordableClass gps GPS (from Companion App) 281-1 GpsRecordableClass gps-app Wi-Fi 282-1 WifiBeaconRecordableClass wps Bluetooth 283-1 BluetoothBeaconRecordableClass\tbluetooth Ambient light sensor 500-1 AlsRecordableClass als SLAM camera front left 1201-1 SlamCameraData slam-front-left SLAM camera front right\t1201-2 SlamCameraData slam-front-right SLAM camera side left 1201-3 SlamCameraData slam-side-left SLAM camera side right 1201-4 SlamCameraData slam-side-right IMU left 1202-1 SlamImuData imu-left IMU right 1202-2 SlamImuData imu-right Magnetometer 1203-1 SlamMagnetometerData mag0 Eye gaze (on-device MP) 373-1 EyeGazeRecordableClass eyegaze Hand tracking (on-device MP) 371-* (dynamic) PoseRecordableClass handtracking VIO (on-device MP) 371-* (dynamic) PoseRecordableClass vio VIO high frequency (on-device MP)\t371-* (dynamic) PoseRecordableClass vio_high_frequency Dynamic Stream IDs The handtracking, vio, and vio_high_frequency streams have dynamic stream IDs that are determined at runtime by querying the VRS file. Case 1: If handtracking stream exists VRS Stream ID\tSensor Label371-1\thandtracking 371-2\tvio 371-3\tvio_high_frequency Case 2: If handtracking stream does not exist VRS Stream ID\tSensor Label371-1\tvio 371-2\tvio_high_frequency These streams are identified by their RecordableTypeId (PoseRecordableClass) and their specific flavor strings: device/oatmeal/hand, device/oatmeal/vio, and device/oatmeal/vio_high_frequency.","keywords":"","version":"Next"},{"title":"MPS Command Line Interface","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/technical-specs/mps/mps_cli_guide","content":"","keywords":"","version":"Next"},{"title":"Overview‚Äã","type":1,"pageTitle":"MPS Command Line Interface","url":"/projectaria_tools/gen2/technical-specs/mps/mps_cli_guide#overview","content":" The Project Aria MPS CLI Guide provides detailed information about how to use this tool. The guide contain:  About the MPS CLIHow the MPS CLI works Go to MPS Data Lifecycle for more details about how sequences are processed on the server MPS CLI SettingsMPS CLI Command Line Reference    ","version":"Next","tagName":"h2"},{"title":"About‚Äã","type":1,"pageTitle":"MPS Command Line Interface","url":"/projectaria_tools/gen2/technical-specs/mps/mps_cli_guide#about","content":" The Project Aria MPS Command Line Interface (MPS CLI), part of Project Aria Tools, is the preferred way to request Machine Perception Services (MPS). MPS CLI supports both Aria Gen1 and Aria Gen2 devices.  The MPS CLI has two modes:  Single Process each recording individuallyThe input can be a file and/or directory, so you can batch process multiple recordings with a single commandOutput is always saved next to the input fileThe most common way to request MPS Multi Not yet supported for Aria Gen2  ","version":"Next","tagName":"h2"},{"title":"How the MPS CLI works‚Äã","type":1,"pageTitle":"MPS Command Line Interface","url":"/projectaria_tools/gen2/technical-specs/mps/mps_cli_guide#how-the-mps-cli-works","content":" The MPS CLI enables you to upload VRS files from your computer to the Meta servers for processing. The outputs are then saved to your local directory. The MPS CLI will try to process multiple recordings concurrently. The concurrency for various stages can be controlled via Settings.    Once you submit your request, the MPS CLI for the selected mode will open and show the status of your requests. See the Getting Started or the Command Line Reference below for how to submit a request. Once authenticated, the request tool checks with the server to see if this recording was previously processed. We use unique IDs (Hashing stage) to check if this is a new or a previously known recording.If it is a known recording, we skip processing and directly download the results (Download Results stage) or show the error code.If this is a new recording, we run health checks on the recording (HealthCheck stage), to minimize the chances that it will fail during processing. While this check catches obvious errors, like gaps in data and ensures presence of the right sensor streams, but server side processing may still fail.If the health check passes, the recording is encrypted on your machine (Encrypting stage).After encryption, the recordings are uploaded to the MPS servers (Uploading stage) for processing. Uploads are resumable.Interrupted uploads can be resumed within 24 hours. Data is processed on MPS servers (Processing stage). The MPS CLI periodically checks the MPS request's status on the server.It is safe to close the MPS Request tool once the data is processing. When the MPS CLI is reopened, it will check the status of your data and progress to Downloading if it is ready.  If you get an error code The server re-attempts processing multiple times before it stops and provides an error message Check Error Codes to see what the error was. In the MPS CLI, you can click (Cmd + click on Mac, Ctrl + click on Linux) on the error code to reach the troubleshooting page.We encourage you to send a bug report with log files to Aria User Support if it is not an error code 1xx. By default, logs are stored in /tmp/logs/projectaria/mps/.  Once the processing is complete, and the tool is open, outputs are automatically downloaded (Downloading stage).Recordings in the MPS CLI UI will show the Success status once the outputs been successfully downloaded.  ","version":"Next","tagName":"h2"},{"title":"Logs‚Äã","type":1,"pageTitle":"MPS Command Line Interface","url":"/projectaria_tools/gen2/technical-specs/mps/mps_cli_guide#logs","content":" Each run will write the console logs to a log file on the local drive. Since the MPS CLI can run concurrently on multiple recordings, these logs are useful for debugging purposes. The logs are named by the current time when the request was initiated via CLI.  By default, logs are stored in /tmp/logs/projectaria/mps/. The location can be modified in settings.     ","version":"Next","tagName":"h3"},{"title":"CLI Settings‚Äã","type":1,"pageTitle":"MPS Command Line Interface","url":"/projectaria_tools/gen2/technical-specs/mps/mps_cli_guide#cli-settings","content":" Project Aria MPS CLI settings can be customized via the mps.ini file. This file is located in the $HOME/.projectaria/mps.ini  Setting\tDescription\tDefault Value General settings log_dir\tWhere log files are saved for each run. The filename is the timestamp from when the request tool started running.\t/tmp/logs/projectaria/mps/ status_check_interval\tHow long the MPS CLI waits to check the status of data during the Processing stage.\t30 secs HASH concurrent_hashes\tMaximum number of files that can be concurrently hashed\t4 chunk_size\tChunk size to use for hashing\t10MB Encryption chunk_size\tChunk size to use for encryption\t50MB concurrent_encryptions\tMaximum number of files that can be concurrently encrypted\t4 delete_encrypted_files\tWhether to delete the encrypted files after upload is done. If you set this to false local disk usage will double, due to an encrypted copy of each file.\tTrue. Health Check concurrent_health_checks\tMaximum number of VRS file healthchecks that can be run concurrently\t2 Uploads backoff\tThe exponential back off factor for retries during failed uploads. The wait time between successive retries will increase with this factor.\t1.5 interval\tBase delay between retries.\t20 secs retries\tMaximum number of retries before giving up.\t10 concurrent_uploads\tMaximum number of concurrent uploads.\t4 max_chunk_size\tMaximum chunk size that can be used during uploads.\t100 MB min_chunk_size\tThe minimum upload chunk size.\t5 MB smoothing_window_size\tSize of the smoothing window to adjust the chunk size. This value defines the number of uploaded chunks that will be used to determine the next chunk size.\t10 target_chunk_upload_secs\tTarget time to upload a single chunk. If the chunks in a smoothing window take longer, we reduce the chunk size. If it takes less time, we increase the chunk size.\t3 secs Query the MPS backend for MPS Status backoff\tThis the exponential back off factor for retries for failed queries. The wait time between successive retries will increase with this factor\t1.5 interval\tBase delay between retries\t4 secs retries\tMaximum number of retries before giving up\t3 Download backoff\tThis the exponential back off factor for retries during failed downloads. The wait time between successive retries will increase with this factor.\t1.5 interval\tBase delay between retries\t20 secs retries\tMaximum number of retries before giving up\t10 chunk_size\tThe chunk size to use for downloads\t10MB concurrent_downloads\tNumber of concurrent downloads\t10 delete_zip\tThe server will send the results in a zip file. This flag controls whether to delete the zip file after extraction or not\tTrue     ","version":"Next","tagName":"h2"},{"title":"Command line reference‚Äã","type":1,"pageTitle":"MPS Command Line Interface","url":"/projectaria_tools/gen2/technical-specs/mps/mps_cli_guide#command-line-reference","content":" The MPS CLI has two distinct modes: single (process each recording individually) and multi (SLAM outputs for multiple recordings in a shared co-ordinate frame). The latest version of the MPS CLI only supports single mode for Aria Gen2 and both single and multi modes for Aria Gen1. Multi mode is not yet supported for Aria Gen2.  aria_mps single &lt;options&gt;   or  aria_mps multi &lt;options&gt;   ","version":"Next","tagName":"h2"},{"title":"Help‚Äã","type":1,"pageTitle":"MPS Command Line Interface","url":"/projectaria_tools/gen2/technical-specs/mps/mps_cli_guide#help","content":" To see the available options and subcommands, use:  --help   or  -h   ","version":"Next","tagName":"h3"},{"title":"Authentication‚Äã","type":1,"pageTitle":"MPS Command Line Interface","url":"/projectaria_tools/gen2/technical-specs/mps/mps_cli_guide#authentication","content":" Log in‚Äã  The first time you use the MPS CLI, you‚Äôll be prompted to enter your username. Use the Project Aria credentials you use to sign into the Mobile Companion app. Follow the directions to either enter your password or complete the authentication via the https://work.meta.com/cli link, as directed by the CLI.  To log in using the legacy Aria credentials, use the following command:  -u USERNAME -p PASSWORD   or  --username USERNAME --password PASSWORD   Note This will only work for legacy Aria accounts.  If you have a legacy Aria credentials, you can also supply the username and password via CLI input. This option is more suited for running MPS as part of a batch script or other automated workflows.  If you have a new Aria account, that requires you to authenticate via the https://work.meta.com/cli link, you will have to use the UI in the CLI to complete the authentication. Once you have authenticated, the auth token is cached on the machine and you can run MPS CLI as part of a script or other automated workflows.  Token storage‚Äã  If keychain is accessible, the login token is saved there automatically. This means that once you‚Äôve authenticated, you can run the MPS CLI without having to re-enter your username and password.  If you request MPS using --no-ui , you'll have the option to pass --no-save-token. This means the token won't be saved. Once processing is complete the MPS CLI will also logout and invalidate any existing tokens.  Log out‚Äã  Use the following command to log out the authentication token and delete it from the system. Next time you run the CLI, it will ask for username and password again.  aria_mps logout   ","version":"Next","tagName":"h3"},{"title":"Request options for any mode‚Äã","type":1,"pageTitle":"MPS Command Line Interface","url":"/projectaria_tools/gen2/technical-specs/mps/mps_cli_guide#request-options-for-any-mode","content":" These options are shared between both modes.  Define input path‚Äã  Provides the path for a directory or file that will be uploaded for processing. Where a directory is provided, all subdirectories will be scanned for VRS files. At least one input file must be provided. There is no limit of how many files or folders can be included in a single request.  -i INPUT   or  --input INPUT   Force the provided files to be reprocessed‚Äã  Force the server to reprocess all of the provided files, regardless of their current state on the server.  --force   Automatically retry processing if it fails‚Äã  By default the MPS server will retry processing data multiple times before generating a failure code. By adding this flag requests automatically retries again if the processing fails. This command is generally only worth using if you‚Äôve done some debugging to warrant it.  --retry-failed   note If you retry 30 days after the recording was uploaded, you'll also need to re-upload the data.  Don‚Äôt show the UI‚Äã  Instead of the MPS CLI UI, you‚Äôll see the raw outputs and processes in the command line.  --no-ui   note You will need to pass username and password via CLI input if you use this option or you should have already authenticated via the UI.  ","version":"Next","tagName":"h3"},{"title":"Single Recording mode‚Äã","type":1,"pageTitle":"MPS Command Line Interface","url":"/projectaria_tools/gen2/technical-specs/mps/mps_cli_guide#single-recording-mode","content":" Select the MPS you wish to generate‚Äã  By default, SLAM MPS is generated and that is the only supported service for Aria gen2 at this time. In the future, we will support more MPS services.  --features {SLAM}  ","version":"Next","tagName":"h3"},{"title":"Camera Intrinsic Models for Project Aria devices","type":0,"sectionRef":"#","url":"/projectaria_tools/gen2/technical-specs/device/calibration_insights/camera_intrinsics_models","content":"","keywords":"","version":"Next"},{"title":"The linear camera model‚Äã","type":1,"pageTitle":"Camera Intrinsic Models for Project Aria devices","url":"/projectaria_tools/gen2/technical-specs/device/calibration_insights/camera_intrinsics_models#the-linear-camera-model","content":" The linear camera model (a.k.a pinhole model) is parametrized by 4 coefficients : f_x, f_y, c_x, c_y.  (fx,fy)(f_x, f_y)(fx‚Äã,fy‚Äã) are the focal lengths, and cx,cyc_x, c_ycx‚Äã,cy‚Äã are the coordinate of the projection of the optical axis. It maps from world point (x,y,z)(x,y,z)(x,y,z) to 2D camera pixel p=(u,v)\\mathbf{p}=(u, v)p=(u,v) with the following formulae.  u=fxx/z+cxv=fyy/z+cy u = f_x x/z + c_x \\\\ v = f_y y/z + c_yu=fx‚Äãx/z+cx‚Äãv=fy‚Äãy/z+cy‚Äã  Or, in polar coordinates:  u=fxtan(Œ∏)cos‚Å°(œÜ)+cx,v=fytan(Œ∏)sin‚Å°(œÜ)+cy. u = f_x tan(\\theta) \\cos(\\varphi) + c_x, \\\\ v = f_y tan(\\theta) \\sin(\\varphi) + c_y.u=fx‚Äãtan(Œ∏)cos(œÜ)+cx‚Äã,v=fy‚Äãtan(Œ∏)sin(œÜ)+cy‚Äã.  Inversely, we can unproject from 2D camera pixel p=(u,v)\\mathbf{p}=(u, v)p=(u,v) to the homogeneous coordinate of the world point by  x/z=(u‚àícx)/fx,y/z=(v‚àícy)/fy.x/z=(u-c_x)/f_x, \\\\ y/z=(v-c_y)/f_y.x/z=(u‚àícx‚Äã)/fx‚Äã,y/z=(v‚àícy‚Äã)/fy‚Äã.  The linear camera model preserves linearity in 3D space, thus straight lines in the real world are supposed to look straight under the linear camera model.  ","version":"Next","tagName":"h2"},{"title":"The spherical camera model‚Äã","type":1,"pageTitle":"Camera Intrinsic Models for Project Aria devices","url":"/projectaria_tools/gen2/technical-specs/device/calibration_insights/camera_intrinsics_models#the-spherical-camera-model","content":" The spherical camera model is, similarly from the linear camera model parametrized by 4 coefficients : f_x, f_y, c_x, c_y. The pixel coordinates are linear to solid angles rather than the homography coordinate system. The projection function can be written in polar coordinates  u=fxŒ∏cos‚Å°(œÜ)+cx,v=fyŒ∏sin‚Å°(œÜ)+cy. u = f_x \\theta \\cos(\\varphi) + c_x, \\\\ v = f_y \\theta \\sin(\\varphi) + c_y.u=fx‚ÄãŒ∏cos(œÜ)+cx‚Äã,v=fy‚ÄãŒ∏sin(œÜ)+cy‚Äã.  Note the difference from the linear camera model ‚Äî under spherical projection, 3D straight lines look curved in images.  Inversely, we can unproject from 2D camera pixel p=(u,v)\\mathbf{p}=(u, v)p=(u,v) to the homogeneous coordinate of the world point by  Œ∏=(u‚àícx)2/fx2+(v‚àícy)2/fy2,œÜ=arctan‚Å°((u‚àícx)/fx,(v‚àícy)/fy). \\theta = \\sqrt{(u - c_x)^2/f_x^2 + (v - c_y)^2/f_y^2}, \\\\ \\varphi = \\arctan((u - c_x)/f_x, (v - c_y)/f_y).Œ∏=(u‚àícx‚Äã)2/fx2‚Äã+(v‚àícy‚Äã)2/fy2‚Äã‚Äã,œÜ=arctan((u‚àícx‚Äã)/fx‚Äã,(v‚àícy‚Äã)/fy‚Äã).  ","version":"Next","tagName":"h2"},{"title":"The KannalaBrandtK3 (KB3) model‚Äã","type":1,"pageTitle":"Camera Intrinsic Models for Project Aria devices","url":"/projectaria_tools/gen2/technical-specs/device/calibration_insights/camera_intrinsics_models#the-kannalabrandtk3-kb3-model","content":" The KannalaBrandtK3 model adds radial distortion to the linear model  u=fxr(Œ∏)cos‚Å°(œÜ)+cx,v=fyr(Œ∏)sin‚Å°(œÜ)+cy. u = f_x r(\\theta) \\cos(\\varphi) + c_x, \\quad v = f_y r(\\theta) \\sin(\\varphi) + c_y.u=fx‚Äãr(Œ∏)cos(œÜ)+cx‚Äã,v=fy‚Äãr(Œ∏)sin(œÜ)+cy‚Äã.  where  r(Œ∏)=Œ∏+k0Œ∏3+k1Œ∏5+k2Œ∏7+k3Œ∏9+... r(\\theta) = \\theta + k_0 \\theta^3 + k_1 \\theta^5 + k_2 \\theta^7 + k_3 \\theta^9 + ...r(Œ∏)=Œ∏+k0‚ÄãŒ∏3+k1‚ÄãŒ∏5+k2‚ÄãŒ∏7+k3‚ÄãŒ∏9+...  In KannalaBrandtK3 model we use a 9-th order polynomial with four radial distortion parameters k0,...k3k_0, ... k_3k0‚Äã,...k3‚Äã.  To unproject from camera pixel (u,v)(u, v)(u,v) to the world point (Œ∏,œÜ)(\\theta, \\varphi)(Œ∏,œÜ), we first compute  œÜ=arctan‚Å°((u‚àícx)/fx,(v‚àícy)/fy)r(Œ∏)=(u‚àícx)2/fx2+(v‚àícy)2/fy2 \\varphi = \\arctan((u - c_x)/f_x, (v - c_y)/f_y) \\\\ r(\\theta) = \\sqrt{(u - c_x)^2/f_x^2 + (v - c_y)^2/f_y^2}œÜ=arctan((u‚àícx‚Äã)/fx‚Äã,(v‚àícy‚Äã)/fy‚Äã)r(Œ∏)=(u‚àícx‚Äã)2/fx2‚Äã+(v‚àícy‚Äã)2/fy2‚Äã‚Äã  Then we use Newton method to inverse the function r(Œ∏)r(\\theta)r(Œ∏) to compute Œ∏\\thetaŒ∏. See the code here.  ","version":"Next","tagName":"h2"},{"title":"The Fisheye62 model‚Äã","type":1,"pageTitle":"Camera Intrinsic Models for Project Aria devices","url":"/projectaria_tools/gen2/technical-specs/device/calibration_insights/camera_intrinsics_models#the-fisheye62-model","content":" The Fisheye62 model adds tangential distortion on top of the KB3 model parametrized by two new coefficients: p_0 p_1.  u=fx.(ur+tx(ur,vr))+cx,v=fy.(vr+ty(ur,vr))+cy. u = f_x . (u_r + t_x(u_r, v_r)) + c_x, \\\\ v = f_y . (v_r + t_y(u_r, v_r)) + c_y.u=fx‚Äã.(ur‚Äã+tx‚Äã(ur‚Äã,vr‚Äã))+cx‚Äã,v=fy‚Äã.(vr‚Äã+ty‚Äã(ur‚Äã,vr‚Äã))+cy‚Äã.  where  ur=r(Œ∏)cos‚Å°(œÜ),vr=r(Œ∏)sin‚Å°(œÜ). u_r = r(\\theta) \\cos(\\varphi), \\\\ v_r = r(\\theta) \\sin(\\varphi).ur‚Äã=r(Œ∏)cos(œÜ),vr‚Äã=r(Œ∏)sin(œÜ).  and  tx(ur,vr)=p0(2ur2+r(Œ∏)2)+2p1urvr,ty(ur,vr)=p1(2vr2+r(Œ∏)2)+2p0urvr. t_x(u_r, v_r) = p_0(2 u_r^2 + r(\\theta)^2) + 2p_1u_rv_r, \\\\ t_y(u_r, v_r) = p_1(2 v_r^2 + r(\\theta)^2) + 2p_0u_rv_r.tx‚Äã(ur‚Äã,vr‚Äã)=p0‚Äã(2ur2‚Äã+r(Œ∏)2)+2p1‚Äãur‚Äãvr‚Äã,ty‚Äã(ur‚Äã,vr‚Äã)=p1‚Äã(2vr2‚Äã+r(Œ∏)2)+2p0‚Äãur‚Äãvr‚Äã.  To unproject from camera pixel (u,v)(u, v)(u,v) to the world point (Œ∏,œÜ)(\\theta, \\varphi)(Œ∏,œÜ), we first use Newton method to compute uru_rur‚Äã and vrv_rvr‚Äã from (u‚àícx)/fx(u - c_x)/f_x(u‚àícx‚Äã)/fx‚Äã and (v‚àícy)/fy(v - cy)/f_y(v‚àícy)/fy‚Äã, and then compute (Œ∏,œÜ)(\\theta, \\varphi)(Œ∏,œÜ) using the above KB3 unproject method.  ","version":"Next","tagName":"h2"},{"title":"The FisheyeRadTanThinPrism (Fisheye624) model‚Äã","type":1,"pageTitle":"Camera Intrinsic Models for Project Aria devices","url":"/projectaria_tools/gen2/technical-specs/device/calibration_insights/camera_intrinsics_models#the-fisheyeradtanthinprism-fisheye624-model","content":" The FisheyeRadTanThinPrism (also called Fisheye624 in file and codebase) models thin-prism distortion (noted tptptp) on top of the Fisheye62 model above. Its parametrization contains 4 additional coefficients: s_0 s_1 s_2 s_3. The projection function writes:  u=fx‚ãÖ(ur+tx(ur,vr)+tpx(ur,vr))+cx,v=fy‚ãÖ(vr+ty(ur,vr)+tpy(ur,vr))+cy. u = f_x \\cdot (u_r + t_x(u_r, v_r) + tp_x(u_r, v_r)) + c_x, \\\\ v = f_y \\cdot (v_r + t_y(u_r, v_r) + tp_y(u_r, v_r)) + c_y.u=fx‚Äã‚ãÖ(ur‚Äã+tx‚Äã(ur‚Äã,vr‚Äã)+tpx‚Äã(ur‚Äã,vr‚Äã))+cx‚Äã,v=fy‚Äã‚ãÖ(vr‚Äã+ty‚Äã(ur‚Äã,vr‚Äã)+tpy‚Äã(ur‚Äã,vr‚Äã))+cy‚Äã.  u_r, v_r, t_x, t_y are defined as in the Fisheye62 model, while tpxtp_xtpx‚Äã and tpytp_ytpy‚Äã are defined as:  tpx(ur,vr)=s0r(Œ∏)2+s1r(Œ∏)4,tpy(ur,vr)=s2r(Œ∏)2+s3r(Œ∏)4. tp_x(u_r, v_r) = s_0 r(\\theta)^2 + s_1 r(\\theta)^4, \\\\ tp_y(u_r, v_r) = s_2 r(\\theta)^2 + s_3 r(\\theta)^4.tpx‚Äã(ur‚Äã,vr‚Äã)=s0‚Äãr(Œ∏)2+s1‚Äãr(Œ∏)4,tpy‚Äã(ur‚Äã,vr‚Äã)=s2‚Äãr(Œ∏)2+s3‚Äãr(Œ∏)4.  To unproject from camera pixel (u,v)(u, v)(u,v) to the world point (Œ∏,œÜ)(\\theta, \\varphi)(Œ∏,œÜ), we first use Newton method to compute uru_rur‚Äã and vrv_rvr‚Äã from (u‚àícx)/fx(u - c_x)/f_x(u‚àícx‚Äã)/fx‚Äã and (v‚àícy)/fy(v - cy)/f_y(v‚àícy)/fy‚Äã, and then compute (Œ∏,œÜ)(\\theta, \\varphi)(Œ∏,œÜ) using the above KB3 unproject method.  Note that in practice, in our codebase and calibration file we assume fxf_xfx‚Äã and fyf_yfy‚Äã are equal. ","version":"Next","tagName":"h2"}]