"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[4461],{42919(e,a,r){r.r(a),r.d(a,{assets:()=>d,contentTitle:()=>n,default:()=>h,frontMatter:()=>o,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"vrs/data-format","title":"VRS Data Format","description":"Project Aria data is stored in VRS, which is optimized to record and playback streams of multi-modal sensor data, such as images, audio, and other discrete sensors (IMU, temperature, etc.).","source":"@site/docs-technical-specs/vrs/data-format.mdx","sourceDirName":"vrs","slug":"/vrs/data-format","permalink":"/projectaria_tools/gen2/technical-specs/vrs/data-format","draft":false,"unlisted":false,"editUrl":"https://www.internalfb.com/code/fbsource/arvr/projects/ariane/aria_research_kit/projectaria_tools/website/docs-technical-specs/vrs/data-format.mdx","tags":[],"version":"current","sidebarPosition":0,"frontMatter":{"sidebar_position":0,"title":"VRS Data Format"},"sidebar":"technicalSpecsSidebar","previous":{"title":"CAD File Downloads","permalink":"/projectaria_tools/gen2/technical-specs/device/cad"},"next":{"title":"VRS Stream ID to Label Mapping in Aria Data","permalink":"/projectaria_tools/gen2/technical-specs/vrs/streamid-label-mapper"}}');var t=r(74848),s=r(28453);const o={sidebar_position:0,title:"VRS Data Format"},n="Project Aria VRS Data Format",d={},c=[{value:"Aria data streams",id:"aria-data-streams",level:2},{value:"Aria sensor data and configuration",id:"aria-sensor-data-and-configuration",level:2},{value:"How data is stored for image recordings",id:"how-data-is-stored-for-image-recordings",level:3},{value:"How data is stored for audio recordings",id:"how-data-is-stored-for-audio-recordings",level:3},{value:"Sensor configuration blob",id:"sensor-configuration-blob",level:3}];function l(e){const a={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(a.header,{children:(0,t.jsx)(a.h1,{id:"project-aria-vrs-data-format",children:"Project Aria VRS Data Format"})}),"\n",(0,t.jsxs)(a.p,{children:["Project Aria data is stored in ",(0,t.jsx)(a.a,{href:"https://facebookresearch.github.io/vrs/",children:"VRS"}),", which is optimized to record and playback streams of multi-modal sensor data, such as images, audio, and other discrete sensors (IMU, temperature, etc.).\nIn this page, we will introduce some basic concepts of ",(0,t.jsx)(a.code,{children:"VRS"})," file format in Aria recordings.\nUsers are referred to the official ",(0,t.jsxs)(a.a,{href:"https://facebookresearch.github.io/vrs/",children:[(0,t.jsx)(a.code,{children:"VRS"})," repo documentation page"]})," for details."]}),"\n",(0,t.jsx)(a.h2,{id:"aria-data-streams",children:"Aria data streams"}),"\n",(0,t.jsx)(a.p,{children:"In VRS, data is organized by streams, each storing data measured by a specific sensor, or calculated from an on-device machine perception algorithm."}),"\n",(0,t.jsxs)(a.p,{children:["The VRS streams are uniquely identified by their ",(0,t.jsx)(a.code,{children:"StreamId"}),", each consisting of a ",(0,t.jsx)(a.code,{children:"RecordableTypeId"})," to categorize the type of the stream data, and an ",(0,t.jsx)(a.code,{children:"InstanceId"})," for identifying the specific instance of the sensor.\nFor example, the first SLAM camera is identified with ",(0,t.jsx)(a.code,{children:"StreamId = 1201-1"}),", where ",(0,t.jsx)(a.code,{children:"1201"})," is the numerical ID for SLAM camera data type, and ",(0,t.jsx)(a.code,{children:"-1"})," identifies the first of all the SLAM cameras."]}),"\n",(0,t.jsxs)(a.p,{children:["For convenience, we also provide a short label for each stream within our library ",(0,t.jsx)(a.code,{children:"projectaria-tools"}),", which provides a human-readable way to access the data.\nSee ",(0,t.jsx)(a.a,{href:"/projectaria_tools/gen2/technical-specs/vrs/streamid-label-mapper",children:"this page"})," to learn more."]}),"\n",(0,t.jsx)(a.h2,{id:"aria-sensor-data-and-configuration",children:"Aria sensor data and configuration"}),"\n",(0,t.jsx)(a.p,{children:"Sensor data includes:"}),"\n",(0,t.jsxs)(a.ul,{children:["\n",(0,t.jsx)(a.li,{children:"Sensor readout"}),"\n",(0,t.jsx)(a.li,{children:"Timestamps"}),"\n",(0,t.jsx)(a.li,{children:"Acquisition parameters (exposure and gain settings)"}),"\n",(0,t.jsx)(a.li,{children:"Conditions (e.g. temperature) during data collection"}),"\n"]}),"\n",(0,t.jsx)(a.p,{children:"Most sensor data of a single stream and at a specific timestamp is stored as a single piece, except for image and audio."}),"\n",(0,t.jsx)(a.h3,{id:"how-data-is-stored-for-image-recordings",children:"How data is stored for image recordings"}),"\n",(0,t.jsxs)(a.ul,{children:["\n",(0,t.jsx)(a.li,{children:"Each camera stores a single image frame at a time."}),"\n",(0,t.jsxs)(a.li,{children:["The image frame contains two parts, the image itself and the image record.","\n",(0,t.jsxs)(a.ul,{children:["\n",(0,t.jsx)(a.li,{children:"The image record stores timestamps, frame id, and acquisition parameters, such as exposure and gain. This avoids having to read image data to get the information in the record."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(a.h3,{id:"how-data-is-stored-for-audio-recordings",children:"How data is stored for audio recordings"}),"\n",(0,t.jsxs)(a.ul,{children:["\n",(0,t.jsx)(a.li,{children:"The audio data is grouped into data chunks of 4096 audio samples from all microphones."}),"\n",(0,t.jsx)(a.li,{children:"Each chunk contains two parts, the data part for the audio signal, and the report part for the timestamps of each audio signal."}),"\n"]}),"\n",(0,t.jsx)(a.h3,{id:"sensor-configuration-blob",children:"Sensor configuration blob"}),"\n",(0,t.jsx)(a.p,{children:"The sensor configuration blob stores the static information of a stream. Common sensor configuration stores information, such as sensor model, sensor serial (if available) as well as frame rate.\nStream-specific information, such as image resolution, is also stored in configurations."}),"\n",(0,t.jsxs)(a.p,{children:["Go to this ",(0,t.jsx)(a.a,{href:"/research-tools/projectariatools/pythontutorials/queue",children:"python tutorial"})," to learn how to access and use the sensor data using Python data utilities.\nGo to the ",(0,t.jsx)(a.a,{href:"https://github.com/facebookresearch/projectaria_tools/blob/main/core/data_provider/players",children:"source code"})," for the detailed implementation of sensor data and configurations."]})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,t.jsx)(a,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}},28453(e,a,r){r.d(a,{R:()=>o,x:()=>n});var i=r(96540);const t={},s=i.createContext(t);function o(e){const a=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(a):{...a,...e}},[a,e])}function n(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),i.createElement(s.Provider,{value:a},e.children)}}}]);