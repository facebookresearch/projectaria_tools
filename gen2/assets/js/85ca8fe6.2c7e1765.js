"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[3841],{83080(e,t,i){i.r(t),i.d(t,{assets:()=>d,contentTitle:()=>l,default:()=>_,frontMatter:()=>o,metadata:()=>n,toc:()=>m});const n=JSON.parse('{"id":"projectariatools/pythontutorials/time-sync","title":"Timestamp Alignment in Aria Gen2","description":"<TutorialButtons","source":"@site/docs-research-tools/projectariatools/pythontutorials/time-sync.mdx","sourceDirName":"projectariatools/pythontutorials","slug":"/projectariatools/pythontutorials/time-sync","permalink":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/time-sync","draft":false,"unlisted":false,"editUrl":"https://www.internalfb.com/code/fbsource/arvr/projects/ariane/aria_research_kit/projectaria_tools/website/docs-research-tools/projectariatools/pythontutorials/time-sync.mdx","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"sidebar_position":6,"title":"Timestamp Alignment in Aria Gen2"},"sidebar":"researchToolsSidebar","previous":{"title":"Using On-Device VIO Data","permalink":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/vio"},"next":{"title":"Loading and Visualizing MPS Output Data","permalink":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/mps"}}');var s=i(74848),r=i(28453),a=i(69470);const o={sidebar_position:6,title:"Timestamp Alignment in Aria Gen2"},l="Tutorial 6: Device Time Alignment in Aria Gen2",d={},m=[{value:"Introduction",id:"introduction",level:2},{value:"Single-Device Timestamp alignment",id:"single-device-timestamp-alignment",level:2},{value:"Supported Time Domains",id:"supported-time-domains",level:3},{value:"Data API to query by timestamp",id:"data-api-to-query-by-timestamp",level:3},{value:"Boundary Behavior",id:"boundary-behavior",level:3},{value:"Single VRS Timestamp-Based Query Example",id:"single-vrs-timestamp-based-query-example",level:3},{value:"Visualizing Synchronized Multi-sensor Data",id:"visualizing-synchronized-multi-sensor-data",level:3},{value:"Multi-Device Timestamp alignment",id:"multi-device-timestamp-alignment",level:2},{value:"Timestamp Converter APIs",id:"timestamp-converter-apis",level:3},{value:"Multi-Device Query APIs",id:"multi-device-query-apis",level:3}];function c(e){const t={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.header,{children:(0,s.jsx)(t.h1,{id:"tutorial-6-device-time-alignment-in-aria-gen2",children:"Tutorial 6: Device Time Alignment in Aria Gen2"})}),"\n",(0,s.jsx)(a.A,{notebookUrl:"https://github.com/facebookresearch/projectaria_tools/blob/2.1.0/examples/Gen2/python_notebooks/Tutorial_6_timestamp_alignment_in_aria_gen2.ipynb",colabUrl:"https://colab.research.google.com/github/facebookresearch/projectaria_tools/blob/2.1.0/examples/Gen2/python_notebooks/Tutorial_6_timestamp_alignment_in_aria_gen2.ipynb"}),"\n",(0,s.jsx)(t.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsxs)(t.p,{children:["In Project Aria glasses, one of the key features is that it provides multi-sensor data that are ",(0,s.jsx)(t.strong,{children:"temporally aligned"})," to a shared, device-time domain for each single device, and also provide ",(0,s.jsx)(t.strong,{children:"multi-device Time Alignment"})," using SubGHz signals (Aria Gen2), TICSync (Aria Gen1), or TimeCode signals (Aria Gen1). In this tutorial, we will demonstrate how to use such temporal aligned data from Aria Gen2 recordings."]}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.strong,{children:"What you'll learn:"})}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"How to access temporally aligned sensor data on a single VRS recording."}),"\n",(0,s.jsx)(t.li,{children:"How to access temporally aligned sensor data across multiple recordings using SubGHz signals."}),"\n"]}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.strong,{children:"Prerequisites"})}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"Complete Tutorial 1 (VrsDataProvider Basics) to understand basic data provider concepts"}),"\n",(0,s.jsx)(t.li,{children:"Complete Tutorial 3 (Sequential Access multi-sensor data) to understand how to create a queue of sensor data from VRS file."}),"\n",(0,s.jsxs)(t.li,{children:["Download Aria Gen2 sample data: ",(0,s.jsx)(t.a,{href:"https://www.projectaria.com/async/sample/download/?bucket=core&filename=aria_gen2_sample_data_host_1.vrs",children:"host recording"})," and ",(0,s.jsx)(t.a,{href:"https://www.projectaria.com/async/sample/download/?bucket=core&filename=aria_gen2_sample_data_client_1.vrs",children:"client recording"})]}),"\n"]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Note on visualization:"}),"\nIf visualization window is not showing up, this is due to ",(0,s.jsx)(t.code,{children:"Rerun"})," lib's caching issue. Just rerun the specific code cell, or restart the Python kernel."]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'from projectaria_tools.core import data_provider\n\n# Load local VRS file\nvrs_file_path = "path/to/your/recording.vrs"\nvrs_data_provider = data_provider.create_vrs_data_provider(vrs_file_path)\n'})}),"\n",(0,s.jsx)(t.h2,{id:"single-device-timestamp-alignment",children:"Single-Device Timestamp alignment"}),"\n",(0,s.jsxs)(t.p,{children:["In ",(0,s.jsx)(t.code,{children:"projectaria_tools"}),", every timestamp is linked to a specific ",(0,s.jsx)(t.code,{children:"TimeDomain"}),", which represents the time reference or clock used to generate that timestamp. Timestamps from different ",(0,s.jsx)(t.code,{children:"TimeDomain"}),"s are not directly comparable\u2014only timestamps within the same ",(0,s.jsx)(t.code,{children:"TimeDomain"})," are consistent and can be accurately compared or aligned."]}),"\n",(0,s.jsx)(t.h3,{id:"supported-time-domains",children:"Supported Time Domains"}),"\n",(0,s.jsxs)(t.blockquote,{children:["\n",(0,s.jsx)(t.p,{children:(0,s.jsxs)(t.strong,{children:["Important: Use ",(0,s.jsx)(t.code,{children:"DEVICE_TIME"})," for single-device Aria data analysis"]})}),"\n"]}),"\n",(0,s.jsxs)(t.p,{children:["The following table shows all supported time domains in ",(0,s.jsx)(t.code,{children:"projectaria_tools"}),". ",(0,s.jsxs)(t.strong,{children:["For single-device Aria data analysis, use ",(0,s.jsx)(t.code,{children:"DEVICE_TIME"})," for accurate temporal alignment between sensors."]})]}),"\n",(0,s.jsxs)(t.table,{children:[(0,s.jsx)(t.thead,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.th,{children:"Time Domain"}),(0,s.jsx)(t.th,{children:"Description"}),(0,s.jsx)(t.th,{children:"Usage"})]})}),(0,s.jsxs)(t.tbody,{children:[(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.strong,{children:"DEVICE_TIME (Recommended)"})}),(0,s.jsx)(t.td,{children:"Capture time in device's time domain. Accurate and reliable. All sensors on the same Aria device share the same device time domain."}),(0,s.jsx)(t.td,{children:(0,s.jsx)(t.strong,{children:"Use this for single-device Aria data analysis"})})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.strong,{children:"RECORD_TIME"})}),(0,s.jsx)(t.td,{children:"Timestamps stored in the index of VRS files. For Aria glasses, these are equal to device timestamp converted to double-precision floating point."}),(0,s.jsx)(t.td,{children:"Fast access, but use DEVICE_TIME for accuracy"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.strong,{children:"HOST_TIME"})}),(0,s.jsx)(t.td,{children:"Timestamps when sensor data is saved to the device (not when captured)."}),(0,s.jsx)(t.td,{children:"Should not be needed for any purpose"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.strong,{children:"--- Time Domains for Multi-device time alignment ---"})}),(0,s.jsx)(t.td,{}),(0,s.jsx)(t.td,{})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"SUBGHZ"}),(0,s.jsx)(t.td,{children:"Multi-device time alignment option for Aria Gen2"}),(0,s.jsx)(t.td,{children:"See next part in this tutorial"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"UTC"}),(0,s.jsx)(t.td,{children:"Multi-device time alignment option"}),(0,s.jsx)(t.td,{children:"See next part in this tutorial"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"TIME_CODE"}),(0,s.jsx)(t.td,{children:"Multi-device time alignment option for Aria Gen1"}),(0,s.jsxs)(t.td,{children:["See ",(0,s.jsx)(t.a,{href:"https://github.com/facebookresearch/projectaria_tools/blob/2.1.0/examples/Gen1/python_notebooks/ticsync_tutorial.ipynb",children:"Gen1 multi-device tutorial"})]})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"TIC_SYNC"}),(0,s.jsx)(t.td,{children:"Multi-device time alignment option for Aria Gen1"}),(0,s.jsxs)(t.td,{children:["See ",(0,s.jsx)(t.a,{href:"https://github.com/facebookresearch/projectaria_tools/blob/2.1.0/examples/Gen1/python_notebooks/ticsync_tutorial.ipynb",children:"Gen1 multi-device tutorial"})]})]})]})]}),"\n",(0,s.jsx)(t.h3,{id:"data-api-to-query-by-timestamp",children:"Data API to query by timestamp"}),"\n",(0,s.jsxs)(t.p,{children:["The VRS data provider offers powerful timestamp-based data access through the ",(0,s.jsx)(t.code,{children:"get_<SENSOR>_data_by_time_ns()"})," API family. This is the recommended approach for temporal alignment across sensors and precise timestamp-based data retrieval."]}),"\n",(0,s.jsxs)(t.p,{children:["For any sensor type, you can query data by timestamp using the ",(0,s.jsx)(t.code,{children:"get_<SENSOR>_data_by_time_ns()"})," function, where ",(0,s.jsx)(t.code,{children:"<SENSOR>"})," can be replaced by any sensor data type available in Aria VRS. See the ",(0,s.jsx)(t.a,{href:"https://github.com/facebookresearch/projectaria_tools/blob/2.1.0/core/data_provider/VrsDataProvider.h",children:"VrsDataProvider.h"})," for a complete list of supported sensor types."]}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.strong,{children:"TimeQueryOptions"})}),"\n",(0,s.jsxs)(t.p,{children:["This ",(0,s.jsx)(t.code,{children:"TimeQueryOptions"})," parameter controls how the system finds data when your query timestamp doesn't exactly match a recorded timestamp:"]}),"\n",(0,s.jsxs)(t.table,{children:[(0,s.jsx)(t.thead,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.th,{children:"Option"}),(0,s.jsx)(t.th,{children:"Behavior"}),(0,s.jsx)(t.th,{children:"Use Case"})]})}),(0,s.jsxs)(t.tbody,{children:[(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.strong,{children:"BEFORE"})}),(0,s.jsxs)(t.td,{children:["Returns the last valid data with ",(0,s.jsx)(t.code,{children:"timestamp \u2264 query_time"})]}),(0,s.jsxs)(t.td,{children:[(0,s.jsx)(t.strong,{children:"Default and most common"})," - Get the most recent data before or at the query time"]})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.strong,{children:"AFTER"})}),(0,s.jsxs)(t.td,{children:["Returns the first valid data with ",(0,s.jsx)(t.code,{children:"timestamp \u2265 query_time"})]}),(0,s.jsx)(t.td,{children:"Get the next available data after or at the query time"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.strong,{children:"CLOSEST"})}),(0,s.jsx)(t.td,{children:"Returns data with smallest `"}),(0,s.jsx)(t.td,{children:"timestamp - query_time"})]})]})]}),"\n",(0,s.jsx)(t.h3,{id:"boundary-behavior",children:"Boundary Behavior"}),"\n",(0,s.jsx)(t.p,{children:"The API handles edge cases automatically:"}),"\n",(0,s.jsxs)(t.table,{children:[(0,s.jsx)(t.thead,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.th,{children:"Query Condition"}),(0,s.jsx)(t.th,{children:"BEFORE"}),(0,s.jsx)(t.th,{children:"AFTER"}),(0,s.jsx)(t.th,{children:"CLOSEST"})]})}),(0,s.jsxs)(t.tbody,{children:[(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"query_time < first_timestamp"})}),(0,s.jsx)(t.td,{children:"Returns invalid data"}),(0,s.jsx)(t.td,{children:"Returns first data"}),(0,s.jsx)(t.td,{children:"Returns first data"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"first_timestamp \u2264 query_time \u2264 last_timestamp"})}),(0,s.jsxs)(t.td,{children:["Returns data with ",(0,s.jsx)(t.code,{children:"timestamp \u2264 query_time"})]}),(0,s.jsxs)(t.td,{children:["Returns data with ",(0,s.jsx)(t.code,{children:"timestamp \u2265 query_time"})]}),(0,s.jsx)(t.td,{children:"Returns temporally closest data"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:(0,s.jsx)(t.code,{children:"query_time > last_timestamp"})}),(0,s.jsx)(t.td,{children:"Returns last data"}),(0,s.jsx)(t.td,{children:"Returns invalid data"}),(0,s.jsx)(t.td,{children:"Returns last data"})]})]})]}),"\n",(0,s.jsx)(t.h3,{id:"single-vrs-timestamp-based-query-example",children:"Single VRS Timestamp-Based Query Example"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'from projectaria_tools.core.sensor_data import SensorDataType, TimeDomain, TimeQueryOptions\n\nprint("=== Single VRS timestamp based query ===")\n\n# Select RGB stream ID\nrgb_stream_id = vrs_data_provider.get_stream_id_from_label("camera-rgb")\n\n# Get a timestamp within the recording (3 seconds after start)\nstart_timestamp_ns = vrs_data_provider.get_first_time_ns(rgb_stream_id, TimeDomain.DEVICE_TIME)\nselected_timestamp_ns = start_timestamp_ns + int(3e9)\n\n# Fetch the RGB frame that is CLOSEST to this selected timestamp_ns\nclosest_rgb_data_and_record = vrs_data_provider.get_image_data_by_time_ns(\n    stream_id = rgb_stream_id,\n    time_ns = selected_timestamp_ns,\n    time_domain = TimeDomain.DEVICE_TIME,\n    time_query_options = TimeQueryOptions.CLOSEST\n)\nclosest_timestamp_ns = closest_rgb_data_and_record[1].capture_timestamp_ns\nclosest_frame_number = closest_rgb_data_and_record[1].frame_number\nprint(f" The closest RGB frame to query timestamp {selected_timestamp_ns} is the {closest_frame_number}-th frame, with capture timestamp of {closest_timestamp_ns}")\n\n# Fetch the frame BEFORE this frame\nprev_rgb_data_and_record = vrs_data_provider.get_image_data_by_time_ns(\n    stream_id = rgb_stream_id,\n    time_ns = closest_timestamp_ns - 1,\n    time_domain = TimeDomain.DEVICE_TIME,\n    time_query_options = TimeQueryOptions.BEFORE\n)\nprev_timestamp_ns = prev_rgb_data_and_record[1].capture_timestamp_ns\nprev_frame_number = prev_rgb_data_and_record[1].frame_number\nprint(f" The previous RGB frame is the {prev_frame_number}-th frame, with capture timestamp of {prev_timestamp_ns}")\n\n# Fetch the frame AFTER this frame\nnext_rgb_data_and_record = vrs_data_provider.get_image_data_by_time_ns(\n    stream_id = rgb_stream_id,\n    time_ns = closest_timestamp_ns + 1,\n    time_domain = TimeDomain.DEVICE_TIME,\n    time_query_options = TimeQueryOptions.AFTER\n)\nnext_timestamp_ns = next_rgb_data_and_record[1].capture_timestamp_ns\nnext_frame_number = next_rgb_data_and_record[1].frame_number\nprint(f" The next RGB frame is the {next_frame_number}-th frame, with capture timestamp of {next_timestamp_ns}")\n'})}),"\n",(0,s.jsx)(t.h3,{id:"visualizing-synchronized-multi-sensor-data",children:"Visualizing Synchronized Multi-sensor Data"}),"\n",(0,s.jsx)(t.p,{children:'In this additional example, we demonstrate how to query and visualize "groups" of RGB + SLAM images approximately at the same timestamp.'}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'import rerun as rr\n\nprint("=== Single VRS timestamp-based query visualization examples ===")\nrr.init("rerun_viz_single_vrs_timestamp_based_query")\n\n# Select RGB and SLAM stream IDs to visualize\nall_labels = vrs_data_provider.get_device_calibration().get_camera_labels()\nslam_labels = [label for label in all_labels if "slam" in label ]\nslam_stream_ids = [vrs_data_provider.get_stream_id_from_label(label) for label in slam_labels]\nrgb_stream_id = vrs_data_provider.get_stream_id_from_label("camera-rgb")\n\n# Starting from +3 seconds into the recording, and at 5Hz frequency\ntarget_period_ns = int(2e8)\nstart_timestamp_ns = vrs_data_provider.get_first_time_ns(rgb_stream_id, TimeDomain.DEVICE_TIME) + int(3e9)\n\n# Plot 20 samples\ncurrent_timestamp_ns = start_timestamp_ns\nfor frame_i in range(20):\n    # Query and plot RGB image\n    rgb_image_data, rgb_image_record = vrs_data_provider.get_image_data_by_time_ns(\n        stream_id = rgb_stream_id,\n        time_ns = current_timestamp_ns,\n        time_domain = TimeDomain.DEVICE_TIME,\n        time_query_options = TimeQueryOptions.CLOSEST)\n    rr.set_time_nanos("device_time", rgb_image_record.capture_timestamp_ns)\n    rr.log("rgb_image", rr.Image(rgb_image_data.to_numpy_array()))\n\n    # Query and plot SLAM images\n    for slam_i in range(len(slam_labels)):\n        single_slam_label = slam_labels[slam_i]\n        single_slam_stream_id = slam_stream_ids[slam_i]\n\n        slam_image_data, slam_image_record = vrs_data_provider.get_image_data_by_time_ns(\n            stream_id = single_slam_stream_id,\n            time_ns = current_timestamp_ns,\n            time_domain = TimeDomain.DEVICE_TIME,\n            time_query_options = TimeQueryOptions.CLOSEST)\n        rr.set_time_nanos("device_time", slam_image_record.capture_timestamp_ns)\n        rr.log(single_slam_label, rr.Image(slam_image_data.to_numpy_array()))\n\n    # Increment query timestamp\n    current_timestamp_ns += target_period_ns\n\nrr.notebook_show()\n'})}),"\n",(0,s.jsx)(t.h2,{id:"multi-device-timestamp-alignment",children:"Multi-Device Timestamp alignment"}),"\n",(0,s.jsx)(t.p,{children:"While recording, multiple Aria-Gen2 glasses can enable a feature that allows their timestamps to be mapped across devices using SubGHz signals. Please refer to the multi-device recording wiki page from ARK (TODO: add link) to learn how to record with this feature."}),"\n",(0,s.jsxs)(t.p,{children:["Basically, one pair of glasses acts as the ",(0,s.jsx)(t.strong,{children:"host"})," device, that actively broadcasts SubGHz signals to a specified channel;\nall other glasses act as ",(0,s.jsx)(t.strong,{children:"client"})," devices, that receives the SubGHz signals, and record a new ",(0,s.jsx)(t.code,{children:"Time Domain Mapping"})," data streams in their VRS file.\nIt is essentially a timestamp hash mapping from ",(0,s.jsx)(t.strong,{children:"host"})," ",(0,s.jsx)(t.code,{children:"DEVICE_TIME"})," -> ",(0,s.jsx)(t.strong,{children:"client"})," ",(0,s.jsx)(t.code,{children:"DEVICE_TIME"}),".\nTherefore this mapping data stream ",(0,s.jsx)(t.strong,{children:"only exists in client VRS"}),", but not ",(0,s.jsx)(t.strong,{children:"host VRS"}),"."]}),"\n",(0,s.jsxs)(t.p,{children:["In ",(0,s.jsx)(t.code,{children:"projectaria_tools"}),", we provide 2 types of APIs to easily perform timestamp-based query across multi-device recordings:"]}),"\n",(0,s.jsxs)(t.ol,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Converter APIs"})," provides direct convert functions that maps timestamps between any 2 ",(0,s.jsx)(t.code,{children:"TimeDomain"}),"."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Query APIs"})," that allows users to specifies ",(0,s.jsx)(t.code,{children:"time_domain = TimeDomain.SUBGHZ"}),' in a client VRS, to query "from timestamp of the host".']}),"\n"]}),"\n",(0,s.jsx)(t.p,{children:"The following code shows examples of using each type of API.\nNote that in the visualization example, the host and client windows will play intermittently.\nThis is expected and correct, because the host and client devices' RGB cameras are NOT trigger aligned by nature."}),"\n",(0,s.jsx)(t.h3,{id:"timestamp-converter-apis",children:"Timestamp Converter APIs"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'import rerun as rr\nfrom projectaria_tools.core.sensor_data import (\n    SensorData,\n    ImageData,\n    TimeDomain,\n    TimeQueryOptions,\n    TimeSyncMode,\n)\n\n# Create data providers for both host and client recordings\nhost_recording = "path/to/host.vrs"\nhost_data_provider = data_provider.create_vrs_data_provider(host_recording)\n\nclient_recording = "path/to/client.vrs"\nclient_data_provider = data_provider.create_vrs_data_provider(client_recording)\n\nprint("======= Multi-VRS time mapping example: Timestamp converter APIs ======")\n\n# Because host and client recordings may start at different times,\n# we manually pick a timestamp in the middle of the host recording.\n# Note that for host, we always use DEVICE_TIME domain.\nselected_timestamp_host = (host_data_provider.get_first_time_ns_all_streams(time_domain = TimeDomain.DEVICE_TIME) +\n    host_data_provider.get_last_time_ns_all_streams(time_domain = TimeDomain.DEVICE_TIME)) // 2\n\n# Convert from host time to client time\nselected_timestamp_client = client_data_provider.convert_from_synctime_to_device_time_ns(selected_timestamp_host, TimeSyncMode.SUBGHZ)\n\n# Convert from client time back to host time. Note that there could be some small numerical differences compared\nselected_timestamp_host_roundtrip = client_data_provider.convert_from_device_time_to_synctime_ns(selected_timestamp_client, TimeSyncMode.SUBGHZ)\n\nprint(f" Selected host timestamp is {selected_timestamp_host}; ")\nprint(f" Converted to client timestamp is {selected_timestamp_client}; ")\nprint(f" Then roundtrip convert back to host:{selected_timestamp_host_roundtrip}, "\n     f" And delta value from original host timestamp is {selected_timestamp_host_roundtrip - selected_timestamp_host}. This is mainly due to numerical errors. ")\n'})}),"\n",(0,s.jsx)(t.h3,{id:"multi-device-query-apis",children:"Multi-Device Query APIs"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'print("======= Multi-VRS time mapping example: Query APIs ======")\nrr.init("rerun_viz_multi_vrs_time_mapping")\n\n# Set up sensor queue options in host VRS, only turn on RGB stream\nhost_deliver_options = host_data_provider.get_default_deliver_queued_options()\nhost_deliver_options.deactivate_stream_all()\nrgb_stream_id = host_data_provider.get_stream_id_from_label("camera-rgb")\nhost_deliver_options.activate_stream(rgb_stream_id)\n\n# Select only a segment to plot\nhost_vrs_start_timestamp = host_data_provider.get_first_time_ns_all_streams(time_domain = TimeDomain.DEVICE_TIME)\nhost_segment_start = host_vrs_start_timestamp + int(20e9) # 20 seconds after start\nhost_segment_duration = int(5e9)\nhost_segment_end = host_segment_start + host_segment_duration\nhost_vrs_end_timestamp = host_data_provider.get_last_time_ns_all_streams(time_domain = TimeDomain.DEVICE_TIME)\nhost_deliver_options.set_truncate_first_device_time_ns(host_segment_start - host_vrs_start_timestamp)\nhost_deliver_options.set_truncate_last_device_time_ns(host_vrs_end_timestamp - host_segment_end)\n\n# Plot RGB image data from both host and client\nfor sensor_data in host_data_provider.deliver_queued_sensor_data(host_deliver_options):\n    # ---------\n    # Plotting in host.\n    # Everything is done in DEVICE_TIME domain.\n    # ---------\n    host_image_data, host_image_record = sensor_data.image_data_and_record()\n\n    # Set timestamps directly from host image record\n    host_timestamp_ns = host_image_record.capture_timestamp_ns\n    rr.set_time_nanos("device_time", host_timestamp_ns)\n\n    rr.log("rgb_image_in_host", rr.Image(host_image_data.to_numpy_array()))\n\n    # ---------\n    # Plotting in client.\n    # All the query APIs are done in SUBGHZ domain.\n    # ---------\n    # Query the closest RGB image from client VRS\n    client_image_data, client_image_record = client_data_provider.get_image_data_by_time_ns(\n        stream_id = rgb_stream_id,\n        time_ns = host_timestamp_ns,\n        time_domain = TimeDomain.SUBGHZ,\n        time_query_options = TimeQueryOptions.CLOSEST)\n\n    # Still need to convert client\'s device time back to host\'s time,\n    # because we want to log this image data on host\'s timeline in Rerun\n    client_timestamp_ns = client_image_record.capture_timestamp_ns\n    converted_client_timestamp_ns = client_data_provider.convert_from_device_time_to_synctime_ns(client_timestamp_ns, TimeSyncMode.SUBGHZ)\n    rr.set_time_nanos("device_time", converted_client_timestamp_ns)\n\n    # Plot client image\n    rr.log("rgb_image_in_client", rr.Image(client_image_data.to_numpy_array()))\n\nrr.notebook_show()\n'})})]})}function _(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},69470(e,t,i){i.d(t,{A:()=>s});i(96540);var n=i(74848);const s=({notebookUrl:e,colabUrl:t,colabDisabled:i=!1})=>{const s=()=>(0,n.jsx)("svg",{style:{marginRight:"8px",width:"16px",height:"16px"},fill:"currentColor",viewBox:"0 0 24 24",children:(0,n.jsx)("path",{d:"M16.9414 4.9757a7.033 7.033 0 0 0-4.9308 2.0324 7.033 7.033 0 0 0-.1232 9.8068l2.395-2.395a3.6455 3.6455 0 0 1 5.1497-5.1478l2.397-2.3989a7.033 7.033 0 0 0-4.8877-1.9175zM7.07 4.9855a7.033 7.033 0 0 0-4.8878 1.9175l2.395 2.3989a3.6434 3.6434 0 0 1 5.1497 5.1478l2.395 2.395a7.033 7.033 0 0 0-.1232-9.8068A7.033 7.033 0 0 0 7.07 4.9855zm15.0191 2.5923l-2.395 2.395a3.6455 3.6455 0 0 1-5.1497 5.1497l-2.395 2.395a7.033 7.033 0 0 0 9.9397-.123z"})});return(0,n.jsxs)("div",{style:{display:"flex",gap:"10px",marginBottom:"20px",flexWrap:"wrap"},children:[(0,n.jsxs)("a",{href:e,target:"_blank",rel:"noopener noreferrer",style:{display:"inline-flex",alignItems:"center",padding:"8px 16px",backgroundColor:"#f6f8fa",border:"1px solid #d0d7de",borderRadius:"6px",textDecoration:"none",color:"#24292f",fontSize:"14px",fontWeight:"500",transition:"background-color 0.2s"},onMouseEnter:e=>{e.target.style.backgroundColor="#f3f4f6"},onMouseLeave:e=>{e.target.style.backgroundColor="#f6f8fa"},children:[(0,n.jsx)("svg",{style:{marginRight:"8px",width:"16px",height:"16px"},fill:"currentColor",viewBox:"0 0 16 16",children:(0,n.jsx)("path",{d:"M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z"})}),"View Notebook on GitHub"]}),!i&&t?(0,n.jsxs)("a",{href:t,target:"_blank",rel:"noopener noreferrer",style:{display:"inline-flex",alignItems:"center",padding:"8px 16px",backgroundColor:"#f9ab00",border:"1px solid #d0d7de",borderRadius:"6px",textDecoration:"none",color:"#fff",fontSize:"14px",fontWeight:"500",transition:"background-color 0.2s"},onMouseEnter:e=>{e.target.style.backgroundColor="#e8a500"},onMouseLeave:e=>{e.target.style.backgroundColor="#f9ab00"},children:[(0,n.jsx)(s,{}),"Run in Google Colab"]}):(0,n.jsxs)("button",{onClick:()=>{alert("Google Colab integration is not available yet. Please download the notebook and run it locally.")},style:{display:"inline-flex",alignItems:"center",padding:"8px 16px",backgroundColor:"#f6f8fa",border:"1px solid #d0d7de",borderRadius:"6px",color:"#656d76",fontSize:"14px",fontWeight:"500",cursor:"not-allowed",transition:"background-color 0.2s"},children:[(0,n.jsx)(s,{}),"Colab (Coming Soon)"]})]})}},28453(e,t,i){i.d(t,{R:()=>a,x:()=>o});var n=i(96540);const s={},r=n.createContext(s);function a(e){const t=n.useContext(r);return n.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),n.createElement(r.Provider,{value:t},e.children)}}}]);