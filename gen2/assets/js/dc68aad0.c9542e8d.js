"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[3840],{28453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>s});var t=i(96540);const r={},a=t.createContext(r);function o(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),t.createElement(a.Provider,{value:n},e.children)}},69470:(e,n,i)=>{i.d(n,{A:()=>r});i(96540);var t=i(74848);const r=({notebookUrl:e,colabDisabled:n=!1})=>(0,t.jsxs)("div",{style:{display:"flex",gap:"10px",marginBottom:"20px",flexWrap:"wrap"},children:[(0,t.jsxs)("a",{href:e,target:"_blank",rel:"noopener noreferrer",style:{display:"inline-flex",alignItems:"center",padding:"8px 16px",backgroundColor:"#f6f8fa",border:"1px solid #d0d7de",borderRadius:"6px",textDecoration:"none",color:"#24292f",fontSize:"14px",fontWeight:"500",transition:"background-color 0.2s"},onMouseEnter:e=>{e.target.style.backgroundColor="#f3f4f6"},onMouseLeave:e=>{e.target.style.backgroundColor="#f6f8fa"},children:[(0,t.jsx)("svg",{style:{marginRight:"8px",width:"16px",height:"16px"},fill:"currentColor",viewBox:"0 0 16 16",children:(0,t.jsx)("path",{d:"M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z"})}),"View Notebook on GitHub"]}),(0,t.jsxs)("button",{onClick:()=>{alert("Google Colab integration is not available yet. Please download the notebook and run it locally.")},style:{display:"inline-flex",alignItems:"center",padding:"8px 16px",backgroundColor:n?"#f6f8fa":"#f9ab00",border:"1px solid #d0d7de",borderRadius:"6px",color:n?"#656d76":"#fff",fontSize:"14px",fontWeight:"500",cursor:n?"not-allowed":"pointer",transition:"background-color 0.2s"},onMouseEnter:e=>{n||(e.target.style.backgroundColor="#e8a500")},onMouseLeave:e=>{n||(e.target.style.backgroundColor="#f9ab00")},children:[(0,t.jsx)("svg",{style:{marginRight:"8px",width:"16px",height:"16px"},fill:"currentColor",viewBox:"0 0 24 24",children:(0,t.jsx)("path",{d:"M16.9414 4.9757a7.033 7.033 0 0 0-4.9308 2.0324 7.033 7.033 0 0 0-.1232 9.8068l2.395-2.395a3.6455 3.6455 0 0 1 5.1497-5.1478l2.397-2.3989a7.033 7.033 0 0 0-4.8877-1.9175zM7.07 4.9855a7.033 7.033 0 0 0-4.8878 1.9175l2.395 2.3989a3.6434 3.6434 0 0 1 5.1497 5.1478l2.395 2.395a7.033 7.033 0 0 0-.1232-9.8068A7.033 7.033 0 0 0 7.07 4.9855zm15.0191 2.5923l-2.395 2.395a3.6455 3.6455 0 0 1-5.1497 5.1497l-2.395 2.395a7.033 7.033 0 0 0 9.9397-.123z"})}),n?"Colab (Coming Soon)":"Run in Google Colab"]})]})},75514:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>d,default:()=>h,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"projectariatools/pythontutorials/vio","title":"Using On-Device VIO Data","description":"<TutorialButtons","source":"@site/docs-research-tools/projectariatools/pythontutorials/vio.mdx","sourceDirName":"projectariatools/pythontutorials","slug":"/projectariatools/pythontutorials/vio","permalink":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/vio","draft":false,"unlisted":false,"editUrl":"https://www.internalfb.com/code/fbsource/arvr/projects/ariane/aria_research_kit/projectaria_tools/website/docs-research-tools/projectariatools/pythontutorials/vio.mdx","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5,"title":"Using On-Device VIO Data"},"sidebar":"researchToolsSidebar","previous":{"title":"Using On-Device Eye-tracking and Hand-tracking","permalink":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/eyetracking-handtracking"},"next":{"title":"Timestamp Alignment in Aria Gen2","permalink":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/time-sync"}}');var r=i(74848),a=i(28453),o=i(69470);const s={sidebar_position:5,title:"Using On-Device VIO Data"},d="Tutorial 5: On-Device VIO data streams",l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"On-Device VIO Data Stream",id:"on-device-vio-data-stream",level:2},{value:"Data Type: <code>FrontendOutput</code>",id:"data-type-frontendoutput",level:3},{value:"Data Access API",id:"data-access-api",level:3},{value:"On-Device VIO High Frequency Data Stream",id:"on-device-vio-high-frequency-data-stream",level:2},{value:"Data Type: <code>OpenLoopTrajectoryPose</code>",id:"data-type-openlooptrajectorypose",level:3},{value:"Visualizing On-Device VIO trajectory",id:"visualizing-on-device-vio-trajectory",level:2},{value:"3D Trajectory Visualization",id:"3d-trajectory-visualization",level:3}];function _(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"tutorial-5-on-device-vio-data-streams",children:"Tutorial 5: On-Device VIO data streams"})}),"\n",(0,r.jsx)(o.A,{notebookUrl:"https://github.com/facebookresearch/projectaria_tools/blob/main/examples/Gen2/python_notebooks/Tutorial_5_on_device_vio.ipynb",colabDisabled:!0}),"\n",(0,r.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,r.jsx)(n.p,{children:"In Aria-Gen2 glasses, one of the key upgrade from Aria-Gen1 is the capability to run Machine Perception (MP) algorithms on the device during streaming / recording. Currently supported on-device MP algorithms include Eye-tracking, Hand-tracking, and VIO. These algorithm results are stored as separate data streams in the VRS file."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"VIO (Visual Inertial Odometry)"})," combines camera images and IMU (Inertial Measurement Unit) data to estimate device pose and motion in real-time. VIO tracks the device's position, orientation, and velocity by performing visual tracking, IMU integration, sensor fusion, etc, making it the foundation for spatial tracking and understanding."]}),"\n",(0,r.jsxs)(n.p,{children:["In Aria-Gen2 devices, the VIO algorithm are run on device to produce 2 types of tracking results as part of the VRS file: VIO and VIO High Frequency.\nThis tutorial focuses on demonstration of how to use the ",(0,r.jsx)(n.strong,{children:"on-device VIO and VIO_high_frequency"})," results."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"What you'll learn:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"How to access on-device VIO and VIO_high_frequency data from VRS files"}),"\n",(0,r.jsx)(n.li,{children:"How to visualize 3D trajectory from on-device VIO data."}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Prerequisites"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Complete Tutorial 1 (VrsDataProvider Basics) to understand basic data provider concepts"}),"\n",(0,r.jsx)(n.li,{children:"Complete Tutorial 2 (Device Calibration) to understand how to properly use calibration in Aria data."}),"\n",(0,r.jsxs)(n.li,{children:["Download Aria Gen2 sample data from ",(0,r.jsx)(n.a,{href:"https://www.projectaria.com/async/sample/download/?bucket=core&filename=aria_gen2_sample_data_1.vrs",children:"link"})]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Note on visualization:"}),"\nIf visualization window is not showing up, this is due to ",(0,r.jsx)(n.code,{children:"Rerun"})," lib's caching issue. Just rerun the specific code cell."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from projectaria_tools.core import data_provider\n\n# Load local VRS file\nvrs_file_path = "path/to/your/recording.vrs"\nvrs_data_provider = data_provider.create_vrs_data_provider(vrs_file_path)\n\n# Query VIO data streams\nvio_label = "vio"\nvio_stream_id = vrs_data_provider.get_stream_id_from_label(vio_label)\nif vio_stream_id is None:\n    raise RuntimeError(\n        f"{vio_label} data stream does not exist! Please use a VRS that contains valid VIO data for this tutorial."\n    )\n\n# Query VIO_high_frequency data streams\nvio_high_freq_label = "vio_high_frequency"\nvio_high_freq_stream_id = vrs_data_provider.get_stream_id_from_label(vio_high_freq_label)\nif vio_high_freq_stream_id is None:\n    raise RuntimeError(\n        f"{vio_high_freq_label} data stream does not exist! Please use a VRS that contains valid VIO high frequency data for this tutorial."\n    )\n\n'})}),"\n",(0,r.jsx)(n.h2,{id:"on-device-vio-data-stream",children:"On-Device VIO Data Stream"}),"\n",(0,r.jsxs)(n.h3,{id:"data-type-frontendoutput",children:["Data Type: ",(0,r.jsx)(n.code,{children:"FrontendOutput"})]}),"\n",(0,r.jsx)(n.p,{children:"This a new data type introduced to store the results from the VIO system, containing the following fields:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Field Name"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"frontend_session_uid"})}),(0,r.jsx)(n.td,{children:"Session identifier (resets on VIO restart)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"frame_id"})}),(0,r.jsx)(n.td,{children:"Frame set identifier"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"capture_timestamp_ns"})}),(0,r.jsx)(n.td,{children:"Center capture time in nanoseconds"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"unix_timestamp_ns"})}),(0,r.jsx)(n.td,{children:"Unix timestamp in nanoseconds"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"status"})}),(0,r.jsx)(n.td,{children:"VIO status (VALID/INVALID)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"pose_quality"})}),(0,r.jsx)(n.td,{children:"Pose quality (GOOD/BAD/UNKNOWN)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"visual_tracking_quality"})}),(0,r.jsx)(n.td,{children:"Visual-only tracking quality"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"online_calib"})}),(0,r.jsx)(n.td,{children:"Online calibration estimates for SLAM cameras and IMUs"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"gravity_in_odometry"})}),(0,r.jsx)(n.td,{children:"Gravity vector in odometry frame"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"transform_odometry_bodyimu"})}),(0,r.jsx)(n.td,{children:"Body IMU's pose in odometry reference frame"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"transform_bodyimu_device"})}),(0,r.jsx)(n.td,{children:"Transform from body IMU to device frame"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"linear_velocity_in_odometry"})}),(0,r.jsx)(n.td,{children:"Linear velocity in odometry frame in m/s"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"angular_velocity_in_bodyimu"})}),(0,r.jsx)(n.td,{children:"Angular velocity in body IMU frame in rad/s"})]})]})]}),"\n",(0,r.jsxs)(n.p,{children:["Here, ",(0,r.jsx)(n.strong,{children:"body IMU"})," is the IMU that is picked as the reference for motion tracking. For Aria-Gen2' on-device VIO algorithm, this is often ",(0,r.jsx)(n.code,{children:"imu-left"}),"."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Important Note"}),": Always check ",(0,r.jsx)(n.code,{children:"status == VioStatus.VALID"})," and\n",(0,r.jsx)(n.code,{children:"pose_quality == TrackingQuality.GOOD"})," for VIO data validity!"]}),"\n",(0,r.jsx)(n.h3,{id:"data-access-api",children:"Data Access API"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from projectaria_tools.core.sensor_data import VioStatus, TrackingQuality\n\nprint("=== VIO Data Sample ===")\n\n# Find the first valid VIO data sample\nnum_vio_samples = vrs_data_provider.get_num_data(vio_stream_id)\nfirst_valid_index = None\nfor idx in range(num_vio_samples):\n    vio_data = vrs_data_provider.get_vio_data_by_index(vio_stream_id, idx)\n    if (\n        vio_data.status == VioStatus.VALID\n        and vio_data.pose_quality == TrackingQuality.GOOD\n    ):\n        first_valid_index = idx\n        break\n\nif first_valid_index is not None:\n    print("=" * 50)\n    print(f"First VALID VIO Data Sample (Index: {first_valid_index})")\n    print("=" * 50)\n\n    # Session Information\n    print(f"Session UID: {vio_data.frontend_session_uid}")\n    print(f"Frame ID: {vio_data.frame_id}")\n\n    # Timestamps\n    print(f"Capture Time: {vio_data.capture_timestamp_ns} ns")\n    print(f"Unix Time: {vio_data.unix_timestamp_ns} ns")\n\n    # Quality Status\n    print(f"Status: {vio_data.status}")\n    print(f"Pose Quality: {vio_data.pose_quality}")\n    print(f"Visual Quality: {vio_data.visual_tracking_quality}")\n\n    # Transforms\n    print(f"Transform Odometry \u2192 Body IMU:\\n{vio_data.transform_odometry_bodyimu.to_matrix()}")\n    print(f"Transform Body IMU \u2192 Device:\\n{vio_data.transform_bodyimu_device.to_matrix()}")\n\n    # Motion\n    print(f"Linear Velocity: {vio_data.linear_velocity_in_odometry}")\n    print(f"Angular Velocity: {vio_data.angular_velocity_in_bodyimu}")\n    print(f"Gravity Vector: {vio_data.gravity_in_odometry}")\nelse:\n    print("\u26a0\ufe0f  No valid VIO sample found")\n\n'})}),"\n",(0,r.jsx)(n.h2,{id:"on-device-vio-high-frequency-data-stream",children:"On-Device VIO High Frequency Data Stream"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"VIO High Frequency"})," results are generated directly from the on-device VIO results by performing IMU integration between VIO poses, hence provides a much higher data rate at approximately ",(0,r.jsx)(n.strong,{children:"800Hz"}),"."]}),"\n",(0,r.jsxs)(n.h3,{id:"data-type-openlooptrajectorypose",children:["Data Type: ",(0,r.jsx)(n.code,{children:"OpenLoopTrajectoryPose"})]}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.strong,{children:"VioHighFrequency"})," stream ",(0,r.jsx)(n.strong,{children:"re-uses"})," the ",(0,r.jsx)(n.code,{children:"OpenLoopTrajectoryPose"})," data\nstructure ",(0,r.jsx)(n.a,{href:"https://github.com/facebookresearch/projectaria_tools/blob/main/core/mps/Trajectory.h",children:"defined in MPS"}),"."]}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Field Name"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"tracking_timestamp"})}),(0,r.jsx)(n.td,{children:"Timestamp in device time domain, in microseconds"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"transform_odometry_device"})}),(0,r.jsx)(n.td,{children:"Transformation from device to odometry coordinate frame, represented as a SE3 instance."})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"device_linear_velocity_odometry"})}),(0,r.jsx)(n.td,{children:"Translational velocity of device in odometry frame, in m/s"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"angular_velocity_device"})}),(0,r.jsx)(n.td,{children:"Angular velocity of device in device frame, in rad/s"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"quality_score"})}),(0,r.jsx)(n.td,{children:"Quality of pose estimation (higher = better)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"gravity_odometry"})}),(0,r.jsx)(n.td,{children:"Earth gravity vector in odometry frame"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"session_uid"})}),(0,r.jsx)(n.td,{children:"Unique identifier for VIO tracking session"})]})]})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Important Note"}),": Due to the high frequency nature of this data (~800Hz), consider\nsubsampling for visualization to maintain performance."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'print("=== VIO High-Frequency Data Sample ===")\n\n# Find the first VIO high_frequency data sample with high quality value\nnum_vio_high_freq_samples = vrs_data_provider.get_num_data(vio_high_freq_stream_id)\nfirst_valid_index = None\nfor idx in range(num_vio_samples):\n    vio_high_freq_data = vrs_data_provider.get_vio_high_freq_data_by_index(vio_high_freq_stream_id, idx)\n    if (\n        vio_high_freq_data.quality_score > 0.5\n    ):\n        first_valid_index = idx\n        break\n\nif first_valid_index is not None:\n    print("=" * 50)\n    print(f"First VIO High Freq Data Sample with good quality score (Index: {first_valid_index})")\n    print("=" * 50)\n\n    # Timestamps, convert timedelta to nanoseconds\n    capture_timestamp_ns = int(vio_high_freq_data.tracking_timestamp.total_seconds() * 1e9)\n\n    # Session Information\n    print(f"Session UID: {vio_high_freq_data.session_uid}")\n\n    # Timestamps\n    print(f"Tracking Time: {capture_timestamp_ns} ns")\n\n    # Quality\n    print(f"Quality Score: {vio_high_freq_data.quality_score:.3f}")\n\n    # Transform\n    print(f"Transform Odometry \u2192 Device:\\n{vio_high_freq_data.transform_odometry_device.to_matrix()}")\n\n    # Motion\n    print(f"Linear Velocity: {vio_high_freq_data.device_linear_velocity_odometry}")\n    print(f"Angular Velocity: {vio_high_freq_data.angular_velocity_device}")\n    print(f"Gravity Vector: {vio_high_freq_data.gravity_odometry}")\n\n'})}),"\n",(0,r.jsx)(n.h2,{id:"visualizing-on-device-vio-trajectory",children:"Visualizing On-Device VIO trajectory"}),"\n",(0,r.jsx)(n.p,{children:"The following code snippets demonstrate how to visualize a VIO trajectory, along with glass frame + hand tracking results, in a 3D view."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def plot_single_hand_3d(\n    hand_joints_in_device, hand_label\n):\n    """\n    A helper function to plot single hand data in 3D view\n    """\n    marker_color = [255,64,0] if hand_label == "left" else [255, 255, 0]\n\n    hand_skeleton_3d = create_hand_skeleton_from_landmarks(hand_joints_in_device)\n    rr.log(\n        f"world/device/handtracking/{hand_label}/landmarks",\n        rr.Points3D(\n            positions=hand_joints_in_device,\n            colors= marker_color,\n            radii=5e-3,\n        ),\n    )\n    rr.log(\n        f"world/device/handtracking/{hand_label}/hand_skeleton",\n        rr.LineStrips3D(\n            hand_skeleton_3d,\n            colors=[0, 255, 0],\n            radii=3e-3,\n        ),\n    )\n\n\ndef plot_hand_pose_data_3d(hand_pose_data):\n    """\n    A helper function to plot hand pose data in 3D world view\n    """\n    # Clear the canvas (only if hand_tracking_label exists for this device version)\n    rr.log(\n        f"world/device/handtracking",\n        rr.Clear.recursive(),\n    )\n\n    # Plot both hands\n    if hand_pose_data.left_hand is not None:\n        plot_single_hand_3d(\n            hand_joints_in_device=hand_pose_data.left_hand.landmark_positions_device,\n            hand_label="left",\n        )\n    if hand_pose_data.right_hand is not None:\n        plot_single_hand_3d(\n            hand_joints_in_device=hand_pose_data.right_hand.landmark_positions_device,\n            hand_label="right",\n        )\n'})}),"\n",(0,r.jsx)(n.h3,{id:"3d-trajectory-visualization",children:"3D Trajectory Visualization"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import rerun as rr\nfrom projectaria_tools.core.sensor_data import SensorDataType, TimeDomain, TimeQueryOptions\nfrom projectaria_tools.utils.rerun_helpers import (\n    create_hand_skeleton_from_landmarks,\n    AriaGlassesOutline,\n    ToTransform3D\n)\n\nprint("\\n=== Visualizing on-device VIO trajectory + HandTracking in 3D view ===")\n\nrr.init("rerun_viz_vio_trajectory")\n\ndevice_calib = vrs_data_provider.get_device_calibration()\nhandtracking_stream_id = vrs_data_provider.get_stream_id_from_label("handtracking")\n\n# Set up a data queue\ndeliver_options = vrs_data_provider.get_default_deliver_queued_options()\ndeliver_options.deactivate_stream_all()\ndeliver_options.activate_stream(vio_stream_id)\n\n# Play for only 3 seconds\ntotal_length_ns = vrs_data_provider.get_last_time_ns_all_streams(TimeDomain.DEVICE_TIME) - vrs_data_provider.get_first_time_ns_all_streams(TimeDomain.DEVICE_TIME)\nskip_begin_ns = int(15 * 1e9) # Skip 15 seconds\nduration_ns = int(3 * 1e9) # 3 seconds\nskip_end_ns = max(total_length_ns - skip_begin_ns - duration_ns, 0)\ndeliver_options.set_truncate_first_device_time_ns(skip_begin_ns)\ndeliver_options.set_truncate_last_device_time_ns(skip_end_ns)\n\n# Plot VIO trajectory in 3D view.\n# Need to keep a cache to store already-loaded trajectory\nvio_traj_cached_full = []\nfor sensor_data in vrs_data_provider.deliver_queued_sensor_data(deliver_options):\n    # Convert sensor data to VIO data\n    vio_data = sensor_data.vio_data()\n\n    # Check VIO data validity, only plot for valid data\n    if ( vio_data.status != VioStatus.VALID or vio_data.pose_quality != TrackingQuality.GOOD):\n        print(f"VIO data is invalid for timestamp {sensor_data.get_time_ns(TimeDomain.DEVICE_TIME)}")\n        continue\n\n    # Set timestamp\n    rr.set_time_nanos("device_time", vio_data.capture_timestamp_ns)\n\n    # Set and plot the Device pose for the current timestamp, as a RGB axis\n    T_World_Device = (\n        vio_data.transform_odometry_bodyimu @ vio_data.transform_bodyimu_device\n    )\n    rr.log(\n        "world/device",\n        ToTransform3D(\n            T_World_Device,\n            axis_length=0.05,\n        ),\n    )\n\n    # Also plot Aria glass outline for visualization\n    aria_glasses_point_outline = AriaGlassesOutline(\n        device_calib, use_cad_calib=True\n    )\n    rr.log(\n        "world/device/glasses_outline",\n        rr.LineStrips3D(\n            aria_glasses_point_outline,\n            colors=[200,200,200],\n            radii=5e-4,\n        ),\n    )\n\n    # Plot gravity direction vector\n    rr.log(\n        "world/vio_gravity",\n        rr.Arrows3D(\n            origins=[T_World_Device.translation()[0]],\n            vectors=[\n                vio_data.gravity_in_odometry * 1e-2\n            ],  # length converted from 9.8 meter -> 10 cm\n            colors=[101,67,33],\n            radii=1.5e-3,\n        ),\n        static=False,\n    )\n\n    # Plot VIO trajectory that are cached so far\n    vio_traj_cached_full.append(T_World_Device.translation()[0])\n    rr.log(\n        "world/vio_trajectory",\n        rr.LineStrips3D(\n            vio_traj_cached_full,\n            colors=[173, 216, 255],\n            radii=1.5e-3,\n        ),\n        static=False,\n    )\n\n    # For visualization purpose, also plot the hand tracking results\n    interpolated_hand_pose = vrs_data_provider.get_interpolated_hand_pose_data(handtracking_stream_id, vio_data.capture_timestamp_ns, TimeDomain.DEVICE_TIME)\n    if interpolated_hand_pose is not None:\n        plot_hand_pose_data_3d(hand_pose_data = interpolated_hand_pose)\n\nrr.notebook_show()\n\n'})})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(_,{...e})}):_(e)}}}]);