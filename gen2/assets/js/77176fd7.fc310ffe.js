"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[1834],{28453:(e,r,t)=>{t.d(r,{R:()=>n,x:()=>o});var s=t(96540);const i={},a=s.createContext(i);function n(e){const r=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(r):{...r,...e}},[r,e])}function o(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:n(e.components),s.createElement(a.Provider,{value:r},e.children)}},69470:(e,r,t)=>{t.d(r,{A:()=>i});t(96540);var s=t(74848);const i=({notebookUrl:e,colabDisabled:r=!1})=>(0,s.jsxs)("div",{style:{display:"flex",gap:"10px",marginBottom:"20px",flexWrap:"wrap"},children:[(0,s.jsxs)("a",{href:e,target:"_blank",rel:"noopener noreferrer",style:{display:"inline-flex",alignItems:"center",padding:"8px 16px",backgroundColor:"#f6f8fa",border:"1px solid #d0d7de",borderRadius:"6px",textDecoration:"none",color:"#24292f",fontSize:"14px",fontWeight:"500",transition:"background-color 0.2s"},onMouseEnter:e=>{e.target.style.backgroundColor="#f3f4f6"},onMouseLeave:e=>{e.target.style.backgroundColor="#f6f8fa"},children:[(0,s.jsx)("svg",{style:{marginRight:"8px",width:"16px",height:"16px"},fill:"currentColor",viewBox:"0 0 16 16",children:(0,s.jsx)("path",{d:"M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z"})}),"View Notebook on GitHub"]}),(0,s.jsxs)("button",{onClick:()=>{alert("Google Colab integration is not available yet. Please download the notebook and run it locally.")},style:{display:"inline-flex",alignItems:"center",padding:"8px 16px",backgroundColor:r?"#f6f8fa":"#f9ab00",border:"1px solid #d0d7de",borderRadius:"6px",color:r?"#656d76":"#fff",fontSize:"14px",fontWeight:"500",cursor:r?"not-allowed":"pointer",transition:"background-color 0.2s"},onMouseEnter:e=>{r||(e.target.style.backgroundColor="#e8a500")},onMouseLeave:e=>{r||(e.target.style.backgroundColor="#f9ab00")},children:[(0,s.jsx)("svg",{style:{marginRight:"8px",width:"16px",height:"16px"},fill:"currentColor",viewBox:"0 0 24 24",children:(0,s.jsx)("path",{d:"M16.9414 4.9757a7.033 7.033 0 0 0-4.9308 2.0324 7.033 7.033 0 0 0-.1232 9.8068l2.395-2.395a3.6455 3.6455 0 0 1 5.1497-5.1478l2.397-2.3989a7.033 7.033 0 0 0-4.8877-1.9175zM7.07 4.9855a7.033 7.033 0 0 0-4.8878 1.9175l2.395 2.3989a3.6434 3.6434 0 0 1 5.1497 5.1478l2.395 2.395a7.033 7.033 0 0 0-.1232-9.8068A7.033 7.033 0 0 0 7.07 4.9855zm15.0191 2.5923l-2.395 2.395a3.6455 3.6455 0 0 1-5.1497 5.1497l-2.395 2.395a7.033 7.033 0 0 0 9.9397-.123z"})}),r?"Colab (Coming Soon)":"Run in Google Colab"]})]})},84226:(e,r,t)=>{t.r(r),t.d(r,{assets:()=>l,contentTitle:()=>d,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"projectariatools/pythontutorials/dataprovider","title":"VrsDataProvider Basics","description":"<TutorialButtons","source":"@site/docs-research-tools/projectariatools/pythontutorials/dataprovider.mdx","sourceDirName":"projectariatools/pythontutorials","slug":"/projectariatools/pythontutorials/dataprovider","permalink":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/dataprovider","draft":false,"unlisted":false,"editUrl":"https://www.internalfb.com/code/fbsource/arvr/projects/ariane/aria_research_kit/projectaria_tools/website/docs-research-tools/projectariatools/pythontutorials/dataprovider.mdx","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"VrsDataProvider Basics"},"sidebar":"researchToolsSidebar","previous":{"title":"Export Gen2 On-device Machine Perception data to CSV","permalink":"/projectaria_tools/gen2/research-tools/projectariatools/tools/exportcsv"},"next":{"title":"Using Device Calibration Data from VRS","permalink":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/calibration"}}');var i=t(74848),a=t(28453),n=t(69470);const o={sidebar_position:1,title:"VrsDataProvider Basics"},d="Tutorial 1: VrsDataProvider Basics",l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Basic File Loading",id:"basic-file-loading",level:2},{value:"Stream Discovery and Navigation",id:"stream-discovery-and-navigation",level:2},{value:"Understanding Stream IDs",id:"understanding-stream-ids",level:3},{value:"Query StreamId By Label",id:"query-streamid-by-label",level:3},{value:"Sensor Data Query APIs",id:"sensor-data-query-apis",level:2},{value:"Query by index",id:"query-by-index",level:3},{value:"Query by Timestamp: TimeDomain and TimeQueryOptions",id:"query-by-timestamp-timedomain-and-timequeryoptions",level:3},{value:"TimeDomain and TimeQueryOptions",id:"timedomain-and-timequeryoptions",level:4},{value:"TimeDomain Options",id:"timedomain-options",level:4},{value:"TimeQueryOptions",id:"timequeryoptions",level:4}];function m(e){const r={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(r.header,{children:(0,i.jsx)(r.h1,{id:"tutorial-1-vrsdataprovider-basics",children:"Tutorial 1: VrsDataProvider Basics"})}),"\n",(0,i.jsx)(n.A,{notebookUrl:"https://github.com/facebookresearch/projectaria_tools/blob/main/examples/Gen2/python_notebooks/Tutorial_1_vrs_data_provider_basics.ipynb",colabDisabled:!0}),"\n",(0,i.jsx)(r.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsxs)(r.p,{children:[(0,i.jsx)(r.a,{href:"https://www.projectaria.com/",children:"Aria Gen2"})," glasses are Meta's dedicated research tool in an always-on glasses form factor. The data recorded by Aria Gen2 glasses are stored in VRS files, where each VRS file captures time-synchronized data streams from various sensors, such cameras, IMUs, audio, and more.\nThe ",(0,i.jsx)(r.strong,{children:"VrsDataProvider"})," interface provides a unified way to access multimodal sensor data from these VRS files."]}),"\n",(0,i.jsx)(r.p,{children:(0,i.jsx)(r.strong,{children:"What you'll learn:"})}),"\n",(0,i.jsxs)(r.ul,{children:["\n",(0,i.jsxs)(r.li,{children:["How to create a ",(0,i.jsx)(r.code,{children:"VrsDataProvider"})," from a VRS file."]}),"\n",(0,i.jsx)(r.li,{children:"Discover available sensor data streams, and check their configurations"}),"\n",(0,i.jsx)(r.li,{children:"Retrieve data using either sequential (index-based) or temporal (timestamp-based) access APIs."}),"\n",(0,i.jsx)(r.li,{children:"Learn about timing domains and time query options"}),"\n"]}),"\n",(0,i.jsx)(r.p,{children:(0,i.jsx)(r.strong,{children:"Prerequisites:"})}),"\n",(0,i.jsxs)(r.ul,{children:["\n",(0,i.jsx)(r.li,{children:"Basic Python knowledge and a general understanding of multimodal sensor data."}),"\n",(0,i.jsxs)(r.li,{children:["Download Aria Gen2 sample data from ",(0,i.jsx)(r.a,{href:"https://www.projectaria.com/async/sample/download/?bucket=core&filename=aria_gen2_sample_data_1.vrs",children:"link"})]}),"\n"]}),"\n",(0,i.jsxs)(r.p,{children:[(0,i.jsx)(r.strong,{children:"Note on Visualization"}),"\nIf visualization window is not showing up, this is due to ",(0,i.jsx)(r.code,{children:"Rerun"})," lib's caching issue. Just rerun the specific code cell."]}),"\n",(0,i.jsx)(r.h2,{id:"basic-file-loading",children:"Basic File Loading"}),"\n",(0,i.jsxs)(r.p,{children:["The ",(0,i.jsx)(r.code,{children:"create_vrs_data_provider($FILE_PATH)"})," factory function will create ",(0,i.jsxs)(r.strong,{children:["a ",(0,i.jsx)(r.code,{children:"vrs_data_provider"})," object"]}),". This object is ",(0,i.jsx)(r.strong,{children:"your entry point"})," for accessing VRS data."]}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'from projectaria_tools.core import data_provider\n\n# Load local VRS file\nvrs_file_path = "path/to/your/recording.vrs"\nvrs_data_provider = data_provider.create_vrs_data_provider(vrs_file_path)\n'})}),"\n",(0,i.jsx)(r.h2,{id:"stream-discovery-and-navigation",children:"Stream Discovery and Navigation"}),"\n",(0,i.jsx)(r.h3,{id:"understanding-stream-ids",children:"Understanding Stream IDs"}),"\n",(0,i.jsxs)(r.p,{children:["A VRS file contains multiple ",(0,i.jsx)(r.strong,{children:"streams"}),", each storing data from a specific sensor or on-device algorithm result."]}),"\n",(0,i.jsxs)(r.p,{children:["Each VRS stream is identified by a unique ",(0,i.jsx)(r.strong,{children:(0,i.jsx)(r.code,{children:"StreamId"})})," (e.g. ",(0,i.jsx)(r.code,{children:"1201-1"}),"), consisting ",(0,i.jsx)(r.code,{children:"RecordableTypeId"})," (sensor type, e.g. ",(0,i.jsx)(r.code,{children:"1201"}),', standing for "SLAM camera"), and an ',(0,i.jsx)(r.code,{children:"instance_id"})," (for multiple sensors of the same type, e.g. ",(0,i.jsx)(r.code,{children:"-1"}),', standing for "instance #1 of this sensor type"). Below are some common ',(0,i.jsx)(r.code,{children:"RecordableTypeId"})," in Aria recordings. Full definitions of all Recordable Types are given in this wiki page (TODO: Add website page), or refer to ",(0,i.jsxs)(r.a,{href:"https://github.com/facebookresearch/vrs/blob/main/vrs/StreamId.h#L49",children:["the ",(0,i.jsx)(r.code,{children:"StreamId.h"})," file in the VRS repo"]}),"."]}),"\n",(0,i.jsxs)(r.table,{children:[(0,i.jsx)(r.thead,{children:(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.th,{style:{textAlign:"left"},children:"RecordableTypeId"}),(0,i.jsx)(r.th,{style:{textAlign:"left"},children:"Description"})]})}),(0,i.jsxs)(r.tbody,{children:[(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{style:{textAlign:"left"},children:"214"}),(0,i.jsx)(r.td,{style:{textAlign:"left"},children:"RGB camera stream"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{style:{textAlign:"left"},children:"1201"}),(0,i.jsx)(r.td,{style:{textAlign:"left"},children:"SLAM camera stream"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{style:{textAlign:"left"},children:"211"}),(0,i.jsx)(r.td,{style:{textAlign:"left"},children:"EyeTracking camera stream"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{style:{textAlign:"left"},children:"1202"}),(0,i.jsx)(r.td,{style:{textAlign:"left"},children:"IMU sensor stream"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{style:{textAlign:"left"},children:"231"}),(0,i.jsx)(r.td,{style:{textAlign:"left"},children:"Audio sensor stream"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{style:{textAlign:"left"},children:"373"}),(0,i.jsx)(r.td,{style:{textAlign:"left"},children:"EyeGaze data stream from on-device EyeTracking algorithm."})]})]})]}),"\n",(0,i.jsx)(r.h3,{id:"query-streamid-by-label",children:"Query StreamId By Label"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'# Get all available streams\nall_streams = vrs_data_provider.get_all_streams()\nprint(f"Found {len(all_streams)} streams in the VRS file:")\n\n# Print out each stream id, and their corresponding sensor label\nfor stream_id in all_streams:\n    label = vrs_data_provider.get_label_from_stream_id(stream_id)\n    print(f" --- Data stream {stream_id}\'s label is: {label}")\n'})}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'# Find a specific stream\'s StreamId by sensor label\nprint("Seeking RGB data stream...")\nrgb_stream_id = vrs_data_provider.get_stream_id_from_label("camera-rgb")\nif rgb_stream_id is not None:\n    print(f"Found camera-rgb stream in VRS file: {rgb_stream_id}")\nelse:\n    print("Cannot find camera-rgb stream in VRS file.")\n'})}),"\n",(0,i.jsx)(r.h2,{id:"sensor-data-query-apis",children:"Sensor Data Query APIs"}),"\n",(0,i.jsxs)(r.ol,{children:["\n",(0,i.jsxs)(r.li,{children:["\n",(0,i.jsx)(r.h3,{id:"query-by-index",children:"Query by index"}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(r.p,{children:"The query-by-index API allows you to retrieve the k-th data sample from a specific stream using the following syntax:"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:"get_<SENSOR>_data_by_index(stream_id, index)\n"})}),"\n",(0,i.jsxs)(r.p,{children:["where ",(0,i.jsx)(r.code,{children:"<SENSOR>"})," can be replaced by any sensor data type available in Aria VRS. See here for ",(0,i.jsx)(r.a,{href:"https://github.com/facebookresearch/projectaria_tools/blob/main/core/data_provider/VrsDataProvider.h#L377",children:"a full list of supported sensor types"})]}),"\n",(0,i.jsx)(r.p,{children:"This API is commonly used for sequential processing\u2014such as iterating through all frames of a single stream\u2014or when you know the exact frame number you want to query within a specific stream."}),"\n",(0,i.jsxs)(r.p,{children:[(0,i.jsx)(r.strong,{children:"Important Note:"}),"\nThe indices in each stream are independent and not correlated across different sensor streams. For example, the i-th RGB image does not necessarily correspond to the i-th SLAM image. This is because different sensors may operate at different frequencies or have missing frames, so their data streams are not synchronized by index."]}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'# Visualize with Rerun\nimport rerun as rr\nrr.init("rerun_viz_query_by_index")\n\n# Get number of samples in stream\nnum_samples = vrs_data_provider.get_num_data(rgb_stream_id)\nprint(f"RGB stream has a total of {num_samples} frames\\n")\n\n# Access frames sequentially, and plot the first few frames\nfirst_few = min(10, num_samples)\nprint(f"Printing the capture timestamps from the first {first_few} frames")\nfor i in range(first_few):  # First 10 frames\n    image_data, image_record = vrs_data_provider.get_image_data_by_index(\n        rgb_stream_id, i\n    )\n\n    # Access image properties\n    timestamp_ns = image_record.capture_timestamp_ns\n    print(f"Frame {i}: timestamp = {timestamp_ns}")\n\n    # Process image data\n    if image_data.is_valid():\n        rr.set_time_nanos("device_time", timestamp_ns)\n        rr.log("camera_rgb", rr.Image(image_data.to_numpy_array()))\n\nrr.notebook_show()\n'})}),"\n",(0,i.jsxs)(r.ol,{start:"2",children:["\n",(0,i.jsxs)(r.li,{children:["\n",(0,i.jsx)(r.h3,{id:"query-by-timestamp-timedomain-and-timequeryoptions",children:"Query by Timestamp: TimeDomain and TimeQueryOptions"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(r.p,{children:["A key feature of Aria devices is the ability to capture time-synchronized, multi-modal sensor data. To help you access this data with precise temporal control, ",(0,i.jsx)(r.code,{children:"projectaria_tools"})," provides a comprehensive suite of time-based APIs."]}),"\n",(0,i.jsxs)(r.p,{children:["The most commonly used is the ",(0,i.jsx)(r.strong,{children:"timestamp-based query"}),":"]}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:"get_<SENSOR>_by_time_ns(stream_id, time_ns, time_domain=None, time_query_options=None)\n"})}),"\n",(0,i.jsxs)(r.p,{children:["where ",(0,i.jsx)(r.code,{children:"<SENSOR>"})," can be replaced by any sensor data type available in Aria VRS. See here for ",(0,i.jsx)(r.a,{href:"https://github.com/facebookresearch/projectaria_tools/blob/main/core/data_provider/VrsDataProvider.h#L377",children:"a full list of supported sensor types"})]}),"\n",(0,i.jsx)(r.p,{children:"This API is often used to synchronize data across multiple sensor streams, fetch sensor data at specific timestamps, or perform temporal analysis."}),"\n",(0,i.jsx)(r.h4,{id:"timedomain-and-timequeryoptions",children:"TimeDomain and TimeQueryOptions"}),"\n",(0,i.jsx)(r.p,{children:"When querying sensor data by timestamp, two important concepts are:"}),"\n",(0,i.jsxs)(r.ul,{children:["\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"TimeDomain"}),": Specifies the time reference for your query."]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"TimeQueryOptions"}),": Controls how the API selects data relative to your requested timestamp."]}),"\n"]}),"\n",(0,i.jsx)(r.p,{children:"Below are all available options for each:"}),"\n",(0,i.jsx)(r.h4,{id:"timedomain-options",children:"TimeDomain Options"}),"\n",(0,i.jsxs)(r.table,{children:[(0,i.jsx)(r.thead,{children:(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.th,{children:"Name"}),(0,i.jsx)(r.th,{children:"Description"}),(0,i.jsx)(r.th,{children:"Typical Use Case"})]})}),(0,i.jsxs)(r.tbody,{children:[(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"RECORD_TIME"}),(0,i.jsx)(r.td,{children:"Timestamp stored directly in the VRS index. Fast access, but time domain may vary."}),(0,i.jsx)(r.td,{children:"Quick access, not recommended for sync."})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"DEVICE_TIME"}),(0,i.jsx)(r.td,{children:"Accurate device capture time. All sensors on the same Aria device share this domain."}),(0,i.jsx)(r.td,{children:(0,i.jsx)(r.strong,{children:"Recommended for single-device data."})})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"HOST_TIME"}),(0,i.jsx)(r.td,{children:"Arrival time in the host computer's domain. May not be accurate."}),(0,i.jsx)(r.td,{children:"Debugging, host-side analysis."})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"TIME_CODE"}),(0,i.jsx)(r.td,{children:"[Aria-Gen1 only] TimeSync server's domain using external time-code devices, accurate across devices in multi-device capture."}),(0,i.jsx)(r.td,{children:"Multi-device synchronization."})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"TIC_SYNC"}),(0,i.jsx)(r.td,{children:"[Aria-Gen1 only] TimeSync server's domain using tic-sync, accurate for multi-device capture."}),(0,i.jsx)(r.td,{children:"Multi-device synchronization."})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"SubGhz"}),(0,i.jsx)(r.td,{children:"[Aria-Gen2 only] TimeSync server's domain using SubGhz signals, accurate for multi-device capture."}),(0,i.jsx)(r.td,{children:"Multi-device synchronization."})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Utc"}),(0,i.jsx)(r.td,{children:"UTC time domain, only seconds-level accuracy."}),(0,i.jsx)(r.td,{children:"Coarse, global time reference."})]})]})]}),"\n",(0,i.jsx)(r.h4,{id:"timequeryoptions",children:"TimeQueryOptions"}),"\n",(0,i.jsxs)(r.table,{children:[(0,i.jsx)(r.thead,{children:(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.th,{children:"Name"}),(0,i.jsx)(r.th,{children:"Description"})]})}),(0,i.jsxs)(r.tbody,{children:[(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Before"}),(0,i.jsxs)(r.td,{children:["Returns the last valid data with ",(0,i.jsx)(r.code,{children:"timestamp <= t_query"}),"."]})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"After"}),(0,i.jsxs)(r.td,{children:["Returns the first valid data with ",(0,i.jsx)(r.code,{children:"timestamp >= t_query"}),"."]})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Closest"}),(0,i.jsxs)(r.td,{children:["Returns the data sample closest to ",(0,i.jsx)(r.code,{children:"t_query"}),". If two are equally close, returns the one ",(0,i.jsx)(r.strong,{children:"before"})," the query."]})]})]})]}),"\n",(0,i.jsxs)(r.blockquote,{children:["\n",(0,i.jsxs)(r.p,{children:["For detailed usage and best practices\u2014especially for time-sync across multiple devices\u2014see ",(0,i.jsx)(r.strong,{children:"Tutorial_6"}),"."]}),"\n"]}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'from projectaria_tools.core.sensor_data import TimeDomain, TimeQueryOptions\n\nrr.init("rerun_viz_query_by_timestamp")\n\n# Get time bounds for RGB images\nfirst_timestamp_ns = vrs_data_provider.get_first_time_ns(rgb_stream_id, TimeDomain.DEVICE_TIME)\nlast_timestamp_ns = vrs_data_provider.get_last_time_ns(rgb_stream_id, TimeDomain.DEVICE_TIME)\n\n# Query specific timestamp\ntarget_time_ns = first_timestamp_ns + int(1e9) # 1 second later\nimage_data, image_record = vrs_data_provider.get_image_data_by_time_ns(\n    rgb_stream_id,\n    target_time_ns,\n    TimeDomain.DEVICE_TIME,\n    TimeQueryOptions.CLOSEST\n)\n\nactual_time_ns = image_record.capture_timestamp_ns\nprint(f"Requested RGB data that is closest to: {target_time_ns} ns, Got closest sample at: {actual_time_ns} ns")\n\n# Plot RGB and SLAM images at approx 1 hz\ncamera_label_list = ["camera-rgb", "slam-front-left", "slam-front-right", "slam-side-left", "slam-side-right"]\ncamera_stream_ids = [vrs_data_provider.get_stream_id_from_label(camera_label) for camera_label in camera_label_list]\n\nquery_timestamp_ns = first_timestamp_ns\nfor _ in range(10):\n    for label, stream_id in zip(camera_label_list, camera_stream_ids):\n        # Query each camera\'s data according to query timestamp\n        image_data, image_record = vrs_data_provider.get_image_data_by_time_ns(\n            stream_id,\n            query_timestamp_ns,\n            TimeDomain.DEVICE_TIME,\n            TimeQueryOptions.CLOSEST)\n        # note that the actual timestamp of the image data is stored within image_record. It can be different from query_time.\n        capture_time_ns = image_record.capture_timestamp_ns\n\n        # Plot to Rerun\n        if image_data.is_valid():\n            rr.set_time_nanos("device_time", capture_time_ns)\n            rr.log(label, rr.Image(image_data.to_numpy_array()))\n\n    query_timestamp_ns = query_timestamp_ns + int(1e9) # 1 second\n\nrr.notebook_show()\n'})})]})}function h(e={}){const{wrapper:r}={...(0,a.R)(),...e.components};return r?(0,i.jsx)(r,{...e,children:(0,i.jsx)(m,{...e})}):m(e)}}}]);