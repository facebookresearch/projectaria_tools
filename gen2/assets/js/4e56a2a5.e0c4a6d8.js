"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[168],{28453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>a});var o=i(96540);const t={},s=o.createContext(t);function r(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),o.createElement(s.Provider,{value:n},e.children)}},52718:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>l,default:()=>_,frontMatter:()=>a,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"projectariatools/pythontutorials/mps","title":"Loading and Visualizing MPS Output Data","description":"<TutorialButtons","source":"@site/docs-research-tools/projectariatools/pythontutorials/mps.mdx","sourceDirName":"projectariatools/pythontutorials","slug":"/projectariatools/pythontutorials/mps","permalink":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/mps","draft":false,"unlisted":false,"editUrl":"https://www.internalfb.com/code/fbsource/arvr/projects/ariane/aria_research_kit/projectaria_tools/website/docs-research-tools/projectariatools/pythontutorials/mps.mdx","tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"sidebar_position":7,"title":"Loading and Visualizing MPS Output Data"},"sidebar":"researchToolsSidebar","previous":{"title":"Timestamp Alignment in Aria Gen2","permalink":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/time-sync"},"next":{"title":"Advanced Installation From Source Code","permalink":"/projectaria_tools/gen2/research-tools/projectariatools/advanced-installation"}}');var t=i(74848),s=i(28453),r=i(69470);const a={sidebar_position:7,title:"Loading and Visualizing MPS Output Data"},l="Tutorial 7: Loading and Visualizing MPS Output Data",d={},c=[{value:"Introduction",id:"introduction",level:2},{value:"MPS SLAM Trajectories",id:"mps-slam-trajectories",level:2},{value:"Understanding Open Loop vs Closed Loop Trajectories",id:"understanding-open-loop-vs-closed-loop-trajectories",level:3},{value:"Loading Closed Loop Trajectory",id:"loading-closed-loop-trajectory",level:3},{value:"Loading Open Loop Trajectory",id:"loading-open-loop-trajectory",level:3},{value:"MPS Semi-dense Point Cloud and Observations",id:"mps-semi-dense-point-cloud-and-observations",level:2},{value:"Understanding Point Cloud Data",id:"understanding-point-cloud-data",level:3},{value:"Loading Semi-dense Point Cloud",id:"loading-semi-dense-point-cloud",level:3},{value:"Loading Point Observations",id:"loading-point-observations",level:3},{value:"Visualizing MPS SLAM Results",id:"visualizing-mps-slam-results",level:2},{value:"Color Mapping Helper Function",id:"color-mapping-helper-function",level:3},{value:"Preparing Data for Visualization",id:"preparing-data-for-visualization",level:3},{value:"3D Visualization with Rerun",id:"3d-visualization-with-rerun",level:3},{value:"Understanding MPS Data Structures",id:"understanding-mps-data-structures",level:2},{value:"Trajectory Data Types",id:"trajectory-data-types",level:3},{value:"ClosedLoopTrajectoryPose",id:"closedlooptrajectorypose",level:4},{value:"OpenLoopTrajectoryPose",id:"openlooptrajectorypose",level:4},{value:"Point Cloud Data Types",id:"point-cloud-data-types",level:3},{value:"GlobalPointPosition",id:"globalpointposition",level:4},{value:"PointObservation",id:"pointobservation",level:4},{value:"MPS vs On-Device Comparisons",id:"mps-vs-on-device-comparisons",level:2},{value:"Key Differences",id:"key-differences",level:3},{value:"Use Cases",id:"use-cases",level:3},{value:"Summary",id:"summary",level:2}];function p(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"tutorial-7-loading-and-visualizing-mps-output-data",children:"Tutorial 7: Loading and Visualizing MPS Output Data"})}),"\n",(0,t.jsx)(r.A,{notebookUrl:"https://github.com/facebookresearch/projectaria_tools/blob/main/examples/Gen2/python_notebooks/Tutorial_7_mps_data_provider_basics.ipynb",colabDisabled:!0}),"\n",(0,t.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,t.jsxs)(n.p,{children:["This tutorial demonstrates how to access and visualize ",(0,t.jsx)(n.strong,{children:"Machine Perception Services (MPS)"})," results. MPS provides cloud-based processing of Aria data to generate high-quality 3D reconstruction, SLAM trajectories, and other perception outputs."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"What you'll learn:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"How to load and access MPS SLAM trajectory data (open loop and closed loop)"}),"\n",(0,t.jsx)(n.li,{children:"How to load and visualize MPS semi-dense point clouds and observations"}),"\n",(0,t.jsx)(n.li,{children:"How to create 3D visualizations of MPS SLAM results"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Prerequisites"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Complete Tutorial 1 (VrsDataProvider Basics) to understand basic data provider concepts"}),"\n",(0,t.jsx)(n.li,{children:"Complete Tutorial 2 (Device Calibration) to understand how to properly use calibration in Aria data."}),"\n",(0,t.jsxs)(n.li,{children:["Download Aria Gen2 sample data: ",(0,t.jsx)(n.a,{href:"https://www.projectaria.com/async/sample/download/?bucket=core&filename=aria_gen2_sample_data_1.vrs",children:"VRS"})," and ",(0,t.jsx)(n.a,{href:"https://www.projectaria.com/async/sample/download/?bucket=core&filename=aria_gen2_sample_data_1_mps_output.zip",children:"MPS output zip file"}),"."]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Note on Visualization"}),"\nIf visualization window is not showing up, this is due to ",(0,t.jsx)(n.code,{children:"Rerun"})," lib's caching issue. Just rerun the specific code cell."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import os\nfrom projectaria_tools.core import mps\n\n# Set up paths to your MPS data\nmps_folder_path = "path/to/your/mps/folder/"\nvrs_file_path = "path/to/your/recording.vrs"\n\n# Load VRS data provider for additional context\nvrs_data_provider = data_provider.create_vrs_data_provider(vrs_file_path)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"mps-slam-trajectories",children:"MPS SLAM Trajectories"}),"\n",(0,t.jsx)(n.h3,{id:"understanding-open-loop-vs-closed-loop-trajectories",children:"Understanding Open Loop vs Closed Loop Trajectories"}),"\n",(0,t.jsxs)(n.p,{children:["MPS SLAM algorithm outputs 2 trajectory files (see ",(0,t.jsx)(n.a,{href:"https://facebookresearch.github.io/projectaria_tools/docs/data_formats/mps/slam/mps_trajectory",children:"wiki page"})," for data type definitions):"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Open loop trajectory"}),": High-frequency (1kHz) odometry from visual-inertial odometry (VIO), accurate over short periods but drifts over time and distance."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Closed loop trajectory"}),": High-frequency (1kHz) pose from mapping with loop closure corrections, reducing drift but possibly less accurate locally over short spans."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"loading-closed-loop-trajectory",children:"Loading Closed Loop Trajectory"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from projectaria_tools.core.mps.utils import (\n    filter_points_from_confidence,\n    get_nearest_pose,\n)\n\nprint("=== MPS - Closed loop trajectory ===")\n\n# Load MPS closed-loop trajectory data\nclosed_loop_trajectory_file = os.path.join(\n    mps_folder_path, "slam", "closed_loop_trajectory.csv"\n)\nclosed_loop_trajectory = mps.read_closed_loop_trajectory(closed_loop_trajectory_file)\n\n# Print out the content of the first sample in closed_loop_trajectory\nif closed_loop_trajectory:\n    sample = closed_loop_trajectory[0]\n    print("ClosedLoopTrajectoryPose sample:")\n    print(f"  tracking_timestamp: {int(sample.tracking_timestamp.total_seconds() * 1e6)} us")\n    print(f"  utc_timestamp: {int(sample.utc_timestamp.total_seconds() * 1e6)} us")\n    print(f"  transform_world_device:\\n{sample.transform_world_device}")\n    print(f"  device_linear_velocity_device: {sample.device_linear_velocity_device}")\n    print(f"  angular_velocity_device: {sample.angular_velocity_device}")\n    print(f"  quality_score: {sample.quality_score}")\n    print(f"  gravity_world: {sample.gravity_world}")\n    print(f"  graph_uid: {sample.graph_uid}")\nelse:\n    print("closed_loop_trajectory is empty.")\n'})}),"\n",(0,t.jsx)(n.h3,{id:"loading-open-loop-trajectory",children:"Loading Open Loop Trajectory"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'print("=== MPS - Open loop trajectory ===")\n\n# Load MPS open-loop trajectory data\nopen_loop_trajectory_file = os.path.join(\n    mps_folder_path, "slam", "open_loop_trajectory.csv"\n)\nopen_loop_trajectory = mps.read_open_loop_trajectory(open_loop_trajectory_file)\n\n# Print out the content of the first sample in open_loop_trajectory\nif open_loop_trajectory:\n    sample = open_loop_trajectory[0]\n    print("OpenLoopTrajectoryPose sample:")\n    print(f"  tracking_timestamp: {int(sample.tracking_timestamp.total_seconds() * 1e6)} us")\n    print(f"  utc_timestamp: {int(sample.utc_timestamp.total_seconds() * 1e6)} us")\n    print(f"  transform_odometry_device:\\n{sample.transform_odometry_device}")\n    print(f"  device_linear_velocity_odometry: {sample.device_linear_velocity_odometry}")\n    print(f"  angular_velocity_device: {sample.angular_velocity_device}")\n    print(f"  quality_score: {sample.quality_score}")\n    print(f"  gravity_odometry: {sample.gravity_odometry}")\n    print(f"  session_uid: {sample.session_uid}")\nelse:\n    print("open_loop_trajectory is empty.")\n'})}),"\n",(0,t.jsx)(n.h2,{id:"mps-semi-dense-point-cloud-and-observations",children:"MPS Semi-dense Point Cloud and Observations"}),"\n",(0,t.jsx)(n.h3,{id:"understanding-point-cloud-data",children:"Understanding Point Cloud Data"}),"\n",(0,t.jsxs)(n.p,{children:["MPS SLAM algorithm outputs 2 files related to semi-dense point cloud (see ",(0,t.jsx)(n.a,{href:"https://facebookresearch.github.io/projectaria_tools/docs/data_formats/mps/slam/mps_pointcloud",children:"wiki page"})," for data type definitions):"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"semidense_points.csv.gz"}),": Global points in the world coordinate frame."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"semidense_observations.csv.gz"}),": Point observations for each camera, at each timestamp."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Note that semidense point files are normally large, therefore loading them may take some time."}),"\n",(0,t.jsx)(n.h3,{id:"loading-semi-dense-point-cloud",children:"Loading Semi-dense Point Cloud"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'print("=== MPS - Semi-dense Point Cloud ===")\n\n# Load MPS semi-dense point cloud data\nsemidense_points_file = os.path.join(\n    mps_folder_path, "slam", "semidense_points.csv.gz"\n)\nsemidense_points = mps.read_global_point_cloud(semidense_points_file)\n\n# Print out the content of the first sample in semidense_points\nif semidense_points:\n    sample = semidense_points[0]\n    print("GlobalPointPosition sample:")\n    print(f"  uid: {sample.uid}")\n    print(f"  graph_uid: {sample.graph_uid}")\n    print(f"  position_world: {sample.position_world}")\n    print(f"  inverse_distance_std: {sample.inverse_distance_std}")\n    print(f"  distance_std: {sample.distance_std}")\n    print(f"Total number of semi-dense points: {len(semidense_points)}")\nelse:\n    print("semidense_points is empty.")\n\n# Filter semidense points by inv_dep or depth.\n# The filter will KEEP points with (inv_dep or depth < threshold)\nfiltered_semidense_points = filter_points_from_confidence(raw_points = semidense_points, threshold_invdep = 1e-3, threshold_dep = 5e-2)\nprint(f"Filtering semidense points from a total of {len(semidense_points)} points down to {len(filtered_semidense_points)}")\n'})}),"\n",(0,t.jsx)(n.h3,{id:"loading-point-observations",children:"Loading Point Observations"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'print("=== MPS - Semi-dense Point Observations ===")\n\n# Load MPS semi-dense point observations data\nsemidense_observations_file = os.path.join(\n    mps_folder_path, "slam", "semidense_observations.csv.gz"\n)\nsemidense_observations = mps.read_point_observations(semidense_observations_file)\n\n# Print out the content of the first sample in semidense_observations\nif semidense_observations:\n    sample = semidense_observations[0]\n    print("PointObservation sample:")\n    print(f"  point_uid: {sample.point_uid}")\n    print(f"  frame_capture_timestamp: {int(sample.frame_capture_timestamp.total_seconds() * 1e6)} us")\n    print(f"  camera_serial: {sample.camera_serial}")\n    print(f"  uv: {sample.uv}")\n    print(f"Total number of point observations: {len(semidense_observations)}")\nelse:\n    print("semidense_observations is empty.")\n'})}),"\n",(0,t.jsx)(n.h2,{id:"visualizing-mps-slam-results",children:"Visualizing MPS SLAM Results"}),"\n",(0,t.jsx)(n.p,{children:"In the following code snippet, we demonstrate how to visualize the MPS SLAM results in a 3D view."}),"\n",(0,t.jsx)(n.p,{children:"We first prepare a short trajectory segment, then extract the semidense points position, along with timestamp-mapped observations for visualization purpose. Finally we plot everything in Rerun."}),"\n",(0,t.jsx)(n.h3,{id:"color-mapping-helper-function",children:"Color Mapping Helper Function"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from collections import defaultdict\nimport numpy as np\n\n# A helper coloring function\ndef color_from_zdepth(z_depth_m: float) -> np.ndarray:\n    """\n    Map z-depth (meters, along the camera\'s forward axis) to a bright Viridis-like RGB color.\n    - If z_depth_m <= 0 (point is behind the camera), return black [0, 0, 0].\n    - Near (0.2 m) -> yellow, Far (5.0 m) -> purple.\n    Returns an array of shape (3,) with dtype=uint8.\n    """\n    if not np.isfinite(z_depth_m) or z_depth_m <= 0.0:\n        return np.array([0, 0, 0], dtype=np.uint8)\n\n    NEAR_METERS, FAR_METERS = 0.2, 5.0\n\n    # Normalize to [0,1], then flip so near \u2192 bright (yellow), far \u2192 dark (purple)\n    clamped = min(max(float(z_depth_m), NEAR_METERS), FAR_METERS)\n    normalized_position = (clamped - NEAR_METERS) / (FAR_METERS - NEAR_METERS + 1e-12)\n    gradient_position = 1.0 - normalized_position\n\n    # Viridis-like anchor colors: purple \u2192 blue \u2192 teal \u2192 green \u2192 yellow\n    color_stops = [\n        (68, 1, 84),\n        (59, 82, 139),\n        (33, 145, 140),\n        (94, 201, 98),\n        (253, 231, 37),\n    ]\n\n    # Locate segment and blend between its endpoints\n    segment_count = len(color_stops) - 1\n    continuous_index = gradient_position * segment_count\n    lower_segment_index = int(continuous_index)\n\n    if lower_segment_index >= segment_count:\n        red, green, blue = color_stops[-1]\n    else:\n        segment_fraction = continuous_index - lower_segment_index\n        r0, g0, b0 = color_stops[lower_segment_index]\n        r1, g1, b1 = color_stops[lower_segment_index + 1]\n        red   = r0 + segment_fraction * (r1 - r0)\n        green = g0 + segment_fraction * (g1 - g0)\n        blue  = b0 + segment_fraction * (b1 - b0)\n\n    return np.array([int(red), int(green), int(blue)], dtype=np.uint8)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"preparing-data-for-visualization",children:"Preparing Data for Visualization"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'print("=== Preparing MPS SLAM results for visualization ===")\n\n# Check if we have valid SLAM data to visualize\nif not closed_loop_trajectory or not semidense_points:\n    raise RuntimeError("Warning: This tutorial requires valid MPS SLAM data to run.")\n\n# --\n# Prepare Trajectory data\n# --\n# Select a short segment of trajectory (e.g., first 5000 samples, subsampled by 50)\nsegment_length = min(50000, len(closed_loop_trajectory))\ntrajectory_segment = closed_loop_trajectory[:segment_length:50]\ntimestamp_to_pose = {\n    pose.tracking_timestamp: pose for pose in trajectory_segment\n}\nprint(f"Finished preparing a trajectory of length {len(trajectory_segment)}... ")\n\n# -----------\n# Prepare Semidense point data\n# -----------\n# Filter the semidense point cloud by confidence and limit max point count, and extract the point positions\nfiltered_semidense_point_cloud_data = filter_points_from_confidence(semidense_points)\npoints_positions = np.array(\n    [\n        point.position_world for point in filtered_semidense_point_cloud_data\n    ]\n)\nprint(f"Finished preparing filtered semidense points cloud of {len(filtered_semidense_point_cloud_data)} points... ")\n\n# -----------\n# Prepare Semidense observation data\n# -----------\n# Based on RGB observations, create a per-timestamp point position list, and color them according to its distance from RGB camera\npoint_uid_to_position = {\n    point.uid: np.array(point.position_world) for point in filtered_semidense_point_cloud_data\n}\n\n# A helper function that creates a easier-to-query mapping to obtain observations according to timestamps\nslam_1_serial = vrs_data_provider.get_device_calibration().get_camera_calib("slam-front-left").get_serial_number()\ntimestamp_to_point_positions = defaultdict(list)  # t_ns -> [position, position, ...]\ntimestamp_to_point_colors = defaultdict(list) # t_ns -> [color, color, ...]\n\nfor obs in semidense_observations:\n    # Only add observations for SLAM_1 camera, and if the timestamp is in the chosen trajectory segment\n    if (\n        obs.camera_serial == slam_1_serial and\n        obs.frame_capture_timestamp in timestamp_to_pose and\n        obs.point_uid in point_uid_to_position):\n        # Insert point position\n        obs_timestamp = obs.frame_capture_timestamp\n        point_position = point_uid_to_position[obs.point_uid]\n        timestamp_to_point_positions[obs_timestamp].append(point_position)\n\n        # Insert point color\n        T_world_device = timestamp_to_pose[obs_timestamp].transform_world_device\n        point_in_device = T_world_device.inverse() @ point_position\n        point_z_depth = point_in_device.squeeze()[2]\n        point_color = color_from_zdepth(point_z_depth)\n        timestamp_to_point_colors[obs_timestamp].append(point_color)\n\nfrom itertools import islice\nprint(f"Finished preparing semidense points observations: ")\nfor timestamp, points in islice(timestamp_to_point_positions.items(), 5):\n    print(f"\\t timestamp {int(timestamp.total_seconds() * 1e9)} ns has {len(points)} observed points in slam-front-left view. ")\nprint(f"\\t ...")\n'})}),"\n",(0,t.jsx)(n.h3,{id:"3d-visualization-with-rerun",children:"3D Visualization with Rerun"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import rerun as rr\nimport numpy as np\nfrom projectaria_tools.utils.rerun_helpers import (\n    AriaGlassesOutline,\n    ToTransform3D,\n    ToBox3D,\n)\nfrom projectaria_tools.core.mps.utils import (\n    filter_points_from_confidence,\n    get_nearest_pose,\n)\n\nprint("=== Visualizing MPS SLAM Results in 3D ===")\n\n# Initialize Rerun\nrr.init("MPS SLAM Visualization")\n\n# Set up the 3D scene\nrr.log("world", rr.ViewCoordinates.RIGHT_HAND_Z_UP, static=True)\n\n# Log point cloud\nrr.log(\n    "world/semidense_points",\n    rr.Points3D(\n        positions=points_positions,\n        colors=[255, 255, 255, 125],\n        radii=0.001\n    ),\n    static=True\n)\n\n# Aria glass outline for visualization purpose\ndevice_calib = vrs_data_provider.get_device_calibration()\naria_glasses_point_outline = AriaGlassesOutline(\n    device_calib, use_cad_calib=True\n)\n\n# Plot Closed loop trajectory\nclosed_loop_traj_cached_full = []\nobservation_points_cached = None\nobservation_colors_cached = None\nfor closed_loop_pose in trajectory_segment:\n    capture_timestamp_ns = int(closed_loop_pose.tracking_timestamp.total_seconds() * 1e9)\n    rr.set_time_nanos("device_time", capture_timestamp_ns)\n\n    T_world_device = closed_loop_pose.transform_world_device\n\n    # Log device pose as a coordinate frame\n    rr.log(\n        "world/device",\n        ToTransform3D(\n            T_world_device,\n            axis_length=0.05,\n        ),\n    )\n\n    # Plot Aria glass outline\n    rr.log(\n        "world/device/glasses_outline",\n        rr.LineStrips3D(\n            aria_glasses_point_outline,\n            colors=[150,200,40],\n            radii=5e-3,\n        ),\n    )\n\n    # Plot gravity direction vector\n    rr.log(\n        "world/vio_gravity",\n        rr.Arrows3D(\n            origins=[T_world_device.translation()[0]],\n            vectors=[\n                closed_loop_pose.gravity_world * 1e-2\n            ],  # length converted from 9.8 meter -> 10 cm\n            colors=[101,67,33],\n            radii=5e-3,\n        ),\n        static=False,\n    )\n\n    # Update cached results for observations. Cache is needed because observation has a much lower freq than high-freq trajectory.\n    if closed_loop_pose.tracking_timestamp in timestamp_to_point_positions.keys():\n        observation_points_cached = timestamp_to_point_positions[closed_loop_pose.tracking_timestamp]\n        observation_colors_cached = timestamp_to_point_colors[closed_loop_pose.tracking_timestamp]\n    if observation_points_cached is not None:\n        rr.log(\n            "world/semidense_observations",\n            rr.Points3D(\n            positions = observation_points_cached,\n            colors = observation_colors_cached,\n            radii=0.01\n            ),\n            static = False\n        )\n\n\n    # Plot the entire VIO trajectory that are cached so far\n    closed_loop_traj_cached_full.append(T_world_device.translation()[0])\n    rr.log(\n        "world/vio_trajectory",\n        rr.LineStrips3D(\n            closed_loop_traj_cached_full,\n            colors=[173, 216, 255],\n            radii=5e-3,\n        ),\n        static=False,\n    )\n\nrr.notebook_show()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"understanding-mps-data-structures",children:"Understanding MPS Data Structures"}),"\n",(0,t.jsx)(n.h3,{id:"trajectory-data-types",children:"Trajectory Data Types"}),"\n",(0,t.jsx)(n.h4,{id:"closedlooptrajectorypose",children:"ClosedLoopTrajectoryPose"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"tracking_timestamp"}),": Device timestamp when pose was computed"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"utc_timestamp"}),": UTC timestamp"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"transform_world_device"}),": 6DOF pose in world coordinate frame"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"device_linear_velocity_device"}),": Linear velocity in device frame"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"angular_velocity_device"}),": Angular velocity in device frame"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"quality_score"}),": Pose estimation quality (higher = better)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"gravity_world"}),": Gravity vector in world frame"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"graph_uid"}),": Unique identifier for the pose graph"]}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"openlooptrajectorypose",children:"OpenLoopTrajectoryPose"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"tracking_timestamp"}),": Device timestamp when pose was computed"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"utc_timestamp"}),": UTC timestamp"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"transform_odometry_device"}),": 6DOF pose in odometry coordinate frame"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"device_linear_velocity_odometry"}),": Linear velocity in odometry frame"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"angular_velocity_device"}),": Angular velocity in device frame"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"quality_score"}),": Pose estimation quality (higher = better)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"gravity_odometry"}),": Gravity vector in odometry frame"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"session_uid"}),": Unique identifier for the session"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"point-cloud-data-types",children:"Point Cloud Data Types"}),"\n",(0,t.jsx)(n.h4,{id:"globalpointposition",children:"GlobalPointPosition"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"uid"}),": Unique identifier for the 3D point"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"graph_uid"}),": Identifier linking point to pose graph"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"position_world"}),": 3D position in world coordinate frame"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"inverse_distance_std"}),": Inverse distance standard deviation (quality metric)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"distance_std"}),": Distance standard deviation (quality metric)"]}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"pointobservation",children:"PointObservation"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"point_uid"}),": Links observation to 3D point"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"frame_capture_timestamp"}),": When the observation was captured"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"camera_serial"}),": Serial number of the observing camera"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"uv"}),": 2D pixel coordinates of the observation"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"mps-vs-on-device-comparisons",children:"MPS vs On-Device Comparisons"}),"\n",(0,t.jsx)(n.h3,{id:"key-differences",children:"Key Differences"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Aspect"}),(0,t.jsx)(n.th,{children:"On-Device (VIO/SLAM)"}),(0,t.jsx)(n.th,{children:"MPS SLAM"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Processing"})}),(0,t.jsx)(n.td,{children:"Real-time during recording"}),(0,t.jsx)(n.td,{children:"Cloud-based post-processing"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Accuracy"})}),(0,t.jsx)(n.td,{children:"Good for real-time use"}),(0,t.jsx)(n.td,{children:"Higher accuracy with global optimization"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Frequency"})}),(0,t.jsx)(n.td,{children:"20Hz (VIO), 800Hz (high-freq)"}),(0,t.jsx)(n.td,{children:"1kHz (both open/closed loop)"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Drift"})}),(0,t.jsx)(n.td,{children:"Accumulates over time"}),(0,t.jsx)(n.td,{children:"Minimized with loop closure"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Point Cloud"})}),(0,t.jsx)(n.td,{children:"Not available"}),(0,t.jsx)(n.td,{children:"Dense semi-dense reconstructions"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Coordinate Frame"})}),(0,t.jsx)(n.td,{children:"Odometry frame"}),(0,t.jsx)(n.td,{children:"Global world frame"})]})]})]}),"\n",(0,t.jsx)(n.h3,{id:"use-cases",children:"Use Cases"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"On-Device Data"}),": Real-time applications, live feedback, immediate processing"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"MPS Data"}),": High-quality reconstruction, research analysis, detailed mapping"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:"This tutorial covered the essential aspects of working with MPS data:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Trajectory Access"}),": Loading both open loop and closed loop trajectories"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Point Cloud Data"}),": Accessing semi-dense 3D reconstructions and observations"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Data Filtering"}),": Using confidence thresholds to improve point cloud quality"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"3D Visualization"}),": Creating comprehensive visualizations with trajectories and point clouds"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Data Structures"}),": Understanding the comprehensive MPS data formats"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"MPS provides high-quality, globally consistent 3D reconstructions that are ideal for:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Research Applications"}),": Detailed spatial analysis and mapping"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"3D Reconstruction"}),": High-fidelity environmental modeling"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Motion Analysis"}),": Accurate trajectory analysis without drift"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multi-modal Studies"}),": Combining precise 3D data with sensor information"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Benchmarking"}),": Comparing against ground truth for algorithm development"]}),"\n"]})]})}function _(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(p,{...e})}):p(e)}},69470:(e,n,i)=>{i.d(n,{A:()=>t});i(96540);var o=i(74848);const t=({notebookUrl:e,colabDisabled:n=!1})=>(0,o.jsxs)("div",{style:{display:"flex",gap:"10px",marginBottom:"20px",flexWrap:"wrap"},children:[(0,o.jsxs)("a",{href:e,target:"_blank",rel:"noopener noreferrer",style:{display:"inline-flex",alignItems:"center",padding:"8px 16px",backgroundColor:"#f6f8fa",border:"1px solid #d0d7de",borderRadius:"6px",textDecoration:"none",color:"#24292f",fontSize:"14px",fontWeight:"500",transition:"background-color 0.2s"},onMouseEnter:e=>{e.target.style.backgroundColor="#f3f4f6"},onMouseLeave:e=>{e.target.style.backgroundColor="#f6f8fa"},children:[(0,o.jsx)("svg",{style:{marginRight:"8px",width:"16px",height:"16px"},fill:"currentColor",viewBox:"0 0 16 16",children:(0,o.jsx)("path",{d:"M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z"})}),"View Notebook on GitHub"]}),(0,o.jsxs)("button",{onClick:()=>{alert("Google Colab integration is not available yet. Please download the notebook and run it locally.")},style:{display:"inline-flex",alignItems:"center",padding:"8px 16px",backgroundColor:n?"#f6f8fa":"#f9ab00",border:"1px solid #d0d7de",borderRadius:"6px",color:n?"#656d76":"#fff",fontSize:"14px",fontWeight:"500",cursor:n?"not-allowed":"pointer",transition:"background-color 0.2s"},onMouseEnter:e=>{n||(e.target.style.backgroundColor="#e8a500")},onMouseLeave:e=>{n||(e.target.style.backgroundColor="#f9ab00")},children:[(0,o.jsx)("svg",{style:{marginRight:"8px",width:"16px",height:"16px"},fill:"currentColor",viewBox:"0 0 24 24",children:(0,o.jsx)("path",{d:"M16.9414 4.9757a7.033 7.033 0 0 0-4.9308 2.0324 7.033 7.033 0 0 0-.1232 9.8068l2.395-2.395a3.6455 3.6455 0 0 1 5.1497-5.1478l2.397-2.3989a7.033 7.033 0 0 0-4.8877-1.9175zM7.07 4.9855a7.033 7.033 0 0 0-4.8878 1.9175l2.395 2.3989a3.6434 3.6434 0 0 1 5.1497 5.1478l2.395 2.395a7.033 7.033 0 0 0-.1232-9.8068A7.033 7.033 0 0 0 7.07 4.9855zm15.0191 2.5923l-2.395 2.395a3.6455 3.6455 0 0 1-5.1497 5.1497l-2.395 2.395a7.033 7.033 0 0 0 9.9397-.123z"})}),n?"Colab (Coming Soon)":"Run in Google Colab"]})]})}}]);