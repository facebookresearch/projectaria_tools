"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[8340],{83922(e,n,i){i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>m,frontMatter:()=>s,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"projectariatools/pythontutorials/calibration","title":"Using Device Calibration Data from VRS","description":"<TutorialButtons","source":"@site/docs-research-tools/projectariatools/pythontutorials/calibration.mdx","sourceDirName":"projectariatools/pythontutorials","slug":"/projectariatools/pythontutorials/calibration","permalink":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/calibration","draft":false,"unlisted":false,"editUrl":"https://www.internalfb.com/code/fbsource/arvr/projects/ariane/aria_research_kit/projectaria_tools/website/docs-research-tools/projectariatools/pythontutorials/calibration.mdx","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Using Device Calibration Data from VRS"},"sidebar":"researchToolsSidebar","previous":{"title":"VrsDataProvider Basics","permalink":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/dataprovider"},"next":{"title":"Access Multi-Sensor Data Sequentially","permalink":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/queue"}}');var r=i(74848),t=i(28453),o=i(69470);const s={sidebar_position:2,title:"Using Device Calibration Data from VRS"},l="Tutorial 2: Device calibration",c={},d=[{value:"Introduction",id:"introduction",level:2},{value:"What You&#39;ll Learn",id:"what-youll-learn",level:3},{value:"Obtaining Device Calibration Content",id:"obtaining-device-calibration-content",level:3},{value:"Accessing Individual Sensor Calibration",id:"accessing-individual-sensor-calibration",level:3},{value:"1. Camera calibration content",id:"1-camera-calibration-content",level:4},{value:"2. IMU calibration content",id:"2-imu-calibration-content",level:4},{value:"3. Magnetometer, barometer, and microphone calibration content",id:"3-magnetometer-barometer-and-microphone-calibration-content",level:4},{value:"Camera Intrinsics: Project and Unproject",id:"camera-intrinsics-project-and-unproject",level:3},{value:"Camera Intrinsics: undistortion",id:"camera-intrinsics-undistortion",level:3},{value:"IMU Intrinsics: Measurement Rectification",id:"imu-intrinsics-measurement-rectification",level:3},{value:"5. Accessing Sensor Extrinsics",id:"5-accessing-sensor-extrinsics",level:3}];function _(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"tutorial-2-device-calibration",children:"Tutorial 2: Device calibration"})}),"\n",(0,r.jsx)(o.A,{notebookUrl:"https://github.com/facebookresearch/projectaria_tools/blob/2.1.0/examples/Gen2/python_notebooks/Tutorial_2_device_calibration.ipynb",colabUrl:"https://colab.research.google.com/github/facebookresearch/projectaria_tools/blob/2.1.0/examples/Gen2/python_notebooks/Tutorial_2_device_calibration.ipynb"}),"\n",(0,r.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,r.jsxs)(n.p,{children:["Most sensors in Aria glasses are calibrated both extrinsically and intrinsically, allowing you to rectify sensor measurements to real-world quantities. Calibration is performed per device, and the information is stored in the VRS file. This tutorial demonstrates how to work with device calibration in Project Aria using ",(0,r.jsx)(n.code,{children:"projectaria_tools"}),"."]}),"\n",(0,r.jsx)(n.h3,{id:"what-youll-learn",children:"What You'll Learn"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"How to obtain each sensor's calibration data"}),"\n",(0,r.jsx)(n.li,{children:"Camera calibration: projection and unprojection, and how to post-process images according to calibration (distort)."}),"\n",(0,r.jsx)(n.li,{children:"IMU calibration: measurement rectification."}),"\n",(0,r.jsx)(n.li,{children:'Multi-sensor coordination and sensor poses, and the concept of the "Device" frame.'}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Pre-requisite:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Familiarity with VRS basics from ",(0,r.jsx)(n.code,{children:"Tutorial_1_vrs_Data_provider_basics.ipynb"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["Download Aria Gen2 sample data from ",(0,r.jsx)(n.a,{href:"https://www.projectaria.com/async/sample/download/?bucket=core&filename=aria_gen2_sample_data_1.vrs",children:"link"})]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Note on Visualization"}),"\nIf visualization window is not showing up, this is due to ",(0,r.jsx)(n.code,{children:"Rerun"})," lib's caching issue. Just rerun the specific code cell."]}),"\n",(0,r.jsx)(n.h3,{id:"obtaining-device-calibration-content",children:"Obtaining Device Calibration Content"}),"\n",(0,r.jsxs)(n.p,{children:["Each VRS file's device calibration can be accessed as a ",(0,r.jsx)(n.code,{children:"DeviceCalibration"})," instance via the ",(0,r.jsx)(n.code,{children:"VrsDataProvider"})," API."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from projectaria_tools.core import data_provider\n\n# Load local VRS file\nvrs_file_path = "path/to/your/recording.vrs"\nvrs_data_provider = data_provider.create_vrs_data_provider(vrs_file_path)\n\n# Obtain device calibration\ndevice_calib = vrs_data_provider.get_device_calibration()\nif device_calib is None:\n    raise RuntimeError(\n        "device calibration does not exist! Please use a VRS that contains valid device calibration for this tutorial. "\n    )\n\n# You can obtain device version (Aria Gen1 vs Gen2), or device subtype (DVT with small/large frame width + short/long temple arms, etc) information from calibration\nif device_calib is not None:\n    device_version = device_calib.get_device_version()\n    device_subtype = device_calib.get_device_subtype()\n    print(f"Aria Device Version: {device_version}")\n    print(f"Device Subtype: {device_subtype}")\n'})}),"\n",(0,r.jsx)(n.h3,{id:"accessing-individual-sensor-calibration",children:"Accessing Individual Sensor Calibration"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"DeviceCalibration"})," provides APIs to query the intrinsics and extrinsics of each calibrated sensor."]}),"\n",(0,r.jsx)(n.h4,{id:"1-camera-calibration-content",children:"1. Camera calibration content"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Get sensor labels within device calibration\nall_labels = device_calib.get_all_labels()\nprint(f"All sensors within device calibration: {all_labels}")\nprint(f"Cameras: {device_calib.get_camera_labels()}")\n\n# Query a specific camera\'s calibration\nrgb_camera_label = "camera-rgb"\ncamera_calib = device_calib.get_camera_calib(rgb_camera_label)\n\nif camera_calib is None:\n    raise RuntimeError(\n        "camera-rgb calibration does not exist! Please use a VRS that contains valid RGB camera calibration for this tutorial. "\n    )\n\nprint(f"-------------- camera calibration for {rgb_camera_label} ----------------")\nprint(f"Image Size: {camera_calib.get_image_size()}")\nprint(f"Camera Model Type: {camera_calib.get_model_name()}")\nprint(\n    f"Camera Intrinsics Params: {camera_calib.get_projection_params()}, \\n"\n    f"where focal is {camera_calib.get_focal_lengths()}, "\n    f"and principal point is {camera_calib.get_principal_point()}\\n"\n)\n\n# Get extrinsics (device to camera transformation)\nT_device_camera = camera_calib.get_transform_device_camera()\nprint(f"Camera Extrinsics T_Device_Camera:\\n{T_device_camera.to_matrix()}")\n'})}),"\n",(0,r.jsx)(n.h4,{id:"2-imu-calibration-content",children:"2. IMU calibration content"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'imu_label = "imu-right"\nimu_calib = device_calib.get_imu_calib(imu_label)\n\nif imu_calib is None:\n    raise RuntimeError(\n        "imu-right calibration does not exist! Please use a VRS that contains valid IMU calibration for this tutorial. "\n    )\n\nprint(f"-------------- IMU calibration for {imu_label} ----------------")\n\n# Get IMU intrinsics parameters\naccel_bias = imu_calib.get_accel_model().get_bias()\naccel_rectification_matrix = imu_calib.get_accel_model().get_rectification()\ngyro_bias = imu_calib.get_gyro_model().get_bias()\ngyro_rectification_matrix = imu_calib.get_gyro_model().get_rectification()\n\nprint(f"Accelerometer Intrinsics:")\nprint(f"  Bias: {accel_bias}")\nprint(f"  Rectification Matrix:\\n{accel_rectification_matrix}")\n\nprint(f"Gyroscope Intrinsics:")\nprint(f"  Bias: {gyro_bias}")\nprint(f"  Rectification Matrix:\\n{gyro_rectification_matrix}")\n\n# Get extrinsics (device to IMU transformation)\nT_device_imu = imu_calib.get_transform_device_imu()\nprint(f"IMU Extrinsics T_Device_IMU:\\n{T_device_imu.to_matrix()}")\n\nprint(f"  \\n ------IMPORTANT----- \\n "\n      f"Please use .raw_to_rectified_[accel,gyro]() and .rectified_to_raw_[accel,gyro]() APIs to apply IMU calibration! \\n")\n'})}),"\n",(0,r.jsx)(n.h4,{id:"3-magnetometer-barometer-and-microphone-calibration-content",children:"3. Magnetometer, barometer, and microphone calibration content"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'print(f"Magnetometers: {device_calib.get_magnetometer_labels()}")\nprint(f"Barometers: {device_calib.get_barometer_labels()}")\nprint(f"Microphones: {device_calib.get_microphone_labels()}")\n\n# ----------------\n# Magnetometer calibration\n# ----------------\nmagnetometer_label = "mag0"\nmagnetometer_calib = device_calib.get_magnetometer_calib(magnetometer_label)\n\nif magnetometer_calib is None:\n    raise RuntimeError(\n        f"{magnetometer_label} calibration does not exist! Please use a VRS that contains valid magnetometer calibration for this tutorial."\n    )\n\n# Get magnetometer intrinsics parameters\nmag_bias = magnetometer_calib.get_model().get_bias()\nmag_rectification_matrix = magnetometer_calib.get_model().get_rectification()\n\nprint(f"Magnetometer calibration for {magnetometer_label} only have intrinsics:")\nprint(f"  Bias: {mag_bias}")\nprint(f"  Rectification Matrix:\\n{mag_rectification_matrix}")\n\n# ----------------\n# Barometer calibration\n# ----------------\nbaro_label = "baro0"\nbaro_calib = device_calib.get_barometer_calib(baro_label)\n\nif baro_calib is None:\n    raise RuntimeError(\n        f"{baro_label} calibration does not exist! Please use a VRS that contains valid barometer calibration for this tutorial."\n    )\n\nprint(f"Barometer calibration for {baro_label} only have intrinsics:")\nprint(f"  Slope: {baro_calib.get_slope()}")\nprint(f"  Offset in Pascal:\\n{baro_calib.get_offset_pa()}")\n\n# ----------------\n# Microphone calibration, containing both mic and speaker calibrations.\n# ----------------\nmicrophone_labels = device_calib.get_microphone_labels()\nspeaker_labels = device_calib.get_speaker_labels()\naudio_sensor_labels = device_calib.get_audio_labels()\nprint(f"Both mic and speakers are calibrated. \\n"\nf"List of mics that are calibrated: {microphone_labels} \\n"\nf"List of speakers that are calibrated: {speaker_labels}")\n\nfor audio_label in audio_sensor_labels:\n    audio_calib = device_calib.get_microphone_calib(audio_label)\n    if audio_calib is None:\n        print(f"Audio sensor calibration for {audio_label} is not available.")\n        continue\n\n    print(f"Audio sensor calibration for {audio_label} only has intrinsics:")\n    print(f"  sensitivity delta: {audio_calib.get_d_sensitivity_1k_dbv()}")\n\nprint(f"  \\n ------IMPORTANT----- \\n "\n    f"Please use .raw_to_rectified() and .rectified_to_raw() APIs to apply magnetometer, barometer, and microphone calibration!\\n")\n'})}),"\n",(0,r.jsx)(n.h3,{id:"camera-intrinsics-project-and-unproject",children:"Camera Intrinsics: Project and Unproject"}),"\n",(0,r.jsx)(n.p,{children:"A camera intrinsic model maps between a 3D point in the camera coordinate system and its corresponding 2D pixel on the sensor. This supports:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Projection:"})," 3D point \u2192 2D pixel"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Unprojection:"})," 2D pixel \u2192 3D ray"]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import numpy as np\n\n# Project 3D point to pixel\npoint_3d = np.array([0.1, 0.05, 1.0])  # Point in camera frame (meters)\npixel = camera_calib.project(point_3d)\nif pixel is not None:\n    print(f"3D point {point_3d} projected to -> pixel {pixel}")\nelse:\n    print(f"3D point {point_3d} projected out of camera sensor plane")\n\n# Unproject pixel to 3D ray.\ntest_pixel = np.array([400, 300])\nray_3d = camera_calib.unproject(test_pixel)\nprint(f"Pixel {test_pixel} unprojected to -> 3D ray {ray_3d}")\n'})}),"\n",(0,r.jsx)(n.h3,{id:"camera-intrinsics-undistortion",children:"Camera Intrinsics: undistortion"}),"\n",(0,r.jsx)(n.p,{children:"Camera calibration enables post-processing of Aria images, such as undistorting images from a fisheye to a linear camera model. Steps:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["Use ",(0,r.jsx)(n.code,{children:"vrs_data_provider"})," to access the camera image and calibration."]}),"\n",(0,r.jsxs)(n.li,{children:["Create a linear camera model using ",(0,r.jsx)(n.code,{children:"get_linear_camera_calibration"})," function."]}),"\n",(0,r.jsxs)(n.li,{children:["Apply ",(0,r.jsx)(n.code,{children:"distort_by_calibration"})," to distort or undistort the image from the actual Fisheye camera model to linear camera model."]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import rerun as rr\nfrom projectaria_tools.core import calibration\n\nrr.init("rerun_viz_image_undistortion")\n\n# We already obtained RGB camera calibration as `camera_calib`.\n# Now, create a linear camera model that is similar to camera_calib\nlinear_camera_model = calibration.get_linear_camera_calibration(\n    image_width=camera_calib.get_image_size()[0],\n    image_height=camera_calib.get_image_size()[1],\n    focal_length=camera_calib.get_focal_lengths()[0],\n    label="test_linear_camera",\n)\n\nrgb_stream_id = vrs_data_provider.get_stream_id_from_label("camera-rgb")\nnum_samples = vrs_data_provider.get_num_data(rgb_stream_id)\n\n# Plot a few frames from RGB camera, and also plot the undistorted images\nfirst_few = min(10, num_samples)\nfor i in range(first_few):\n    # Query RGB images\n    image_data, image_record = vrs_data_provider.get_image_data_by_index(\n        rgb_stream_id, i\n    )\n    if not image_data.is_valid():\n        continue\n\n    # Plot original RGB image\n    timestamp_ns = image_record.capture_timestamp_ns\n    rr.set_time_nanos("device_time", timestamp_ns)\n    rr.log("camera_rgb", rr.Image(image_data.to_numpy_array()))\n\n    # Undistort RGB image to a linear camera model\n    undistorted_image = calibration.distort_by_calibration(\n        arraySrc=image_data.to_numpy_array(),\n        dstCalib=linear_camera_model,\n        srcCalib=camera_calib,\n    )\n    rr.log("undistorted_camera_rgb", rr.Image(undistorted_image))\n\nrr.notebook_show()\n'})}),"\n",(0,r.jsx)(n.h3,{id:"imu-intrinsics-measurement-rectification",children:"IMU Intrinsics: Measurement Rectification"}),"\n",(0,r.jsxs)(n.p,{children:["IMU intrinsics are represented by an affine model. The raw sensor readout (",(0,r.jsx)(n.code,{children:"value_raw"}),") is compensated to obtain the real acceleration or angular velocity (",(0,r.jsx)(n.code,{children:"value_compensated"}),"):"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"value_compensated = M^-1 * (value_raw - bias)\n"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"M"})," is an upper triangular matrix (no global rotation between IMU body and accelerometer frame)."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"To simulate sensor readout from real values:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"value_raw = M * value_compensated + bias\n"})}),"\n",(0,r.jsx)(n.p,{children:"Note that in the following example, the difference between raw reading and compensated IMU signals are pretty close, therefore the plotting may look similar."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def _set_imu_plot_colors(rerun_plot_label):\n    """\n    A helper function to set colors for the IMU plots in rerun\n    """\n    rr.log(\n        f"{rerun_plot_label}/accl/x[m/sec2]",\n        rr.SeriesLine(color=[230, 25, 75], name="accel/x[m/sec2]"),\n        static=True,\n    )  # Red\n    rr.log(\n        f"{rerun_plot_label}/accl/y[m/sec2]",\n        rr.SeriesLine(color=[60, 180, 75], name="accel/y[m/sec2]"),\n        static=True,\n    )  # Green\n    rr.log(\n        f"{rerun_plot_label}/accl/z[m/sec2]",\n        rr.SeriesLine(color=[0, 130, 200], name="accel/z[m/sec2]"),\n        static=True,\n    )  # Blue\n    rr.log(\n        f"{rerun_plot_label}/gyro/x[rad/sec2]",\n        rr.SeriesLine(color=[245, 130, 48], name="gyro/x[rad/sec2]"),\n        static=True,\n    )  # Orange\n    rr.log(\n        f"{rerun_plot_label}/gyro/y[rad/sec2]",\n        rr.SeriesLine(color=[145, 30, 180], name="gyro/y[rad/sec2]"),\n        static=True,\n    )  # Purple\n    rr.log(\n        f"{rerun_plot_label}/gyro/z[rad/sec2]",\n        rr.SeriesLine(color=[70, 240, 240], name="gyro/z[rad/sec2]"),\n        static=True,\n    )  # Cyan\n\n\ndef _plot_imu_signals(accel_data, gyro_data, rerun_plot_label):\n    """\n    This is a helper function to plot IMU signals in Rerun 1D plot\n    """\n    rr.log(\n        f"{rerun_plot_label}/accl/x[m/sec2]",\n        rr.Scalar(accel_data[0]),\n    )\n    rr.log(\n        f"{rerun_plot_label}/accl/y[m/sec2]",\n        rr.Scalar(accel_data[1]),\n    )\n    rr.log(\n        f"{rerun_plot_label}/accl/z[m/sec2]",\n        rr.Scalar(accel_data[2]),\n    )\n    rr.log(\n        f"{rerun_plot_label}/gyro/x[rad/sec2]",\n        rr.Scalar(gyro_data[0]),\n    )\n    rr.log(\n        f"{rerun_plot_label}/gyro/y[rad/sec2]",\n        rr.Scalar(gyro_data[1]),\n    )\n    rr.log(\n        f"{rerun_plot_label}/gyro/z[rad/sec2]",\n        rr.Scalar(gyro_data[2]),\n    )\n\n\nrr.init("rerun_viz_imu_rectification")\n\nimu_label = "imu-right"\nimu_calib = device_calib.get_imu_calib(imu_label)\nimu_stream_id = vrs_data_provider.get_stream_id_from_label(imu_label)\nif imu_calib is None or imu_stream_id is None:\n    raise RuntimeError(\n        "imu-right calibration or stream data does not exist! Please use a VRS that contains valid IMU calibration and data for this tutorial. "\n    )\n\nnum_samples = vrs_data_provider.get_num_data(imu_stream_id)\nfirst_few = min(5000, num_samples)\n\n# Set same colors for both plots\n_set_imu_plot_colors("imu_right")\n_set_imu_plot_colors("imu_right_compensated")\n\nfor i in range(0, first_few, 50):\n    # Query IMU data\n    imu_data = vrs_data_provider.get_imu_data_by_index(imu_stream_id, i)\n\n    # Plot raw IMU readings\n    rr.set_time_nanos("device_time", imu_data.capture_timestamp_ns)\n\n    # Get compensated imu data\n    compensated_accel = imu_calib.raw_to_rectified_accel(imu_data.accel_msec2)\n    compensated_gyro = imu_calib.raw_to_rectified_gyro(imu_data.gyro_radsec)\n\n    # print one sample content\n    if i == 0:\n        print(\n            f"IMU compensation: raw accel {imu_data.accel_msec2} , compensated accel {compensated_accel}"\n        )\n        print(\n            f"IMU compensation: raw gyro {imu_data.gyro_radsec} , compensated gyro {compensated_gyro}"\n        )\n\n    # Plot raw IMU readings\n    _plot_imu_signals(imu_data.accel_msec2, imu_data.gyro_radsec, "imu_right")\n\n    # Plot compensated IMU readings in a separate plot\n    _plot_imu_signals(compensated_accel, compensated_gyro, "imu_right_compensated")\n\nrr.notebook_show()\n'})}),"\n",(0,r.jsx)(n.h3,{id:"5-accessing-sensor-extrinsics",children:"5. Accessing Sensor Extrinsics"}),"\n",(0,r.jsx)(n.p,{children:"The core API to query sensor extrinsics is:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"get_transform_device_sensor(label = sensor_label, use_cad_calib = False)\n"})}),"\n",(0,r.jsxs)(n.p,{children:["This API returns the extrinsics of the sensor, represented as a ",(0,r.jsx)(n.code,{children:"Sophus::SE3"})," (translation + rotation).  in the reference coordinate frame of ",(0,r.jsx)(n.code,{children:"Device"}),"."]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["The ",(0,r.jsx)(n.code,{children:"Device"})," frame is the reference coordinate system for all sensors."]}),"\n",(0,r.jsxs)(n.li,{children:['For Aria-Gen2, the "Device" frame is the left front-facing SLAM camera (',(0,r.jsx)(n.code,{children:"slam-front-left"}),")."]}),"\n",(0,r.jsx)(n.li,{children:"All sensor extrinsics are defined relative to this frame."}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["The optional parameter ",(0,r.jsx)(n.code,{children:"use_cad_calib"}),' controls the "source" of the sensor extrinsics.']}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"use_cad_calib=False"})," (default): this will return the sensor extrinsics from factory calibration, if the sensor's extrinsics is factory-calibrated. This include:","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Cameras"}),"\n",(0,r.jsx)(n.li,{children:"IMUs"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"use_cad_calib=True"}),": this will return the sensor's extrinsics in their designed location in CAD.  This is useful for sensors without factory-calibrated extrinsics, including:","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Magnetometer"}),"\n",(0,r.jsx)(n.li,{children:"Barometer"}),"\n",(0,r.jsx)(n.li,{children:"Microphones"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from projectaria_tools.utils.rerun_helpers import (\n    AriaGlassesOutline,\n    ToTransform3D,\n    ToBox3D,\n)\n\nrr.init("rerun_viz_sensor_extrinsics")\n\n# Obtain a glass outline for visualization. This outline uses factory calibration extrinsics if possible, uses CAD extrinsics if factory calibration is not available.\nglass_outline = AriaGlassesOutline(device_calib, use_cad_calib=False)\nrr.log("device/glasses_outline", rr.LineStrips3D([glass_outline]), static=True)\n\n# Plot all the sensor locations from either factory calibration (if available) or CAD\nsensor_labels = device_calib.get_all_labels()\ncamera_labels = device_calib.get_camera_labels()\nfor sensor in sensor_labels:\n    # Query for sensor extrinsics from factory calibration if possible. Fall back to CAD values if unavailable.\n    if ("camera" in sensor) or ("imu" in sensor):\n        T_device_sensor = device_calib.get_transform_device_sensor(label = sensor, get_cad_value = False)\n    else:\n        T_device_sensor = device_calib.get_transform_device_sensor(label = sensor, get_cad_value = True)\n\n    # Skip if extrinsics cannot be obtained\n    if T_device_sensor is None:\n        print(f"Warning: sensor {sensor} does not have extrinsics from neither factory calibration nor CAD, skipping the plotting.")\n        continue\n\n    # Plot sensor labels\n    rr.log(f"device/{sensor}", ToTransform3D(T_device_sensor), static=True)\n    rr.log(\n        f"device/{sensor}/text",\n        ToBox3D(sensor, [1e-5, 1e-5, 1e-5]),\n        static=True,\n    )\n\n    # For cameras, also plot camera frustum\n    if sensor in camera_labels:\n        camera_calibration = device_calib.get_camera_calib(sensor)\n        rr.log(f"device/{sensor}_frustum", ToTransform3D(T_device_sensor), static=True)\n        rr.log(\n            f"device/{sensor}_frustum",\n            rr.Pinhole(\n                resolution=[\n                    camera_calibration.get_image_size()[0],\n                    camera_calibration.get_image_size()[1],\n                ],\n                focal_length=float(camera_calibration.get_focal_lengths()[0]),\n            ),\n            static=True,\n        )\nrr.notebook_show()\n'})})]})}function m(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(_,{...e})}):_(e)}},69470(e,n,i){i.d(n,{A:()=>r});i(96540);var a=i(74848);const r=({notebookUrl:e,colabUrl:n,colabDisabled:i=!1})=>{const r=()=>(0,a.jsx)("svg",{style:{marginRight:"8px",width:"16px",height:"16px"},fill:"currentColor",viewBox:"0 0 24 24",children:(0,a.jsx)("path",{d:"M16.9414 4.9757a7.033 7.033 0 0 0-4.9308 2.0324 7.033 7.033 0 0 0-.1232 9.8068l2.395-2.395a3.6455 3.6455 0 0 1 5.1497-5.1478l2.397-2.3989a7.033 7.033 0 0 0-4.8877-1.9175zM7.07 4.9855a7.033 7.033 0 0 0-4.8878 1.9175l2.395 2.3989a3.6434 3.6434 0 0 1 5.1497 5.1478l2.395 2.395a7.033 7.033 0 0 0-.1232-9.8068A7.033 7.033 0 0 0 7.07 4.9855zm15.0191 2.5923l-2.395 2.395a3.6455 3.6455 0 0 1-5.1497 5.1497l-2.395 2.395a7.033 7.033 0 0 0 9.9397-.123z"})});return(0,a.jsxs)("div",{style:{display:"flex",gap:"10px",marginBottom:"20px",flexWrap:"wrap"},children:[(0,a.jsxs)("a",{href:e,target:"_blank",rel:"noopener noreferrer",style:{display:"inline-flex",alignItems:"center",padding:"8px 16px",backgroundColor:"#f6f8fa",border:"1px solid #d0d7de",borderRadius:"6px",textDecoration:"none",color:"#24292f",fontSize:"14px",fontWeight:"500",transition:"background-color 0.2s"},onMouseEnter:e=>{e.target.style.backgroundColor="#f3f4f6"},onMouseLeave:e=>{e.target.style.backgroundColor="#f6f8fa"},children:[(0,a.jsx)("svg",{style:{marginRight:"8px",width:"16px",height:"16px"},fill:"currentColor",viewBox:"0 0 16 16",children:(0,a.jsx)("path",{d:"M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z"})}),"View Notebook on GitHub"]}),!i&&n?(0,a.jsxs)("a",{href:n,target:"_blank",rel:"noopener noreferrer",style:{display:"inline-flex",alignItems:"center",padding:"8px 16px",backgroundColor:"#f9ab00",border:"1px solid #d0d7de",borderRadius:"6px",textDecoration:"none",color:"#fff",fontSize:"14px",fontWeight:"500",transition:"background-color 0.2s"},onMouseEnter:e=>{e.target.style.backgroundColor="#e8a500"},onMouseLeave:e=>{e.target.style.backgroundColor="#f9ab00"},children:[(0,a.jsx)(r,{}),"Run in Google Colab"]}):(0,a.jsxs)("button",{onClick:()=>{alert("Google Colab integration is not available yet. Please download the notebook and run it locally.")},style:{display:"inline-flex",alignItems:"center",padding:"8px 16px",backgroundColor:"#f6f8fa",border:"1px solid #d0d7de",borderRadius:"6px",color:"#656d76",fontSize:"14px",fontWeight:"500",cursor:"not-allowed",transition:"background-color 0.2s"},children:[(0,a.jsx)(r,{}),"Colab (Coming Soon)"]})]})}},28453(e,n,i){i.d(n,{R:()=>o,x:()=>s});var a=i(96540);const r={},t=a.createContext(r);function o(e){const n=a.useContext(t);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),a.createElement(t.Provider,{value:n},e.children)}}}]);