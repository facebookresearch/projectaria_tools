"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[7315],{28453:(e,t,i)=>{i.d(t,{R:()=>o,x:()=>s});var n=i(96540);const a={},r=n.createContext(a);function o(e){const t=n.useContext(r);return n.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function s(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),n.createElement(r.Provider,{value:t},e.children)}},43271:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>l,contentTitle:()=>d,default:()=>m,frontMatter:()=>s,metadata:()=>n,toc:()=>_});const n=JSON.parse('{"id":"dataset/pilot/tutorials/algorithm_loading","title":"Aria Gen2 Pilot Dataset Tutorial - Algorithm Data Loading","description":"<TutorialButtons","source":"@site/docs-research-tools/dataset/pilot/tutorials/algorithm_loading.mdx","sourceDirName":"dataset/pilot/tutorials","slug":"/dataset/pilot/tutorials/algorithm_loading","permalink":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/algorithm_loading","draft":false,"unlisted":false,"editUrl":"https://www.internalfb.com/code/fbsource/arvr/projects/ariane/aria_research_kit/projectaria_tools/website/docs-research-tools/dataset/pilot/tutorials/algorithm_loading.mdx","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Aria Gen2 Pilot Dataset Tutorial - Algorithm Data Loading"},"sidebar":"researchToolsSidebar","previous":{"title":"Aria Gen2 Pilot Dataset Tutorial - MPS Data Loading","permalink":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/mps_loading"},"next":{"title":"Aria Gen2 Pilot Dataset Tutorial - Multi-Sequence Timestamp Alignment","permalink":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/multi_sequences_timestamp_alignment"}}');var a=i(74848),r=i(28453),o=i(69470);const s={sidebar_position:2,title:"Aria Gen2 Pilot Dataset Tutorial - Algorithm Data Loading"},d=void 0,l={},_=[{value:"What You&#39;ll Learn",id:"what-youll-learn",level:2},{value:"Algorithm Data Overview",id:"algorithm-data-overview",level:2},{value:"Import Required Libraries",id:"import-required-libraries",level:2},{value:"Initialize Data Provider",id:"initialize-data-provider",level:2},{value:"Check Available Algorithm Data",id:"check-available-algorithm-data",level:2},{value:"Heart Rate Monitoring",id:"heart-rate-monitoring",level:2},{value:"Heart Rate Data Structure",id:"heart-rate-data-structure",level:3},{value:"Heart Rate API Reference",id:"heart-rate-api-reference",level:3},{value:"Diarization",id:"diarization",level:2},{value:"Diarization Data Structure",id:"diarization-data-structure",level:3},{value:"Diarization API Reference",id:"diarization-api-reference",level:3},{value:"Hand-Object Interaction",id:"hand-object-interaction",level:2},{value:"Hand-Object Interaction Data Structure",id:"hand-object-interaction-data-structure",level:3},{value:"Hand-Object Interaction API Reference",id:"hand-object-interaction-api-reference",level:3},{value:"Egocentric Voxel Lifting",id:"egocentric-voxel-lifting",level:2},{value:"Egocentric Voxel Lifting Data Structure",id:"egocentric-voxel-lifting-data-structure",level:3},{value:"Egocentric Voxel Lifting API Reference",id:"egocentric-voxel-lifting-api-reference",level:3},{value:"Foundation Stereo Depth",id:"foundation-stereo-depth",level:2},{value:"Foundation Stereo Data Structure",id:"foundation-stereo-data-structure",level:3},{value:"Foundation Stereo API Reference",id:"foundation-stereo-api-reference",level:3},{value:"Summary",id:"summary",level:2},{value:"Key Concepts Covered",id:"key-concepts-covered",level:3},{value:"Important Notes",id:"important-notes",level:3}];function c(e){const t={a:"a",code:"code",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(o.A,{notebookUrl:"https://github.com/facebookresearch/projectaria_gen2_pilot_dataset/blob/main/examples/tutorial_3_algorithm_data_loading.ipynb",colabDisabled:!0}),"\n",(0,a.jsxs)(t.p,{children:["This tutorial demonstrates how to load and visualize algorithm output data from the Aria Gen2 Pilot Dataset using the ",(0,a.jsx)(t.code,{children:"AriaGen2PilotDataProvider"}),"."]}),"\n",(0,a.jsx)(t.h2,{id:"what-youll-learn",children:"What You'll Learn"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:"Load and visualize Heart Rate monitoring data"}),"\n",(0,a.jsx)(t.li,{children:"Access Diarization (speaker identification) results"}),"\n",(0,a.jsx)(t.li,{children:"Work with Hand-Object Interaction segmentation data"}),"\n",(0,a.jsx)(t.li,{children:"Explore Egocentric Voxel Lifting 3D scene reconstruction"}),"\n",(0,a.jsx)(t.li,{children:"Process Foundation Stereo depth estimation data"}),"\n",(0,a.jsx)(t.li,{children:"Understand data structures and API patterns for algorithm outputs"}),"\n"]}),"\n",(0,a.jsx)(t.h2,{id:"algorithm-data-overview",children:"Algorithm Data Overview"}),"\n",(0,a.jsxs)(t.p,{children:["The Aria Gen2 Pilot Dataset includes 5 types of algorithm outputs. Please find the introduction to algorithms ",(0,a.jsx)(t.a,{href:"https://facebookresearch.github.io/projectaria_tools/gen2/research-tools/dataset/pilot/content#additional-perception-algorithms",children:"here"}),"."]}),"\n",(0,a.jsxs)(t.ol,{children:["\n",(0,a.jsx)(t.li,{children:(0,a.jsx)(t.strong,{children:"Heart Rate Monitoring"})}),"\n",(0,a.jsx)(t.li,{children:(0,a.jsx)(t.strong,{children:"Diarization"})}),"\n",(0,a.jsx)(t.li,{children:(0,a.jsx)(t.strong,{children:"Hand-Object Interaction"})}),"\n",(0,a.jsx)(t.li,{children:(0,a.jsx)(t.strong,{children:"Egocentric Voxel Lifting"})}),"\n",(0,a.jsx)(t.li,{children:(0,a.jsx)(t.strong,{children:"Foundation Stereo"})}),"\n"]}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.strong,{children:"Important Notes:"})}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:["These are ",(0,a.jsx)(t.strong,{children:"algorithm outputs"})," (post-processed results), distinct from raw VRS sensor data"]}),"\n",(0,a.jsx)(t.li,{children:"Algorithm data availability varies by sequence - not all sequences contain all algorithm outputs"}),"\n",(0,a.jsx)(t.li,{children:"Each algorithm has its own data structure and query patterns"}),"\n"]}),"\n",(0,a.jsx)(t.h2,{id:"import-required-libraries",children:"Import Required Libraries"}),"\n",(0,a.jsx)(t.p,{children:"The following libraries are required for this tutorial:"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"# Standard library imports\nimport numpy as np\nimport os\nfrom pathlib import Path\nfrom datetime import timedelta\n\n# Project Aria Tools imports\nfrom projectaria_tools.core.stream_id import StreamId\nfrom projectaria_tools.core.sensor_data import TimeDomain, TimeQueryOptions\nfrom projectaria_tools.core.calibration import DeviceCalibration\nfrom projectaria_tools.utils.rerun_helpers import (\n    create_hand_skeleton_from_landmarks,\n    AriaGlassesOutline,\n    ToTransform3D\n)\n\n# Aria Gen2 Pilot Dataset imports\nfrom aria_gen2_pilot_dataset import AriaGen2PilotDataProvider\nfrom aria_gen2_pilot_dataset.data_provider.aria_gen2_pilot_dataset_data_types import (\n    HeartRateData,\n    DiarizationData,\n    HandObjectInteractionData,\n    BoundingBox3D,\n    BoundingBox2D,\n    CameraIntrinsicsAndPose\n)\n\n# Visualization library\nimport rerun as rr\n"})}),"\n",(0,a.jsx)(t.h2,{id:"initialize-data-provider",children:"Initialize Data Provider"}),"\n",(0,a.jsxs)(t.p,{children:["The ",(0,a.jsx)(t.code,{children:"AriaGen2PilotDataProvider"})," is the main interface for accessing data from the Aria Gen2 Pilot Dataset. It provides methods to query algorithm data, check availability, and access device calibration information."]}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"\u26a0\ufe0f Important:"})," Update the ",(0,a.jsx)(t.code,{children:"sequence_path"})," below to point to your downloaded Aria Gen2 Pilot Dataset sequence folder."]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'# Replace with the actual path to your downloaded sequence folder\nsequence_path = "path/to/your/sequence_folder"\n\n# Initialize the data provider\npilot_data_provider = AriaGen2PilotDataProvider(sequence_path)\n'})}),"\n",(0,a.jsx)(t.h2,{id:"check-available-algorithm-data",children:"Check Available Algorithm Data"}),"\n",(0,a.jsx)(t.p,{children:"Each Aria Gen2 Pilot dataset sequence may contain different algorithm outputs. Let's check what's available in this sequence."}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"# Check what algorithm data types are available in this sequence\nprint(\"Algorithm Data Availability in This Sequence:\")\nprint(\"=\" * 60)\nprint(f\"Heart Rate Monitoring:      {'\u2705' if pilot_data_provider.has_heart_rate_data() else '\u274c'}\")\nprint(f\"Diarization:               {'\u2705' if pilot_data_provider.has_diarization_data() else '\u274c'}\")\nprint(f\"Hand-Object Interaction:    {'\u2705' if pilot_data_provider.has_hand_object_interaction_data() else '\u274c'}\")\nprint(f\"Egocentric Voxel Lifting:   {'\u2705' if pilot_data_provider.has_egocentric_voxel_lifting_data() else '\u274c'}\")\nprint(f\"Foundation Stereo:         {'\u2705' if pilot_data_provider.has_stereo_depth_data() else '\u274c'}\")\nprint(\"=\" * 60)\n\n# Count available algorithms\navailable_algorithms = [\n    pilot_data_provider.has_heart_rate_data(),\n    pilot_data_provider.has_diarization_data(),\n    pilot_data_provider.has_hand_object_interaction_data(),\n    pilot_data_provider.has_egocentric_voxel_lifting_data(),\n    pilot_data_provider.has_stereo_depth_data()\n]\navailable_count = sum(available_algorithms)\nprint(f\"\\nTotal available algorithms: {available_count}/5\")\n"})}),"\n",(0,a.jsx)(t.h2,{id:"heart-rate-monitoring",children:"Heart Rate Monitoring"}),"\n",(0,a.jsx)(t.p,{children:"Heart rate monitoring provides physiological data extracted from PPG (Photoplethysmography) sensors in the Aria glasses."}),"\n",(0,a.jsx)(t.h3,{id:"heart-rate-data-structure",children:"Heart Rate Data Structure"}),"\n",(0,a.jsxs)(t.p,{children:["The ",(0,a.jsx)(t.code,{children:"HeartRateData"})," class contains:"]}),"\n",(0,a.jsxs)(t.table,{children:[(0,a.jsx)(t.thead,{children:(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.th,{children:"Field Name"}),(0,a.jsx)(t.th,{children:"Type"}),(0,a.jsx)(t.th,{children:"Description"})]})}),(0,a.jsxs)(t.tbody,{children:[(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:(0,a.jsx)(t.code,{children:"timestamp_ns"})}),(0,a.jsx)(t.td,{children:"int"}),(0,a.jsx)(t.td,{children:"Timestamp in device time domain (nanoseconds)"})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:(0,a.jsx)(t.code,{children:"heart_rate_bpm"})}),(0,a.jsx)(t.td,{children:"int"}),(0,a.jsx)(t.td,{children:"Heart rate in beats per minute"})]})]})]}),"\n",(0,a.jsx)(t.h3,{id:"heart-rate-api-reference",children:"Heart Rate API Reference"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.code,{children:"has_heart_rate_data()"}),": Check if heart rate data is available"]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.code,{children:"get_heart_rate_by_index(index)"}),": Get heart rate data by index"]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.code,{children:"get_heart_rate_by_timestamp_ns(timestamp_ns, time_domain, time_query_options)"}),": Get heart rate data by timestamp"]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.code,{children:"get_heart_rate_total_number()"}),": Get total number of heart rate entries"]}),"\n"]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'# Heart Rate Data Loading and Analysis\nif pilot_data_provider.has_heart_rate_data():\n    print("\u2705 Heart Rate data is available")\n\n    # Get total number of heart rate entries\n    total_heart_rate = pilot_data_provider.get_heart_rate_total_number()\n    print(f"Total heart rate entries: {total_heart_rate}")\n\n    # Sample first few heart rate entries\n    print("\\n=== Heart Rate Data Sample ===")\n    sample_count = min(5, total_heart_rate)\n    for i in range(sample_count):\n        heart_rate_data = pilot_data_provider.get_heart_rate_by_index(i)\n        if heart_rate_data is not None:\n            print(f"Entry {i}: timestamp={heart_rate_data.timestamp_ns} ns, heart_rate={heart_rate_data.heart_rate_bpm} bpm")\n\n    # Query heart rate data by timestamp\n    if total_heart_rate > 0:\n        # Get a sample timestamp from the middle of the sequence\n        sample_heart_rate = pilot_data_provider.get_heart_rate_by_index(total_heart_rate // 2)\n        if sample_heart_rate is not None:\n            query_timestamp = sample_heart_rate.timestamp_ns\n\n            # Query heart rate at this timestamp\n            heart_rate_at_time = pilot_data_provider.get_heart_rate_by_timestamp_ns(\n                query_timestamp, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST\n            )\n\n            if heart_rate_at_time is not None:\n                print(f"\\nHeart rate at timestamp {query_timestamp}: {heart_rate_at_time.heart_rate_bpm} bpm")\nelse:\n    print("\u274c Heart Rate data is not available in this sequence")\n'})}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'# Heart Rate Visualization\nif pilot_data_provider.has_heart_rate_data():\n    print("\\n=== Visualizing Heart Rate Data ===")\n\n    # Initialize Rerun for visualization\n    rr.init("rerun_viz_heart_rate")\n    rr.notebook_show()\n\n    # Get all heart rate data for time series visualization\n    total_heart_rate = pilot_data_provider.get_heart_rate_total_number()\n\n    # Sample heart rate data (every 10th entry for performance)\n    sample_indices = range(0, total_heart_rate, max(1, total_heart_rate // 50))\n\n    for i in sample_indices:\n        heart_rate_data = pilot_data_provider.get_heart_rate_by_index(i)\n        if heart_rate_data is not None:\n            # Convert timestamp to seconds for visualization\n            timestamp_seconds = heart_rate_data.timestamp_ns / 1e9\n\n            # Set time and log heart rate as scalar (following visualizer pattern)\n            rr.set_time_seconds("device_time", timestamp_seconds)\n            rr.log("heart_rate_bpm", rr.Scalar(heart_rate_data.heart_rate_bpm))\nelse:\n    print("Skipping heart rate visualization - no heart rate data available.")\n'})}),"\n",(0,a.jsx)(t.h2,{id:"diarization",children:"Diarization"}),"\n",(0,a.jsx)(t.p,{children:"Diarization provides speaker identification and voice activity detection from audio data."}),"\n",(0,a.jsx)(t.h3,{id:"diarization-data-structure",children:"Diarization Data Structure"}),"\n",(0,a.jsxs)(t.p,{children:["The ",(0,a.jsx)(t.code,{children:"DiarizationData"})," class contains:"]}),"\n",(0,a.jsxs)(t.table,{children:[(0,a.jsx)(t.thead,{children:(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.th,{children:"Field Name"}),(0,a.jsx)(t.th,{children:"Type"}),(0,a.jsx)(t.th,{children:"Description"})]})}),(0,a.jsxs)(t.tbody,{children:[(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:(0,a.jsx)(t.code,{children:"start_timestamp_ns"})}),(0,a.jsx)(t.td,{children:"int"}),(0,a.jsx)(t.td,{children:"Start timestamp in device time domain (nanoseconds)"})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:(0,a.jsx)(t.code,{children:"end_timestamp_ns"})}),(0,a.jsx)(t.td,{children:"int"}),(0,a.jsx)(t.td,{children:"End timestamp in device time domain (nanoseconds)"})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:(0,a.jsx)(t.code,{children:"speaker"})}),(0,a.jsx)(t.td,{children:"str"}),(0,a.jsx)(t.td,{children:"Unique identifier of the speaker"})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:(0,a.jsx)(t.code,{children:"content"})}),(0,a.jsx)(t.td,{children:"str"}),(0,a.jsx)(t.td,{children:"ASR transcription text"})]})]})]}),"\n",(0,a.jsx)(t.h3,{id:"diarization-api-reference",children:"Diarization API Reference"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.code,{children:"has_diarization_data()"}),": Check if diarization data is available"]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.code,{children:"get_diarization_data_by_index(index)"}),": Get diarization data by index"]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.code,{children:"get_diarization_data_by_timestamp_ns(timestamp_ns, time_domain)"}),": Get diarization data containing timestamp (returns list)"]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.code,{children:"get_diarization_data_by_start_and_end_timestamps(start_ns, end_ns, time_domain)"}),": Get diarization data in time range"]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.code,{children:"get_diarization_data_total_number()"}),": Get total number of diarization entries"]}),"\n"]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'# Diarization Data Loading and Analysis\nif pilot_data_provider.has_diarization_data():\n    print("\u2705 Diarization data is available")\n\n    # Get total number of diarization entries\n    total_diarization = pilot_data_provider.get_diarization_data_total_number()\n    print(f"Total diarization entries: {total_diarization}")\n\n    # Sample first few diarization entries\n    print("\\n=== Diarization Data Sample ===")\n    sample_count = min(3, total_diarization)\n    for i in range(sample_count):\n        diarization_data = pilot_data_provider.get_diarization_data_by_index(i)\n        if diarization_data is not None:\n            duration_ms = (diarization_data.end_timestamp_ns - diarization_data.start_timestamp_ns) / 1e6\n            print(f"Entry {i}:")\n            print(f"  Speaker: {diarization_data.speaker}")\n            print(f"  Duration: {duration_ms:.1f} ms")\n            print(f"  Content: {diarization_data.content[:100]}{\'...\' if len(diarization_data.content) > 100 else \'\'}")\n            print()\n\n    # Query diarization data by timestamp\n    if total_diarization > 0:\n        # Get a sample timestamp from the middle of the sequence\n        sample_diarization = pilot_data_provider.get_diarization_data_by_index(total_diarization // 2)\n        if sample_diarization is not None:\n            query_timestamp = sample_diarization.start_timestamp_ns\n\n            # Query diarization at this timestamp\n            diarization_at_time = pilot_data_provider.get_diarization_data_by_timestamp_ns(\n                query_timestamp, TimeDomain.DEVICE_TIME\n            )\n\n            print(f"Diarization entries at timestamp {query_timestamp}: {len(diarization_at_time)}")\n            for entry in diarization_at_time[:2]:  # Show first 2 entries\n                print(f"  Speaker: {entry.speaker}, Content: {entry.content[:50]}...")\nelse:\n    print("\u274c Diarization data is not available in this sequence")\n'})}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'# Diarization Visualization\nif pilot_data_provider.has_diarization_data():\n    print("\\n=== Visualizing Diarization Data ===")\n\n    # Initialize Rerun for visualization\n    rr.init("rerun_viz_diarization")\n    rr.notebook_show()\n\n    # Get RGB camera stream for overlay\n    rgb_stream_id = pilot_data_provider.get_vrs_stream_id_from_label("camera-rgb")\n\n    if rgb_stream_id is not None:\n        # Get time bounds for RGB images\n        first_timestamp_ns = pilot_data_provider.get_vrs_timestamps_ns(rgb_stream_id, TimeDomain.DEVICE_TIME)[0]\n\n        # Sample a few RGB frames for visualization\n        sample_timestamps = []\n        for i in range(50, min(100, pilot_data_provider.get_vrs_num_data(rgb_stream_id)), 2):\n            rgb_data, rgb_record = pilot_data_provider.get_vrs_image_data_by_index(rgb_stream_id, i)\n            sample_timestamps.append(rgb_record.capture_timestamp_ns)\n\n        # Visualize RGB images with diarization overlay\n        for timestamp_ns in sample_timestamps:\n            # Get RGB image\n            rgb_data, rgb_record = pilot_data_provider.get_vrs_image_data_by_time_ns(\n                rgb_stream_id, timestamp_ns, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST\n            )\n\n            if rgb_data.is_valid():\n                # Visualize the RGB image\n                rr.set_time_nanos("device_time", rgb_record.capture_timestamp_ns)\n                rr.log("camera_rgb", rr.Image(rgb_data.to_numpy_array()))\n\n                # Get diarization data for this timestamp\n                diarization_entries = pilot_data_provider.get_diarization_data_by_timestamp_ns(\n                    timestamp_ns, TimeDomain.DEVICE_TIME\n                )\n\n                # Add diarization text overlay (following visualizer pattern)\n                if diarization_entries:\n                    # Get image dimensions for positioning (following visualizer logic)\n                    width, height = rgb_data.get_width(), rgb_data.get_height()\n\n                    # Clear previous diarization overlays\n                    rr.log("camera_rgb/diarization", rr.Clear.recursive())\n\n                    # Plot each diarization entry (following visualizer pattern exactly)\n                    for i, conv_data in enumerate(diarization_entries[:3]):  # Show first 3 entries\n                        text_content = f"{conv_data.speaker}: {conv_data.content}"\n                        text_x = width // 2  # Center horizontally\n                        text_y = height - height // 15 - (i * 10 * 7)  # Bottom positioning with vertical spacing\n\n                        rr.log(\n                            f"camera_rgb/diarization/conversation_text_{i}",\n                            rr.Points2D(\n                                positions=[[text_x, text_y]],\n                                labels=[text_content],\n                                colors=[255, 255, 255],  # White text from plot_style.py DIARIZATION_TEXT\n                                radii=10  # Text size from plot_style.py\n                            )\n                        )\nelse:\n    print("Skipping diarization visualization - no diarization data available.")\n'})}),"\n",(0,a.jsx)(t.h2,{id:"hand-object-interaction",children:"Hand-Object Interaction"}),"\n",(0,a.jsx)(t.p,{children:"Hand-Object Interaction provides segmentation masks for hands and interacting objects, enabling analysis of hand-object relationships."}),"\n",(0,a.jsx)(t.h3,{id:"hand-object-interaction-data-structure",children:"Hand-Object Interaction Data Structure"}),"\n",(0,a.jsxs)(t.p,{children:["The ",(0,a.jsx)(t.code,{children:"HandObjectInteractionData"})," class contains:"]}),"\n",(0,a.jsxs)(t.table,{children:[(0,a.jsx)(t.thead,{children:(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.th,{children:"Field Name"}),(0,a.jsx)(t.th,{children:"Type"}),(0,a.jsx)(t.th,{children:"Description"})]})}),(0,a.jsxs)(t.tbody,{children:[(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:(0,a.jsx)(t.code,{children:"timestamp_ns"})}),(0,a.jsx)(t.td,{children:"int"}),(0,a.jsx)(t.td,{children:"Timestamp in device time domain (nanoseconds)"})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:(0,a.jsx)(t.code,{children:"category_id"})}),(0,a.jsx)(t.td,{children:"int"}),(0,a.jsx)(t.td,{children:"Category: 1=left_hand, 2=right_hand, 3=interacting_object"})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:(0,a.jsx)(t.code,{children:"masks"})}),(0,a.jsx)(t.td,{children:"List[np.ndarray]"}),(0,a.jsx)(t.td,{children:"List of decoded binary masks (height, width) uint8 arrays"})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:(0,a.jsx)(t.code,{children:"bboxes"})}),(0,a.jsx)(t.td,{children:"List[List[float]]"}),(0,a.jsx)(t.td,{children:"List of bounding boxes [x, y, width, height] for each mask"})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:(0,a.jsx)(t.code,{children:"scores"})}),(0,a.jsx)(t.td,{children:"List[float]"}),(0,a.jsx)(t.td,{children:"List of confidence scores [0.0, 1.0] for each mask"})]})]})]}),"\n",(0,a.jsx)(t.h3,{id:"hand-object-interaction-api-reference",children:"Hand-Object Interaction API Reference"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.code,{children:"has_hand_object_interaction_data()"}),": Check if HOI data is available"]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.code,{children:"get_hoi_data_by_timestamp_ns(timestamp_ns, time_domain, time_query_options)"}),": Get HOI data by timestamp (returns list)"]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.code,{children:"get_hoi_data_by_index(index)"}),": Get HOI data by index"]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.code,{children:"get_hoi_total_number()"}),": Get total number of HOI timestamps"]}),"\n"]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'# Hand-Object Interaction Data Loading and Analysis\nif pilot_data_provider.has_hand_object_interaction_data():\n    print("\u2705 Hand-Object Interaction data is available")\n\n    # Get total number of HOI entries\n    total_hoi = pilot_data_provider.get_hoi_total_number()\n    print(f"Total HOI timestamps: {total_hoi}")\n\n    # Sample first few HOI entries\n    print("\\n=== Hand-Object Interaction Data Sample ===")\n    sample_count = min(3, total_hoi)\n    for i in range(sample_count):\n        hoi_data_list = pilot_data_provider.get_hoi_data_by_index(i)\n        if hoi_data_list is not None and len(hoi_data_list) > 0:\n            print(f"Timestamp {i}: {len(hoi_data_list)} HOI entries")\n            for j, hoi_data in enumerate(hoi_data_list[:2]):  # Show first 2 entries\n                category_names = {1: "left_hand", 2: "right_hand", 3: "interacting_object"}\n                category_name = category_names.get(hoi_data.category_id, "unknown")\n                print(f"  Entry {j}: {category_name}, {len(hoi_data.masks)} masks, avg_score={np.mean(hoi_data.scores):.3f}")\n                if len(hoi_data.masks) > 0:\n                    print(f"    Mask shape: {hoi_data.masks[0].shape}")\n\n    # Query HOI data by timestamp\n    if total_hoi > 0:\n        # Get a sample timestamp from the middle of the sequence\n        sample_hoi_list = pilot_data_provider.get_hoi_data_by_index(total_hoi // 2)\n        if sample_hoi_list is not None and len(sample_hoi_list) > 0:\n            query_timestamp = sample_hoi_list[0].timestamp_ns\n\n            # Query HOI at this timestamp\n            hoi_at_time = pilot_data_provider.get_hoi_data_by_timestamp_ns(\n                query_timestamp, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST\n            )\n\n            if hoi_at_time is not None:\n                print(f"\\nHOI entries at timestamp {query_timestamp}: {len(hoi_at_time)}")\n                for entry in hoi_at_time[:2]:  # Show first 2 entries\n                    category_names = {1: "left_hand", 2: "right_hand", 3: "interacting_object"}\n                    category_name = category_names.get(entry.category_id, "unknown")\n                    print(f"  {category_name}: {len(entry.masks)} masks, scores={[f\'{s:.2f}\' for s in entry.scores[:3]]}")\nelse:\n    print("\u274c Hand-Object Interaction data is not available in this sequence")\n'})}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'# Hand-Object Interaction Visualization\nif pilot_data_provider.has_hand_object_interaction_data():\n    print("\\n=== Visualizing Hand-Object Interaction Data ===")\n\n    # Initialize Rerun for visualization\n    rr.init("rerun_viz_hoi")\n    rr.notebook_show()\n\n    # Get RGB camera stream for overlay\n    rgb_stream_id = pilot_data_provider.get_vrs_stream_id_from_label("camera-rgb")\n\n    if rgb_stream_id is not None:\n        # Get time bounds for RGB images\n        first_timestamp_ns = pilot_data_provider.get_vrs_timestamps_ns(rgb_stream_id, TimeDomain.DEVICE_TIME)[0]\n\n        # Sample a few RGB frames for visualization\n        sample_timestamps = []\n        for i in range(0, min(10, pilot_data_provider.get_vrs_num_data(rgb_stream_id)), 2):\n            rgb_data, rgb_record = pilot_data_provider.get_vrs_image_data_by_index(rgb_stream_id, i)\n            sample_timestamps.append(rgb_record.capture_timestamp_ns)\n\n        # Visualize RGB images with HOI overlay\n        for timestamp_ns in sample_timestamps:\n            # Get RGB image\n            rgb_data, rgb_record = pilot_data_provider.get_vrs_image_data_by_time_ns(\n                rgb_stream_id, timestamp_ns, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST\n            )\n\n            if rgb_data.is_valid():\n                # Visualize the RGB image\n                rr.set_time_nanos("device_time", rgb_record.capture_timestamp_ns)\n                rr.log("camera_rgb", rr.Image(rgb_data.to_numpy_array()))\n\n                # Get HOI data for this timestamp\n                hoi_entries = pilot_data_provider.get_hoi_data_by_timestamp_ns(\n                    timestamp_ns, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST\n                )\n\n                # Visualize HOI masks as overlays (following visualizer pattern exactly)\n                if hoi_entries:\n                    # Clear previous HOI overlays (following visualizer pattern)\n                    rr.log("camera_rgb/hoi_overlay", rr.Clear.recursive())\n\n                    # Filter out HOI data too far away from the current frame (following visualizer logic)\n                    rgb_frame_interval_ns = 33_333_333  # ~30 FPS\n                    if abs(hoi_entries[0].timestamp_ns - timestamp_ns) > rgb_frame_interval_ns / 2:\n                        continue\n\n                    # Color mapping from plot_style.py (following visualizer pattern)\n                    category_to_plot_style = {\n                        1: [119, 172, 48, 128],    # Green for left hand (HOI_LEFT_HAND)\n                        2: [217, 83, 255, 128],    # Purple for right hand (HOI_RIGHT_HAND)\n                        3: [237, 177, 32, 128]     # Orange for interacting object (HOI_INTERACTING_OBJECT)\n                    }\n\n                    # Determine mask shape from the first valid mask (following visualizer logic)\n                    mask_shape = next(\n                        (\n                            mask.shape\n                            for hoi_data in hoi_entries\n                            for mask in hoi_data.masks\n                            if mask is not None and mask.size > 0\n                        ),\n                        None,\n                    )\n                    if mask_shape is None:\n                        continue\n\n                    # Initialize combined RGBA overlay (following visualizer pattern)\n                    combined_rgba_overlay = np.zeros((*mask_shape, 4), dtype=np.uint8)\n\n                    # Overlay each category\'s mask with its color (following visualizer logic)\n                    for hoi_data in hoi_entries:\n                        category_id = hoi_data.category_id\n                        plot_style_color = category_to_plot_style.get(category_id, None)\n                        if not plot_style_color:\n                            continue\n\n                        for mask in hoi_data.masks:\n                            if mask is None or mask.size == 0:\n                                continue\n                            foreground_pixels = mask > 0\n                            combined_rgba_overlay[foreground_pixels] = plot_style_color\n\n                    # Log the combined segmentation overlay as an image (following visualizer pattern)\n                    rr.log(\n                        "camera_rgb/hoi_overlay/combined",\n                        rr.Image(combined_rgba_overlay)\n                    )\nelse:\n    print("Skipping HOI visualization - no HOI data available.")\n'})}),"\n",(0,a.jsx)(t.h2,{id:"egocentric-voxel-lifting",children:"Egocentric Voxel Lifting"}),"\n",(0,a.jsx)(t.p,{children:"Egocentric Voxel Lifting provides 3D scene reconstruction from egocentric view, including 3D bounding boxes and object instance information."}),"\n",(0,a.jsx)(t.h3,{id:"egocentric-voxel-lifting-data-structure",children:"Egocentric Voxel Lifting Data Structure"}),"\n",(0,a.jsx)(t.p,{children:"The EVL system provides two main data types:"}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"BoundingBox3D"})," (3D world coordinates):"]}),"\n",(0,a.jsxs)(t.table,{children:[(0,a.jsx)(t.thead,{children:(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.th,{children:"Field Name"}),(0,a.jsx)(t.th,{children:"Type"}),(0,a.jsx)(t.th,{children:"Description"})]})}),(0,a.jsxs)(t.tbody,{children:[(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:(0,a.jsx)(t.code,{children:"start_timestamp_ns"})}),(0,a.jsx)(t.td,{children:"int"}),(0,a.jsx)(t.td,{children:"Timestamp in device time domain (nanoseconds)"})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:(0,a.jsx)(t.code,{children:"bbox3d"})}),(0,a.jsx)(t.td,{children:"BoundingBox3dData"}),(0,a.jsx)(t.td,{children:"3D bounding box data (AABB, transform, etc.)"})]})]})]}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"BoundingBox3dData"})," structure:"]}),"\n",(0,a.jsxs)(t.table,{children:[(0,a.jsx)(t.thead,{children:(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.th,{children:"Field Name"}),(0,a.jsx)(t.th,{children:"Type"}),(0,a.jsx)(t.th,{children:"Description"})]})}),(0,a.jsxs)(t.tbody,{children:[(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:(0,a.jsx)(t.code,{children:"transform_scene_object"})}),(0,a.jsx)(t.td,{children:"SE3"}),(0,a.jsx)(t.td,{children:"Object 6DoF pose in the scene (world), where: point_in_scene = T_Scene_Object * point_in_object"})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:(0,a.jsx)(t.code,{children:"aabb"})}),(0,a.jsx)(t.td,{children:"List[float]"}),(0,a.jsx)(t.td,{children:"Object AABB (axes-aligned-bounding-box) in the object's local coordinate frame, represented as [xmin, xmax, ymin, ymax, zmin, zmax]"})]})]})]}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"BoundingBox2D"})," (2D camera projections):"]}),"\n",(0,a.jsxs)(t.table,{children:[(0,a.jsx)(t.thead,{children:(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.th,{children:"Field Name"}),(0,a.jsx)(t.th,{children:"Type"}),(0,a.jsx)(t.th,{children:"Description"})]})}),(0,a.jsxs)(t.tbody,{children:[(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:(0,a.jsx)(t.code,{children:"start_timestamp_ns"})}),(0,a.jsx)(t.td,{children:"int"}),(0,a.jsx)(t.td,{children:"Timestamp in device time domain (nanoseconds)"})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:(0,a.jsx)(t.code,{children:"bbox2d"})}),(0,a.jsx)(t.td,{children:"BoundingBox2dData"}),(0,a.jsx)(t.td,{children:"2D bounding box data"})]})]})]}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"BoundingBox2dData"})," structure:"]}),"\n",(0,a.jsxs)(t.table,{children:[(0,a.jsx)(t.thead,{children:(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.th,{children:"Field Name"}),(0,a.jsx)(t.th,{children:"Type"}),(0,a.jsx)(t.th,{children:"Description"})]})}),(0,a.jsxs)(t.tbody,{children:[(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:(0,a.jsx)(t.code,{children:"box_range"})}),(0,a.jsx)(t.td,{children:"List[float]"}),(0,a.jsx)(t.td,{children:"2D bounding box range as [xmin, xmax, ymin, ymax]"})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:(0,a.jsx)(t.code,{children:"visibility_ratio"})}),(0,a.jsx)(t.td,{children:"float"}),(0,a.jsx)(t.td,{children:"Visibility ratio calculated by occlusion between objects. visibility_ratio = 1: object is not occluded, visibility_ratio = 0: object is fully occluded"})]})]})]}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"InstanceInfo"})," (object metadata):"]}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.code,{children:"category"}),": Object category name"]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.code,{children:"name"}),": Specific object name"]}),"\n"]}),"\n",(0,a.jsx)(t.h3,{id:"egocentric-voxel-lifting-api-reference",children:"Egocentric Voxel Lifting API Reference"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.code,{children:"has_egocentric_voxel_lifting_data()"}),": Check if EVL data is available"]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.code,{children:"get_evl_3d_bounding_boxes_by_timestamp_ns(timestamp_ns, time_domain, time_query_options)"}),": Get 3D bounding boxes (returns Dict[int, BoundingBox3D])"]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.code,{children:"get_evl_2d_bounding_boxes_by_timestamp_ns(timestamp_ns, time_domain, camera_label)"}),": Get 2D bounding boxes for specific camera"]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.code,{children:"get_evl_instance_info_by_id(instance_id)"}),": Get object category/name information"]}),"\n"]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'# Egocentric Voxel Lifting Data Loading and Analysis\nif pilot_data_provider.has_egocentric_voxel_lifting_data():\n    print("\u2705 Egocentric Voxel Lifting data is available")\n\n    # Get RGB camera stream for 2D projection\n    rgb_stream_id = pilot_data_provider.get_vrs_stream_id_from_label("camera-rgb")\n\n    if rgb_stream_id is not None:\n        # Get a sample timestamp from RGB stream\n        first_timestamp_ns = pilot_data_provider.get_vrs_timestamps_ns(rgb_stream_id, TimeDomain.DEVICE_TIME)[0]\n        sample_timestamp = first_timestamp_ns + int(5e9)  # 5 seconds into sequence\n\n        # Query 3D bounding boxes\n        evl_3d_bboxes = pilot_data_provider.get_evl_3d_bounding_boxes_by_timestamp_ns(\n            sample_timestamp, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST\n        )\n\n        if evl_3d_bboxes is not None:\n            print(f"\\n=== EVL 3D Bounding Boxes at timestamp {sample_timestamp} ===")\n            print(f"Found {len(evl_3d_bboxes)} 3D bounding boxes")\n\n            for instance_id, bbox_3d in list(evl_3d_bboxes.items())[:3]:  # Show first 3\n                # Get instance info\n                instance_info = pilot_data_provider.get_evl_instance_info_by_id(instance_id)\n                if instance_info is not None:\n                    print(f"Instance {instance_id}: {instance_info.category} - {instance_info.name}")\n                    print(f"  AABB: {bbox_3d.bbox3d.aabb}")\n                    print(f"  Transform: {bbox_3d.bbox3d.transform_scene_object.to_matrix()[:3, 3]}")\n\n        # Query 2D bounding boxes for RGB camera\n        evl_2d_bboxes = pilot_data_provider.get_evl_2d_bounding_boxes_by_timestamp_ns(\n            sample_timestamp, TimeDomain.DEVICE_TIME, "camera-rgb"\n        )\n\n        if evl_2d_bboxes is not None:\n            print(f"\\n=== EVL 2D Bounding Boxes for RGB camera ===")\n            print(f"Found {len(evl_2d_bboxes)} 2D bounding boxes")\n\n            for instance_id, bbox_2d in list(evl_2d_bboxes.items())[:3]:  # Show first 3\n                print(f"Instance {instance_id}: 2D bbox {bbox_2d.bbox2d.box_range}")\nelse:\n    print("\u274c Egocentric Voxel Lifting data is not available in this sequence")\n'})}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'from aria_gen2_pilot_dataset.visualization.plot_utils import extract_bbox_projection_data, project_3d_bbox_to_2d_camera\n# Egocentric Voxel Lifting Visualization\nif pilot_data_provider.has_egocentric_voxel_lifting_data():\n    print("\\n=== Visualizing Egocentric Voxel Lifting Data ===")\n\n    # Initialize Rerun for visualization\n    rr.init("rerun_viz_evl")\n    rr.notebook_show()\n\n    # Get RGB camera stream for 2D projection\n    rgb_stream_id = pilot_data_provider.get_vrs_stream_id_from_label("camera-rgb")\n\n    if rgb_stream_id is not None:\n        # Get time bounds for RGB images\n        first_timestamp_ns = pilot_data_provider.get_vrs_timestamps_ns(rgb_stream_id, TimeDomain.DEVICE_TIME)[0]\n\n        # Sample a few RGB frames for visualization\n        sample_timestamps = []\n        for i in range(0, min(10, pilot_data_provider.get_vrs_num_data(rgb_stream_id)), 2):\n            rgb_data, rgb_record = pilot_data_provider.get_vrs_image_data_by_index(rgb_stream_id, i)\n            sample_timestamps.append(rgb_record.capture_timestamp_ns)\n\n        # Visualize RGB images with EVL 2D and 3D bounding boxes\n        for timestamp_ns in sample_timestamps:\n            # Get RGB image\n            rgb_data, rgb_record = pilot_data_provider.get_vrs_image_data_by_time_ns(\n                rgb_stream_id, timestamp_ns, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST\n            )\n\n            if rgb_data.is_valid():\n                # Visualize the RGB image\n                rr.set_time_nanos("device_time", rgb_record.capture_timestamp_ns)\n                rr.log("camera_rgb", rr.Image(rgb_data.to_numpy_array()))\n\n                # Get EVL 3D bounding boxes for this timestamp\n                evl_3d_bboxes = pilot_data_provider.get_evl_3d_bounding_boxes_by_timestamp_ns(\n                    timestamp_ns, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST\n                )\n\n                # Visualize projected 3D bounding boxes (following visualizer pattern)\n                if evl_3d_bboxes is not None:\n                    # Clear previous EVL overlays (following visualizer pattern)\n                    rr.log("camera_rgb/evl_3d_bboxes_projected", rr.Clear.recursive())\n\n                    # Get trajectory pose from MPS data (following visualizer pattern exactly)\n                    trajectory_pose = pilot_data_provider.get_mps_closed_loop_pose(\n                        timestamp_ns, TimeDomain.DEVICE_TIME\n                    )\n\n                    if trajectory_pose is not None:\n                        # Get RGB camera calibration for projection\n                        device_calibration = pilot_data_provider.get_vrs_device_calibration()\n                        rgb_camera_calibration = device_calibration.get_camera_calib("camera-rgb")\n\n                        if rgb_camera_calibration is not None:\n                            # Get transforms and image dimensions (following visualizer pattern exactly)\n                            T_world_device = trajectory_pose.transform_world_device\n                            T_device_camera = rgb_camera_calibration.get_transform_device_camera()\n                            T_world_camera = T_world_device @ T_device_camera\n\n                            # Get image dimensions\n                            image_width, image_height = rgb_camera_calibration.get_image_size()\n\n                            # Extract bbox data for projection using utility function (following visualizer pattern)\n                            projection_data = extract_bbox_projection_data(pilot_data_provider, evl_3d_bboxes)\n\n                            # Collect all projection results for batching\n                            all_projected_lines = []\n                            all_line_colors = []\n                            label_positions = []\n                            label_texts = []\n                            label_colors = []\n\n                            # Project each bounding box using utility function (following visualizer pattern)\n                            for data in projection_data:\n                                projection_result = project_3d_bbox_to_2d_camera(\n                                    corners_in_world=data["corners_world"],\n                                    T_world_camera=T_world_camera,\n                                    camera_calibration=rgb_camera_calibration,\n                                    image_width=image_width,\n                                    image_height=image_height,\n                                    label=data["label"],\n                                )\n\n                                if projection_result:\n                                    projected_lines, line_colors, label_position = projection_result\n\n                                    # Collect projection data for batching\n                                    if projected_lines:\n                                        all_projected_lines.extend(projected_lines)\n                                        if line_colors and len(line_colors) >= len(projected_lines):\n                                            all_line_colors.extend(line_colors[:len(projected_lines)])\n                                        else:\n                                            all_line_colors.extend([0, 255, 0] * len(projected_lines))  # Green color\n\n                                        if label_position and data["label"]:\n                                            label_positions.append(label_position)\n                                            label_texts.append(data["label"])\n                                            label_colors.append([0, 255, 0])  # Green text\n\n                            # Log all projected lines in batch (following visualizer pattern)\n                            if all_projected_lines:\n                                rr.log(\n                                    "camera_rgb/evl_3d_bboxes_projected/wireframes",\n                                    rr.LineStrips2D(\n                                        all_projected_lines,\n                                        colors=all_line_colors,\n                                        radii=1.5  # Match plot_style.py EVL line thickness\n                                    )\n                                )\n\n                            # Log all labels in batch (following visualizer pattern)\n                            if label_positions:\n                                rr.log(\n                                    "camera_rgb/evl_3d_bboxes_projected/labels",\n                                    rr.Points2D(\n                                        positions=label_positions,\n                                        labels=label_texts,\n                                        colors=label_colors,\n                                        radii=10  # Text size from plot_style.py\n                                    )\n                                )\n\n                # Visualize 3D bounding boxes in world coordinates (following visualizer pattern exactly)\n                if evl_3d_bboxes is not None:\n                    # Clear previous 3D bounding boxes (following visualizer pattern)\n                    rr.log("world/evl_3d_bboxes", rr.Clear.recursive())\n\n                    bb3d_sizes = []\n                    bb3d_centers = []\n                    bb3d_quats_xyzw = []\n                    bb3d_labels = []\n\n                    for instance_id, boundingBox3d in evl_3d_bboxes.items():\n                        # Extract BoundingBox3dData from our BoundingBox3D wrapper (following visualizer logic)\n                        bbox3d_data = boundingBox3d.bbox3d\n\n                        # Get AABB in object\'s local coordinates: [xmin, xmax, ymin, ymax, zmin, zmax]\n                        aabb = bbox3d_data.aabb\n\n                        # Calculate dimensions (following visualizer logic)\n                        object_dimensions = np.array([\n                            aabb[1] - aabb[0],  # width (xmax - xmin)\n                            aabb[3] - aabb[2],  # height (ymax - ymin)\n                            aabb[5] - aabb[4],  # depth (zmax - zmin)\n                        ])\n\n                        # Get world center and rotation from transform_scene_object (following visualizer logic)\n                        T_scene_object = bbox3d_data.transform_scene_object\n                        quat_and_translation = np.squeeze(T_scene_object.to_quat_and_translation())\n                        quaternion_wxyz = quat_and_translation[0:4]  # [w, x, y, z]\n                        world_center = quat_and_translation[4:7]  # [x, y, z]\n\n                        # Convert quaternion to ReRun format [x, y, z, w] (following visualizer logic)\n                        quat_xyzw = [\n                            quaternion_wxyz[1],\n                            quaternion_wxyz[2],\n                            quaternion_wxyz[3],\n                            quaternion_wxyz[0],\n                        ]\n\n                        # Get label (following visualizer logic)\n                        label = f"instance_{instance_id}"\n                        instance_info = pilot_data_provider.get_evl_instance_info_by_id(instance_id)\n                        if instance_info:\n                            if hasattr(instance_info, "category") and instance_info.category:\n                                label = instance_info.category\n                            elif hasattr(instance_info, "name") and instance_info.name:\n                                label = instance_info.name\n\n                        # Add to lists (following visualizer pattern)\n                        bb3d_centers.append(world_center)\n                        bb3d_sizes.append(object_dimensions)\n                        bb3d_quats_xyzw.append(quat_xyzw)\n                        bb3d_labels.append(label)\n\n                    # Visualize using ReRun Boxes3D with plot style (following visualizer pattern exactly)\n                    if bb3d_sizes:\n                        # Split into batches of 20 (ReRun limitation, following visualizer logic)\n                        MAX_BOXES_PER_BATCH = 20\n                        batch_id = 0\n\n                        while batch_id * MAX_BOXES_PER_BATCH < len(bb3d_sizes):\n                            start_idx = batch_id * MAX_BOXES_PER_BATCH\n                            end_idx = min(len(bb3d_sizes), start_idx + MAX_BOXES_PER_BATCH)\n                            rr.log(\n                                f"world/evl_3d_bboxes/batch_{batch_id}",\n                                rr.Boxes3D(\n                                    sizes=bb3d_sizes[start_idx:end_idx],\n                                    centers=bb3d_centers[start_idx:end_idx],\n                                    rotations=bb3d_quats_xyzw[start_idx:end_idx],\n                                    labels=bb3d_labels[start_idx:end_idx],\n                                    colors=[0, 255, 0, 70],  # Green with alpha from plot_style.py EVL_BBOX_3D\n                                    radii=0.005,  # From plot_style.py EVL_BBOX_3D plot_3d_size\n                                    show_labels=False,\n                                )\n                            )\n                            batch_id += 1\nelse:\n    print("Skipping EVL visualization - no EVL data available.")\n'})}),"\n",(0,a.jsx)(t.h2,{id:"foundation-stereo-depth",children:"Foundation Stereo Depth"}),"\n",(0,a.jsx)(t.p,{children:"Foundation Stereo provides depth estimation from stereo camera pairs, including depth maps and rectified images."}),"\n",(0,a.jsx)(t.h3,{id:"foundation-stereo-data-structure",children:"Foundation Stereo Data Structure"}),"\n",(0,a.jsxs)(t.p,{children:["The ",(0,a.jsx)(t.code,{children:"CameraIntrinsicsAndPose"})," class contains:"]}),"\n",(0,a.jsxs)(t.table,{children:[(0,a.jsx)(t.thead,{children:(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.th,{children:"Field Name"}),(0,a.jsx)(t.th,{children:"Type"}),(0,a.jsx)(t.th,{children:"Description"})]})}),(0,a.jsxs)(t.tbody,{children:[(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:(0,a.jsx)(t.code,{children:"timestamp_ns"})}),(0,a.jsx)(t.td,{children:"int"}),(0,a.jsx)(t.td,{children:"Timestamp in device time domain (nanoseconds)"})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:(0,a.jsx)(t.code,{children:"camera_projection"})}),(0,a.jsx)(t.td,{children:"CameraProjection"}),(0,a.jsx)(t.td,{children:"Camera intrinsics and model information"})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:(0,a.jsx)(t.code,{children:"transform_world_camera"})}),(0,a.jsx)(t.td,{children:"SE3"}),(0,a.jsx)(t.td,{children:"Camera pose in world coordinates"})]})]})]}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.strong,{children:"Depth Map Format:"})}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:"Rectified depth maps of slam-front-left camera, 512 x 512, 16-bit grayscale PNG(1 unit = 1mm)."}),"\n"]}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.strong,{children:"Rectified SLAM Image:"})}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:"Matching rectified slam-front-left camera images, 8-bit grayscale PNG."}),"\n"]}),"\n",(0,a.jsx)(t.h3,{id:"foundation-stereo-api-reference",children:"Foundation Stereo API Reference"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.code,{children:"has_stereo_depth_data()"}),": Check if stereo depth data is available"]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.code,{children:"get_stereo_depth_depth_map_by_index(index)"}),": Get depth map by index"]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.code,{children:"get_stereo_depth_depth_map_by_timestamp_ns(timestamp_ns, time_domain, time_query_option)"}),": Get depth map by timestamp"]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.code,{children:"get_stereo_depth_rectified_slam_front_left_by_index(index)"}),": Get rectified image by index"]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.code,{children:"get_stereo_depth_rectified_slam_front_left_by_timestamp_ns(timestamp_ns, time_domain, time_query_option)"}),": Get rectified image by timestamp"]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.code,{children:"get_stereo_depth_camera_intrinsics_and_pose_by_index(index)"}),": Get camera info by index"]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.code,{children:"get_stereo_depth_camera_intrinsics_and_pose_by_timestamp_ns(timestamp_ns, time_domain, time_query_option)"}),": Get camera info by timestamp"]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.code,{children:"get_stereo_depth_data_total_number()"}),": Get total number of depth entries"]}),"\n"]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'# Foundation Stereo Depth Data Loading and Analysis\nif pilot_data_provider.has_stereo_depth_data():\n    print("\u2705 Foundation Stereo data is available")\n\n    # Get total number of stereo depth entries\n    total_stereo = pilot_data_provider.get_stereo_depth_data_total_number()\n    print(f"Total stereo depth entries: {total_stereo}")\n\n    # Sample first few stereo depth entries\n    print("\\n=== Foundation Stereo Data Sample ===")\n    sample_count = min(3, total_stereo)\n    for i in range(sample_count):\n        # Get depth map\n        depth_map = pilot_data_provider.get_stereo_depth_depth_map_by_index(i)\n\n        # Get rectified image\n        rectified_image = pilot_data_provider.get_stereo_depth_rectified_slam_front_left_by_index(i)\n\n        # Get camera info\n        camera_info = pilot_data_provider.get_stereo_depth_camera_intrinsics_and_pose_by_index(i)\n\n        if depth_map is not None:\n            print(f"Entry {i}:")\n            print(f"  Depth map shape: {depth_map.shape}, dtype: {depth_map.dtype}")\n            print(f"  Depth range: {depth_map[depth_map > 0].min()}-{depth_map[depth_map > 0].max()} mm")\n            print(f"  Valid pixels: {np.sum(depth_map > 0)}/{depth_map.size} ({100*np.sum(depth_map > 0)/depth_map.size:.1f}%)")\n\n            if rectified_image is not None:\n                print(f"  Rectified image shape: {rectified_image.shape}")\n\n            if camera_info is not None:\n                print(f"  Camera model: {camera_info.camera_projection.model_name()}")\n                print(f"  Focal lengths: {camera_info.camera_projection.get_focal_lengths()}")\n                print(f"  Principal point: {camera_info.camera_projection.get_principal_point()}")\n                print(f"  Projection params: {camera_info.camera_projection.projection_params()}")\n\n    # Query stereo depth data by timestamp\n    if total_stereo > 0:\n        slam_front_left_stream_id = pilot_data_provider.get_vrs_stream_id_from_label("slam-front-left")\n        sample_timestamps = []\n        for i in range(50, min(100, pilot_data_provider.get_vrs_num_data(slam_front_left_stream_id)), 2):\n            rgb_data, rgb_record = pilot_data_provider.get_vrs_image_data_by_index(rgb_stream_id, i)\n            sample_timestamps.append(rgb_record.capture_timestamp_ns)\n\n        if sample_timestamp is not None:\n            # Query depth map at this timestamp\n            depth_at_time = pilot_data_provider.get_stereo_depth_depth_map_by_timestamp_ns(\n                sample_timestamp, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST\n            )\n\n            if depth_at_time is not None:\n                print(f"\\nDepth map at timestamp {sample_timestamp}:")\n                print(f"  Shape: {depth_at_time.shape}")\n                print(f"  Valid depth range: {depth_at_time[depth_at_time > 0].min()}-{depth_at_time[depth_at_time > 0].max()} mm")\nelse:\n    print("\u274c Foundation Stereo data is not available in this sequence")\n'})}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'# Foundation Stereo Depth Visualization\nif pilot_data_provider.has_stereo_depth_data():\n    print("\\n=== Visualizing Foundation Stereo Depth Data ===")\n\n    # Initialize Rerun for visualization\n    rr.init("rerun_viz_stereo_depth")\n    rr.notebook_show()\n\n    # Get total number of stereo depth entries\n    total_stereo = pilot_data_provider.get_stereo_depth_data_total_number()\n\n    slam_front_left_stream_id = pilot_data_provider.get_vrs_stream_id_from_label("slam-front-left")\n    sample_timestamps = []\n    for i in range(50, min(100, pilot_data_provider.get_vrs_num_data(slam_front_left_stream_id)), 2):\n        rgb_data, rgb_record = pilot_data_provider.get_vrs_image_data_by_index(rgb_stream_id, i)\n        sample_timestamps.append(rgb_record.capture_timestamp_ns)\n\n    for query_timestamp_ns in sample_timestamps:\n        # Get depth map\n        depth_map = pilot_data_provider.get_stereo_depth_depth_map_by_timestamp_ns(timestamp_ns=query_timestamp_ns, time_domain=TimeDomain.DEVICE_TIME, time_query_option=TimeQueryOptions.CLOSEST)\n\n        # Get rectified image\n        rectified_image = pilot_data_provider.get_stereo_depth_rectified_slam_front_left_by_timestamp_ns(timestamp_ns=query_timestamp_ns, time_domain=TimeDomain.DEVICE_TIME, time_query_option=TimeQueryOptions.CLOSEST)\n\n        # Get camera info\n        camera_info = pilot_data_provider.get_stereo_depth_camera_intrinsics_and_pose_by_timestamp_ns(timestamp_ns=query_timestamp_ns, time_domain=TimeDomain.DEVICE_TIME, time_query_option=TimeQueryOptions.CLOSEST)\n\n        if depth_map is not None and rectified_image is not None and camera_info is not None:\n            # Set timestamp\n            rr.set_time_nanos("device_time", camera_info.timestamp_ns)\n\n            # Clear previous depth visualizations\n            rr.log("depth_image", rr.Clear.recursive())\n            rr.log("rectified_slam_front_left", rr.Clear.recursive())\n            rr.log("world/stereo_depth_depth_camera", rr.Clear.recursive())\n\n            # Visualize rectified SLAM image (following visualizer pattern)\n            if rectified_image is not None:\n                rr.log("rectified_slam_front_left", rr.Image(rectified_image))\n\n            # Visualize depth as 3D point cloud\n            # Get original camera intrinsics\n            original_fx, original_fy = camera_info.camera_projection.get_focal_lengths()\n            original_ux, original_uy = camera_info.camera_projection.get_principal_point()\n\n            # Apply downsampling factor (following visualizer logic)\n            factor = 4  # depth_image_downsample_factor\n            scaled_fx = original_fx / factor\n            scaled_fy = original_fy / factor\n            scaled_ux = original_ux / factor\n            scaled_uy = original_uy / factor\n\n            # Resize depth map (following visualizer pattern)\n            subsampled_depth_map = depth_map[::factor, ::factor] if factor > 1 else depth_map\n\n            # Set up depth camera in world coordinate system (following visualizer pattern)\n            rr.log(\n                "world/stereo_depth",\n                rr.Pinhole(\n                    resolution=[subsampled_depth_map.shape[1], subsampled_depth_map.shape[0]],\n                    focal_length=[scaled_fx, scaled_fy],\n                    principal_point=[scaled_ux, scaled_uy],\n                ),\n                static=True,\n            )\n\n            # Log camera transform (following visualizer pattern)\n            rr.log(\n                "world/stereo_depth",\n                ToTransform3D(camera_info.transform_world_camera, axis_length=0.02)\n            )\n\n            # Log depth image with proper scaling (following visualizer pattern exactly)\n            DEPTH_IMAGE_SCALING = 1000  # mm to meters\n            rr.log(\n                "world/stereo_depth",\n                rr.DepthImage(\n                    subsampled_depth_map,\n                    meter=DEPTH_IMAGE_SCALING,\n                    colormap="Magma",\n                    point_fill_ratio=0.3\n                )\n            )\n\nelse:\n    print("Skipping stereo depth visualization - no stereo depth data available.")\n'})}),"\n",(0,a.jsx)(t.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsxs)(t.p,{children:["This tutorial has demonstrated how to use the ",(0,a.jsx)(t.code,{children:"AriaGen2PilotDataProvider"})," to access and visualize algorithm output data from the Aria Gen2 Pilot Dataset:"]}),"\n",(0,a.jsx)(t.h3,{id:"key-concepts-covered",children:"Key Concepts Covered"}),"\n",(0,a.jsxs)(t.ol,{children:["\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"Heart Rate Monitoring"})," - Physiological data from PPG sensors with time series visualization"]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"Diarization"})," - Speaker identification and voice activity detection with text overlay"]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"Hand-Object Interaction"})," - Segmentation masks for hands and objects with colored overlays"]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"Egocentric Voxel Lifting"})," - 3D scene reconstruction with 2D/3D bounding box visualization"]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"Foundation Stereo"})," - Depth estimation with 2D depth maps and 3D point clouds"]}),"\n"]}),"\n",(0,a.jsx)(t.h3,{id:"important-notes",children:"Important Notes"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"Data Availability"}),": Algorithm data availability varies by sequence - always check availability before processing"]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"Data Structures"}),": Each algorithm has its own data structure with specific fields and formats"]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"Query Patterns"}),": Use index-based queries for sequential processing, timestamp-based queries for synchronization"]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"Visualization"}),": Use appropriate visualization methods for each data type (scalars, images, bounding box, etc.)"]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"Performance"}),": Consider subsampling for large datasets and high-frequency data"]}),"\n"]})]})}function m(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},69470:(e,t,i)=>{i.d(t,{A:()=>a});i(96540);var n=i(74848);const a=({notebookUrl:e,colabDisabled:t=!1})=>(0,n.jsxs)("div",{style:{display:"flex",gap:"10px",marginBottom:"20px",flexWrap:"wrap"},children:[(0,n.jsxs)("a",{href:e,target:"_blank",rel:"noopener noreferrer",style:{display:"inline-flex",alignItems:"center",padding:"8px 16px",backgroundColor:"#f6f8fa",border:"1px solid #d0d7de",borderRadius:"6px",textDecoration:"none",color:"#24292f",fontSize:"14px",fontWeight:"500",transition:"background-color 0.2s"},onMouseEnter:e=>{e.target.style.backgroundColor="#f3f4f6"},onMouseLeave:e=>{e.target.style.backgroundColor="#f6f8fa"},children:[(0,n.jsx)("svg",{style:{marginRight:"8px",width:"16px",height:"16px"},fill:"currentColor",viewBox:"0 0 16 16",children:(0,n.jsx)("path",{d:"M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z"})}),"View Notebook on GitHub"]}),(0,n.jsxs)("button",{onClick:()=>{alert("Google Colab integration is not available yet. Please download the notebook and run it locally.")},style:{display:"inline-flex",alignItems:"center",padding:"8px 16px",backgroundColor:t?"#f6f8fa":"#f9ab00",border:"1px solid #d0d7de",borderRadius:"6px",color:t?"#656d76":"#fff",fontSize:"14px",fontWeight:"500",cursor:t?"not-allowed":"pointer",transition:"background-color 0.2s"},onMouseEnter:e=>{t||(e.target.style.backgroundColor="#e8a500")},onMouseLeave:e=>{t||(e.target.style.backgroundColor="#f9ab00")},children:[(0,n.jsx)("svg",{style:{marginRight:"8px",width:"16px",height:"16px"},fill:"currentColor",viewBox:"0 0 24 24",children:(0,n.jsx)("path",{d:"M16.9414 4.9757a7.033 7.033 0 0 0-4.9308 2.0324 7.033 7.033 0 0 0-.1232 9.8068l2.395-2.395a3.6455 3.6455 0 0 1 5.1497-5.1478l2.397-2.3989a7.033 7.033 0 0 0-4.8877-1.9175zM7.07 4.9855a7.033 7.033 0 0 0-4.8878 1.9175l2.395 2.3989a3.6434 3.6434 0 0 1 5.1497 5.1478l2.395 2.395a7.033 7.033 0 0 0-.1232-9.8068A7.033 7.033 0 0 0 7.07 4.9855zm15.0191 2.5923l-2.395 2.395a3.6455 3.6455 0 0 1-5.1497 5.1497l-2.395 2.395a7.033 7.033 0 0 0 9.9397-.123z"})}),t?"Colab (Coming Soon)":"Run in Google Colab"]})]})}}]);