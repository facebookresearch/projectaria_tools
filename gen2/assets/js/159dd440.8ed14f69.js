"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[4534],{28453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>o});var i=t(96540);const r={},a=i.createContext(r);function s(e){const n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),i.createElement(a.Provider,{value:n},e.children)}},69470:(e,n,t)=>{t.d(n,{A:()=>r});t(96540);var i=t(74848);const r=({notebookUrl:e,colabDisabled:n=!1})=>(0,i.jsxs)("div",{style:{display:"flex",gap:"10px",marginBottom:"20px",flexWrap:"wrap"},children:[(0,i.jsxs)("a",{href:e,target:"_blank",rel:"noopener noreferrer",style:{display:"inline-flex",alignItems:"center",padding:"8px 16px",backgroundColor:"#f6f8fa",border:"1px solid #d0d7de",borderRadius:"6px",textDecoration:"none",color:"#24292f",fontSize:"14px",fontWeight:"500",transition:"background-color 0.2s"},onMouseEnter:e=>{e.target.style.backgroundColor="#f3f4f6"},onMouseLeave:e=>{e.target.style.backgroundColor="#f6f8fa"},children:[(0,i.jsx)("svg",{style:{marginRight:"8px",width:"16px",height:"16px"},fill:"currentColor",viewBox:"0 0 16 16",children:(0,i.jsx)("path",{d:"M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z"})}),"View Notebook on GitHub"]}),(0,i.jsxs)("button",{onClick:()=>{alert("Google Colab integration is not available yet. Please download the notebook and run it locally.")},style:{display:"inline-flex",alignItems:"center",padding:"8px 16px",backgroundColor:n?"#f6f8fa":"#f9ab00",border:"1px solid #d0d7de",borderRadius:"6px",color:n?"#656d76":"#fff",fontSize:"14px",fontWeight:"500",cursor:n?"not-allowed":"pointer",transition:"background-color 0.2s"},onMouseEnter:e=>{n||(e.target.style.backgroundColor="#e8a500")},onMouseLeave:e=>{n||(e.target.style.backgroundColor="#f9ab00")},children:[(0,i.jsx)("svg",{style:{marginRight:"8px",width:"16px",height:"16px"},fill:"currentColor",viewBox:"0 0 24 24",children:(0,i.jsx)("path",{d:"M16.9414 4.9757a7.033 7.033 0 0 0-4.9308 2.0324 7.033 7.033 0 0 0-.1232 9.8068l2.395-2.395a3.6455 3.6455 0 0 1 5.1497-5.1478l2.397-2.3989a7.033 7.033 0 0 0-4.8877-1.9175zM7.07 4.9855a7.033 7.033 0 0 0-4.8878 1.9175l2.395 2.3989a3.6434 3.6434 0 0 1 5.1497 5.1478l2.395 2.395a7.033 7.033 0 0 0-.1232-9.8068A7.033 7.033 0 0 0 7.07 4.9855zm15.0191 2.5923l-2.395 2.395a3.6455 3.6455 0 0 1-5.1497 5.1497l-2.395 2.395a7.033 7.033 0 0 0 9.9397-.123z"})}),n?"Colab (Coming Soon)":"Run in Google Colab"]})]})},77698:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>l,default:()=>_,frontMatter:()=>o,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"dataset/pilot/tutorials/multi_sequences_timestamp_alignment","title":"Aria Gen2 Pilot Dataset Tutorial - Multi-Sequence Timestamp Alignment","description":"<TutorialButtons","source":"@site/docs-research-tools/dataset/pilot/tutorials/multi_sequences_timestamp_alignment.mdx","sourceDirName":"dataset/pilot/tutorials","slug":"/dataset/pilot/tutorials/multi_sequences_timestamp_alignment","permalink":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/multi_sequences_timestamp_alignment","draft":false,"unlisted":false,"editUrl":"https://www.internalfb.com/code/fbsource/arvr/projects/ariane/aria_research_kit/projectaria_tools/website/docs-research-tools/dataset/pilot/tutorials/multi_sequences_timestamp_alignment.mdx","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"Aria Gen2 Pilot Dataset Tutorial - Multi-Sequence Timestamp Alignment"},"sidebar":"researchToolsSidebar","previous":{"title":"Aria Gen2 Pilot Dataset Tutorial - Algorithm Data Loading","permalink":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/algorithm_loading"}}');var r=t(74848),a=t(28453),s=t(69470);const o={sidebar_position:3,title:"Aria Gen2 Pilot Dataset Tutorial - Multi-Sequence Timestamp Alignment"},l=void 0,d={},c=[{value:"What You&#39;ll Learn",id:"what-youll-learn",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"SubGHz Timestamp Alignment Overview",id:"subghz-timestamp-alignment-overview",level:2},{value:"Import Required Libraries",id:"import-required-libraries",level:2},{value:"Part 1: Single-Device Timestamp Alignment",id:"part-1-single-device-timestamp-alignment",level:2},{value:"Understanding Time Domains",id:"understanding-time-domains",level:3},{value:"Supported Time Domains for Aria Gen2",id:"supported-time-domains-for-aria-gen2",level:4},{value:"Load a Single Sequence",id:"load-a-single-sequence",level:3},{value:"Data API to Query by Timestamp",id:"data-api-to-query-by-timestamp",level:3},{value:"TimeQueryOptions",id:"timequeryoptions",level:4},{value:"Boundary Behavior",id:"boundary-behavior",level:4},{value:"Visualizing Timestamp-Aligned Multi-Sensor Data",id:"visualizing-timestamp-aligned-multi-sensor-data",level:3},{value:"Part 2: Multi-Device Time Alignment",id:"part-2-multi-device-time-alignment",level:2},{value:"Understanding Pilot Dataset Multi-Device Naming Convention",id:"understanding-pilot-dataset-multi-device-naming-convention",level:3},{value:"Load Multiple Sequences",id:"load-multiple-sequences",level:3},{value:"Understanding SubGHz Timestamp Conversion",id:"understanding-subghz-timestamp-conversion",level:3},{value:"Query APIs with TimeDomain.SUBGHZ",id:"query-apis-with-timedomainsubghz",level:3},{value:"Part 3: Timestamp-Aligned Trajectory and Hand Tracking Visualization",id:"part-3-timestamp-aligned-trajectory-and-hand-tracking-visualization",level:2},{value:"Load MPS Data",id:"load-mps-data",level:3},{value:"Visualization Setup",id:"visualization-setup",level:3},{value:"Visualization Part 1: Timestamp-Aligned RGB Frames",id:"visualization-part-1-timestamp-aligned-rgb-frames",level:3},{value:"Visualization Part 2: 3D World View",id:"visualization-part-2-3d-world-view",level:3},{value:"Summary",id:"summary",level:2},{value:"Key Takeaways",id:"key-takeaways",level:3}];function m(e){const n={code:"code",h2:"h2",h3:"h3",h4:"h4",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(s.A,{notebookUrl:"https://github.com/facebookresearch/projectaria_gen2_pilot_dataset/blob/main/examples/tutorial_4_multi_sequences_timestamp_alignment.ipynb",colabDisabled:!0}),"\n",(0,r.jsx)(n.p,{children:"This tutorial demonstrates how to work with timestamp-aligned multi-device Aria Gen2 recordings. When multiple Aria Gen2 glasses record simultaneously with SubGHz timestamp alignment enabled, their timestamps can be mapped across devices, enabling multi-person activity analysis, multi-view reconstruction, and collaborative tasks."}),"\n",(0,r.jsx)(n.h2,{id:"what-youll-learn",children:"What You'll Learn"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Understanding SubGHz time timestamp alignment between multiple Aria Gen2 devices"}),"\n",(0,r.jsx)(n.li,{children:"Converting timestamps between host and client devices"}),"\n",(0,r.jsx)(n.li,{children:"Querying timestamp-aligned sensor data across multiple recordings"}),"\n",(0,r.jsx)(n.li,{children:"Visualizing timestamp-aligned RGB frames from multiple devices"}),"\n",(0,r.jsx)(n.li,{children:"Plotting trajectories and hand tracking from multiple devices in a shared world coordinate frame"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Complete Tutorial 1 (VRS Data Loading) to understand basic data provider concepts"}),"\n",(0,r.jsx)(n.li,{children:"Complete Tutorial 2 (MPS Data Loading) to understand MPS trajectories and hand tracking"}),"\n",(0,r.jsx)(n.li,{children:"Download a multi-device sequence from the Aria Gen2 Pilot Dataset"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"subghz-timestamp-alignment-overview",children:"SubGHz Timestamp Alignment Overview"}),"\n",(0,r.jsx)(n.p,{children:"During multi-device recording, Aria Gen2 glasses use SubGHz radio signals for timestamp alignment:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Host Device"}),": One device acts as the host, actively broadcasting SubGHz signals to a specified channel"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Client Device(s)"}),": Other devices act as clients, receiving SubGHz signals and recording a ",(0,r.jsx)(n.code,{children:"Time Domain Mapping"})," data stream in their VRS file"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Important Notes:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["The time domain mapping stream ",(0,r.jsx)(n.strong,{children:"only exists in client VRS files"}),", not in the host VRS"]}),"\n",(0,r.jsxs)(n.li,{children:["This mapping enables converting timestamps: host ",(0,r.jsx)(n.code,{children:"DEVICE_TIME"})," \u2194 client ",(0,r.jsx)(n.code,{children:"DEVICE_TIME"})]}),"\n",(0,r.jsx)(n.li,{children:"MPS trajectories from timestamp-aligned recordings share the same world coordinate frame"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"import-required-libraries",children:"Import Required Libraries"}),"\n",(0,r.jsx)(n.p,{children:"The following libraries are required for this tutorial:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Standard library imports\nimport numpy as np\nimport os\nfrom pathlib import Path\n\n# Project Aria Tools imports\nfrom projectaria_tools.core.stream_id import StreamId\nfrom projectaria_tools.core.sensor_data import TimeDomain, TimeQueryOptions, TimeSyncMode\nfrom projectaria_tools.core import mps\nfrom projectaria_tools.utils.rerun_helpers import (\n    create_hand_skeleton_from_landmarks,\n    ToTransform3D\n)\n\n# Aria Gen2 Pilot Dataset imports\nfrom aria_gen2_pilot_dataset import AriaGen2PilotDataProvider\nfrom aria_gen2_pilot_dataset.visualization.plot_style import get_plot_style, PlotEntity\n\n# Visualization library\nimport rerun as rr\nimport rerun.blueprint as rrb\n"})}),"\n",(0,r.jsx)(n.h2,{id:"part-1-single-device-timestamp-alignment",children:"Part 1: Single-Device Timestamp Alignment"}),"\n",(0,r.jsx)(n.p,{children:"Before diving into multi-device synchronization, let's understand how timestamp alignment works within a single Aria Gen2 device."}),"\n",(0,r.jsx)(n.h3,{id:"understanding-time-domains",children:"Understanding Time Domains"}),"\n",(0,r.jsxs)(n.p,{children:["In ",(0,r.jsx)(n.code,{children:"projectaria_tools"}),", every timestamp is linked to a specific ",(0,r.jsx)(n.code,{children:"TimeDomain"}),", which represents the time reference or clock used to generate that timestamp. Timestamps from different ",(0,r.jsx)(n.code,{children:"TimeDomain"}),"s are not directly comparable\u2014only timestamps within the same ",(0,r.jsx)(n.code,{children:"TimeDomain"})," are consistent and can be accurately compared or aligned."]}),"\n",(0,r.jsx)(n.h4,{id:"supported-time-domains-for-aria-gen2",children:"Supported Time Domains for Aria Gen2"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsxs)(n.strong,{children:["Important: Use ",(0,r.jsx)(n.code,{children:"DEVICE_TIME"})," for single-device Aria data analysis"]})}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Time Domain"}),(0,r.jsx)(n.th,{children:"Description"}),(0,r.jsx)(n.th,{children:"Usage"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"DEVICE_TIME (Recommended)"})}),(0,r.jsx)(n.td,{children:"Capture time in device's time domain. Accurate and reliable. All sensors on the same Aria device share the same device time domain."}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Use this for single-device Aria data analysis"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"RECORD_TIME"})}),(0,r.jsx)(n.td,{children:"Timestamps stored in the index of VRS files. For Aria glasses, these are equal to device timestamp converted to double-precision floating point."}),(0,r.jsx)(n.td,{children:"Fast access, but use DEVICE_TIME for accuracy"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"HOST_TIME"})}),(0,r.jsx)(n.td,{children:"Timestamps when sensor data is saved to the device (not when captured)."}),(0,r.jsx)(n.td,{children:"Should not be needed for any purpose"})]})]})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"For multi-device time alignment"})," (covered in Part 2), we use:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"SUBGHZ"}),": Multi-device time alignment for Aria Gen2 using SubGHz signals"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"load-a-single-sequence",children:"Load a Single Sequence"}),"\n",(0,r.jsx)(n.p,{children:"Let's start by loading a single Aria Gen2 sequence to demonstrate timestamp-based queries:"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"\u26a0\ufe0f Important:"})," Update the path below to point to your downloaded sequence folder."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# TODO: Update this path to your dataset location\nsequence_path = "path/to/your/sequence_folder"\n\n# Initialize data provider\nprint("Loading sequence data...")\npilot_data_provider = AriaGen2PilotDataProvider(sequence_path)\n\nprint("\\n" + "="*60)\nprint("Data Loaded Successfully!")\nprint("="*60)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"data-api-to-query-by-timestamp",children:"Data API to Query by Timestamp"}),"\n",(0,r.jsxs)(n.p,{children:["The data provider offers powerful timestamp-based data access through the ",(0,r.jsx)(n.code,{children:"get_vrs_$SENSOR_data_by_time_ns()"})," API family. This is the recommended approach for temporal alignment across sensors and precise timestamp-based data retrieval."]}),"\n",(0,r.jsx)(n.p,{children:"For any sensor type, you can query data by timestamp using these functions:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"get_vrs_image_data_by_time_ns()"})," - Query image data (RGB, SLAM cameras)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"get_vrs_imu_data_by_time_ns()"})," - Query IMU data"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"get_vrs_audio_data_by_time_ns()"})," - Query audio data"]}),"\n",(0,r.jsx)(n.li,{children:"And more..."}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"timequeryoptions",children:"TimeQueryOptions"}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.code,{children:"TimeQueryOptions"})," parameter controls how the system finds data when your query timestamp doesn't exactly match a recorded timestamp:"]}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Option"}),(0,r.jsx)(n.th,{children:"Behavior"}),(0,r.jsx)(n.th,{children:"Use Case"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"BEFORE"})}),(0,r.jsxs)(n.td,{children:["Returns the last valid data with ",(0,r.jsx)(n.code,{children:"timestamp \u2264 query_time"})]}),(0,r.jsxs)(n.td,{children:[(0,r.jsx)(n.strong,{children:"Default and most common"})," - Get the most recent data before or at the query time"]})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"AFTER"})}),(0,r.jsxs)(n.td,{children:["Returns the first valid data with ",(0,r.jsx)(n.code,{children:"timestamp \u2265 query_time"})]}),(0,r.jsx)(n.td,{children:"Get the next available data after or at the query time"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"CLOSEST"})}),(0,r.jsx)(n.td,{children:"Returns data with smallest `"}),(0,r.jsx)(n.td,{children:"timestamp - query_time"})]})]})]}),"\n",(0,r.jsx)(n.h4,{id:"boundary-behavior",children:"Boundary Behavior"}),"\n",(0,r.jsx)(n.p,{children:"The API handles edge cases automatically:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Query Condition"}),(0,r.jsx)(n.th,{children:"BEFORE"}),(0,r.jsx)(n.th,{children:"AFTER"}),(0,r.jsx)(n.th,{children:"CLOSEST"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"query_time < first_timestamp"})}),(0,r.jsx)(n.td,{children:"Returns invalid data"}),(0,r.jsx)(n.td,{children:"Returns first data"}),(0,r.jsx)(n.td,{children:"Returns first data"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"first_timestamp \u2264 query_time \u2264 last_timestamp"})}),(0,r.jsxs)(n.td,{children:["Returns data with ",(0,r.jsx)(n.code,{children:"timestamp \u2264 query_time"})]}),(0,r.jsxs)(n.td,{children:["Returns data with ",(0,r.jsx)(n.code,{children:"timestamp \u2265 query_time"})]}),(0,r.jsx)(n.td,{children:"Returns temporally closest data"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"query_time > last_timestamp"})}),(0,r.jsx)(n.td,{children:"Returns last data"}),(0,r.jsx)(n.td,{children:"Returns invalid data"}),(0,r.jsx)(n.td,{children:"Returns last data"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:"Let's demonstrate timestamp-based queries:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'print("="*60)\nprint("Single Device Timestamp-Based Query Example")\nprint("="*60)\n\n# Select RGB stream ID\nrgb_stream_id = pilot_data_provider.get_vrs_stream_id_from_label("camera-rgb")\n\n# Get a timestamp within the recording (3 seconds after start)\nstart_timestamp_ns = pilot_data_provider.vrs_data_provider.get_first_time_ns(rgb_stream_id, TimeDomain.DEVICE_TIME)\nselected_timestamp_ns = start_timestamp_ns + int(3e9)\n\nprint(f"\\nQuery timestamp: {selected_timestamp_ns} ns (3 seconds after start)")\n\n# Fetch the RGB frame that is CLOSEST to this selected timestamp_ns\nclosest_rgb_data, closest_rgb_record = pilot_data_provider.get_vrs_image_data_by_time_ns(\n    stream_id=rgb_stream_id,\n    time_ns=selected_timestamp_ns,\n    time_domain=TimeDomain.DEVICE_TIME,\n    time_query_options=TimeQueryOptions.CLOSEST\n)\nclosest_timestamp_ns = closest_rgb_record.capture_timestamp_ns\nclosest_frame_number = closest_rgb_record.frame_number\nprint(f"\\n\u2705 CLOSEST frame to query timestamp:")\nprint(f"   Frame #{closest_frame_number}")\nprint(f"   Capture timestamp: {closest_timestamp_ns} ns")\nprint(f"   Time difference: {abs(closest_timestamp_ns - selected_timestamp_ns) / 1e6:.2f} ms")\n\n# Fetch the frame BEFORE this frame\nprev_rgb_data, prev_rgb_record = pilot_data_provider.get_vrs_image_data_by_time_ns(\n    stream_id=rgb_stream_id,\n    time_ns=closest_timestamp_ns - 1,\n    time_domain=TimeDomain.DEVICE_TIME,\n    time_query_options=TimeQueryOptions.BEFORE\n)\nprev_timestamp_ns = prev_rgb_record.capture_timestamp_ns\nprev_frame_number = prev_rgb_record.frame_number\nprint(f"\\n\u2b05\ufe0f BEFORE frame:")\nprint(f"   Frame #{prev_frame_number}")\nprint(f"   Capture timestamp: {prev_timestamp_ns} ns")\n\n# Fetch the frame AFTER this frame\nnext_rgb_data, next_rgb_record = pilot_data_provider.get_vrs_image_data_by_time_ns(\n    stream_id=rgb_stream_id,\n    time_ns=closest_timestamp_ns + 1,\n    time_domain=TimeDomain.DEVICE_TIME,\n    time_query_options=TimeQueryOptions.AFTER\n)\nnext_timestamp_ns = next_rgb_record.capture_timestamp_ns\nnext_frame_number = next_rgb_record.frame_number\nprint(f"\\n\u27a1\ufe0f AFTER frame:")\nprint(f"   Frame #{next_frame_number}")\nprint(f"   Capture timestamp: {next_timestamp_ns} ns")\n'})}),"\n",(0,r.jsx)(n.h3,{id:"visualizing-timestamp-aligned-multi-sensor-data",children:"Visualizing Timestamp-Aligned Multi-Sensor Data"}),"\n",(0,r.jsx)(n.p,{children:"A common use case is to query and visualize data from multiple sensors at approximately the same timestamp. Let's demonstrate querying RGB and SLAM camera images at the same time:"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Use Case:"})," Get RGB + SLAM images at 5Hz to see timestamp-aligned multi-camera views"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'print("="*60)\nprint("Multi-Sensor Synchronized Query Example")\nprint("="*60)\n\n# Initialize a simple Rerun viewer for single-device visualization\nrr.init("single_device_sync_demo", spawn=False)\n\n# Get stream IDs for RGB and SLAM cameras\nall_labels = pilot_data_provider.vrs_data_provider.get_device_calibration().get_camera_labels()\nslam_labels = [label for label in all_labels if "slam" in label]\nslam_stream_ids = [pilot_data_provider.get_vrs_stream_id_from_label(label) for label in slam_labels]\nrgb_stream_id = pilot_data_provider.get_vrs_stream_id_from_label("camera-rgb")\n\n# Starting from +3 seconds into the recording, sample at 5Hz for 10 frames\ntarget_period_ns = int(2e8)  # 200ms = 5 Hz\nstart_timestamp_ns = pilot_data_provider.vrs_data_provider.get_first_time_ns(rgb_stream_id, TimeDomain.DEVICE_TIME) + int(3e9)\n\nprint(f"\\nQuerying RGB + SLAM images at 5Hz...")\nprint(f"Starting from +3 seconds, sampling 10 frames\\n")\n\n# Plot 10 samples\ncurrent_timestamp_ns = start_timestamp_ns\nfor frame_i in range(10):\n    # Set time for Rerun\n    rr.set_time_nanos("device_time", current_timestamp_ns)\n\n    # Query and log RGB image\n    rgb_image_data, rgb_image_record = pilot_data_provider.get_vrs_image_data_by_time_ns(\n        stream_id=rgb_stream_id,\n        time_ns=current_timestamp_ns,\n        time_domain=TimeDomain.DEVICE_TIME,\n        time_query_options=TimeQueryOptions.CLOSEST\n    )\n    rr.log("single_device/rgb_image", rr.Image(rgb_image_data.to_numpy_array()))\n\n    # Query and log SLAM images\n    for slam_i, (slam_label, slam_stream_id) in enumerate(zip(slam_labels, slam_stream_ids)):\n        slam_image_data, slam_image_record = pilot_data_provider.get_vrs_image_data_by_time_ns(\n            stream_id=slam_stream_id,\n            time_ns=current_timestamp_ns,\n            time_domain=TimeDomain.DEVICE_TIME,\n            time_query_options=TimeQueryOptions.CLOSEST\n        )\n        rr.log(f"single_device/{slam_label}", rr.Image(slam_image_data.to_numpy_array()))\n\n    if frame_i == 0:\n        print(f"Frame {frame_i}: RGB timestamp {rgb_image_record.capture_timestamp_ns} ns")\n\n    # Increment query timestamp\n    current_timestamp_ns += target_period_ns\nrr.notebook_show()\nprint(f"\\n\u2705 Successfully queried and logged 10 frames of synchronized multi-sensor data!")\n'})}),"\n",(0,r.jsx)(n.h2,{id:"part-2-multi-device-time-alignment",children:"Part 2: Multi-Device Time Alignment"}),"\n",(0,r.jsx)(n.p,{children:"Now that we understand single-device timestamp alignment, let's explore how to align timestamps across multiple Aria Gen2 devices using SubGHz signals."}),"\n",(0,r.jsx)(n.h3,{id:"understanding-pilot-dataset-multi-device-naming-convention",children:"Understanding Pilot Dataset Multi-Device Naming Convention"}),"\n",(0,r.jsx)(n.p,{children:"In the Aria Gen2 Pilot Dataset, timestamp-aligned multi-device sequences share the same base name with different numeric suffixes:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Host Device"}),": Sequences ending with ",(0,r.jsx)(n.code,{children:"_0"})," (e.g., ",(0,r.jsx)(n.code,{children:"eat_0"}),", ",(0,r.jsx)(n.code,{children:"play_0"}),", ",(0,r.jsx)(n.code,{children:"walk_0"}),")"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Client Devices"}),": Sequences ending with ",(0,r.jsx)(n.code,{children:"_1"}),", ",(0,r.jsx)(n.code,{children:"_2"}),", ",(0,r.jsx)(n.code,{children:"_3"}),", etc. (e.g., ",(0,r.jsx)(n.code,{children:"eat_1"}),", ",(0,r.jsx)(n.code,{children:"eat_2"}),", ",(0,r.jsx)(n.code,{children:"eat_3"}),")"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Example:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"eat_0"})," \u2192 Host device recording"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"eat_1"})," \u2192 Client device 1 recording"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"eat_2"})," \u2192 Client device 2 recording"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"eat_3"})," \u2192 Client device 3 recording"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["All sequences with the same base name (e.g., all ",(0,r.jsx)(n.code,{children:"eat_*"}),", ",(0,r.jsx)(n.code,{children:"play_*"}),", ",(0,r.jsx)(n.code,{children:"walk_*"})," sequences) share a SubGHz timestamp mapping and can be aligned using the time domain mapping."]}),"\n",(0,r.jsx)(n.h3,{id:"load-multiple-sequences",children:"Load Multiple Sequences"}),"\n",(0,r.jsx)(n.p,{children:"We'll need data from both host and client devices. Make sure you have downloaded a multi-device sequence from the Aria Gen2 Pilot Dataset."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"\u26a0\ufe0f Important:"})," Update the paths below to point to your downloaded host and client sequence folders."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# TODO: Update these paths to your multi-device dataset location\nhost_sequence_path = "/path/to/host_sequence"\nclient_sequence_path = "/path/to/client_sequence"\n\n# Initialize data providers for both devices\nprint("Loading host device data...")\nhost_data_provider = AriaGen2PilotDataProvider(host_sequence_path)\n\nprint("Loading client device data...")\nclient_data_provider = AriaGen2PilotDataProvider(client_sequence_path)\n\nprint("\\n" + "="*60)\nprint("Multi-Device Data Loaded Successfully!")\nprint("="*60)\nprint(f"Host has MPS data: {\'\u2705\' if host_data_provider.has_mps_data() else \'\u274c\'}")\nprint(f"Client has MPS data: {\'\u2705\' if client_data_provider.has_mps_data() else \'\u274c\'}")\n'})}),"\n",(0,r.jsx)(n.h3,{id:"understanding-subghz-timestamp-conversion",children:"Understanding SubGHz Timestamp Conversion"}),"\n",(0,r.jsx)(n.p,{children:"The SubGHz synchronization creates a mapping between host and client device times. We can convert timestamps bidirectionally:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Host \u2192 Client"}),": ",(0,r.jsx)(n.code,{children:"client_vrs_provider.convert_from_synctime_to_device_time_ns(host_time, TimeSyncMode.SUBGHZ)"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Client \u2192 Host"}),": ",(0,r.jsx)(n.code,{children:"client_vrs_provider.convert_from_device_time_to_synctime_ns(client_time, TimeSyncMode.SUBGHZ)"})]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Important"}),": Both conversion functions are called on the ",(0,r.jsx)(n.strong,{children:"client's VRS data provider"}),", since only the client has the time domain mapping data."]}),"\n",(0,r.jsx)(n.p,{children:"Let's demonstrate timestamp conversion:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'print("="*60)\nprint("Multi-Device Timestamp Conversion Example")\nprint("="*60)\n\n# Get RGB stream ID from host\nrgb_stream_id = host_data_provider.get_vrs_stream_id_from_label("camera-rgb")\n\n# Pick a timestamp in the middle of the host recording\nhost_start_time = host_data_provider.vrs_data_provider.get_first_time_ns_all_streams(TimeDomain.DEVICE_TIME)\nhost_end_time = host_data_provider.vrs_data_provider.get_last_time_ns_all_streams(TimeDomain.DEVICE_TIME)\nselected_host_timestamp_ns = (host_start_time + host_end_time) // 2\n\nprint(f"\\nSelected host timestamp: {selected_host_timestamp_ns} ns")\nprint(f"  (Host recording spans {(host_end_time - host_start_time) / 1e9:.2f} seconds)")\n\n# Convert from host time to client time\nconverted_client_timestamp_ns = client_data_provider.vrs_data_provider.convert_from_synctime_to_device_time_ns(\n    selected_host_timestamp_ns,\n    TimeSyncMode.SUBGHZ\n)\nprint(f"\\nConverted to client timestamp: {converted_client_timestamp_ns} ns")\n\n# Convert back from client time to host time (roundtrip)\nroundtrip_host_timestamp_ns = client_data_provider.vrs_data_provider.convert_from_device_time_to_synctime_ns(\n    converted_client_timestamp_ns,\n    TimeSyncMode.SUBGHZ\n)\nprint(f"\\nRoundtrip back to host timestamp: {roundtrip_host_timestamp_ns} ns")\n\n# Calculate numerical difference\nroundtrip_error_ns = roundtrip_host_timestamp_ns - selected_host_timestamp_ns\nprint(f"\\nRoundtrip error: {roundtrip_error_ns} ns ({roundtrip_error_ns / 1e3:.3f} \u03bcs)")\nprint("  Note: Small numerical differences are expected due to interpolation")\n'})}),"\n",(0,r.jsx)(n.h3,{id:"query-apis-with-timedomainsubghz",children:"Query APIs with TimeDomain.SUBGHZ"}),"\n",(0,r.jsxs)(n.p,{children:["Instead of manually converting timestamps, you can directly query client data using host timestamps by specifying ",(0,r.jsx)(n.code,{children:"time_domain=TimeDomain.SUBGHZ"}),". This is more convenient when querying multiple data types."]}),"\n",(0,r.jsx)(n.p,{children:"The following example shows how to query client RGB images using host timestamps:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'print("="*60)\nprint("Query Client Data Using Host Timestamps")\nprint("="*60)\n\n# Query host RGB image at the selected timestamp\nhost_image_data, host_image_record = host_data_provider.get_vrs_image_data_by_time_ns(\n    stream_id=rgb_stream_id,\n    time_ns=selected_host_timestamp_ns,\n    time_domain=TimeDomain.DEVICE_TIME,\n    time_query_options=TimeQueryOptions.CLOSEST\n)\n\nprint(f"\\nHost RGB frame:")\nprint(f"  Query timestamp: {selected_host_timestamp_ns} ns")\nprint(f"  Actual capture timestamp: {host_image_record.capture_timestamp_ns} ns")\nprint(f"  Frame number: {host_image_record.frame_number}")\nprint(f"  Image shape: {host_image_data.to_numpy_array().shape}")\n\n# Query client RGB image using the SAME host timestamp, but with TimeDomain.SUBGHZ\nclient_image_data, client_image_record = client_data_provider.get_vrs_image_data_by_time_ns(\n    stream_id=rgb_stream_id,\n    time_ns=selected_host_timestamp_ns,  # Using host timestamp!\n    time_domain=TimeDomain.SUBGHZ,  # Specify SUBGHZ domain\n    time_query_options=TimeQueryOptions.CLOSEST\n)\n\nprint(f"\\nClient RGB frame (queried with host timestamp):")\nprint(f"  Query timestamp (host domain): {selected_host_timestamp_ns} ns")\nprint(f"  Actual capture timestamp (client device time): {client_image_record.capture_timestamp_ns} ns")\nprint(f"  Frame number: {client_image_record.frame_number}")\nprint(f"  Image shape: {client_image_data.to_numpy_array().shape}")\n\nprint("\\n\u2705 Successfully queried synchronized frames from both devices!")\n'})}),"\n",(0,r.jsx)(n.h2,{id:"part-3-timestamp-aligned-trajectory-and-hand-tracking-visualization",children:"Part 3: Timestamp-Aligned Trajectory and Hand Tracking Visualization"}),"\n",(0,r.jsx)(n.p,{children:"Now we'll visualize timestamp-aligned data from both devices in 3D. Since MPS processes multi-device recordings together, the trajectories from both devices are already in the same world coordinate frame."}),"\n",(0,r.jsx)(n.h3,{id:"load-mps-data",children:"Load MPS Data"}),"\n",(0,r.jsx)(n.p,{children:"First, let's load the MPS trajectories and hand tracking data from both devices:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'print("="*60)\nprint("Loading MPS Data")\nprint("="*60)\n\n# Load closed loop trajectories\nhost_trajectory = host_data_provider.get_mps_closed_loop_trajectory()\nclient_trajectory = client_data_provider.get_mps_closed_loop_trajectory()\n\nprint(f"\\nHost trajectory: {len(host_trajectory)} poses")\nprint(f"  Duration: {(host_trajectory[-1].tracking_timestamp - host_trajectory[0].tracking_timestamp).total_seconds():.2f} seconds")\n\nprint(f"\\nClient trajectory: {len(client_trajectory)} poses")\nprint(f"  Duration: {(client_trajectory[-1].tracking_timestamp - client_trajectory[0].tracking_timestamp).total_seconds():.2f} seconds")\n\n# Load hand tracking results\nhost_hand_tracking = host_data_provider.get_mps_hand_tracking_result_list()\nclient_hand_tracking = client_data_provider.get_mps_hand_tracking_result_list()\n\nprint(f"\\nHost hand tracking: {len(host_hand_tracking)} frames")\nprint(f"Client hand tracking: {len(client_hand_tracking)} frames")\n\nprint("\\n\u2705 MPS data loaded successfully!")\nprint("\\n\u26a0\ufe0f Note: Both trajectories are in the same world coordinate frame")\n'})}),"\n",(0,r.jsx)(n.h3,{id:"visualization-setup",children:"Visualization Setup"}),"\n",(0,r.jsx)(n.p,{children:"We'll create a Rerun visualization with two views:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"RGB View"}),": Synchronized RGB frames from both host and client devices"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"3D World View"}),": Trajectories and hand tracking from both devices in the shared world coordinate frame"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Let's define helper functions for plotting:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Cache for accumulated trajectory points\nhost_trajectory_cache = []\nclient_trajectory_cache = []\n\ndef plot_device_pose_and_trajectory(device_label: str, pose: mps.ClosedLoopTrajectoryPose, trajectory_cache: list, color: list):\n    """\n    Plot device pose and accumulated trajectory in 3D world view.\n\n    Args:\n        device_label: Label for the device (e.g., "host" or "client")\n        pose: ClosedLoopTrajectoryPose object\n        trajectory_cache: List to accumulate trajectory points\n        color: RGB color for trajectory line\n    """\n    if pose is None:\n        return\n\n    # Get transform and add to trajectory cache\n    T_world_device = pose.transform_world_device\n    trajectory_cache.append(T_world_device.translation()[0])\n\n    # Plot device pose\n    rr.log(\n        f"world/{device_label}/device",\n        ToTransform3D(T_world_device, axis_length=0.05),\n    )\n\n    # Plot accumulated trajectory\n    if len(trajectory_cache) > 1:\n        rr.log(\n            f"world/{device_label}/trajectory",\n            rr.LineStrips3D(\n                [trajectory_cache],\n                colors=[color],\n                radii=0.005,\n            ),\n        )\n\ndef plot_hand_tracking(device_label: str, hand_tracking_result: mps.hand_tracking.HandTrackingResult):\n    """\n    Plot hand tracking landmarks and skeleton in 3D world view.\n\n    Args:\n        device_label: Label for the device (e.g., "host" or "client")\n        hand_tracking_result: HandTrackingResult object\n    """\n    # Clear previous hand tracking data\n    rr.log(f"world/{device_label}/device/hand-tracking", rr.Clear.recursive())\n\n    if hand_tracking_result is None:\n        return\n\n    # Plot left hand if available\n    if hand_tracking_result.left_hand is not None:\n        landmarks = hand_tracking_result.left_hand.landmark_positions_device\n        skeleton = create_hand_skeleton_from_landmarks(landmarks)\n\n        landmarks_style = get_plot_style(PlotEntity.HAND_TRACKING_LEFT_HAND_LANDMARKS)\n        skeleton_style = get_plot_style(PlotEntity.HAND_TRACKING_LEFT_HAND_SKELETON)\n\n        rr.log(\n            f"world/{device_label}/device/hand-tracking/left/landmarks",\n            rr.Points3D(\n                positions=landmarks,\n                colors=[landmarks_style.color],\n                radii=landmarks_style.plot_3d_size,\n            ),\n        )\n        rr.log(\n            f"world/{device_label}/device/hand-tracking/left/skeleton",\n            rr.LineStrips3D(\n                skeleton,\n                colors=[skeleton_style.color],\n                radii=skeleton_style.plot_3d_size,\n            ),\n        )\n\n    # Plot right hand if available\n    if hand_tracking_result.right_hand is not None:\n        landmarks = hand_tracking_result.right_hand.landmark_positions_device\n        skeleton = create_hand_skeleton_from_landmarks(landmarks)\n\n        landmarks_style = get_plot_style(PlotEntity.HAND_TRACKING_RIGHT_HAND_LANDMARKS)\n        skeleton_style = get_plot_style(PlotEntity.HAND_TRACKING_RIGHT_HAND_SKELETON)\n\n        rr.log(\n            f"world/{device_label}/device/hand-tracking/right/landmarks",\n            rr.Points3D(\n                positions=landmarks,\n                colors=[landmarks_style.color],\n                radii=landmarks_style.plot_3d_size,\n            ),\n        )\n        rr.log(\n            f"world/{device_label}/device/hand-tracking/right/skeleton",\n            rr.LineStrips3D(\n                skeleton,\n                colors=[skeleton_style.color],\n                radii=skeleton_style.plot_3d_size,\n            ),\n        )\n\ndef plot_rgb_image(device_label: str, image_data, image_record):\n    """\n    Plot RGB image in the RGB view.\n\n    Args:\n        device_label: Label for the device (e.g., "host" or "client")\n        image_data: ImageData object\n        image_record: ImageDataRecord object\n    """\n    if image_data is None:\n        return\n\n    rr.log(\n        f"{device_label}",\n        rr.Image(image_data.to_numpy_array())\n    )\n\nprint("\u2705 Helper functions defined!")\n'})}),"\n",(0,r.jsx)(n.h3,{id:"visualization-part-1-timestamp-aligned-rgb-frames",children:"Visualization Part 1: Timestamp-Aligned RGB Frames"}),"\n",(0,r.jsx)(n.p,{children:"Let's first visualize the timestamp-aligned RGB frames from both devices side-by-side."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Initialize RGB Rerun viewer\nrr.init("rgb_viewer", spawn=False)\nprint("\u2705 RGB viewer initialized")\n# Get RGB stream ID\nrgb_stream_id = host_data_provider.get_vrs_stream_id_from_label("camera-rgb")\n\n# Define sampling parameters\nsampling_period_ns = int(2e8)  # 200ms = 5 Hz (adjust for different frame rates)\nstart_offset_ns = int(3e9)     # Start 3 seconds into recording (skip initial setup)\nduration_ns = int(10e9)        # Visualize 10 seconds (adjust as needed)\n\n# Get host recording time range\nhost_start_time_ns = host_data_provider.vrs_data_provider.get_first_time_ns_all_streams(TimeDomain.DEVICE_TIME)\nhost_end_time_ns = host_data_provider.vrs_data_provider.get_last_time_ns_all_streams(TimeDomain.DEVICE_TIME)\n\n# Calculate query range\nquery_start_ns = host_start_time_ns + start_offset_ns\nquery_end_ns = min(query_start_ns + duration_ns, host_end_time_ns)\n\n# Clear trajectory caches for fresh visualization\nhost_trajectory_cache.clear()\nclient_trajectory_cache.clear()\n\nprint("Visualization Configuration:")\nprint("=" * 60)\nprint(f"Host recording duration: {(host_end_time_ns - host_start_time_ns) / 1e9:.2f} seconds")\nprint(f"Visualization start: {start_offset_ns / 1e9:.1f}s into recording")\nprint(f"Visualization duration: {(query_end_ns - query_start_ns) / 1e9:.2f} seconds")\nprint(f"Sampling rate: {1e9 / sampling_period_ns:.1f} Hz")\nprint(f"Expected frames: ~{int((query_end_ns - query_start_ns) / sampling_period_ns)}")\nprint("=" * 60)\n\nprint("\u23f3 Logging RGB frames...\\n")\n\ncurrent_timestamp_ns = query_start_ns\nframe_count = 0\n\nwhile current_timestamp_ns <= query_end_ns:\n    # Query host RGB image\n    host_image_data, host_image_record = host_data_provider.get_vrs_image_data_by_time_ns(\n        stream_id=rgb_stream_id,\n        time_ns=current_timestamp_ns,\n        time_domain=TimeDomain.DEVICE_TIME,\n        time_query_options=TimeQueryOptions.CLOSEST\n    )\n\n    # Query client RGB image using host timestamp with SUBGHZ\n    client_image_data, client_image_record = client_data_provider.get_vrs_image_data_by_time_ns(\n        stream_id=rgb_stream_id,\n        time_ns=current_timestamp_ns,  # Host timestamp!\n        time_domain=TimeDomain.SUBGHZ,  # SUBGHZ domain for cross-device query\n        time_query_options=TimeQueryOptions.CLOSEST\n    )\n\n    # Set Rerun timestamp to the host\'s actual capture timestamp\n    rr.set_time_nanos("device_time", host_image_record.capture_timestamp_ns)\n\n    # Plot RGB images with the correct timestamp\n    rr.log("rgb_image_in_host", rr.Image(host_image_data.to_numpy_array()))\n    rr.log("rgb_image_in_client", rr.Image(client_image_data.to_numpy_array()))\n\n    # Move to next timestamp\n    current_timestamp_ns += sampling_period_ns\n    frame_count += 1\n\n    # Print progress every 10 frames\n    if frame_count % 10 == 0:\n        print(f"  Processed {frame_count} RGB frames...")\n\nprint(f"\\n\u2705 RGB data logging complete! Processed {frame_count} frames.")\n# Display RGB viewer\nrr.notebook_show()\n\nprint("\\n\ud83d\udca1 RGB Viewer - What to observe:")\nprint("  - Top: Host device RGB frames")\nprint("  - Bottom: Client device RGB frames")\nprint("  - Frames are synchronized using SubGHz time alignment")\nprint("  - May have slight timing differences (cameras not trigger-aligned)")\n'})}),"\n",(0,r.jsx)(n.h3,{id:"visualization-part-2-3d-world-view",children:"Visualization Part 2: 3D World View"}),"\n",(0,r.jsx)(n.p,{children:"Now let's visualize the 3D trajectories, hand tracking, and point cloud in a shared world coordinate frame."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Initialize 3D World Rerun viewer\nrr.init("world_3d_viewer", spawn=False)\nworld_blueprint = rrb.Blueprint(\n    rrb.Spatial3DView(\n        origin="world",\n        name="3D World View",\n        background=[0, 0, 0],\n    ),\n    collapse_panels=True,\n)\n\nprint("\u2705 3D world viewer initialized")\n\n# Load filtered point cloud from host\nprint("Loading filtered point cloud from host...")\nhost_point_cloud_filtered = host_data_provider.get_mps_semidense_point_cloud_filtered(\n    filter_confidence=True,\n    max_point_count=50000  # Limit to 50k points for performance\n)\n\n# Convert to numpy array for plotting\nif host_point_cloud_filtered:\n    points_array = np.array([point.position_world for point in host_point_cloud_filtered])\n\n    # Plot point cloud as static data\n    plot_style = get_plot_style(PlotEntity.SEMI_DENSE_POINT_CLOUD)\n    rr.log(\n        f"world/{plot_style.label}",\n        rr.Points3D(\n            positions=points_array,\n            colors=[plot_style.color] * len(points_array),\n            radii=plot_style.plot_3d_size,\n        ),\n        static=True,\n    )\n    print(f"\u2705 Plotted {len(points_array)} filtered points from host")\nelse:\n    print("\u26a0\ufe0f No point cloud data available")\n\n# Clear trajectory caches for fresh visualization\nhost_trajectory_cache.clear()\nclient_trajectory_cache.clear()\n\nprint("\u23f3 Logging 3D trajectories and hand tracking...\\n")\n\ncurrent_timestamp_ns = query_start_ns\nframe_count = 0\n\nwhile current_timestamp_ns <= query_end_ns:\n    # Query host MPS pose and hand tracking\n    host_pose = host_data_provider.get_mps_interpolated_closed_loop_pose(\n        timestamp_ns=current_timestamp_ns,\n        time_domain=TimeDomain.DEVICE_TIME\n    )\n\n    host_hand = host_data_provider.get_mps_interpolated_hand_tracking_result(\n        timestamp_ns=current_timestamp_ns,\n        time_domain=TimeDomain.DEVICE_TIME\n    )\n\n    # Query client MPS pose and hand tracking using host timestamp with SUBGHZ\n    client_pose = client_data_provider.get_mps_interpolated_closed_loop_pose(\n        timestamp_ns=current_timestamp_ns,  # Host timestamp!\n        time_domain=TimeDomain.SUBGHZ  # SUBGHZ domain for cross-device query\n    )\n\n    client_hand = client_data_provider.get_mps_interpolated_hand_tracking_result(\n        timestamp_ns=current_timestamp_ns,  # Host timestamp!\n        time_domain=TimeDomain.SUBGHZ  # SUBGHZ domain for cross-device query\n    )\n\n    # Set Rerun timestamp to the host\'s actual pose timestamp (if available)\n    if host_pose is not None:\n        rr.set_time_nanos("device_time", int(host_pose.tracking_timestamp.total_seconds() * 1e9))\n    else:\n        rr.set_time_nanos("device_time", current_timestamp_ns)\n\n    # Plot host trajectory and pose (blue color) with correct timestamp\n    plot_device_pose_and_trajectory(\n        "host",\n        host_pose,\n        host_trajectory_cache,\n        color=[0, 100, 255]\n    )\n    plot_hand_tracking("host", host_hand)\n\n    # Plot client trajectory and pose (red color) with the same timestamp\n    plot_device_pose_and_trajectory(\n        "client",\n        client_pose,\n        client_trajectory_cache,\n        color=[255, 100, 0]\n    )\n    plot_hand_tracking("client", client_hand)\n\n    # Move to next timestamp\n    current_timestamp_ns += sampling_period_ns\n    frame_count += 1\n\n    # Print progress every 10 frames\n    if frame_count % 10 == 0:\n        print(f"  Processed {frame_count} 3D frames...")\n\nprint(f"\\n\u2705 3D data logging complete! Processed {frame_count} frames.")\nprint(f"\\n\ud83d\udcca Trajectory Statistics:")\nprint(f"  Host trajectory points: {len(host_trajectory_cache)}")\nprint(f"  Client trajectory points: {len(client_trajectory_cache)}")\n# Display 3D world viewer\nrr.notebook_show(blueprint=world_blueprint)\n\nprint("\\n\ud83d\udca1 3D World Viewer - What to observe:")\nprint("  - Gray points: Filtered semi-dense point cloud from host")\nprint("  - Blue trajectory: Host device path")\nprint("  - Red trajectory: Client device path")\nprint("  - Hand skeletons: Real-time hand tracking from both devices")\nprint("  - Both trajectories share the same world coordinate frame")\n'})}),"\n",(0,r.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsx)(n.p,{children:"This tutorial covered multi-device timestamp alignment in the Aria Gen2 Pilot Dataset:"}),"\n",(0,r.jsx)(n.h3,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"SubGHz timestamp alignment"}),": Understanding the host/client model for multi-device time alignment"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Host broadcasts SubGHz signals"}),"\n",(0,r.jsx)(n.li,{children:"Client records time domain mapping (only in client VRS)"}),"\n",(0,r.jsx)(n.li,{children:"Enables bidirectional timestamp conversion"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Timestamp Conversion APIs"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"convert_from_synctime_to_device_time_ns()"}),": Host time \u2192 Client device time"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"convert_from_device_time_to_synctime_ns()"}),": Client device time \u2192 Host time"]}),"\n",(0,r.jsx)(n.li,{children:"Both functions called on client's VRS data provider"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Query APIs with TimeDomain.SUBGHZ"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Query client data directly using host timestamps"}),"\n",(0,r.jsxs)(n.li,{children:["Specify ",(0,r.jsx)(n.code,{children:"time_domain=TimeDomain.SUBGHZ"})," in query functions"]}),"\n",(0,r.jsx)(n.li,{children:"More convenient than manual timestamp conversion"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Shared World Coordinate Frame"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"MPS trajectories from multi-device recordings are in the same world coordinate frame"}),"\n",(0,r.jsx)(n.li,{children:"Enables direct comparison and multi-view analysis"}),"\n",(0,r.jsx)(n.li,{children:"No additional alignment needed"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Visualization Best Practices"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Break large visualization code into manageable steps"}),"\n",(0,r.jsx)(n.li,{children:"Use different colors for different devices"}),"\n",(0,r.jsx)(n.li,{children:"Show timestamp-aligned RGB frames side-by-side"}),"\n",(0,r.jsx)(n.li,{children:"Display trajectories and hand tracking in unified 3D world view"}),"\n"]}),"\n"]}),"\n"]})]})}function _(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(m,{...e})}):m(e)}}}]);