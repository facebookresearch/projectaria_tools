"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[134],{93476(e,r,a){a.r(r),a.d(r,{assets:()=>c,contentTitle:()=>n,default:()=>p,frontMatter:()=>o,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"mps/mps","title":"Machine Perception Services (MPS)","description":"To accelerate research with Project Aria gen2, we provide Spatial AI machine perception capabilities that help form the foundation for future Contextualized AI applications and analysis of egocentric data. These capabilities are powered by a set of proprietary machine perception algorithms, designed for Project Aria glasses, that provide superior accuracy and robustness on Aria gen2 data compared to off-the-shelf open source algorithms.","source":"@site/docs-ark/mps/mps.mdx","sourceDirName":"mps","slug":"/mps/","permalink":"/projectaria_tools/gen2/ark/mps/","draft":false,"unlisted":false,"editUrl":"https://www.internalfb.com/code/fbsource/arvr/projects/ariane/aria_research_kit/projectaria_tools/website/docs-ark/mps/mps.mdx","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Machine Perception Services (MPS)"},"sidebar":"arkSidebar","previous":{"title":"Changelog","permalink":"/projectaria_tools/gen2/ark/companion-app/changelog"},"next":{"title":"Getting Started","permalink":"/projectaria_tools/gen2/ark/mps/start"}}');var t=a(74848),s=a(28453);const o={sidebar_position:2,title:"Machine Perception Services (MPS)"},n="Project Aria Machine Perception Services",c={},l=[{value:"Current MPS offerings",id:"current-mps-offerings",level:2},{value:"SLAM services",id:"slam-services",level:2},{value:"6DoF trajectory",id:"6dof-trajectory",level:3},{value:"Semi-dense point cloud",id:"semi-dense-point-cloud",level:3},{value:"Online sensor calibration",id:"online-sensor-calibration",level:3},{value:"Hand Tracking services",id:"hand-tracking-services",level:2},{value:"About MPS Data Loader APIs",id:"about-mps-data-loader-apis",level:2},{value:"Questions &amp; Feedback",id:"questions--feedback",level:2}];function d(e){const r={a:"a",admonition:"admonition",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(r.header,{children:(0,t.jsx)(r.h1,{id:"project-aria-machine-perception-services",children:"Project Aria Machine Perception Services"})}),"\n",(0,t.jsx)(r.p,{children:"To accelerate research with Project Aria gen2, we provide Spatial AI machine perception capabilities that help form the foundation for future Contextualized AI applications and analysis of egocentric data. These capabilities are powered by a set of proprietary machine perception algorithms, designed for Project Aria glasses, that provide superior accuracy and robustness on Aria gen2 data compared to off-the-shelf open source algorithms."}),"\n",(0,t.jsx)(r.p,{children:"For research partners with access to the Aria Research Kit, Machine Perception Services (MPS) are offered as post-processing of VRS files via a cloud service. Use the MPS CLI, to request derived data from any VRS file that contains necessary sensor data."}),"\n",(0,t.jsx)(r.admonition,{type:"note",children:(0,t.jsxs)(r.p,{children:["When research partners submit data for processing, the data is only used to serve MPS requests. Partner data is not made available to Meta researchers or Meta\u2019s affiliates. Go to ",(0,t.jsx)(r.a,{href:"/projectaria_tools/gen2/ark/mps/mps_data_lifecycle",children:"MPS Data Lifecycle"})," for more details about how partner data is processed and stored."]})}),"\n",(0,t.jsx)(r.h2,{id:"current-mps-offerings",children:"Current MPS offerings"}),"\n",(0,t.jsxs)(r.p,{children:["The following MPS can be requested, as long as the data has been recorded with a compatible Recording Profile. Go to the ",(0,t.jsx)(r.a,{href:"/technical-specs/device/profile",children:"Recording Profiles"})," for information about each profile."]}),"\n",(0,t.jsx)(r.p,{children:"MPS offerings are grouped into SLAM and Hand Tracking services."}),"\n",(0,t.jsx)(r.h2,{id:"slam-services",children:"SLAM services"}),"\n",(0,t.jsxs)(r.p,{children:["To get these outputs the ",(0,t.jsx)(r.a,{href:"/technical-specs/device/profile",children:"recording profile"})," must have CV cameras + IMU enabled."]}),"\n",(0,t.jsx)(r.h3,{id:"6dof-trajectory",children:"6DoF trajectory"}),"\n",(0,t.jsx)(r.p,{children:"MPS provides two types of high frequency (1kHz) trajectories:"}),"\n",(0,t.jsxs)(r.ul,{children:["\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.a,{href:"/technical-specs/mps/data_formats/slam/mps_trajectory#open-loop-trajectory",children:"Open loop trajectory"})," - local odometry estimation from visual-inertial odometry (VIO)"]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.a,{href:"/technical-specs/mps/data_formats/slam/mps_trajectory#closed-loop-trajectory",children:"Closed loop trajectory"})," - created via batch optimization, using multi-sensors' input (SLAM, IMU, barometer, Wi-Fi and GPS), fully optimized and provides poses in a consistent frame of reference."]}),"\n"]}),"\n",(0,t.jsx)(r.h3,{id:"semi-dense-point-cloud",children:"Semi-dense point cloud"}),"\n",(0,t.jsxs)(r.p,{children:[(0,t.jsx)(r.a,{href:"/technical-specs/mps/data_formats/slam/mps_pointcloud",children:"Semi-dense point cloud"})," data supports researchers who need static scene 3D reconstructions, reliable 2D images tracks or a representative visualization of the environment."]}),"\n",(0,t.jsx)(r.h3,{id:"online-sensor-calibration",children:"Online sensor calibration"}),"\n",(0,t.jsxs)(r.p,{children:["The ",(0,t.jsx)(r.a,{href:"/technical-specs/mps/data_formats/slam/mps_calibration",children:"time-varying intrinsic and extrinsic calibrations"})," of cameras and IMUs are estimated at the frequency of the SLAM (mono scene) cameras by our multi-sensor state estimation pipeline."]}),"\n",(0,t.jsx)(r.h2,{id:"hand-tracking-services",children:"Hand Tracking services"}),"\n",(0,t.jsx)(r.p,{children:"Cloud Hand Tracking processing is available for all recordings made with CV cameras (>=10fps) and IMU enabled."}),"\n",(0,t.jsx)(r.h2,{id:"about-mps-data-loader-apis",children:"About MPS Data Loader APIs"}),"\n",(0,t.jsxs)(r.p,{children:["Please refer to our ",(0,t.jsx)(r.a,{href:"/research-tools/projectariatools/pythontutorials/mps",children:"MPS data loader APIs"})," in python, to load and visualize the MPS outputs into your application."]}),"\n",(0,t.jsx)(r.h2,{id:"questions--feedback",children:"Questions & Feedback"}),"\n",(0,t.jsxs)(r.p,{children:["If you have feedback you'd like to provide, be it overall trends and experiences or where we can improve, we'd love to hear from you. Go to our ",(0,t.jsx)(r.a,{href:"/ark/support",children:"Support page"})," for different ways to get in touch."]})]})}function p(e={}){const{wrapper:r}={...(0,s.R)(),...e.components};return r?(0,t.jsx)(r,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},28453(e,r,a){a.d(r,{R:()=>o,x:()=>n});var i=a(96540);const t={},s=i.createContext(t);function o(e){const r=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(r):{...r,...e}},[r,e])}function n(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),i.createElement(s.Provider,{value:r},e.children)}}}]);