"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[7795],{28453:(e,i,n)=>{n.d(i,{R:()=>o,x:()=>s});var t=n(96540);const a={},r=t.createContext(a);function o(e){const i=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function s(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),t.createElement(r.Provider,{value:i},e.children)}},82615:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>c,metadata:()=>t,toc:()=>h});const t=JSON.parse('{"id":"on_device_mp","title":"On-device Machine Perception","description":"Aria Gen2 device has energy efficient hardware acceleration technology that enables on-device machine perception algorithms to be run with all-day wearability in mind. Machine perception algorithm output can be either recorded onto the device or streamed to a host PC. Profiles are used to precisely configure the frequency and types of machine perception algorithm calculations for a variety of research applications.","source":"@site/docs-ark/on_device_mp.mdx","sourceDirName":".","slug":"/on_device_mp","permalink":"/projectaria_tools/gen2/ark/on_device_mp","draft":false,"unlisted":false,"editUrl":"https://www.internalfb.com/code/fbsource/arvr/projects/ariane/aria_research_kit/projectaria_tools/website/docs-ark/on_device_mp.mdx","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"On-device Machine Perception"},"sidebar":"arkSidebar","previous":{"title":"Accessories and Fitment","permalink":"/projectaria_tools/gen2/ark/device/accessories_fitment"},"next":{"title":"Get Started","permalink":"/projectaria_tools/gen2/ark/companion-app/start"}}');var a=n(74848),r=n(28453),o=n(98180),s=n(67581);const c={sidebar_position:1,title:"On-device Machine Perception"},l=void 0,d={},h=[{value:"Visual Inertial Odometry (VIO)",id:"visual-inertial-odometry-vio",level:2},{value:"Eye Tracking",id:"eye-tracking",level:2},{value:"Hand Tracking",id:"hand-tracking",level:2}];function p(e){const i={a:"a",h2:"h2",li:"li",p:"p",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsxs)(i.p,{children:["Aria Gen2 device has energy efficient hardware acceleration technology that enables on-device machine perception algorithms to be run with all-day wearability in mind. Machine perception algorithm output can be either recorded onto the device or streamed to a host PC. ",(0,a.jsx)(i.a,{href:"/technical-specs/device/profile",children:"Profiles"})," are used to precisely configure the frequency and types of machine perception algorithm calculations for a variety of research applications."]}),"\n",(0,a.jsx)(i.p,{children:"At the time of the release of Aria Gen 2, the device is able to run visual-inertial odometry (VIO), 21-keypoint hand tracking, and eye gaze estimation. However, there is significant potential to expand what can be computed on-device in the future, such as algorithms for scene understanding, action recognition, human body pose estimation, and more. We will now describe the currently available machine perception algorithms in detail."}),"\n",(0,a.jsx)(s.A,{alt:"Docusaurus themed image",sources:{light:(0,o.default)("/img/docs-ark/device/on-device-mp.png"),dark:(0,o.default)("/img/docs-ark/device/on-device-mp.png")}}),"\n",(0,a.jsx)(i.h2,{id:"visual-inertial-odometry-vio",children:"Visual Inertial Odometry (VIO)"}),"\n",(0,a.jsx)(i.p,{children:"One of the key features of Aria Gen 2 is its ability to track the glasses in six degrees of freedom (6DOF) within a spatial frame of reference using Visual Inertial Odometry (VIO), by fusing the sensor data from four CV cameras and two IMUs. This allows for seamless navigation and mapping of the environment, opening up new possibilities for research in contextual AI and robotics.\nThe VIO output is generated at 10Hz with the following output:"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"3-DOF position"}),"\n",(0,a.jsx)(i.li,{children:"3-DOF linear velocity"}),"\n",(0,a.jsx)(i.li,{children:"3-DOF orientation in quaternion form"}),"\n",(0,a.jsx)(i.li,{children:"3-DOF angular velocity"}),"\n",(0,a.jsx)(i.li,{children:"Estimated direction of gravity for the odometry frame"}),"\n"]}),"\n",(0,a.jsx)(i.p,{children:"Additionally, Aria Gen2 also produces high-frequency VIO output (the fields of the output are the same regular VIO) at IMU rate (800Hz), by performing IMU pre-integration on top of the regular 10Hz VIO output. The high-frequency VIO output can be useful for applications where low-latency VIO poses are needed."}),"\n",(0,a.jsx)(i.h2,{id:"eye-tracking",children:"Eye Tracking"}),"\n",(0,a.jsx)(i.p,{children:"Aria Gen 2 also boasts an advanced camera-based eye tracking system that tracks the wearer\u2019s gaze. The advanced gaze signal enables a deeper understanding of the wearer\u2019s visual attention and intentions, unlocking new possibilities for human-computer interaction. This system generates the following eye tracking outputs for each eye, up to 90Hz:"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"The origin and direction of the individual gaze ray"}),"\n",(0,a.jsx)(i.li,{children:"The 3-DOF position of the entrance pupil"}),"\n",(0,a.jsx)(i.li,{children:"The diameter of the pupil"}),"\n",(0,a.jsx)(i.li,{children:"Whether the eye is blinking"}),"\n"]}),"\n",(0,a.jsx)(i.p,{children:"Additionally, the system also produces the following signals for the combined gaze estimated from both eyes, including:"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"The original and direction of the combined gaze ray"}),"\n",(0,a.jsx)(i.li,{children:"Vergence depth of the combined gaze"}),"\n",(0,a.jsx)(i.li,{children:"Distance between the left/right eye pupils, a.k.a, IPD (Inter Pupillary Distance)"}),"\n"]}),"\n",(0,a.jsx)(i.h2,{id:"hand-tracking",children:"Hand Tracking"}),"\n",(0,a.jsx)(i.p,{children:"Aria Gen 2 also features a hand detection and tracking solution that tracks the wearer\u2019s hand in 3D space. This produces articulated hand-joint poses in the device frame of reference, facilitating accurate hand annotations for datasets and enabling applications such as dexterous robot hand manipulation that require high precision.\nThe hand tracking pipeline generates the following outputs at 30Hz for each hand (left and right):"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"3-DOF position of the wrist"}),"\n",(0,a.jsx)(i.li,{children:"3-DOF rotation of the wrist"}),"\n",(0,a.jsx)(i.li,{children:"3-DOF positions of the 21 finger joint landmarks"}),"\n"]})]})}function u(e={}){const{wrapper:i}={...(0,r.R)(),...e.components};return i?(0,a.jsx)(i,{...e,children:(0,a.jsx)(p,{...e})}):p(e)}}}]);