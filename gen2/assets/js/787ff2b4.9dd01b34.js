"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[2899],{28453:(e,n,a)=>{a.d(n,{R:()=>s,x:()=>c});var r=a(96540);const t={},i=r.createContext(t);function s(e){const n=r.useContext(i);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),r.createElement(i.Provider,{value:n},e.children)}},99903:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>c,default:()=>h,frontMatter:()=>s,metadata:()=>r,toc:()=>o});const r=JSON.parse('{"id":"client-sdk/python-sdk/streaming-example","title":"Streaming Example","description":"This example demonstrates how to programmatically control streaming on your Aria Gen2 device using the Python SDK. You\'ll learn how to start streaming, implement custom callbacks for real-time data processing, and optionally record streaming data to VRS files.","source":"@site/docs-ark/client-sdk/python-sdk/streaming-example.mdx","sourceDirName":"client-sdk/python-sdk","slug":"/client-sdk/python-sdk/streaming-example","permalink":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/streaming-example","draft":false,"unlisted":false,"editUrl":"https://www.internalfb.com/code/fbsource/arvr/projects/ariane/aria_research_kit/projectaria_tools/website/docs-ark/client-sdk/python-sdk/streaming-example.mdx","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4,"title":"Streaming Example"},"sidebar":"arkSidebar","previous":{"title":"Recording Example","permalink":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/recording-example"},"next":{"title":"Text-to-Speech Example","permalink":"/projectaria_tools/gen2/ark/client-sdk/python-sdk/text-to-speech-example"}}');var t=a(74848),i=a(28453);const s={sidebar_position:4,title:"Streaming Example"},c="Streaming Example",l={},o=[{value:"Quick Start",id:"quick-start",level:2},{value:"What This Example Does",id:"what-this-example-does",level:2},{value:"Code Walkthrough",id:"code-walkthrough",level:2},{value:"Step 1: Import Required Modules",id:"step-1-import-required-modules",level:3},{value:"Step 2: Parse Command-Line Arguments",id:"step-2-parse-command-line-arguments",level:3},{value:"Step 3: Connect to Device and Start Streaming",id:"step-3-connect-to-device-and-start-streaming",level:3},{value:"Step 4: Define Data Callbacks",id:"step-4-define-data-callbacks",level:3},{value:"Image Callback",id:"image-callback",level:4},{value:"Audio Callback",id:"audio-callback",level:4},{value:"IMU Callback",id:"imu-callback",level:4},{value:"Eye Gaze Callback",id:"eye-gaze-callback",level:4},{value:"Hand Tracking Callback",id:"hand-tracking-callback",level:4},{value:"VIO Callback",id:"vio-callback",level:4},{value:"Step 5: Set Up the Streaming Receiver",id:"step-5-set-up-the-streaming-receiver",level:3},{value:"Step 6: Main Function",id:"step-6-main-function",level:3},{value:"Complete Example Code",id:"complete-example-code",level:2},{value:"Usage Examples",id:"usage-examples",level:2},{value:"Basic Streaming with Console Output",id:"basic-streaming-with-console-output",level:3},{value:"Streaming and Recording to VRS",id:"streaming-and-recording-to-vrs",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"No Data in Callbacks",id:"no-data-in-callbacks",level:3},{value:"High Data Drop Rate",id:"high-data-drop-rate",level:3},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"streaming-example",children:"Streaming Example"})}),"\n",(0,t.jsx)(n.p,{children:"This example demonstrates how to programmatically control streaming on your Aria Gen2 device using the Python SDK. You'll learn how to start streaming, implement custom callbacks for real-time data processing, and optionally record streaming data to VRS files."}),"\n",(0,t.jsx)(n.h2,{id:"quick-start",children:"Quick Start"}),"\n",(0,t.jsx)(n.p,{children:"Run the streaming example script:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"python ~/Downloads/projectaria_client_sdk_samples_gen2/device_streaming.py\n"})}),"\n",(0,t.jsx)(n.p,{children:"To save streaming data to a VRS file:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"python ~/Downloads/projectaria_client_sdk_samples_gen2/device_streaming.py --record-to-vrs /path\n"})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"what-this-example-does",children:"What This Example Does"}),"\n",(0,t.jsx)(n.p,{children:"The script performs the following operations:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Connects to the device"}),"\n",(0,t.jsx)(n.li,{children:"Configures streaming settings (profile, interface)"}),"\n",(0,t.jsx)(n.li,{children:"Starts streaming on the device"}),"\n",(0,t.jsx)(n.li,{children:"Sets up a streaming receiver with custom callbacks"}),"\n",(0,t.jsx)(n.li,{children:"Processes real-time data from all sensors (cameras, IMU, audio, machine perception)"}),"\n",(0,t.jsx)(n.li,{children:"Optionally records streaming data to a VRS file"}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"code-walkthrough",children:"Code Walkthrough"}),"\n",(0,t.jsx)(n.h3,{id:"step-1-import-required-modules",children:"Step 1: Import Required Modules"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import argparse\nimport signal\nimport sys\nimport aria.sdk_gen2 as sdk_gen2\nimport aria.stream_receiver as receiver\nfrom projectaria_tools.core.mps import EyeGaze, hand_tracking\nfrom projectaria_tools.core.sensor_data import (\n    AudioData, AudioDataRecord, FrontendOutput, ImageData, ImageDataRecord, MotionData,\n)\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Key Modules:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"aria.sdk_gen2"}),": Main SDK for device control"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"aria.stream_receiver"}),": Receives and processes streaming data"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"projectaria_tools.core"}),": Data structures for sensor data and machine perception"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h3,{id:"step-2-parse-command-line-arguments",children:"Step 2: Parse Command-Line Arguments"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def parse_args() -> argparse.Namespace:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        "--record-to-vrs",\n        dest="record_to_vrs",\n        type=str,\n        default="",\n        required=False,\n        help="Output directory to save the received streaming into VRS",\n    )\n    return parser.parse_args()\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Available Options:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"--record-to-vrs"}),": Optional path to save streaming data as a VRS file"]}),"\n",(0,t.jsx)(n.li,{children:"If not specified, data is only processed in callbacks (not saved)"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Important:"})," Data drops can occur with poor streaming connections. The saved VRS file will reflect any data drops that occurred during streaming."]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h3,{id:"step-3-connect-to-device-and-start-streaming",children:"Step 3: Connect to Device and Start Streaming"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Create device client\ndevice_client = sdk_gen2.DeviceClient()\n\n# Establish connection to the device\nconfig = sdk_gen2.DeviceClientConfig()\ndevice_client.set_client_config(config)\ndevice = device_client.connect()\n\n# Set recording config with profile name\nstreaming_config = sdk_gen2.HttpStreamingConfig()\nstreaming_config.profile_name = "profile9"\ndevice.set_streaming_config(streaming_config)\n\n# Start and stop recording\ndevice.start_streaming()\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Streaming Configuration:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"profile_name"}),": Use ",(0,t.jsx)(n.code,{children:"mp_streaming_demo"})," for smooth visualization with all machine perception data"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"streaming_interface"}),": Current support:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"USB_NCM"}),": USB connection (default)"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h3,{id:"step-4-define-data-callbacks",children:"Step 4: Define Data Callbacks"}),"\n",(0,t.jsx)(n.p,{children:"Callbacks are functions that process data as it arrives from the device. Here's how to implement callbacks for each data type:"}),"\n",(0,t.jsx)(n.h4,{id:"image-callback",children:"Image Callback"}),"\n",(0,t.jsx)(n.p,{children:"Processes all camera streams: RGB (1x), SLAM (4x), and Eye Tracking (2x):"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def image_callback(image_data: ImageData, image_record: ImageDataRecord):\n    """Called for each image frame from any camera."""\n    image_array = image_data.to_numpy_array()\n    timestamp_ns = image_record.capture_timestamp_ns\n\n    print(f"Received image: shape={image_array.shape}, timestamp={timestamp_ns} ns")\n\n    # Example: Process the image\n    # - image_array is a numpy array you can process with OpenCV, PIL, etc.\n    # - image_record contains metadata like timestamp, camera ID, exposure\n'})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h4,{id:"audio-callback",children:"Audio Callback"}),"\n",(0,t.jsx)(n.p,{children:"Processes audio data from all 8 microphone channels:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def audio_callback(audio_data: AudioData, audio_record: AudioDataRecord, num_channels: int):\n    """Called for each audio data packet."""\n    num_samples = len(audio_data.data)\n    num_timestamps = len(audio_record.capture_timestamps_ns)\n\n    print(f"Received audio: samples={num_samples}, timestamps={num_timestamps}, channels={num_channels}")\n\n    # Example: Process audio\n    # - audio_data.data contains the raw audio samples\n    # - audio_record.capture_timestamps_ns contains per-sample timestamps\n'})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h4,{id:"imu-callback",children:"IMU Callback"}),"\n",(0,t.jsx)(n.p,{children:"Processes high-frequency IMU data from both IMU sensors at 800Hz:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def imu_callback(imu_data: MotionData, sensor_label: str):\n    """Called for each IMU data sample from imu-left or imu-right."""\n    accel = imu_data.accel_msec2  # Acceleration in m/s\xb2\n    gyro = imu_data.gyro_radsec   # Gyroscope in rad/s\n\n    print(f"Received {sensor_label}: accel={accel}, gyro={gyro}")\n\n    # Example: Process IMU data\n    # - Use for motion tracking.\n'})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h4,{id:"eye-gaze-callback",children:"Eye Gaze Callback"}),"\n",(0,t.jsx)(n.p,{children:"Processes eye tracking data with gaze direction and depth:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def eyegaze_callback(eyegaze_data: EyeGaze):\n    """Called for each eye gaze estimate."""\n    timestamp_sec = eyegaze_data.tracking_timestamp.total_seconds()\n    yaw_rad = eyegaze_data.yaw\n    pitch_rad = eyegaze_data.pitch\n    depth_m = eyegaze_data.depth\n\n    print(f"Eye Gaze: timestamp={timestamp_sec}s, yaw={yaw_rad:.3f}, pitch={pitch_rad:.3f}, depth={depth_m:.3f}m")\n\n    # Example: Use eye gaze data\n    # - Track where user is looking\n    # - Estimate focus depth\n    # - Build attention maps\n'})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h4,{id:"hand-tracking-callback",children:"Hand Tracking Callback"}),"\n",(0,t.jsx)(n.p,{children:"Processes hand pose estimates for both left and right hands:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def handtracking_callback(handtracking_data: hand_tracking.HandTrackingResult):\n    """Called for each hand tracking estimate."""\n    timestamp_sec = handtracking_data.tracking_timestamp.total_seconds()\n    print(f"Hand Tracking: timestamp={timestamp_sec}s")\n\n    # Process left hand\n    if handtracking_data.left_hand is not None:\n        left_hand = handtracking_data.left_hand\n        print(f"  Left hand confidence: {left_hand.confidence:.3f}")\n        print(f"  Left wrist position: {left_hand.get_wrist_position_device()}")\n        print(f"  Left palm position: {left_hand.get_palm_position_device()}")\n        if left_hand.wrist_and_palm_normal_device is not None:\n            normals = left_hand.wrist_and_palm_normal_device\n            print(f"  Left wrist normal: {normals.wrist_normal_device}")\n            print(f"  Left palm normal: {normals.palm_normal_device}")\n    else:\n        print("  Left hand: No data")\n\n\n    # Process right hand (similar to left hand)\n    if handtracking_data.right_hand is not None:\n        right_hand = handtracking_data.right_hand\n        print(f"  Right hand confidence: {right_hand.confidence:.3f}")\n        print(f"  Right wrist position: {right_hand.get_wrist_position_device()}")\n        print(f"  Right palm position: {right_hand.get_palm_position_device()}")\n        if right_hand.wrist_and_palm_normal_device is not None:\n            normals = right_hand.wrist_and_palm_normal_device\n            print(f"  Right wrist normal: {normals.wrist_normal_device}")\n            print(f"  Right palm normal: {normals.palm_normal_device}")\n    else:\n        print("  Right hand: No data")\n'})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h4,{id:"vio-callback",children:"VIO Callback"}),"\n",(0,t.jsx)(n.p,{children:"Processes Visual-Inertial Odometry data (6-DOF pose estimates):"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def vio_callback(vio_data: FrontendOutput):\n    """Called for each VIO pose estimate."""\n    timestamp_ns = vio_data.capture_timestamp_ns\n    rotation = vio_data.transform_odometry_bodyimu.rotation().log()\n    translation = vio_data.transform_odometry_bodyimu.translation()\n\n    print(f"VIO: timestamp={timestamp_ns}ns, rotation={rotation}, translation={translation}")\n\n    # Example: Use VIO data\n    # - Track device position and orientation\n    # - Build 3D maps\n    # - Enable AR applications\n'})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h3,{id:"step-5-set-up-the-streaming-receiver",children:"Step 5: Set Up the Streaming Receiver"}),"\n",(0,t.jsx)(n.p,{children:"The streaming receiver listens for data and dispatches it to your callbacks:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def setup_streaming_receiver(device, record_to_vrs):\n    """Configure and start the streaming receiver."""\n    # Configure server\n    config = sdk_gen2.HttpServerConfig()\n    config.address = "0.0.0.0"  # Listen on all interfaces\n    config.port = 6768           # Default streaming port\n\n    # Create receiver\n    stream_receiver = receiver.StreamReceiver()\n    stream_receiver.set_server_config(config)\n\n    # Optional: Record to VRS\n    if record_to_vrs != "":\n        stream_receiver.record_to_vrs(record_to_vrs)\n\n    # Register all callbacks\n    stream_receiver.register_slam_callback(image_callback)\n    stream_receiver.register_rgb_callback(image_callback)\n    stream_receiver.register_audio_callback(audio_callback)\n    stream_receiver.register_eye_gaze_callback(eyegaze_callback)\n    stream_receiver.register_hand_pose_callback(handtracking_callback)\n    stream_receiver.register_vio_callback(vio_callback)\n\n    # Start receiving data\n    stream_receiver.start_server()\n\n    return stream_receiver\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Important Configuration Notes:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Port 6768"}),": Ensure this port is open and not blocked by firewall"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"VPN"}),": Disable VPN to allow streaming data to be received"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"0.0.0.0"})}),": Listens on all network interfaces (required for device to connect)"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h3,{id:"step-6-main-function",children:"Step 6: Main Function"}),"\n",(0,t.jsx)(n.p,{children:"Tie everything together:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'if __name__ == "__main__":\n    args = parse_args()\n\n    # Connect and start streaming\n    device = device_streaming()\n\n    # Set up receiver with callbacks\n    stream_receiver = setup_streaming_receiver(device, args.record_to_vrs)\n\n    # Keep running until interrupted\n    print("Streaming... Press Ctrl+C to stop")\n    try:\n        signal.pause()  # Wait for interrupt signal\n    except KeyboardInterrupt:\n        print("\\nStopping streaming...")\n        device.stop_streaming()\n        print("Streaming stopped")\n'})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"complete-example-code",children:"Complete Example Code"}),"\n",(0,t.jsx)(n.p,{children:"Here's the full streaming script structure:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import argparse\nimport time\n\nimport aria.sdk_gen2 as sdk_gen2\nimport aria.stream_receiver as receiver\n\nfrom projectaria_tools.core.mps import EyeGaze, hand_tracking, OpenLoopTrajectoryPose\nfrom projectaria_tools.core.sensor_data import (\n    AudioData,\n    AudioDataRecord,\n    FrontendOutput,\n    ImageData,\n    ImageDataRecord,\n    MotionData,\n)\n\n# Set up the device client to initiate connection to the device\ndevice_client = sdk_gen2.DeviceClient()\n\n\ndef device_streaming():\n    # Set up the device client config to specify the device to be connected to e.g. device serial number.\n    # If nothing is specified, the first device in the list of connected devices will be connected to\n    config = sdk_gen2.DeviceClientConfig()\n    device_client.set_client_config(config)\n    device = device_client.connect()\n\n    # Set recording config with profile name\n    streaming_config = sdk_gen2.HttpStreamingConfig()\n    streaming_config.profile_name = "profile9"\n    device.set_streaming_config(streaming_config)\n\n    # Start and stop recording\n    device.start_streaming()\n    return device\n\n\ndef image_callback(image_data: ImageData, image_record: ImageDataRecord):\n    print(\n        f"Received image data of size {image_data.to_numpy_array().shape} with timestamp {image_record.capture_timestamp_ns} ns"\n    )\n\n\ndef audio_callback(\n    audio_data: AudioData, audio_record: AudioDataRecord, num_channels: int\n):\n    print(\n        f"Received audio data with {len(audio_data.data)} samples and {len(audio_record.capture_timestamps_ns)} timestamps and num channels {num_channels}"\n    )\n\n\ndef imu_callback(imu_data: MotionData, sensor_label: str):\n    print(\n        f"Received {sensor_label} accel data {imu_data.accel_msec2} and gyro {imu_data.gyro_radsec}"\n    )\n\n\ndef eyegaze_callback(eyegaze_data: EyeGaze):\n    print(\n        f"Received EyeGaze data at timestamp {eyegaze_data.tracking_timestamp.total_seconds()} sec "\n        f"with yaw={eyegaze_data.yaw:.3f} rad, pitch={eyegaze_data.pitch:.3f} rad, "\n        f"depth={eyegaze_data.depth:.3f} m"\n    )\n\n\ndef handtracking_callback(handtracking_data: hand_tracking.HandTrackingResult):\n    print(\n        f"Received HandTracking data at timestamp {handtracking_data.tracking_timestamp.total_seconds()} sec"\n    )\n\n    # Check left hand data\n    if handtracking_data.left_hand is not None:\n        left_hand = handtracking_data.left_hand\n        print(f"  Left hand confidence: {left_hand.confidence:.3f}")\n        print(f"  Left wrist position: {left_hand.get_wrist_position_device()}")\n        print(f"  Left palm position: {left_hand.get_palm_position_device()}")\n        if left_hand.wrist_and_palm_normal_device is not None:\n            normals = left_hand.wrist_and_palm_normal_device\n            print(f"  Left wrist normal: {normals.wrist_normal_device}")\n            print(f"  Left palm normal: {normals.palm_normal_device}")\n    else:\n        print("  Left hand: No data")\n\n    # Check right hand data\n    if handtracking_data.right_hand is not None:\n        right_hand = handtracking_data.right_hand\n        print(f"  Right hand confidence: {right_hand.confidence:.3f}")\n        print(f"  Right wrist position: {right_hand.get_wrist_position_device()}")\n        print(f"  Right palm position: {right_hand.get_palm_position_device()}")\n        if right_hand.wrist_and_palm_normal_device is not None:\n            normals = right_hand.wrist_and_palm_normal_device\n            print(f"  Right wrist normal: {normals.wrist_normal_device}")\n            print(f"  Right palm normal: {normals.palm_normal_device}")\n    else:\n        print("  Right hand: No data")\n\n\ndef vio_callback(vio_data: FrontendOutput):\n    print(\n        f"Received VIO data at timestamp {vio_data.capture_timestamp_ns} with transform_odometry_bodyimu: {vio_data.transform_odometry_bodyimu.rotation().log()} and {vio_data.transform_odometry_bodyimu.translation()} ns"\n    )\n\n\ndef calib_callback(calib_json_str: str):\n    print(f"Received calibration: {calib_json_str}")\n\n\ndef setup_streaming_receiver(device, record_to_vrs):\n    # setup the server to receive streaming data from the device\n    # IP address : 0.0.0.0 means that the server is listening on all available interfaces\n    # Port : 6768 is the port number that the server is listening on\n    config = sdk_gen2.HttpServerConfig()\n    config.address = "0.0.0.0"\n    config.port = 6768\n\n    # setup the receiver\n    stream_receiver = receiver.StreamReceiver()\n    stream_receiver.set_server_config(config)\n    if record_to_vrs != "":\n        stream_receiver.record_to_vrs(record_to_vrs)\n\n    # register callbacks for each type of data\n    stream_receiver.register_slam_callback(image_callback)\n    stream_receiver.register_rgb_callback(image_callback)\n    stream_receiver.register_audio_callback(audio_callback)\n    stream_receiver.register_eye_gaze_callback(eyegaze_callback)\n    stream_receiver.register_hand_pose_callback(handtracking_callback)\n    stream_receiver.register_vio_callback(vio_callback)\n\n    # start the server\n    stream_receiver.start_server()\n\n    time.sleep(10)\n\n    # stop streaming and terminate the server\n    device.stop_streaming()\n\n    time.sleep(2)\n\n\ndef parse_args() -> argparse.Namespace:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        "--record-to-vrs",\n        dest="record_to_vrs",\n        type=str,\n        default="",\n        required=False,\n        help="Output directory to save the received streaming into VRS",\n    )\n\n    return parser.parse_args()\n\n\nif __name__ == "__main__":\n    args = parse_args()\n    # setup device to start streaming\n    device = device_streaming()\n\n    # setup streaming receiver to receive streaming data with callbacks\n    setup_streaming_receiver(device, args.record_to_vrs)\n'})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"usage-examples",children:"Usage Examples"}),"\n",(0,t.jsx)(n.h3,{id:"basic-streaming-with-console-output",children:"Basic Streaming with Console Output"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"python ~/Downloads/projectaria_client_sdk_samples_gen2/device_streaming.py\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"What happens:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Connects to device and starts streaming"}),"\n",(0,t.jsx)(n.li,{children:"Prints real-time data from all sensors to console"}),"\n",(0,t.jsx)(n.li,{children:"Data is NOT saved"}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h3,{id:"streaming-and-recording-to-vrs",children:"Streaming and Recording to VRS"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"python ~/Downloads/projectaria_client_sdk_samples_gen2/device_streaming.py \\\n    --record-to-vrs ~/Downloads/streaming_capture.vrs\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"What happens:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Streams data with real-time callbacks"}),"\n",(0,t.jsx)(n.li,{children:"Simultaneously records all data to VRS file"}),"\n",(0,t.jsxs)(n.li,{children:["VRS file can be played back later with ",(0,t.jsx)(n.code,{children:"aria_rerun_viewer"})]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,t.jsx)(n.h3,{id:"no-data-in-callbacks",children:"No Data in Callbacks"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Problem:"})," Streaming starts but callbacks are never called."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Solutions:"})}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Check port 6768 is open:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# On Linux/macOS\nsudo lsof -i :6768\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Disable VPN:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"VPNs block streaming data"}),"\n",(0,t.jsx)(n.li,{children:"Disconnect from VPN/Lighthouse"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Check firewall:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Ensure firewall allows port 6768"}),"\n",(0,t.jsx)(n.li,{children:"Add exception for Python script"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Verify server address:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Must be ",(0,t.jsx)(n.code,{children:"0.0.0.0"})," to listen on all interfaces"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Run aria_doctor:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"aria_doctor\n"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h3,{id:"high-data-drop-rate",children:"High Data Drop Rate"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Problem:"})," VRS file shows many dropped frames."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Solutions:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Close other applications"})," consuming bandwidth"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Check system resources"})," (CPU, memory)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Reduce callback processing"})," time"]}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.strong,{children:"Increase message queue size"})}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Learn about ",(0,t.jsx)(n.a,{href:"/ark/client-sdk/python-sdk/text-to-speech-example",children:"text-to-speech commands"})]}),"\n",(0,t.jsxs)(n.li,{children:["Review ",(0,t.jsx)(n.a,{href:"/ark/client-sdk/python-sdk/python-interface",children:"all Python SDK examples"})]}),"\n",(0,t.jsxs)(n.li,{children:["Explore the ",(0,t.jsx)(n.a,{href:"/ark/client-sdk/streaming",children:"Streaming Guide"})," for CLI streaming"]}),"\n",(0,t.jsxs)(n.li,{children:["Check ",(0,t.jsx)(n.a,{href:"/ark/support/sdk",children:"Troubleshooting"})," for common issues"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}}}]);