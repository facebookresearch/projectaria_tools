"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[1463],{28453:(e,t,a)=>{a.d(t,{R:()=>i,x:()=>o});var n=a(96540);const s={},r=n.createContext(s);function i(e){const t=n.useContext(r);return n.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),n.createElement(r.Provider,{value:t},e.children)}},48174:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>n,toc:()=>c});const n=JSON.parse('{"id":"projectariatools/pythontutorials/queue","title":"Access Multi-Sensor Data Sequentially","description":"<TutorialButtons","source":"@site/docs-research-tools/projectariatools/pythontutorials/queue.mdx","sourceDirName":"projectariatools/pythontutorials","slug":"/projectariatools/pythontutorials/queue","permalink":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/queue","draft":false,"unlisted":false,"editUrl":"https://www.internalfb.com/code/fbsource/arvr/projects/ariane/aria_research_kit/projectaria_tools/website/docs-research-tools/projectariatools/pythontutorials/queue.mdx","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"Access Multi-Sensor Data Sequentially"},"sidebar":"researchToolsSidebar","previous":{"title":"Using Device Calibration Data from VRS","permalink":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/calibration"},"next":{"title":"Using On-Device Eye-tracking and Hand-tracking","permalink":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/eyetracking-handtracking"}}');var s=a(74848),r=a(28453),i=a(69470);const o={sidebar_position:3,title:"Access Multi-Sensor Data Sequentially"},l="Tutorial 3: Sequential Accessing Multi-sensor Data",d={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Basic Sequential Data Access API",id:"basic-sequential-data-access-api",level:2},{value:"Understanding the Unified SensorData Interface",id:"understanding-the-unified-sensordata-interface",level:2},{value:"Customizing Data Access with DeliverQueuedOptions",id:"customizing-data-access-with-deliverqueuedoptions",level:2}];function _(e){const t={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.header,{children:(0,s.jsx)(t.h1,{id:"tutorial-3-sequential-accessing-multi-sensor-data",children:"Tutorial 3: Sequential Accessing Multi-sensor Data"})}),"\n",(0,s.jsx)(i.A,{notebookUrl:"https://github.com/facebookresearch/projectaria_tools/blob/main/examples/Gen2/python_notebooks/Tutorial_3_sequential_access_multi_sensor_data.ipynb",colabDisabled:!0}),"\n",(0,s.jsx)(t.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsxs)(t.p,{children:["This tutorial shows how to use the unified queued API in ",(0,s.jsx)(t.code,{children:"vrs_data_provider"})," to efficiently ",(0,s.jsx)(t.strong,{children:"stream"})," multi-sensor data from Aria VRS files."]}),"\n",(0,s.jsx)(t.p,{children:"We will learn how to use the unified SensorData interface, access time-ordered sensor data queues, and customize stream control and time windowing for efficient processing."}),"\n",(0,s.jsx)(t.p,{children:"In this tutorial, we will learn:"}),"\n",(0,s.jsxs)(t.ol,{children:["\n",(0,s.jsx)(t.li,{children:"Use the basic queued API to iterate through all sensor data."}),"\n",(0,s.jsx)(t.li,{children:"Explore the unified SensorData interface"}),"\n",(0,s.jsx)(t.li,{children:"Customize stream selection and time windowing in this queued API."}),"\n",(0,s.jsx)(t.li,{children:"Apply frame rate subsampling for efficient processing"}),"\n"]}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.strong,{children:"Prerequisites"})}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"Complete Tutorial 1 (VrsDataProvider Basics) to understand basic data provider concepts"}),"\n",(0,s.jsxs)(t.li,{children:["Download Aria Gen2 sample data from ",(0,s.jsx)(t.a,{href:"https://www.projectaria.com/async/sample/download/?bucket=core&filename=aria_gen2_sample_data_1.vrs",children:"link"})]}),"\n"]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Note on Visualization"}),"\nIf visualization window is not showing up, this is due to ",(0,s.jsx)(t.code,{children:"Rerun"})," lib's caching issue. Just rerun the specific code cell."]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'from projectaria_tools.core import data_provider\n\n# Load local VRS file\nvrs_file_path = "path/to/your/recording.vrs"\nvrs_data_provider = data_provider.create_vrs_data_provider(vrs_file_path)\n'})}),"\n",(0,s.jsx)(t.h2,{id:"basic-sequential-data-access-api",children:"Basic Sequential Data Access API"}),"\n",(0,s.jsxs)(t.p,{children:["The ",(0,s.jsx)(t.code,{children:"deliver_queued_sensor_data()"})," method in ",(0,s.jsx)(t.code,{children:"vrs_data_provider"})," provides a ",(0,s.jsx)(t.strong,{children:"unified"})," way to iterate through ",(0,s.jsx)(t.strong,{children:"all sensor data"})," in ",(0,s.jsx)(t.strong,{children:"timestamp order"}),". This is the primary API for sequential access to multi-sensor data."]}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.strong,{children:"Key features of the queued API:"})}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"Returns data from ALL streams in the VRS file"}),"\n",(0,s.jsx)(t.li,{children:"Orders data by device timestamp (chronological order)"}),"\n",(0,s.jsx)(t.li,{children:"Customizable via stream selection, sub-sampling each stream, etc."}),"\n",(0,s.jsx)(t.li,{children:"The returned data can be further converted to each sensor type via a unified interface."}),"\n"]}),"\n",(0,s.jsx)(t.p,{children:"Here is a simple example to query the first K data samples from the VRS, and inspect each data sample's properties:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'print("\\n=== Basic Sequential Data Access ===")\nprint("Processing all sensor data in timestamp order...")\n\n# Variables to store how many data samples has arrived for each sensor stream\ndata_count = 0\nper_stream_data_counts = {}\ntotal_num_samples = 5000\n\n# Call deliver queued sensor data API to obtain a "streamed" data, in sorted timestamp order\n# The iterator would return a unified SensorData instance\nprint(f"Start inspecting the first {total_num_samples} data samples in the VRS")\nfor sensor_data in vrs_data_provider.deliver_queued_sensor_data():\n    # Which stream does this sensor data belong to\n    stream_id = sensor_data.stream_id()\n    stream_label = vrs_data_provider.get_label_from_stream_id(stream_id)\n\n    # Aggregate data count for this stream\n    data_count += 1\n    if stream_label not in per_stream_data_counts:\n        per_stream_data_counts[stream_label] = 0\n    per_stream_data_counts[stream_label] += 1\n\n    # Limit output for demonstration\n    if data_count >= total_num_samples:\n        print("Stopping after 5000 samples for demonstration...")\n        break\n\n\n# Print data counts for each sensor stream\nprint(f"\\nTotal processed: {data_count} sensor data samples")\nprint("Data count per stream:")\nfor stream_label, count in per_stream_data_counts.items():\n    print(f"\\t{stream_label}: {count}")\n'})}),"\n",(0,s.jsx)(t.h2,{id:"understanding-the-unified-sensordata-interface",children:"Understanding the Unified SensorData Interface"}),"\n",(0,s.jsxs)(t.p,{children:["The queued API returns data using a unified ",(0,s.jsx)(t.code,{children:"SensorData"})," interface. This allows you to handle different types of sensor data (images, IMU, audio, etc.) in a ",(0,s.jsx)(t.strong,{children:"consistent way"}),", regardless of the sensor type."]}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.strong,{children:"Each SensorData object provides:"})}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"Stream ID and stream label for identification"}),"\n",(0,s.jsx)(t.li,{children:"Sensor data type (IMAGE, IMU, AUDIO, etc.)"}),"\n",(0,s.jsx)(t.li,{children:"Timestamps in different time domains"}),"\n",(0,s.jsx)(t.li,{children:"Access to the actual sensor data (images, IMU, audio, etc.)"}),"\n"]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'from projectaria_tools.core.sensor_data import SensorDataType, TimeDomain, TimeQueryOptions\n\nprint("\\n=== Exploring the SensorData Interface ===")\n\n# Get a few samples to examine their properties\ndata_count = 0\nfor sensor_data in vrs_data_provider.deliver_queued_sensor_data():\n    if data_count >= 5:\n        break\n\n    # Inspect where this sensor data come from, and what is its data type\n    stream_id = sensor_data.stream_id()\n    stream_label = vrs_data_provider.get_label_from_stream_id(stream_id)\n    data_type = sensor_data.sensor_data_type()\n\n    # Inspect the device timestamp of this sensor data\n    device_time = sensor_data.get_time_ns(TimeDomain.DEVICE_TIME)\n\n    print(f"\\nSample {data_count + 1}:")\n    print(f"  Stream: {stream_label} (Stream ID: {stream_id})")\n    print(f"  Type: {data_type}")\n    print(f"  Device Time: {device_time/1e9:.6f}s")\n\n    # Map sensor data to its specific type, and inspect its actual data content.\n    # Here we use image and IMU as an example\n    if data_type == SensorDataType.IMAGE:\n        image_data = sensor_data.image_data_and_record()[0]\n        print(f"  Image size: {image_data.get_width()} x {image_data.get_height()}")\n        print(f"  Pixel format: {image_data.get_pixel_format()}")\n\n    elif data_type == SensorDataType.IMU:\n        imu_data = sensor_data.imu_data()\n        accel = imu_data.accel_msec2\n        gyro = imu_data.gyro_radsec\n        print(f"  IMU Accel: [{accel[0]:.3f}, {accel[1]:.3f}, {accel[2]:.3f}] m/s\xb2")\n        print(f"  IMU Gyro: [{gyro[0]:.3f}, {gyro[1]:.3f}, {gyro[2]:.3f}] rad/s")\n\n    data_count += 1\n'})}),"\n",(0,s.jsx)(t.h2,{id:"customizing-data-access-with-deliverqueuedoptions",children:"Customizing Data Access with DeliverQueuedOptions"}),"\n",(0,s.jsxs)(t.p,{children:["The real power of the queued API comes from ",(0,s.jsx)(t.strong,{children:"customization options"}),". The ",(0,s.jsx)(t.code,{children:"DeliverQueuedOptions"})," class allows you to:"]}),"\n",(0,s.jsxs)(t.ol,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Apply time windowing"})," - Process only specific time ranges."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Select specific streams"})," - Choose which sensors to include."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Subsample data"})," - Reduce frame rates for specific streams."]}),"\n"]}),"\n",(0,s.jsx)(t.p,{children:"These all provide flexible ways to control the sensor queue, to focus on specific time periods or sensor modalities, or customize data rates for different analysis needs."}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'import rerun as rr\n\nprint("\\n=== Customizing Data Access with DeliverQueuedOptions ===")\n\ncustomized_deliver_options = vrs_data_provider.get_default_deliver_queued_options()\n\n# -----------------\n# 1. Stream selection feature - only select RGB, 1 SLAM camera, and 1 ET camera data.\n# -----------------\nrgb_to_select = vrs_data_provider.get_stream_id_from_label("camera-rgb")\nslam_to_select = vrs_data_provider.get_stream_id_from_label("slam-front-right")\net_to_select = vrs_data_provider.get_stream_id_from_label("camera-et-right")\n\n# First deactivate all streams, then just add back selected streams\ncustomized_deliver_options.deactivate_stream_all()\nfor selected_stream_id in [rgb_to_select,slam_to_select,et_to_select]:\n    customized_deliver_options.activate_stream(selected_stream_id)\n\n# -----------------\n# 2. Time windowing feature - Skip first 2 seconds, and play for 3 seconds, if possible\n# -----------------\ntotal_length_ns = vrs_data_provider.get_last_time_ns_all_streams(TimeDomain.DEVICE_TIME) - vrs_data_provider.get_first_time_ns_all_streams(TimeDomain.DEVICE_TIME)\nskip_begin_ns = int(2 * 1e9) # 2 seconds\nduration_ns = int(3 * 1e9) # 3 seconds\nskip_end_ns = max(total_length_ns - skip_begin_ns - duration_ns, 0)\ncustomized_deliver_options.set_truncate_first_device_time_ns(skip_begin_ns)\ncustomized_deliver_options.set_truncate_last_device_time_ns(skip_end_ns)\n\n# -----------------\n# 3. Per-stream sub-sampling feature - subsample slam camera at rate of 3\n# -----------------\nslam_subsample_rate = 3\ncustomized_deliver_options.set_subsample_rate(stream_id = slam_to_select, rate = slam_subsample_rate)\n\n# -----------------\n# 4. Deliver customized data queue, and visualize\n# -----------------\nprint(f"Start visualizing customized sensor data queue")\n\nrr.init("rerun_viz_customized_sensor_data_queue")\n\nfor sensor_data in vrs_data_provider.deliver_queued_sensor_data(customized_deliver_options):\n    stream_id = sensor_data.stream_id()\n    stream_label = vrs_data_provider.get_label_from_stream_id(stream_id)\n    device_time_ns = sensor_data.get_time_ns(TimeDomain.DEVICE_TIME)\n\n    image_data_and_record = sensor_data.image_data_and_record()\n\n    # Visualize\n    rr.set_time_nanos("device_time", device_time_ns)\n    rr.log(stream_label, rr.Image(image_data_and_record[0].to_numpy_array()))\n\nrr.notebook_show()\n'})})]})}function u(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(_,{...e})}):_(e)}},69470:(e,t,a)=>{a.d(t,{A:()=>s});a(96540);var n=a(74848);const s=({notebookUrl:e,colabDisabled:t=!1})=>(0,n.jsxs)("div",{style:{display:"flex",gap:"10px",marginBottom:"20px",flexWrap:"wrap"},children:[(0,n.jsxs)("a",{href:e,target:"_blank",rel:"noopener noreferrer",style:{display:"inline-flex",alignItems:"center",padding:"8px 16px",backgroundColor:"#f6f8fa",border:"1px solid #d0d7de",borderRadius:"6px",textDecoration:"none",color:"#24292f",fontSize:"14px",fontWeight:"500",transition:"background-color 0.2s"},onMouseEnter:e=>{e.target.style.backgroundColor="#f3f4f6"},onMouseLeave:e=>{e.target.style.backgroundColor="#f6f8fa"},children:[(0,n.jsx)("svg",{style:{marginRight:"8px",width:"16px",height:"16px"},fill:"currentColor",viewBox:"0 0 16 16",children:(0,n.jsx)("path",{d:"M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z"})}),"View Notebook on GitHub"]}),(0,n.jsxs)("button",{onClick:()=>{alert("Google Colab integration is not available yet. Please download the notebook and run it locally.")},style:{display:"inline-flex",alignItems:"center",padding:"8px 16px",backgroundColor:t?"#f6f8fa":"#f9ab00",border:"1px solid #d0d7de",borderRadius:"6px",color:t?"#656d76":"#fff",fontSize:"14px",fontWeight:"500",cursor:t?"not-allowed":"pointer",transition:"background-color 0.2s"},onMouseEnter:e=>{t||(e.target.style.backgroundColor="#e8a500")},onMouseLeave:e=>{t||(e.target.style.backgroundColor="#f9ab00")},children:[(0,n.jsx)("svg",{style:{marginRight:"8px",width:"16px",height:"16px"},fill:"currentColor",viewBox:"0 0 24 24",children:(0,n.jsx)("path",{d:"M16.9414 4.9757a7.033 7.033 0 0 0-4.9308 2.0324 7.033 7.033 0 0 0-.1232 9.8068l2.395-2.395a3.6455 3.6455 0 0 1 5.1497-5.1478l2.397-2.3989a7.033 7.033 0 0 0-4.8877-1.9175zM7.07 4.9855a7.033 7.033 0 0 0-4.8878 1.9175l2.395 2.3989a3.6434 3.6434 0 0 1 5.1497 5.1478l2.395 2.395a7.033 7.033 0 0 0-.1232-9.8068A7.033 7.033 0 0 0 7.07 4.9855zm15.0191 2.5923l-2.395 2.395a3.6455 3.6455 0 0 1-5.1497 5.1497l-2.395 2.395a7.033 7.033 0 0 0 9.9397-.123z"})}),t?"Colab (Coming Soon)":"Run in Google Colab"]})]})}}]);