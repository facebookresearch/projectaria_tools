"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[8802],{49787(e,n,i){i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"osi","title":"Open Science Initiative (OSI)","description":"The Open Science Initiative (OSI) is Meta\'s commitment to advancing egocentric AI research through open collaboration. We provide researchers with access to cutting-edge tools, datasets, and machine learning models built on data from Aria glasses. OSI enables you to analyze, visualize, and build upon rich multimodal egocentric data without needing your own Aria device.","source":"@site/docs-research-tools/osi.mdx","sourceDirName":".","slug":"/","permalink":"/projectaria_tools/gen2/research-tools/","draft":false,"unlisted":false,"editUrl":"https://www.internalfb.com/code/fbsource/arvr/projects/ariane/aria_research_kit/projectaria_tools/website/docs-research-tools/osi.mdx","tags":[],"version":"current","sidebarPosition":0,"frontMatter":{"sidebar_position":0,"title":"Open Science Initiative (OSI)","slug":"/"},"sidebar":"researchToolsSidebar","next":{"title":"Project Aria Tools Overview","permalink":"/projectaria_tools/gen2/research-tools/projectariatools/overview"}}');var r=i(74848),t=i(28453);i(98180),i(67581);const a={sidebar_position:0,title:"Open Science Initiative (OSI)",slug:"/"},o="Welcome to the Open Science Initiative",l={},c=[{value:"What is OSI?",id:"what-is-osi",level:2},{value:"\ud83d\udcca Public Datasets",id:"-public-datasets",level:3},{value:"\ud83d\udee0\ufe0f <code>projectaria-tools</code>",id:"\ufe0f-projectaria-tools",level:3},{value:"\ud83e\udd16 Open Models",id:"-open-models",level:3},{value:"What You Can Do with OSI",id:"what-you-can-do-with-osi",level:2},{value:"\ud83d\udce5 Access Research Data",id:"-access-research-data",level:3},{value:"\ud83d\udd2c Analyze Egocentric Data",id:"-analyze-egocentric-data",level:3},{value:"\ud83e\uddea Build and Train Models",id:"-build-and-train-models",level:3},{value:"\ud83d\udcca Benchmark Your Algorithms",id:"-benchmark-your-algorithms",level:3},{value:"Getting Started",id:"getting-started",level:2},{value:"For Dataset Users",id:"for-dataset-users",level:3},{value:"For Tool Users",id:"for-tool-users",level:3},{value:"For Algorithm Developers",id:"for-algorithm-developers",level:3},{value:"For First-Time Users",id:"for-first-time-users",level:3},{value:"Research Impact",id:"research-impact",level:2},{value:"Community &amp; Support",id:"community--support",level:2},{value:"Citation",id:"citation",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"welcome-to-the-open-science-initiative",children:"Welcome to the Open Science Initiative"})}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.strong,{children:"Open Science Initiative (OSI)"})," is Meta's commitment to advancing egocentric AI research through open collaboration. We provide researchers with access to cutting-edge tools, datasets, and machine learning models built on data from Aria glasses. OSI enables you to analyze, visualize, and build upon rich multimodal egocentric data without needing your own Aria device."]}),"\n",(0,r.jsx)(n.h2,{id:"what-is-osi",children:"What is OSI?"}),"\n",(0,r.jsx)(n.p,{children:"OSI makes egocentric AI research accessible by open-sourcing three key components:"}),"\n",(0,r.jsx)(n.h3,{id:"-public-datasets",children:"\ud83d\udcca Public Datasets"}),"\n",(0,r.jsx)(n.p,{children:"High-quality, multimodal egocentric datasets collected with Aria glasses, featuring:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Raw sensor streams"})," from multiple synchronized cameras, IMUs, audio, and more"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"On-device machine perception"})," outputs (VIO, eye gaze, hand tracking)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Offline machine perception annotations"})," from cloud-based processing"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Rich semantic annotations"})," from state-of-the-art perception algorithms"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Our datasets capture real-world scenarios with time-synchronized multi-participant recordings, enabling research in computer vision, multimodal learning, robotics, and contextual AI."}),"\n",(0,r.jsxs)(n.h3,{id:"\ufe0f-projectaria-tools",children:["\ud83d\udee0\ufe0f ",(0,r.jsx)(n.code,{children:"projectaria-tools"})]}),"\n",(0,r.jsx)(n.p,{children:"A comprehensive Python/C++ library for working with Aria data:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Load and visualize VRS (Video Recording Storage) files"}),"\n",(0,r.jsx)(n.li,{children:"Access device calibration and sensor data"}),"\n",(0,r.jsx)(n.li,{children:"Work with machine perception outputs (VIO, eye gaze, hand tracking)"}),"\n",(0,r.jsx)(n.li,{children:"Process and analyze multimodal sensor streams"}),"\n",(0,r.jsx)(n.li,{children:"Export data to standard formats"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"-open-models",children:"\ud83e\udd16 Open Models"}),"\n",(0,r.jsx)(n.p,{children:"Pre-trained machine learning models and algorithms for egocentric perception:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Coming soon..."}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"what-you-can-do-with-osi",children:"What You Can Do with OSI"}),"\n",(0,r.jsx)(n.h3,{id:"-access-research-data",children:"\ud83d\udce5 Access Research Data"}),"\n",(0,r.jsx)(n.p,{children:"Download and explore publicly available egocentric datasets:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Multi-sensor time-synchronized recordings"}),"\n",(0,r.jsx)(n.li,{children:"Ground truth annotations from advanced perception algorithms"}),"\n",(0,r.jsx)(n.li,{children:"Pre-processed machine perception outputs"}),"\n",(0,r.jsx)(n.li,{children:"Calibration data for all sensors"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"-analyze-egocentric-data",children:"\ud83d\udd2c Analyze Egocentric Data"}),"\n",(0,r.jsx)(n.p,{children:"Use Project Aria Tools to:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Load and visualize VRS recordings in Python"}),"\n",(0,r.jsx)(n.li,{children:"Access raw sensor data (RGB, CV cameras, IMU, audio, etc.)"}),"\n",(0,r.jsx)(n.li,{children:"Work with device calibration and coordinate transformations"}),"\n",(0,r.jsx)(n.li,{children:"Query time-synchronized sensor streams efficiently"}),"\n",(0,r.jsx)(n.li,{children:"Visualize 3D trajectories and point clouds with Rerun"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"-build-and-train-models",children:"\ud83e\uddea Build and Train Models"}),"\n",(0,r.jsx)(n.p,{children:"Leverage open datasets and tools to:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Train computer vision models on egocentric data"}),"\n",(0,r.jsx)(n.li,{children:"Develop multimodal perception algorithms"}),"\n",(0,r.jsx)(n.li,{children:"Research human attention and gaze patterns"}),"\n",(0,r.jsx)(n.li,{children:"Study hand-object interactions"}),"\n",(0,r.jsx)(n.li,{children:"Explore social dynamics in multi-participant scenarios"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"-benchmark-your-algorithms",children:"\ud83d\udcca Benchmark Your Algorithms"}),"\n",(0,r.jsx)(n.p,{children:"Compare your methods against:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Baseline perception algorithms"}),"\n",(0,r.jsx)(n.li,{children:"Pre-trained open models"}),"\n",(0,r.jsx)(n.li,{children:"Published research results"}),"\n",(0,r.jsx)(n.li,{children:"State-of-the-art egocentric AI systems"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"getting-started",children:"Getting Started"}),"\n",(0,r.jsx)(n.p,{children:"Choose your starting point based on your research goals:"}),"\n",(0,r.jsx)(n.h3,{id:"for-dataset-users",children:"For Dataset Users"}),"\n",(0,r.jsxs)(n.p,{children:["Begin with the ",(0,r.jsx)(n.a,{href:"/research-tools/dataset/pilot/content",children:"Aria Gen 2 Pilot Dataset"})," to explore available data. Then follow the ",(0,r.jsx)(n.a,{href:"/research-tools/dataset/pilot/download",children:"Download Guide"})," to access the dataset and start your research."]}),"\n",(0,r.jsx)(n.h3,{id:"for-tool-users",children:"For Tool Users"}),"\n",(0,r.jsxs)(n.p,{children:["Install ",(0,r.jsx)(n.a,{href:"/research-tools/projectariatools/installation",children:"Project Aria Tools"})," to work with Aria data. The library provides Python APIs and tutorials for loading, visualizing, and processing VRS files."]}),"\n",(0,r.jsx)(n.h3,{id:"for-algorithm-developers",children:"For Algorithm Developers"}),"\n",(0,r.jsx)(n.p,{children:"Explore the Open Models section to access pre-trained models and algorithms. Use them as baselines or building blocks for your own research."}),"\n",(0,r.jsx)(n.h3,{id:"for-first-time-users",children:"For First-Time Users"}),"\n",(0,r.jsxs)(n.p,{children:["Start with the ",(0,r.jsx)(n.a,{href:"/research-tools/projectariatools/pythontutorials/dataprovider",children:"Python Tutorials"})," to learn the basics:"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"VrsDataProvider Basics"})," - Load and access Aria data"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Device Calibration"})," - Work with sensor calibration"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Queued Sensor Data"})," - Efficiently stream multi-sensor data"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Eye Tracking & Hand Tracking"})," - Access on-device perception outputs"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"MPS Data Loading"})," - Visualize machine perception results"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"research-impact",children:"Research Impact"}),"\n",(0,r.jsx)(n.p,{children:"OSI enables research in:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Egocentric Vision"}),": First-person computer vision and scene understanding"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Multimodal AI"}),": Fusion of vision, audio, IMU, and physiological signals"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Human-AI Interaction"}),": Gaze-based interfaces and attention modeling"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Robotics"}),": Manipulation learning from human demonstrations"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Social Computing"}),": Multi-participant interaction analysis"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Contextual AI"}),": Understanding user context and intent"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Augmented Reality"}),": Spatial computing and scene reconstruction"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"community--support",children:"Community & Support"}),"\n",(0,r.jsx)(n.p,{children:"Join a growing community of researchers working with Aria data:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Access open-source code and tutorials"}),"\n",(0,r.jsx)(n.li,{children:"Contribute to Project Aria Tools development"}),"\n",(0,r.jsx)(n.li,{children:"Share your research and findings"}),"\n",(0,r.jsx)(n.li,{children:"Collaborate on new datasets and models"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"citation",children:"Citation"}),"\n",(0,r.jsx)(n.p,{children:"If you use OSI datasets or tools in your research, please cite the relevant papers and acknowledge the Project Aria platform."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Ready to explore?"})," Choose a starting point above to begin your journey with the Open Science Initiative."]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},28453(e,n,i){i.d(n,{R:()=>a,x:()=>o});var s=i(96540);const r={},t=s.createContext(r);function a(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);