"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[6983],{81529(e,n,t){t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>l,default:()=>_,frontMatter:()=>a,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"dataset/pilot/tutorials/mps_loading","title":"Aria Gen2 Pilot Dataset Tutorial - MPS Data Loading","description":"<TutorialButtons","source":"@site/docs-research-tools/dataset/pilot/tutorials/mps_loading.mdx","sourceDirName":"dataset/pilot/tutorials","slug":"/dataset/pilot/tutorials/mps_loading","permalink":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/mps_loading","draft":false,"unlisted":false,"editUrl":"https://www.internalfb.com/code/fbsource/arvr/projects/ariane/aria_research_kit/projectaria_tools/website/docs-research-tools/dataset/pilot/tutorials/mps_loading.mdx","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Aria Gen2 Pilot Dataset Tutorial - MPS Data Loading"},"sidebar":"researchToolsSidebar","previous":{"title":"Aria Gen2 Pilot Dataset Tutorial - VRS Data Loading","permalink":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/vrs_loading"},"next":{"title":"Aria Gen2 Pilot Dataset Tutorial - Algorithm Data Loading","permalink":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/algorithm_loading"}}');var o=t(74848),s=t(28453),r=t(69470);const a={sidebar_position:1,title:"Aria Gen2 Pilot Dataset Tutorial - MPS Data Loading"},l=void 0,d={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Dataset Structure",id:"dataset-structure",level:2},{value:"Initialize Data Provider",id:"initialize-data-provider",level:2},{value:"MPS - SLAM",id:"mps---slam",level:2},{value:"[MPS - SLAM] Output Files",id:"mps---slam-output-files",level:3},{value:"[MPS - SLAM] Semi-dense Point Cloud and Observations",id:"mps---slam-semi-dense-point-cloud-and-observations",level:3},{value:"Point Cloud Data Types",id:"point-cloud-data-types",level:4},{value:"Filtering Point Cloud by Confidence",id:"filtering-point-cloud-by-confidence",level:4},{value:"[MPS - SLAM] Closed vs Open Loop Trajectory",id:"mps---slam-closed-vs-open-loop-trajectory",level:3},{value:"Key Differences",id:"key-differences",level:4},{value:"Data Types",id:"data-types",level:4},{value:"MPS - Hand Tracking",id:"mps---hand-tracking",level:2},{value:"Hand Tracking Features",id:"hand-tracking-features",level:3},{value:"Query Methods",id:"query-methods",level:3},{value:"Visualization",id:"visualization",level:2},{value:"Visualization Notes",id:"visualization-notes",level:3},{value:"Summary",id:"summary",level:2},{value:"Key Takeaways",id:"key-takeaways",level:3},{value:"MPS vs On-Device Data",id:"mps-vs-on-device-data",level:3}];function p(e){const n={a:"a",code:"code",h2:"h2",h3:"h3",h4:"h4",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(r.A,{notebookUrl:"https://github.com/facebookresearch/projectaria_gen2_pilot_dataset/blob/main/examples/tutorial_2_mps_data_loading.ipynb",colabDisabled:!0}),"\n",(0,o.jsxs)(n.p,{children:["This tutorial demonstrates how to load and visualize MPS data from the Aria Gen2 Pilot Dataset using the ",(0,o.jsx)(n.code,{children:"AriaGen2PilotDataProvider"}),"."]}),"\n",(0,o.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,o.jsx)(n.p,{children:"Machine Perception Services, or MPS, is a post-processing cloud service that we provide to Aria users.\nIt runs a set of proprietary Spatial AI machine perception algorithms that are designed for Project Aria glasses.\nMPS is designed to provide superior accuracy and robustness compared to off-the-shelf open algorithms."}),"\n",(0,o.jsx)(n.p,{children:"Currently, the supported MPS algorithms for Aria Gen2 include:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"SLAM: Single Sequence Trajectory and Semi-dense point cloud generation."}),"\n",(0,o.jsx)(n.li,{children:"Hand Tracking: 21 landmarks, wrist to device transformation, wrist and palm positions and normals."}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"This tutorial focuses on demonstrating how to load and visualize the MPS results."}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"What You'll Learn"})}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"How to load MPS output data, and definitions of the data types."}),"\n",(0,o.jsx)(n.li,{children:"How to visualize the MPS data"}),"\n",(0,o.jsx)(n.li,{children:"Understanding the difference between MPS and on-device perception outputs"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"dataset-structure",children:"Dataset Structure"}),"\n",(0,o.jsx)(n.p,{children:"The Aria Gen2 Pilot Dataset comprises four primary data content types:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Raw sensor streams acquired directly from Aria Gen2 devices"}),"\n",(0,o.jsx)(n.li,{children:"Real-time machine perception outputs generated on-device via embedded algorithms during data collection"}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Offline machine perception results produced by Machine Perception Services (MPS) during post-processing"})," (focus of this tutorial)"]}),"\n",(0,o.jsx)(n.li,{children:"Outputs from additional offline perception algorithms"}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["Each sequence folder contains an ",(0,o.jsx)(n.code,{children:"mps/"})," directory with the following structure:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"mps/\n\u251c\u2500\u2500 slam/\n\u2502   \u251c\u2500\u2500 closed_loop_trajectory.csv\n\u2502   \u251c\u2500\u2500 open_loop_trajectory.csv\n\u2502   \u251c\u2500\u2500 semidense_observations.csv.gz\n\u2502   \u251c\u2500\u2500 semidense_points.csv.gz\n\u2502   \u251c\u2500\u2500 online_calibration.jsonl\n\u2502   \u2514\u2500\u2500 summary.json\n\u2514\u2500\u2500 hand_tracking/\n    \u251c\u2500\u2500 hand_tracking_results.csv\n    \u2514\u2500\u2500 summary.json\n"})}),"\n",(0,o.jsx)(n.h2,{id:"initialize-data-provider",children:"Initialize Data Provider"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"\u26a0\ufe0f Important:"})," Update the ",(0,o.jsx)(n.code,{children:"sequence_path"})," below to point to your downloaded Aria Gen2 Pilot Dataset sequence folder."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import numpy as np\nfrom aria_gen2_pilot_dataset import AriaGen2PilotDataProvider\nfrom projectaria_tools.core.sensor_data import TimeDomain\nfrom projectaria_tools.core import mps\nfrom aria_gen2_pilot_dataset.visualization.plot_style import get_plot_style, PlotEntity, PlotStyle\nfrom projectaria_tools.utils.rerun_helpers import (\n    create_hand_skeleton_from_landmarks,\n    ToTransform3D,\n)\nimport rerun as rr\nimport rerun.blueprint as rrb\n"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# TODO: Update this path to your dataset location\nsequence_path = "path/to/your/sequence_folder"\npilot_data_provider = AriaGen2PilotDataProvider(sequence_path)\n'})}),"\n",(0,o.jsx)(n.h2,{id:"mps---slam",children:"MPS - SLAM"}),"\n",(0,o.jsx)(n.h3,{id:"mps---slam-output-files",children:"[MPS - SLAM] Output Files"}),"\n",(0,o.jsx)(n.p,{children:"MPS output result files are categorized into sub-folders by algorithm.\nFor SLAM algorithm output, it generates the following files:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"closed_loop_trajectory.csv"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"open_loop_trajectory.csv"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"semidense_observations.csv.gz"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"semidense_points.csv.gz"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"online_calibration.jsonl"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"summary.json"})}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["Please refer to the ",(0,o.jsx)(n.a,{href:"https://facebookresearch.github.io/projectaria_tools/docs/data_formats/mps/slam",children:"MPS Wiki page"})," for details of each file."]}),"\n",(0,o.jsx)(n.h3,{id:"mps---slam-semi-dense-point-cloud-and-observations",children:"[MPS - SLAM] Semi-dense Point Cloud and Observations"}),"\n",(0,o.jsxs)(n.p,{children:["The MPS SLAM algorithm outputs 2 files related to semi-dense point cloud (see ",(0,o.jsx)(n.a,{href:"https://facebookresearch.github.io/projectaria_tools/docs/data_formats/mps/slam/mps_pointcloud",children:"wiki page"})," for data type definitions):"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"semidense_points.csv.gz"}),": Global points in the world coordinate frame."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"semidense_observations.csv.gz"}),": Point observations for each camera, at each timestamp."]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"Note that semidense point files are normally large, therefore loading them may take some time."}),"\n",(0,o.jsx)(n.h4,{id:"point-cloud-data-types",children:"Point Cloud Data Types"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"GlobalPointPosition"})," contains:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"uid"}),": Unique identifier for the 3D point"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"graph_uid"}),": Identifier linking point to pose graph"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"position_world"}),": 3D position in world coordinate frame"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"inverse_distance_std"}),": Inverse distance standard deviation (quality metric)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"distance_std"}),": Distance standard deviation (quality metric)"]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"PointObservation"})," contains:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"point_uid"}),": Links observation to 3D point"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"frame_capture_timestamp"}),": When the observation was captured"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"camera_serial"}),": Serial number of the observing camera"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"uv"}),": 2D pixel coordinates of the observation"]}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"filtering-point-cloud-by-confidence",children:"Filtering Point Cloud by Confidence"}),"\n",(0,o.jsxs)(n.p,{children:["You can filter the semi-dense point cloud using confidence thresholds based on ",(0,o.jsx)(n.code,{children:"inverse_distance_std"})," or ",(0,o.jsx)(n.code,{children:"distance_std"})," to improve quality. The ",(0,o.jsx)(n.code,{children:"AriaGen2PilotDataProvider"})," provides a convenient method ",(0,o.jsx)(n.code,{children:"get_mps_semidense_point_cloud_filtered()"})," for this purpose."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'print("=== MPS - Semi-dense Point Cloud ===")\n\nsemi_dense_point_cloud = pilot_data_provider.get_mps_semidense_point_cloud()\nsemi_dense_point_cloud_filtered = pilot_data_provider.get_mps_semidense_point_cloud_filtered(filter_confidence=True, max_point_count=50000)\n\n# Print out the content of the first sample in semidense_points\nif semi_dense_point_cloud:\n    sample = semi_dense_point_cloud[0]\n    print("GlobalPointPosition sample:")\n    print(f"  uid: {sample.uid}")\n    print(f"  graph_uid: {sample.graph_uid}")\n    print(f"  position_world: {sample.position_world}")\n    print(f"  inverse_distance_std: {sample.inverse_distance_std}")\n    print(f"  distance_std: {sample.distance_std}")\n    print(f"Total number of semi-dense points: {len(semi_dense_point_cloud)}")\n    print(f"Total number of filtered semi-dense points: {len(semi_dense_point_cloud_filtered)}")\nelse:\n    print("semidense_points is empty.")\n'})}),"\n",(0,o.jsx)(n.h3,{id:"mps---slam-closed-vs-open-loop-trajectory",children:"[MPS - SLAM] Closed vs Open Loop Trajectory"}),"\n",(0,o.jsxs)(n.p,{children:["The MPS SLAM algorithm outputs 2 trajectory files ",(0,o.jsx)(n.code,{children:"open_loop_trajectory.csv"})," and ",(0,o.jsx)(n.code,{children:"closed_loop_trajectory.csv"})," (see ",(0,o.jsx)(n.a,{href:"https://facebookresearch.github.io/projectaria_tools/docs/data_formats/mps/slam/mps_trajectory",children:"wiki page"})," for data type definitions):"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Open loop trajectory"}),": High-frequency (1kHz) odometry from visual-inertial odometry (VIO), accurate over short periods but drifts over time and distance."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Closed loop trajectory"}),": High-frequency (1kHz) pose from mapping with loop closure corrections, reducing drift but possibly less accurate locally over short spans. For closed loop, an additional interpolation API is provided."]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"key-differences",children:"Key Differences"}),"\n",(0,o.jsxs)(n.table,{children:[(0,o.jsx)(n.thead,{children:(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.th,{children:"Aspect"}),(0,o.jsx)(n.th,{children:"Open Loop (VIO)"}),(0,o.jsx)(n.th,{children:"Closed Loop (SLAM)"})]})}),(0,o.jsxs)(n.tbody,{children:[(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"Coordinate Frame"})}),(0,o.jsx)(n.td,{children:"Odometry frame"}),(0,o.jsx)(n.td,{children:"Global world frame"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"Drift"})}),(0,o.jsx)(n.td,{children:"Accumulates over time"}),(0,o.jsx)(n.td,{children:"Minimized with loop closure"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"Accuracy"})}),(0,o.jsx)(n.td,{children:"Good for short periods"}),(0,o.jsx)(n.td,{children:"Higher global accuracy"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"Use Case"})}),(0,o.jsx)(n.td,{children:"Real-time odometry"}),(0,o.jsx)(n.td,{children:"High-quality reconstruction"})]})]})]}),"\n",(0,o.jsx)(n.h4,{id:"data-types",children:"Data Types"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"ClosedLoopTrajectoryPose"})," contains:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"tracking_timestamp"}),": Device timestamp when pose was computed"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"transform_world_device"}),": 6DOF pose in world coordinate frame"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"device_linear_velocity_device"}),": Linear velocity in device frame"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"angular_velocity_device"}),": Angular velocity in device frame"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"quality_score"}),": Pose estimation quality (higher = better)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"gravity_world"}),": Gravity vector in world frame"]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"OpenLoopTrajectoryPose"})," contains:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"tracking_timestamp"}),": Device timestamp when pose was computed"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"transform_odometry_device"}),": 6DOF pose in odometry coordinate frame"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"device_linear_velocity_odometry"}),": Linear velocity in odometry frame"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"angular_velocity_device"}),": Angular velocity in device frame"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"quality_score"}),": Pose estimation quality (higher = better)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"gravity_odometry"}),": Gravity vector in odometry frame"]}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'print("=== MPS - query whole trajectory ===")\nmps_closed_loop_trajectory = pilot_data_provider.get_mps_closed_loop_trajectory()\nmps_closed_loop_trajectory_duration = mps_closed_loop_trajectory[-1].tracking_timestamp.total_seconds() - mps_closed_loop_trajectory[0].tracking_timestamp.total_seconds()\nprint("MPS Closed Loop Trajectory duration: " + f"{mps_closed_loop_trajectory_duration:.2f}")\n\nmps_open_loop_trajectory = pilot_data_provider.get_mps_open_loop_trajectory()\nmps_open_loop_trajectory_duration = mps_open_loop_trajectory[-1].tracking_timestamp.total_seconds() - mps_open_loop_trajectory[0].tracking_timestamp.total_seconds()\nprint("MPS Open Loop Trajectory duration: " + f"{mps_open_loop_trajectory_duration:.2f}")\n\nprint("\\n")\nprint("=== MPS - query pose by timestamp ===")\n\nquery_timestamp_ns = int((mps_closed_loop_trajectory[0].tracking_timestamp.total_seconds()+1)*1e9)\nprint("query timestamp ns:", query_timestamp_ns, \'\\n\')\nif mps_closed_loop_trajectory_duration > 1:  # If duration > 1s\n\n    nearest_mps_closed_loop_pose = pilot_data_provider.get_mps_closed_loop_pose(timestamp_ns=query_timestamp_ns, time_domain=TimeDomain.DEVICE_TIME)\n    print("Nearest mps closed loop pose: ", nearest_mps_closed_loop_pose, "\\n")\n\n    interpolated_mps_closed_loop_pose = pilot_data_provider.get_mps_interpolated_closed_loop_pose(timestamp_ns=query_timestamp_ns, time_domain=TimeDomain.DEVICE_TIME)\n    print("Interpolated mps closed loop pose: ", interpolated_mps_closed_loop_pose,  "\\n")\n\nif mps_open_loop_trajectory_duration > 1:  # If duration > 1s\n    query_timestamp_ns = int((mps_open_loop_trajectory[0].tracking_timestamp.total_seconds()+1)*1e9)\n    nearest_mps_open_loop_pose = pilot_data_provider.get_mps_open_loop_pose(timestamp_ns=query_timestamp_ns, time_domain=TimeDomain.DEVICE_TIME)\n    print("Nearest mps closed loop pose: ", nearest_mps_open_loop_pose,  "\\n")\n'})}),"\n",(0,o.jsx)(n.h2,{id:"mps---hand-tracking",children:"MPS - Hand Tracking"}),"\n",(0,o.jsxs)(n.p,{children:["The MPS Hand Tracking algorithm outputs 2 files related to hand-tracking: ",(0,o.jsx)(n.code,{children:"hand_tracking_results.csv"})," and ",(0,o.jsx)(n.code,{children:"summary.json"}),". The MPS Hand Tracking algorithm outputs 2 files related to hand-tracking: ",(0,o.jsx)(n.code,{children:"hand_tracking_results.csv"})," and ",(0,o.jsx)(n.code,{children:"summary.json"}),". See ",(0,o.jsx)(n.a,{href:"https://facebookresearch.github.io/projectaria_tools/gen2/research-tools/dataset/pilot/format#file-format",children:"wiki page"})," for data type definitions."]}),"\n",(0,o.jsx)(n.h3,{id:"hand-tracking-features",children:"Hand Tracking Features"}),"\n",(0,o.jsx)(n.p,{children:"MPS Hand Tracking provides the following outputs for each detected hand:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"21 landmarks"}),": Detailed hand joint positions"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Wrist to device transformation"}),": Spatial relationship between wrist and device"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Wrist and palm positions"}),": Key reference points for hand pose"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Palm normals"}),": Surface orientation for interaction analysis"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"query-methods",children:"Query Methods"}),"\n",(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(n.code,{children:"AriaGen2PilotDataProvider"})," offers two query methods for hand tracking data:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"get_mps_hand_tracking_result()"}),": Returns the nearest hand tracking result for a given timestamp"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"get_mps_interpolated_hand_tracking_result()"}),": Returns interpolated hand tracking result for smoother motion analysis"]}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'print("=== MPS - query whole hand tracking results ===")\nmps_hand_tracking_results = pilot_data_provider.get_mps_hand_tracking_result_list()\nmps_hand_tracking_results_duration = mps_hand_tracking_results[-1].tracking_timestamp.total_seconds() - mps_hand_tracking_results[0].tracking_timestamp.total_seconds()\nprint("MPS hand tracking results duration: " + f"{mps_hand_tracking_results_duration:.2f}", "\\n")\n\nprint("=== MPS - query hand tracking result by timestamp ===")\nif mps_hand_tracking_results_duration > 1:  # If duration > 1s\n    query_timestamp_ns = int((mps_hand_tracking_results[0].tracking_timestamp.total_seconds()+1)*1e9)\n    print("Query timestamp ns:", query_timestamp_ns, \'\\n\')\n    nearest_mps_hand_tracking_result = pilot_data_provider.get_mps_hand_tracking_result(timestamp_ns=query_timestamp_ns, time_domain=TimeDomain.DEVICE_TIME)\n    print("Nearest tracking result: ", nearest_mps_hand_tracking_result, "\\n")\n    interpolated_mps_hand_tracking_result = pilot_data_provider.get_mps_interpolated_hand_tracking_result(timestamp_ns=query_timestamp_ns, time_domain=TimeDomain.DEVICE_TIME)\n    print("Interpolated tracking result: ", interpolated_mps_hand_tracking_result, "\\n")\n'})}),"\n",(0,o.jsx)(n.h2,{id:"visualization",children:"Visualization"}),"\n",(0,o.jsx)(n.p,{children:"The following section shows how to plot:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Semi-dense point cloud"}),"\n",(0,o.jsx)(n.li,{children:"Closed loop trajectory"}),"\n",(0,o.jsx)(n.li,{children:"Hand pose results"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"visualization-notes",children:"Visualization Notes"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Point Cloud Filtering"}),": For better visualization performance, we filter the point cloud by confidence and limit the maximum point count"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Trajectory Caching"}),": We accumulate trajectory points over time to visualize the full path"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Rerun Integration"}),": We use Rerun for interactive 3D visualization with proper coordinate frames"]}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import rerun as rr\ndef plot_mps_semidense_point_cloud(\n        point_cloud_data: list[mps.GlobalPointPosition]\n    ) -> None:\n        if point_cloud_data == []:\n            return\n        points_array = np.array(\n            [\n                point.position_world\n                for point in point_cloud_data\n                if hasattr(point, "position_world")\n            ]\n        )\n        plot_style = get_plot_style(PlotEntity.SEMI_DENSE_POINT_CLOUD)\n        rr.log(\n            f"world/{plot_style.label}",\n            rr.Points3D(\n                positions=points_array,\n                colors=[] * len(points_array),\n                radii=plot_style.plot_3d_size,\n            ),\n            static=True,\n        )\n\ndef plot_closed_loop_pose(\n        closed_loop_trajectory_pose: mps.ClosedLoopTrajectoryPose,\n    ) -> None:\n        """Plot MPS closed loop trajectory"""\n        if not closed_loop_trajectory_pose:\n            return\n        # Get transform and add to trajectory cache\n        T_world_device = closed_loop_trajectory_pose.transform_world_device\n        closed_loop_trajectory_pose_cache.append(T_world_device.translation()[0])\n\n        # Plot device pose\n        rr.log(\n            "world/device",\n            ToTransform3D(T_world_device, axis_length=0.05),\n        )\n\n        # Plot accumulated trajectory\n        if len(closed_loop_trajectory_pose_cache) > 1:\n            plot_style = get_plot_style(PlotEntity.TRAJECTORY)\n            rr.log(\n                f"world/{plot_style.label}",\n                rr.LineStrips3D(\n                    [closed_loop_trajectory_pose_cache],\n                    colors=[plot_style.color],\n                    radii=plot_style.plot_3d_size,\n                ),\n            )\n\ndef _get_hand_plot_style(hand_label: str) -> PlotStyle:\n    if hand_label == "left":\n        landmarks_plot_entity = PlotEntity.HAND_TRACKING_LEFT_HAND_LANDMARKS\n        skeleton_plot_entity = PlotEntity.HAND_TRACKING_LEFT_HAND_SKELETON\n    else:\n        landmarks_plot_entity = PlotEntity.HAND_TRACKING_RIGHT_HAND_LANDMARKS\n        skeleton_plot_entity = PlotEntity.HAND_TRACKING_RIGHT_HAND_SKELETON\n\n    return get_plot_style(landmarks_plot_entity), get_plot_style(\n        skeleton_plot_entity\n    )\n\ndef _plot_single_hand_3d(\n    hand_joints_in_device: list[np.array],\n    hand_label: str,\n) -> None:\n    """\n    Plot single hand data in 3D and 2D camera views\n    """\n    landmarks_style, skeleton_style = _get_hand_plot_style(hand_label=hand_label)\n    if hand_joints_in_device is None:\n        return\n    # Plot 3D hand markers and skeleton\n    hand_skeleton_3d = create_hand_skeleton_from_landmarks(hand_joints_in_device)\n    rr.log(\n        f"world/device/hand-tracking/{hand_label}/{landmarks_style.label}",\n        rr.Points3D(\n            positions=hand_joints_in_device,\n            colors=[landmarks_style.color],\n            radii=landmarks_style.plot_3d_size,\n        ),\n    )\n    rr.log(\n        f"world/device/hand-tracking/{hand_label}/{skeleton_style.label}",\n        rr.LineStrips3D(\n            hand_skeleton_3d,\n            colors=[skeleton_style.color],\n            radii=skeleton_style.plot_3d_size,\n        ),\n    )\n\ndef plot_mps_hand_tracking_result_3d(\n        hand_pose_data: mps.hand_tracking.HandTrackingResult,\n    ) -> None:\n        """\n        Plot hand pose data within 3D world view\n        """\n        rr.log(\n            "world/device/hand-tracking",\n            rr.Clear.recursive(),\n        )\n        if hand_pose_data is None:\n            return\n\n        if hand_pose_data.left_hand is not None:\n            _plot_single_hand_3d(\n                hand_joints_in_device=hand_pose_data.left_hand.landmark_positions_device,\n                hand_label="left",\n            )\n        if hand_pose_data.right_hand is not None:\n            _plot_single_hand_3d(\n                hand_joints_in_device=hand_pose_data.right_hand.landmark_positions_device,\n                hand_label="right",\n            )\n\nclosed_loop_trajectory_pose_cache=[]\nopen_loop_trajectory_pose_cache=[]\n\ndef plot_mps():\n    rr.init("rerun_viz_mps")\n    # Create a Spatial3D view to display the points.\n    blueprint = rrb.Blueprint(\n        rrb.Spatial3DView(\n            origin="/",\n            name="3D Scene",\n            # Set the background color to light blue.\n            background=[0, 0, 0],\n        ),\n        collapse_panels=True,\n    )\n    rr.notebook_show(blueprint=blueprint)\n\n    # plot semi-dense point cloud\n    plot_mps_semidense_point_cloud(semi_dense_point_cloud_filtered)\n    for hand_tracking_result in mps_hand_tracking_results:\n        closed_loop_pose = pilot_data_provider.get_mps_interpolated_closed_loop_pose(int(hand_tracking_result.tracking_timestamp.total_seconds()*1e9), TimeDomain.DEVICE_TIME)\n\n        # plot hand tracking result\n        plot_mps_hand_tracking_result_3d(hand_tracking_result)\n\n        # plot closed loop pose\n        plot_closed_loop_pose(closed_loop_pose)\n\n\nplot_mps()\n'})}),"\n",(0,o.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(n.p,{children:"This tutorial covered the essential aspects of working with MPS data in the Aria Gen2 Pilot Dataset:"}),"\n",(0,o.jsx)(n.h3,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"MPS SLAM Trajectories"}),": Understanding the difference between open loop (VIO-based, with drift) and closed loop (globally optimized, minimal drift) trajectories"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Semi-dense Point Cloud"}),": Accessing high-quality 3D reconstructions with confidence-based filtering"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Hand Tracking"}),": Loading and visualizing detailed hand pose data with 21 landmarks per hand"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Data Querying"}),": Using both nearest-neighbor and interpolated queries for smooth temporal access"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"3D Visualization"}),": Creating interactive visualizations with Rerun"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"mps-vs-on-device-data",children:"MPS vs On-Device Data"}),"\n",(0,o.jsxs)(n.table,{children:[(0,o.jsx)(n.thead,{children:(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.th,{children:"Aspect"}),(0,o.jsx)(n.th,{children:"On-Device (VIO/Tracking)"}),(0,o.jsx)(n.th,{children:"MPS (Post-processing)"})]})}),(0,o.jsxs)(n.tbody,{children:[(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"Processing"})}),(0,o.jsx)(n.td,{children:"Real-time during recording"}),(0,o.jsx)(n.td,{children:"Cloud-based offline processing"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"Accuracy"})}),(0,o.jsx)(n.td,{children:"Good for real-time use"}),(0,o.jsx)(n.td,{children:"Higher accuracy with global optimization"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"Latency"})}),(0,o.jsx)(n.td,{children:"Immediate availability"}),(0,o.jsx)(n.td,{children:"Requires post-processing time"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"Drift"})}),(0,o.jsx)(n.td,{children:"Accumulates over time"}),(0,o.jsx)(n.td,{children:"Minimized with loop closure (SLAM)"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"Point Cloud"})}),(0,o.jsx)(n.td,{children:"Not available"}),(0,o.jsx)(n.td,{children:"Dense semi-dense reconstructions"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"Coordinate Frame"})}),(0,o.jsx)(n.td,{children:"Odometry/device frame"}),(0,o.jsx)(n.td,{children:"Global world frame"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"Use Cases"})}),(0,o.jsx)(n.td,{children:"Live feedback, real-time apps"}),(0,o.jsx)(n.td,{children:"Research, high-quality reconstruction, benchmarking"})]})]})]})]})}function _(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(p,{...e})}):p(e)}},69470(e,n,t){t.d(n,{A:()=>o});t(96540);var i=t(74848);const o=({notebookUrl:e,colabUrl:n,colabDisabled:t=!1})=>{const o=()=>(0,i.jsx)("svg",{style:{marginRight:"8px",width:"16px",height:"16px"},fill:"currentColor",viewBox:"0 0 24 24",children:(0,i.jsx)("path",{d:"M16.9414 4.9757a7.033 7.033 0 0 0-4.9308 2.0324 7.033 7.033 0 0 0-.1232 9.8068l2.395-2.395a3.6455 3.6455 0 0 1 5.1497-5.1478l2.397-2.3989a7.033 7.033 0 0 0-4.8877-1.9175zM7.07 4.9855a7.033 7.033 0 0 0-4.8878 1.9175l2.395 2.3989a3.6434 3.6434 0 0 1 5.1497 5.1478l2.395 2.395a7.033 7.033 0 0 0-.1232-9.8068A7.033 7.033 0 0 0 7.07 4.9855zm15.0191 2.5923l-2.395 2.395a3.6455 3.6455 0 0 1-5.1497 5.1497l-2.395 2.395a7.033 7.033 0 0 0 9.9397-.123z"})});return(0,i.jsxs)("div",{style:{display:"flex",gap:"10px",marginBottom:"20px",flexWrap:"wrap"},children:[(0,i.jsxs)("a",{href:e,target:"_blank",rel:"noopener noreferrer",style:{display:"inline-flex",alignItems:"center",padding:"8px 16px",backgroundColor:"#f6f8fa",border:"1px solid #d0d7de",borderRadius:"6px",textDecoration:"none",color:"#24292f",fontSize:"14px",fontWeight:"500",transition:"background-color 0.2s"},onMouseEnter:e=>{e.target.style.backgroundColor="#f3f4f6"},onMouseLeave:e=>{e.target.style.backgroundColor="#f6f8fa"},children:[(0,i.jsx)("svg",{style:{marginRight:"8px",width:"16px",height:"16px"},fill:"currentColor",viewBox:"0 0 16 16",children:(0,i.jsx)("path",{d:"M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z"})}),"View Notebook on GitHub"]}),!t&&n?(0,i.jsxs)("a",{href:n,target:"_blank",rel:"noopener noreferrer",style:{display:"inline-flex",alignItems:"center",padding:"8px 16px",backgroundColor:"#f9ab00",border:"1px solid #d0d7de",borderRadius:"6px",textDecoration:"none",color:"#fff",fontSize:"14px",fontWeight:"500",transition:"background-color 0.2s"},onMouseEnter:e=>{e.target.style.backgroundColor="#e8a500"},onMouseLeave:e=>{e.target.style.backgroundColor="#f9ab00"},children:[(0,i.jsx)(o,{}),"Run in Google Colab"]}):(0,i.jsxs)("button",{onClick:()=>{alert("Google Colab integration is not available yet. Please download the notebook and run it locally.")},style:{display:"inline-flex",alignItems:"center",padding:"8px 16px",backgroundColor:"#f6f8fa",border:"1px solid #d0d7de",borderRadius:"6px",color:"#656d76",fontSize:"14px",fontWeight:"500",cursor:"not-allowed",transition:"background-color 0.2s"},children:[(0,i.jsx)(o,{}),"Colab (Coming Soon)"]})]})}},28453(e,n,t){t.d(n,{R:()=>r,x:()=>a});var i=t(96540);const o={},s=i.createContext(o);function r(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);