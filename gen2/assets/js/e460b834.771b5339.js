"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[8613],{9516(e,a,n){n.r(a),n.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>d,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"projectariatools/pythontutorials/eyetracking-handtracking","title":"Using On-Device Eye-tracking and Hand-tracking","description":"<TutorialButtons","source":"@site/docs-research-tools/projectariatools/pythontutorials/eyetracking-handtracking.mdx","sourceDirName":"projectariatools/pythontutorials","slug":"/projectariatools/pythontutorials/eyetracking-handtracking","permalink":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/eyetracking-handtracking","draft":false,"unlisted":false,"editUrl":"https://www.internalfb.com/code/fbsource/arvr/projects/ariane/aria_research_kit/projectaria_tools/website/docs-research-tools/projectariatools/pythontutorials/eyetracking-handtracking.mdx","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4,"title":"Using On-Device Eye-tracking and Hand-tracking"},"sidebar":"researchToolsSidebar","previous":{"title":"Access Multi-Sensor Data Sequentially","permalink":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/queue"},"next":{"title":"Using On-Device VIO Data","permalink":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/vio"}}');var i=n(74848),r=n(28453),s=n(69470);const d={sidebar_position:4,title:"Using On-Device Eye-tracking and Hand-tracking"},o="Tutorial 4: Using On-Device Eye-tracking and Hand-tracking",l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"On-Device Eye-tracking results",id:"on-device-eye-tracking-results",level:2},{value:"EyeGaze Data Structure",id:"eyegaze-data-structure",level:3},{value:"EyeGaze API Reference",id:"eyegaze-api-reference",level:3},{value:"EyeGaze visualization in camera images",id:"eyegaze-visualization-in-camera-images",level:3},{value:"Visualizing Eye-tracking Data",id:"visualizing-eye-tracking-data",level:3},{value:"Accessing Hand-tracking Data",id:"accessing-hand-tracking-data",level:2},{value:"Basic Hand-tracking Data Access",id:"basic-hand-tracking-data-access",level:3},{value:"Hand-tracking Data Structure",id:"hand-tracking-data-structure",level:3},{value:"Interpolated Hand-tracking Data",id:"interpolated-hand-tracking-data",level:3},{value:"Visualizing Hand-tracking Results in Cameras",id:"visualizing-hand-tracking-results-in-cameras",level:3},{value:"Understanding Interpolation",id:"understanding-interpolation",level:2},{value:"Summary",id:"summary",level:2}];function _(e){const a={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(a.header,{children:(0,i.jsx)(a.h1,{id:"tutorial-4-using-on-device-eye-tracking-and-hand-tracking",children:"Tutorial 4: Using On-Device Eye-tracking and Hand-tracking"})}),"\n",(0,i.jsx)(s.A,{notebookUrl:"https://github.com/facebookresearch/projectaria_tools/blob/2.1.0/examples/Gen2/python_notebooks/Tutorial_4_on_device_eyetracking_handtracking.ipynb",colabUrl:"https://colab.research.google.com/github/facebookresearch/projectaria_tools/blob/2.1.0/examples/Gen2/python_notebooks/Tutorial_4_on_device_eyetracking_handtracking.ipynb"}),"\n",(0,i.jsx)(a.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsx)(a.p,{children:"In Aria-Gen2 glasses, one of the key upgrade from Aria-Gen1 is the capability to run Machine Perception (MP) algorithms on the device during streaming / recording. Currently supported on-device MP algorithms include Eye-tracking, Hand-tracking, and VIO. These algorithm results are stored as separate data streams in the VRS file."}),"\n",(0,i.jsxs)(a.p,{children:["This tutorial focuses on demonstration of how to use the ",(0,i.jsx)(a.strong,{children:"Eye-tracking and Hand-tracking"})," results."]}),"\n",(0,i.jsx)(a.p,{children:(0,i.jsx)(a.strong,{children:"What you'll learn:"})}),"\n",(0,i.jsxs)(a.ul,{children:["\n",(0,i.jsx)(a.li,{children:"How to access on-device EyeGaze and HandTracking data from VRS files"}),"\n",(0,i.jsx)(a.li,{children:"Understanding the concept of interpolated hand tracking and why interpolation is needed"}),"\n",(0,i.jsx)(a.li,{children:"How to visualize EyeGaze and HandTracking data projected onto 2D camera images using DeviceCalibration"}),"\n",(0,i.jsx)(a.li,{children:"How to match MP data with camera frames using timestamps"}),"\n"]}),"\n",(0,i.jsx)(a.p,{children:(0,i.jsx)(a.strong,{children:"Prerequisites"})}),"\n",(0,i.jsxs)(a.ul,{children:["\n",(0,i.jsx)(a.li,{children:"Complete Tutorial 1 (VrsDataProvider Basics) to understand basic data provider concepts"}),"\n",(0,i.jsx)(a.li,{children:"Complete Tutorial 2 (Device Calibration) to understand how to properly use calibration in Aria data."}),"\n",(0,i.jsxs)(a.li,{children:["Download Aria Gen2 sample data from ",(0,i.jsx)(a.a,{href:"https://www.projectaria.com/async/sample/download/?bucket=core&filename=aria_gen2_sample_data_1.vrs",children:"link"})]}),"\n"]}),"\n",(0,i.jsxs)(a.p,{children:[(0,i.jsx)(a.strong,{children:"Note on Visualization"}),"\nIf visualization window is not showing up, this is due to ",(0,i.jsx)(a.code,{children:"Rerun"})," lib's caching issue. Just rerun the specific code cell, or restart the python kernel."]}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{className:"language-python",children:'from projectaria_tools.core import data_provider\n\n# Load VRS file\nvrs_file_path = "path/to/your/recording.vrs"\nvrs_data_provider = data_provider.create_vrs_data_provider(vrs_file_path)\n\n# Access device calibration\ndevice_calib = vrs_data_provider.get_device_calibration()\n'})}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{className:"language-python",children:'# Query EyeGaze data streams\neyegaze_label = "eyegaze"\neyegaze_stream_id = vrs_data_provider.get_stream_id_from_label(eyegaze_label)\nif eyegaze_stream_id is None:\n    raise RuntimeError(\n        f"{eyegaze_label} data stream does not exist! Please use a VRS that contains valid eyegaze data for this tutorial."\n    )\n\n# Query HandTracking data streams\nhandtracking_label = "handtracking"\nhandtracking_stream_id = vrs_data_provider.get_stream_id_from_label(handtracking_label)\nif handtracking_stream_id is None:\n    raise RuntimeError(\n        f"{handtracking_label} data stream does not exist! Please use a VRS that contains valid handtracking data for this tutorial."\n    )\n'})}),"\n",(0,i.jsx)(a.h2,{id:"on-device-eye-tracking-results",children:"On-Device Eye-tracking results"}),"\n",(0,i.jsx)(a.h3,{id:"eyegaze-data-structure",children:"EyeGaze Data Structure"}),"\n",(0,i.jsxs)(a.p,{children:["The EyeGaze data type represents on-device eye tracking results.\n",(0,i.jsxs)(a.strong,{children:["Importantly, it directly reuses ",(0,i.jsx)(a.a,{href:"https://github.com/facebookresearch/projectaria_tools/blob/2.1.0/core/mps/EyeGaze.h",children:"the EyeGaze data structure"}),"\nfrom MPS (Machine Perception Services)"]}),", providing guaranteed compatibility across VRS and MPS."]}),"\n",(0,i.jsx)(a.p,{children:(0,i.jsxs)(a.strong,{children:["Key ",(0,i.jsx)(a.code,{children:"EyeGaze"})," fields"]})}),"\n",(0,i.jsxs)(a.table,{children:[(0,i.jsx)(a.thead,{children:(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.th,{style:{textAlign:"left"},children:"Field Name"}),(0,i.jsx)(a.th,{style:{textAlign:"left"},children:"Description"})]})}),(0,i.jsxs)(a.tbody,{children:[(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{style:{textAlign:"left"},children:(0,i.jsx)(a.code,{children:"session_uid"})}),(0,i.jsx)(a.td,{style:{textAlign:"left"},children:"Unique ID for the eyetracking session"})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{style:{textAlign:"left"},children:(0,i.jsx)(a.code,{children:"tracking_timestamp"})}),(0,i.jsx)(a.td,{style:{textAlign:"left"},children:"Timestamp of the eye tracking camera frame in device time domain, in us."})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{style:{textAlign:"left"},children:(0,i.jsx)(a.code,{children:"yaw"})}),(0,i.jsx)(a.td,{style:{textAlign:"left"},children:"Gaze direction in yaw (horizontal) in radians"})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{style:{textAlign:"left"},children:(0,i.jsx)(a.code,{children:"pitch"})}),(0,i.jsx)(a.td,{style:{textAlign:"left"},children:"Gaze direction in pitch (vertical) in radians"})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{style:{textAlign:"left"},children:(0,i.jsx)(a.code,{children:"depth"})}),(0,i.jsx)(a.td,{style:{textAlign:"left"},children:"Estimated gaze depth distance, in meters"})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{style:{textAlign:"left"},children:(0,i.jsx)(a.code,{children:"combined_gaze_origin_in_cpf"})}),(0,i.jsx)(a.td,{style:{textAlign:"left"},children:"Combined gaze origin in CPF frame (Gen2 only)"})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{style:{textAlign:"left"},children:(0,i.jsx)(a.code,{children:"spatial_gaze_point_in_cpf"})}),(0,i.jsx)(a.td,{style:{textAlign:"left"},children:"3D spatial gaze point in CPF frame"})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{style:{textAlign:"left"},children:(0,i.jsx)(a.code,{children:"vergence.[left,right]_entrance_pupil_position_meter"})}),(0,i.jsx)(a.td,{style:{textAlign:"left"},children:"Entrance pupil positions for each eye"})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{style:{textAlign:"left"},children:(0,i.jsx)(a.code,{children:"vergence.[left,right]_pupil_diameter_meter"})}),(0,i.jsx)(a.td,{style:{textAlign:"left"},children:"Entrance pupil diameter for each eye"})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{style:{textAlign:"left"},children:(0,i.jsx)(a.code,{children:"vergence.[left,right]_blink"})}),(0,i.jsx)(a.td,{style:{textAlign:"left"},children:"Blink detection for left and right eyes"})]}),(0,i.jsxs)(a.tr,{children:[(0,i.jsx)(a.td,{style:{textAlign:"left"},children:(0,i.jsx)(a.code,{children:"*_valid"})}),(0,i.jsx)(a.td,{style:{textAlign:"left"},children:"Boolean flags to indicating if the corresponding data field in EyeGaze is valid"})]})]})]}),"\n",(0,i.jsx)(a.h3,{id:"eyegaze-api-reference",children:"EyeGaze API Reference"}),"\n",(0,i.jsxs)(a.p,{children:["In ",(0,i.jsx)(a.code,{children:"vrs_data_provider"}),", EyeGaze is treated the same way as any other sensor data, and share similar query APIs covered in ",(0,i.jsx)(a.code,{children:"Tutorial_1_vrs_data_provider_basics"}),":"]}),"\n",(0,i.jsxs)(a.ul,{children:["\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.code,{children:"vrs_data_provider.get_eye_gaze_data_by_index(stream_id, index)"}),": Query by index."]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.code,{children:"vrs_data_provider.get_eye_gaze_data_by_time_ns(stream_id, timestamp, time_domain, query_options)"}),": Query by timestamp."]}),"\n"]}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{className:"language-python",children:'from projectaria_tools.core.mps import get_unit_vector_from_yaw_pitch\nfrom datetime import timedelta\n\nprint("=== EyeGaze Data Sample ===")\nnum_eyegaze_samples = vrs_data_provider.get_num_data(eyegaze_stream_id)\nselected_index = min(5, num_eyegaze_samples)\nprint(f"Sample {selected_index}:")\n\neyegaze_data = vrs_data_provider.get_eye_gaze_data_by_index(eyegaze_stream_id, selected_index)\n\n# Eyegaze timestamp is in format of datetime.deltatime in microseconds, convert it to integer\neyegaze_timestamp_ns = (eyegaze_data.tracking_timestamp // timedelta(microseconds=1)) * 1000\nprint(f"\\tTracking timestamp: {eyegaze_timestamp_ns}")\n\n# check if combined gaze is valid, if so, print out the gaze direction\nprint(f"\\tCombined gaze valid: {eyegaze_data.combined_gaze_valid}")\nif eyegaze_data.combined_gaze_valid:\n    print(f"\\tYaw: {eyegaze_data.yaw:.3f} rad")\n    print(f"\\tPitch: {eyegaze_data.pitch:.3f} rad")\n    print(f"\\tDepth: {eyegaze_data.depth:.3f} m")\n    # Can also print gaze direction in unit vector\n    gaze_direction_in_unit_vec = get_unit_vector_from_yaw_pitch(eyegaze_data.yaw, eyegaze_data.pitch)\n    print(f"\\tGaze direction in unit vec [xyz]: {gaze_direction_in_unit_vec}")\n\n# Check if spatial gaze point is valid, if so, print out the spatial gaze point\nprint(\n    f"\\tSpatial gaze point valid: {eyegaze_data.spatial_gaze_point_valid}"\n)\nif eyegaze_data.spatial_gaze_point_valid:\n    print(\n        f"\\tSpatial gaze point in CPF: {eyegaze_data.spatial_gaze_point_in_cpf}"\n    )\n'})}),"\n",(0,i.jsx)(a.h3,{id:"eyegaze-visualization-in-camera-images",children:"EyeGaze visualization in camera images"}),"\n",(0,i.jsx)(a.p,{children:"To visualize EyeGaze in camera images, you just need to project eye tracking results into the camera images using the camera's calibration. But please note the coordinate frame difference, entailed below."}),"\n",(0,i.jsx)(a.p,{children:(0,i.jsx)(a.strong,{children:"EyeGaze Coordinate System - Central Pupil Frame (CPF)"})}),"\n",(0,i.jsxs)(a.p,{children:["All Eyetracking results in Aria are stored in a reference coordinates system called ",(0,i.jsxs)(a.strong,{children:["Central Pupil Frame (",(0,i.jsx)(a.code,{children:"CPF"}),")"]}),", which is approximately the center of user's two eye positions. Note that this ",(0,i.jsxs)(a.strong,{children:[(0,i.jsx)(a.code,{children:"CPF"})," frame is DIFFERENT from the ",(0,i.jsx)(a.code,{children:"Device"})," frame in device calibration"]}),", where the latter is essentially the ",(0,i.jsx)(a.code,{children:"slam-front-left"})," (for Gen2) or ",(0,i.jsx)(a.code,{children:"camera-slam-left"})," (for Gen1) camera. To transform between ",(0,i.jsx)(a.code,{children:"CPF"})," and ",(0,i.jsx)(a.code,{children:"Device"}),", we provide the following API to query their relative pose, and see the following code cell for usage:"]}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{children:"device_calibration.get_transform_device_cpf()\n"})}),"\n",(0,i.jsx)(a.h3,{id:"visualizing-eye-tracking-data",children:"Visualizing Eye-tracking Data"}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{className:"language-python",children:'import rerun as rr\nimport numpy as np\nfrom projectaria_tools.core.sensor_data import TimeDomain, TimeQueryOptions\n\ndef visualize_eyegaze_in_camera(camera_label, eyegaze_data, camera_calib, device_calib):\n    """\n    Project eye-tracking data onto camera image\n    """\n    # Convert gaze direction to 3D vector\n    yaw, pitch = eyegaze_data.yaw, eyegaze_data.pitch\n    gaze_vector_device = np.array([\n        np.cos(pitch) * np.sin(yaw),\n        np.sin(pitch),\n        np.cos(pitch) * np.cos(yaw)\n    ])\n\n    # Transform to camera coordinate system\n    T_device_camera = camera_calib.get_transform_device_camera()\n    gaze_vector_camera = T_device_camera.inverse().rotationMatrix() @ gaze_vector_device\n\n    # Project to image coordinates (assuming gaze origin at camera center)\n    gaze_distance = 2.0  # meters\n    gaze_point_camera = gaze_vector_camera * gaze_distance\n    gaze_pixel = camera_calib.project(gaze_point_camera)\n\n    # Visualize gaze point on image\n    if camera_calib.is_valid_projection(gaze_pixel):\n        rr.log(\n            f"{camera_label}/eyegaze",\n            rr.Points2D(\n                positions=[gaze_pixel],\n                colors=[255, 0, 0],  # Red color\n                radii=[5.0]\n            )\n        )\n\n# Example usage in visualization loop\nrr.init("rerun_viz_eyegaze")\n\nif eyegaze_stream_id is not None:\n    rgb_stream_id = vrs_data_provider.get_stream_id_from_label("camera-rgb")\n    rgb_camera_calib = device_calib.get_camera_calib("camera-rgb")\n\n    # Visualize first few frames with eye-tracking data\n    for i in range(min(10, num_eyegaze_samples)):\n        eyegaze_data = vrs_data_provider.get_eyegaze_data_by_index(eyegaze_stream_id, i)\n\n        # Find closest RGB frame\n        eyegaze_timestamp_ns = int(eyegaze_data.tracking_timestamp.total_seconds() * 1e9)\n        rgb_data, rgb_record = vrs_data_provider.get_image_data_by_time_ns(\n            rgb_stream_id, eyegaze_timestamp_ns, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST\n        )\n\n        if rgb_data.is_valid():\n            rr.set_time_nanos("device_time", rgb_record.capture_timestamp_ns)\n            rr.log("camera-rgb", rr.Image(rgb_data.to_numpy_array()))\n\n            # Overlay eye-tracking data\n            visualize_eyegaze_in_camera("camera-rgb", eyegaze_data, rgb_camera_calib, device_calib)\n\nrr.notebook_show()\n'})}),"\n",(0,i.jsx)(a.h2,{id:"accessing-hand-tracking-data",children:"Accessing Hand-tracking Data"}),"\n",(0,i.jsx)(a.h3,{id:"basic-hand-tracking-data-access",children:"Basic Hand-tracking Data Access"}),"\n",(0,i.jsx)(a.p,{children:"Hand-tracking provides 3D pose estimation for both hands, including joint positions and hand poses."}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{className:"language-python",children:'# Query HandTracking stream\nhandtracking_stream_id = vrs_data_provider.get_stream_id_from_label("handtracking")\n\nif handtracking_stream_id is None:\n    print("This VRS file does not contain on-device hand-tracking data.")\nelse:\n    print(f"Found hand-tracking stream: {handtracking_stream_id}")\n\n    # Get total number of hand-tracking samples\n    num_handtracking_samples = vrs_data_provider.get_num_data(handtracking_stream_id)\n    print(f"Total hand-tracking samples: {num_handtracking_samples}")\n\n    # Access hand-tracking data\n    print("\\nFirst few hand-tracking samples:")\n    for i in range(min(3, num_handtracking_samples)):\n        hand_pose_data = vrs_data_provider.get_hand_pose_data_by_index(handtracking_stream_id, i)\n\n        print(f"\\nSample {i}:")\n        print(f"\\tTimestamp: {hand_pose_data.tracking_timestamp}")\n\n        # Check left hand\n        if hand_pose_data.left_hand is not None:\n            print(f"\\tLeft hand detected:")\n            print(f"\\tConfidence: {hand_pose_data.left_hand.confidence}")\n            print(f"\\tNumber of landmarks: {len(hand_pose_data.left_hand.landmark_positions_device)}")\n        else:\n            print(f"\\tLeft hand: Not detected")\n\n        # Check right hand\n        if hand_pose_data.right_hand is not None:\n            print(f"\\tRight hand detected:")\n            print(f"\\tConfidence: {hand_pose_data.right_hand.confidence}")\n            print(f"\\tNumber of landmarks: {len(hand_pose_data.right_hand.landmark_positions_device)}")\n        else:\n            print(f"\\tRight hand: Not detected")\n'})}),"\n",(0,i.jsx)(a.h3,{id:"hand-tracking-data-structure",children:"Hand-tracking Data Structure"}),"\n",(0,i.jsx)(a.p,{children:"Hand-tracking data contains:"}),"\n",(0,i.jsxs)(a.ul,{children:["\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Tracking Timestamp"}),": When the hand-tracking measurement was taken"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Left/Right Hand Data"}),": Each hand (when detected) includes:","\n",(0,i.jsxs)(a.ul,{children:["\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Confidence"}),": Detection confidence score"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Landmark Positions"}),": 3D positions of hand joints in device coordinate system"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Wrist Transform"}),": 6DOF pose of the wrist"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Palm Normal"}),": Normal vector of the palm"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(a.h3,{id:"interpolated-hand-tracking-data",children:"Interpolated Hand-tracking Data"}),"\n",(0,i.jsx)(a.p,{children:"Since hand-tracking and camera data may not be perfectly synchronized, Aria provides interpolated hand-tracking data that can be queried at arbitrary timestamps."}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{className:"language-python",children:'from projectaria_tools.core.sensor_data import SensorDataType, TimeDomain, TimeQueryOptions\nfrom datetime import timedelta\n\nprint("\\n=== Demonstrating query interpolated hand tracking results ===")\n\n# Demonstrate how to query interpolated handtracking results\nslam_stream_id = vrs_data_provider.get_stream_id_from_label("slam-front-left")\nrgb_stream_id = vrs_data_provider.get_stream_id_from_label("camera-rgb")\n\n# Retrieve a SLAM frame, use its timestamp as query\nslam_sample_index = min(10, vrs_data_provider.get_num_data(slam_stream_id) - 1)\nslam_data_and_record = vrs_data_provider.get_image_data_by_index(slam_stream_id, slam_sample_index)\nslam_timestamp_ns = slam_data_and_record[1].capture_timestamp_ns\n\n# Retrieve the closest RGB frame\nrgb_data_and_record = vrs_data_provider.get_image_data_by_time_ns(\n    rgb_stream_id, slam_timestamp_ns, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST\n)\nrgb_timestamp_ns = rgb_data_and_record[1].capture_timestamp_ns\n\n# Retrieve the closest hand tracking data sample\nraw_ht_data = vrs_data_provider.get_hand_pose_data_by_time_ns(\n    handtracking_stream_id, slam_timestamp_ns, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST\n)\nraw_ht_timestamp_ns = (raw_ht_data.tracking_timestamp // timedelta(microseconds=1)) * 1000\n\n# Check if hand tracking aligns with RGB or SLAM data\nprint(f"SLAM timestamp: {slam_timestamp_ns}")\nprint(f"RGB timestamp:  {rgb_timestamp_ns}")\nprint(f"hand tracking timestamp:   {raw_ht_timestamp_ns}")\nprint(f"hand tracking-SLAM time diff: {abs(raw_ht_timestamp_ns - slam_timestamp_ns) / 1e6:.2f} ms")\nprint(f"hand tracking- RGB time diff: {abs(raw_ht_timestamp_ns - rgb_timestamp_ns) / 1e6:.2f} ms")\n\n# Now, query interpolated hand tracking data sample using RGB timestamp.\ninterpolated_ht_data = vrs_data_provider.get_interpolated_hand_pose_data(\n    handtracking_stream_id, rgb_timestamp_ns\n)\n\n# Check that interpolated hand tracking now aligns with RGB data\nif interpolated_ht_data is not None:\n    interpolated_ht_timestamp_ns = (interpolated_ht_data.tracking_timestamp// timedelta(microseconds=1)) * 1000\n    print(f"Interpolated hand tracking timestamp: {interpolated_ht_timestamp_ns}")\n    print(f"Interpolated hand tracking-RGB time diff: {abs(interpolated_ht_timestamp_ns - rgb_timestamp_ns) / 1e6:.2f} ms")\nelse:\n    print("Interpolated hand tracking data is None - interpolation failed")\n'})}),"\n",(0,i.jsx)(a.h3,{id:"visualizing-hand-tracking-results-in-cameras",children:"Visualizing Hand-tracking Results in Cameras"}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{className:"language-python",children:'import rerun as rr\nfrom projectaria_tools.utils.rerun_helpers import create_hand_skeleton_from_landmarks\n\ndef plot_single_hand_in_camera(hand_joints_in_device, camera_label, camera_calib, hand_label):\n    """\n    A helper function to plot a single hand data in 2D camera view\n    """\n    # Setting different marker plot sizes for RGB and SLAM since they have different resolutions\n    plot_ratio = 3.0 if camera_label == "camera-rgb" else 1.0\n    marker_color = [255,64,0] if hand_label == "left" else [255, 255, 0]\n\n    # project into camera frame, and also create line segments\n    hand_joints_in_camera = []\n    for pt_in_device in hand_joints_in_device:\n        pt_in_camera = (\n            camera_calib.get_transform_device_camera().inverse() @ pt_in_device\n        )\n        pixel = camera_calib.project(pt_in_camera)\n        hand_joints_in_camera.append(pixel)\n\n    # Create hand skeleton in 2D image space\n    hand_skeleton = create_hand_skeleton_from_landmarks(hand_joints_in_camera)\n\n    # Remove "None" markers from hand joints in camera. This is intentionally done AFTER the hand skeleton creation\n    hand_joints_in_camera = list(\n        filter(lambda x: x is not None, hand_joints_in_camera)\n    )\n\n    rr.log(\n        f"{camera_label}/{hand_label}/landmarks",\n        rr.Points2D(\n            positions=hand_joints_in_camera,\n            colors= marker_color,\n            radii= [3.0 * plot_ratio]\n        ),\n    )\n    rr.log(\n        f"{camera_label}/{hand_label}/skeleton",\n        rr.LineStrips2D(\n            hand_skeleton,\n            colors=[0, 255, 0],\n            radii= [0.5 * plot_ratio],\n        ),\n    )\n\ndef plot_handpose_in_camera(hand_pose, camera_label, camera_calib):\n    """\n    A helper function to plot hand tracking results into a camera image\n    """\n    # Plot both hands\n    if hand_pose.left_hand is not None:\n        plot_single_hand_in_camera(\n            hand_joints_in_device=hand_pose.left_hand.landmark_positions_device,\n            camera_label=camera_label,\n            camera_calib = camera_calib,\n            hand_label="left")\n    if hand_pose.right_hand is not None:\n        plot_single_hand_in_camera(\n            hand_joints_in_device=hand_pose.right_hand.landmark_positions_device,\n            camera_label=camera_label,\n            camera_calib = camera_calib,\n            hand_label="right")\n\nprint("\\n=== Visualizing on-device hand tracking in camera images ===")\n\n# First, query the RGB camera stream id\ndevice_calib = vrs_data_provider.get_device_calibration()\nrgb_camera_label = "camera-rgb"\nslam_camera_labels = ["slam-front-left", "slam-front-right", "slam-side-left", "slam-side-right"]\nrgb_stream_id = vrs_data_provider.get_stream_id_from_label(rgb_camera_label)\nslam_stream_ids = [vrs_data_provider.get_stream_id_from_label(label) for label in slam_camera_labels]\n\nrr.init("rerun_viz_ht_in_cameras")\n\n# Set up a sensor queue with only RGB images.\n# Handtracking data will be queried with interpolated API.\ndeliver_options = vrs_data_provider.get_default_deliver_queued_options()\ndeliver_options.deactivate_stream_all()\nfor stream_id in slam_stream_ids + [rgb_stream_id]:\n    deliver_options.activate_stream(stream_id)\n\n# Play for only 3 seconds\ntotal_length_ns = vrs_data_provider.get_last_time_ns_all_streams(TimeDomain.DEVICE_TIME) - vrs_data_provider.get_first_time_ns_all_streams(TimeDomain.DEVICE_TIME)\nskip_begin_ns = int(15 * 1e9) # Skip 15 seconds\nduration_ns = int(3 * 1e9) # 3 seconds\nskip_end_ns = max(total_length_ns - skip_begin_ns - duration_ns, 0)\ndeliver_options.set_truncate_first_device_time_ns(skip_begin_ns)\ndeliver_options.set_truncate_last_device_time_ns(skip_end_ns)\n\n# Plot image data, and overlay hand tracking data\nfor sensor_data in vrs_data_provider.deliver_queued_sensor_data(deliver_options):\n    # --\n    # Only image data will be obtained.\n    # --\n    device_time_ns = sensor_data.get_time_ns(TimeDomain.DEVICE_TIME)\n    image_data_and_record = sensor_data.image_data_and_record()\n    stream_id = sensor_data.stream_id()\n    camera_label = vrs_data_provider.get_label_from_stream_id(stream_id)\n    camera_calib = device_calib.get_camera_calib(camera_label)\n\n\n    # Visualize the RGB images.\n    rr.set_time_nanos("device_time", device_time_ns)\n    rr.log(f"{camera_label}", rr.Image(image_data_and_record[0].to_numpy_array()))\n\n    # Query and plot interpolated hand tracking result\n    interpolated_hand_pose = vrs_data_provider.get_interpolated_hand_pose_data(handtracking_stream_id, device_time_ns, TimeDomain.DEVICE_TIME)\n    if interpolated_hand_pose is not None:\n        plot_handpose_in_camera(hand_pose = interpolated_hand_pose, camera_label = camera_label, camera_calib = camera_calib)\n\n# Wait for rerun to buffer 1 second of data\nimport time\ntime.sleep(1)\n\nrr.notebook_show()\n'})}),"\n",(0,i.jsx)(a.h2,{id:"understanding-interpolation",children:"Understanding Interpolation"}),"\n",(0,i.jsx)(a.p,{children:"Hand-tracking interpolation is crucial for synchronizing hand data with camera frames:"}),"\n",(0,i.jsxs)(a.ol,{children:["\n",(0,i.jsxs)(a.li,{children:["\n",(0,i.jsxs)(a.p,{children:[(0,i.jsx)(a.strong,{children:"Why Interpolation is Needed"}),": Hand-tracking algorithms may run at different frequencies than cameras, leading to temporal misalignment."]}),"\n"]}),"\n",(0,i.jsxs)(a.li,{children:["\n",(0,i.jsxs)(a.p,{children:[(0,i.jsx)(a.strong,{children:"Interpolation Algorithm"}),": The system uses linear interpolation for 3D positions and SE3 interpolation for poses."]}),"\n"]}),"\n",(0,i.jsxs)(a.li,{children:["\n",(0,i.jsxs)(a.p,{children:[(0,i.jsx)(a.strong,{children:"Interpolation Rules"}),":"]}),"\n",(0,i.jsxs)(a.ul,{children:["\n",(0,i.jsx)(a.li,{children:"Both hands must be valid in both before/after samples for interpolation to work"}),"\n",(0,i.jsxs)(a.li,{children:["If either hand is missing in either sample, the interpolated result for that hand will be ",(0,i.jsx)(a.code,{children:"None"})]}),"\n",(0,i.jsxs)(a.li,{children:["Single-hand interpolation includes:","\n",(0,i.jsxs)(a.ul,{children:["\n",(0,i.jsx)(a.li,{children:"Linear interpolation on 3D hand landmark positions"}),"\n",(0,i.jsx)(a.li,{children:"SE3 interpolation on wrist 3D pose"}),"\n",(0,i.jsx)(a.li,{children:"Re-calculated wrist and palm normal vectors"}),"\n",(0,i.jsx)(a.li,{children:"Minimum confidence values"}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(a.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(a.p,{children:"This tutorial covered accessing and visualizing on-device eye-tracking and hand-tracking data:"}),"\n",(0,i.jsxs)(a.ul,{children:["\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Eye-tracking Data"}),": Access gaze direction information and project onto camera images"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Hand-tracking Data"}),": Access 3D hand pose data including joint positions and confidence scores"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Interpolated Data"}),": Use interpolated hand-tracking for better temporal alignment with camera data"]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Visualization"}),": Project MP data onto 2D camera images for analysis and debugging"]}),"\n"]}),"\n",(0,i.jsx)(a.p,{children:"These on-device MP algorithms provide real-time insights into user behavior and can be combined with other sensor data for comprehensive analysis of user interactions and movements."})]})}function m(e={}){const{wrapper:a}={...(0,r.R)(),...e.components};return a?(0,i.jsx)(a,{...e,children:(0,i.jsx)(_,{...e})}):_(e)}},69470(e,a,n){n.d(a,{A:()=>i});n(96540);var t=n(74848);const i=({notebookUrl:e,colabUrl:a,colabDisabled:n=!1})=>{const i=()=>(0,t.jsx)("svg",{style:{marginRight:"8px",width:"16px",height:"16px"},fill:"currentColor",viewBox:"0 0 24 24",children:(0,t.jsx)("path",{d:"M16.9414 4.9757a7.033 7.033 0 0 0-4.9308 2.0324 7.033 7.033 0 0 0-.1232 9.8068l2.395-2.395a3.6455 3.6455 0 0 1 5.1497-5.1478l2.397-2.3989a7.033 7.033 0 0 0-4.8877-1.9175zM7.07 4.9855a7.033 7.033 0 0 0-4.8878 1.9175l2.395 2.3989a3.6434 3.6434 0 0 1 5.1497 5.1478l2.395 2.395a7.033 7.033 0 0 0-.1232-9.8068A7.033 7.033 0 0 0 7.07 4.9855zm15.0191 2.5923l-2.395 2.395a3.6455 3.6455 0 0 1-5.1497 5.1497l-2.395 2.395a7.033 7.033 0 0 0 9.9397-.123z"})});return(0,t.jsxs)("div",{style:{display:"flex",gap:"10px",marginBottom:"20px",flexWrap:"wrap"},children:[(0,t.jsxs)("a",{href:e,target:"_blank",rel:"noopener noreferrer",style:{display:"inline-flex",alignItems:"center",padding:"8px 16px",backgroundColor:"#f6f8fa",border:"1px solid #d0d7de",borderRadius:"6px",textDecoration:"none",color:"#24292f",fontSize:"14px",fontWeight:"500",transition:"background-color 0.2s"},onMouseEnter:e=>{e.target.style.backgroundColor="#f3f4f6"},onMouseLeave:e=>{e.target.style.backgroundColor="#f6f8fa"},children:[(0,t.jsx)("svg",{style:{marginRight:"8px",width:"16px",height:"16px"},fill:"currentColor",viewBox:"0 0 16 16",children:(0,t.jsx)("path",{d:"M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z"})}),"View Notebook on GitHub"]}),!n&&a?(0,t.jsxs)("a",{href:a,target:"_blank",rel:"noopener noreferrer",style:{display:"inline-flex",alignItems:"center",padding:"8px 16px",backgroundColor:"#f9ab00",border:"1px solid #d0d7de",borderRadius:"6px",textDecoration:"none",color:"#fff",fontSize:"14px",fontWeight:"500",transition:"background-color 0.2s"},onMouseEnter:e=>{e.target.style.backgroundColor="#e8a500"},onMouseLeave:e=>{e.target.style.backgroundColor="#f9ab00"},children:[(0,t.jsx)(i,{}),"Run in Google Colab"]}):(0,t.jsxs)("button",{onClick:()=>{alert("Google Colab integration is not available yet. Please download the notebook and run it locally.")},style:{display:"inline-flex",alignItems:"center",padding:"8px 16px",backgroundColor:"#f6f8fa",border:"1px solid #d0d7de",borderRadius:"6px",color:"#656d76",fontSize:"14px",fontWeight:"500",cursor:"not-allowed",transition:"background-color 0.2s"},children:[(0,t.jsx)(i,{}),"Colab (Coming Soon)"]})]})}},28453(e,a,n){n.d(a,{R:()=>s,x:()=>d});var t=n(96540);const i={},r=t.createContext(i);function s(e){const a=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(a):{...a,...e}},[a,e])}function d(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),t.createElement(r.Provider,{value:a},e.children)}}}]);