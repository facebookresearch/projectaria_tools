"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[168],{52718(n,e,i){i.r(e),i.d(e,{assets:()=>d,contentTitle:()=>l,default:()=>_,frontMatter:()=>a,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"projectariatools/pythontutorials/mps","title":"Loading and Visualizing MPS Output Data","description":"<TutorialButtons","source":"@site/docs-research-tools/projectariatools/pythontutorials/mps.mdx","sourceDirName":"projectariatools/pythontutorials","slug":"/projectariatools/pythontutorials/mps","permalink":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/mps","draft":false,"unlisted":false,"editUrl":"https://www.internalfb.com/code/fbsource/arvr/projects/ariane/aria_research_kit/projectaria_tools/website/docs-research-tools/projectariatools/pythontutorials/mps.mdx","tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"sidebar_position":7,"title":"Loading and Visualizing MPS Output Data"},"sidebar":"researchToolsSidebar","previous":{"title":"Timestamp Alignment in Aria Gen2","permalink":"/projectaria_tools/gen2/research-tools/projectariatools/pythontutorials/time-sync"},"next":{"title":"Advanced Installation From Source Code","permalink":"/projectaria_tools/gen2/research-tools/projectariatools/advanced-installation"}}');var o=i(74848),s=i(28453),r=i(69470);const a={sidebar_position:7,title:"Loading and Visualizing MPS Output Data"},l="Tutorial 7: Loading and Visualizing MPS Output Data",d={},c=[{value:"Introduction",id:"introduction",level:2},{value:"MPS SLAM Trajectories",id:"mps-slam-trajectories",level:2},{value:"Understanding Open Loop vs Closed Loop Trajectories",id:"understanding-open-loop-vs-closed-loop-trajectories",level:3},{value:"Loading Closed Loop Trajectory",id:"loading-closed-loop-trajectory",level:3},{value:"Loading Open Loop Trajectory",id:"loading-open-loop-trajectory",level:3},{value:"MPS Semi-dense Point Cloud and Observations",id:"mps-semi-dense-point-cloud-and-observations",level:2},{value:"Understanding Point Cloud Data",id:"understanding-point-cloud-data",level:3},{value:"Loading Semi-dense Point Cloud",id:"loading-semi-dense-point-cloud",level:3},{value:"Loading Point Observations",id:"loading-point-observations",level:3},{value:"MPS Hand Tracking",id:"mps-hand-tracking",level:2},{value:"Hand Tracking Outputs",id:"hand-tracking-outputs",level:3},{value:"Loading Hand Tracking Results",id:"loading-hand-tracking-results",level:3},{value:"Visualizing MPS Results",id:"visualizing-mps-results",level:2},{value:"Color Mapping Helper Function",id:"color-mapping-helper-function",level:3},{value:"Preparing SLAM Data for Visualization",id:"preparing-slam-data-for-visualization",level:3},{value:"Prepare Hand Tracking Data",id:"prepare-hand-tracking-data",level:3},{value:"3D Visualization with Rerun",id:"3d-visualization-with-rerun",level:3},{value:"Understanding MPS Data Structures",id:"understanding-mps-data-structures",level:2},{value:"Trajectory Data Types",id:"trajectory-data-types",level:3},{value:"ClosedLoopTrajectoryPose",id:"closedlooptrajectorypose",level:4},{value:"OpenLoopTrajectoryPose",id:"openlooptrajectorypose",level:4},{value:"Point Cloud Data Types",id:"point-cloud-data-types",level:3},{value:"GlobalPointPosition",id:"globalpointposition",level:4},{value:"PointObservation",id:"pointobservation",level:4},{value:"MPS vs On-Device Comparisons",id:"mps-vs-on-device-comparisons",level:2},{value:"Key Differences",id:"key-differences",level:3},{value:"Use Cases",id:"use-cases",level:3},{value:"Summary",id:"summary",level:2}];function p(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"tutorial-7-loading-and-visualizing-mps-output-data",children:"Tutorial 7: Loading and Visualizing MPS Output Data"})}),"\n",(0,o.jsx)(r.A,{notebookUrl:"https://github.com/facebookresearch/projectaria_tools/blob/2.1.0/examples/Gen2/python_notebooks/Tutorial_7_mps_data_provider_basics.ipynb",colabUrl:"https://colab.research.google.com/github/facebookresearch/projectaria_tools/blob/2.1.0/examples/Gen2/python_notebooks/Tutorial_7_mps_data_provider_basics.ipynb"}),"\n",(0,o.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,o.jsxs)(e.p,{children:["This tutorial demonstrates how to access and visualize ",(0,o.jsx)(e.strong,{children:"Machine Perception Services (MPS)"})," results. MPS provides cloud-based processing of Aria data to generate high-quality 3D reconstruction, SLAM trajectories, and other perception outputs."]}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"What you'll learn:"})}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"How to load and access MPS SLAM trajectory data (open loop and closed loop)"}),"\n",(0,o.jsx)(e.li,{children:"How to load and visualize MPS semi-dense point clouds and observations"}),"\n",(0,o.jsx)(e.li,{children:"How to load and access MPS hand tracking results"}),"\n",(0,o.jsx)(e.li,{children:"How to create 3D visualizations of MPS results"}),"\n"]}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"Prerequisites"})}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Complete Tutorial 1 (VrsDataProvider Basics) to understand basic data provider concepts"}),"\n",(0,o.jsx)(e.li,{children:"Complete Tutorial 2 (Device Calibration) to understand how to properly use calibration in Aria data."}),"\n",(0,o.jsxs)(e.li,{children:["Download Aria Gen2 sample data: ",(0,o.jsx)(e.a,{href:"https://www.projectaria.com/async/sample/download/?bucket=core&filename=aria_gen2_sample_data_1.vrs",children:"VRS"})," and ",(0,o.jsx)(e.a,{href:"https://www.projectaria.com/async/sample/download/?bucket=core&filename=aria_gen2_sample_data_1_mps_output.zip",children:"MPS output zip file"}),"."]}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Note on Visualization"}),"\nIf visualization window is not showing up, this is due to ",(0,o.jsx)(e.code,{children:"Rerun"})," lib's caching issue. Just rerun the specific code cell."]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import os\nfrom projectaria_tools.core import mps\n\n# Set up paths to your MPS data\nmps_folder_path = "path/to/your/mps/folder/"\nvrs_file_path = "path/to/your/recording.vrs"\n\n# Load VRS data provider for additional context\nvrs_data_provider = data_provider.create_vrs_data_provider(vrs_file_path)\n'})}),"\n",(0,o.jsx)(e.h2,{id:"mps-slam-trajectories",children:"MPS SLAM Trajectories"}),"\n",(0,o.jsx)(e.h3,{id:"understanding-open-loop-vs-closed-loop-trajectories",children:"Understanding Open Loop vs Closed Loop Trajectories"}),"\n",(0,o.jsxs)(e.p,{children:["MPS SLAM algorithm outputs 2 trajectory files (see ",(0,o.jsx)(e.a,{href:"https://facebookresearch.github.io/projectaria_tools/docs/data_formats/mps/slam/mps_trajectory",children:"wiki page"})," for data type definitions):"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Open loop trajectory"}),": High-frequency (1kHz) odometry from visual-inertial odometry (VIO), accurate over short periods but drifts over time and distance."]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Closed loop trajectory"}),": High-frequency (1kHz) pose from mapping with loop closure corrections, reducing drift but possibly less accurate locally over short spans."]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"loading-closed-loop-trajectory",children:"Loading Closed Loop Trajectory"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'from projectaria_tools.core.mps.utils import (\n    filter_points_from_confidence,\n    get_nearest_pose,\n)\n\nprint("=== MPS - Closed loop trajectory ===")\n\n# Load MPS closed-loop trajectory data\nclosed_loop_trajectory_file = os.path.join(\n    mps_folder_path, "slam", "closed_loop_trajectory.csv"\n)\nclosed_loop_trajectory = mps.read_closed_loop_trajectory(closed_loop_trajectory_file)\n\n# Print out the content of the first sample in closed_loop_trajectory\nif closed_loop_trajectory:\n    sample = closed_loop_trajectory[0]\n    print("ClosedLoopTrajectoryPose sample:")\n    print(f"  tracking_timestamp: {int(sample.tracking_timestamp.total_seconds() * 1e6)} us")\n    print(f"  utc_timestamp: {int(sample.utc_timestamp.total_seconds() * 1e6)} us")\n    print(f"  transform_world_device:\\n{sample.transform_world_device}")\n    print(f"  device_linear_velocity_device: {sample.device_linear_velocity_device}")\n    print(f"  angular_velocity_device: {sample.angular_velocity_device}")\n    print(f"  quality_score: {sample.quality_score}")\n    print(f"  gravity_world: {sample.gravity_world}")\n    print(f"  graph_uid: {sample.graph_uid}")\nelse:\n    print("closed_loop_trajectory is empty.")\n'})}),"\n",(0,o.jsx)(e.h3,{id:"loading-open-loop-trajectory",children:"Loading Open Loop Trajectory"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'print("=== MPS - Open loop trajectory ===")\n\n# Load MPS open-loop trajectory data\nopen_loop_trajectory_file = os.path.join(\n    mps_folder_path, "slam", "open_loop_trajectory.csv"\n)\nopen_loop_trajectory = mps.read_open_loop_trajectory(open_loop_trajectory_file)\n\n# Print out the content of the first sample in open_loop_trajectory\nif open_loop_trajectory:\n    sample = open_loop_trajectory[0]\n    print("OpenLoopTrajectoryPose sample:")\n    print(f"  tracking_timestamp: {int(sample.tracking_timestamp.total_seconds() * 1e6)} us")\n    print(f"  utc_timestamp: {int(sample.utc_timestamp.total_seconds() * 1e6)} us")\n    print(f"  transform_odometry_device:\\n{sample.transform_odometry_device}")\n    print(f"  device_linear_velocity_odometry: {sample.device_linear_velocity_odometry}")\n    print(f"  angular_velocity_device: {sample.angular_velocity_device}")\n    print(f"  quality_score: {sample.quality_score}")\n    print(f"  gravity_odometry: {sample.gravity_odometry}")\n    print(f"  session_uid: {sample.session_uid}")\nelse:\n    print("open_loop_trajectory is empty.")\n'})}),"\n",(0,o.jsx)(e.h2,{id:"mps-semi-dense-point-cloud-and-observations",children:"MPS Semi-dense Point Cloud and Observations"}),"\n",(0,o.jsx)(e.h3,{id:"understanding-point-cloud-data",children:"Understanding Point Cloud Data"}),"\n",(0,o.jsxs)(e.p,{children:["MPS SLAM algorithm outputs 2 files related to semi-dense point cloud (see ",(0,o.jsx)(e.a,{href:"https://facebookresearch.github.io/projectaria_tools/docs/data_formats/mps/slam/mps_pointcloud",children:"wiki page"})," for data type definitions):"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.code,{children:"semidense_points.csv.gz"}),": Global points in the world coordinate frame."]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.code,{children:"semidense_observations.csv.gz"}),": Point observations for each camera, at each timestamp."]}),"\n"]}),"\n",(0,o.jsx)(e.p,{children:"Note that semidense point files are normally large, therefore loading them may take some time."}),"\n",(0,o.jsx)(e.h3,{id:"loading-semi-dense-point-cloud",children:"Loading Semi-dense Point Cloud"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'print("=== MPS - Semi-dense Point Cloud ===")\n\n# Load MPS semi-dense point cloud data\nsemidense_points_file = os.path.join(\n    mps_folder_path, "slam", "semidense_points.csv.gz"\n)\nsemidense_points = mps.read_global_point_cloud(semidense_points_file)\n\n# Print out the content of the first sample in semidense_points\nif semidense_points:\n    sample = semidense_points[0]\n    print("GlobalPointPosition sample:")\n    print(f"  uid: {sample.uid}")\n    print(f"  graph_uid: {sample.graph_uid}")\n    print(f"  position_world: {sample.position_world}")\n    print(f"  inverse_distance_std: {sample.inverse_distance_std}")\n    print(f"  distance_std: {sample.distance_std}")\n    print(f"Total number of semi-dense points: {len(semidense_points)}")\nelse:\n    print("semidense_points is empty.")\n\n# Filter semidense points by inv_dep or depth.\n# The filter will KEEP points with (inv_dep or depth < threshold)\nfiltered_semidense_points = filter_points_from_confidence(raw_points = semidense_points, threshold_invdep = 1e-3, threshold_dep = 5e-2)\nprint(f"Filtering semidense points from a total of {len(semidense_points)} points down to {len(filtered_semidense_points)}")\n'})}),"\n",(0,o.jsx)(e.h3,{id:"loading-point-observations",children:"Loading Point Observations"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'print("=== MPS - Semi-dense Point Observations ===")\n\n# Load MPS semi-dense point observations data\nsemidense_observations_file = os.path.join(\n    mps_folder_path, "slam", "semidense_observations.csv.gz"\n)\nsemidense_observations = mps.read_point_observations(semidense_observations_file)\n\n# Print out the content of the first sample in semidense_observations\nif semidense_observations:\n    sample = semidense_observations[0]\n    print("PointObservation sample:")\n    print(f"  point_uid: {sample.point_uid}")\n    print(f"  frame_capture_timestamp: {int(sample.frame_capture_timestamp.total_seconds() * 1e6)} us")\n    print(f"  camera_serial: {sample.camera_serial}")\n    print(f"  uv: {sample.uv}")\n    print(f"Total number of point observations: {len(semidense_observations)}")\nelse:\n    print("semidense_observations is empty.")\n'})}),"\n",(0,o.jsx)(e.h2,{id:"mps-hand-tracking",children:"MPS Hand Tracking"}),"\n",(0,o.jsx)(e.p,{children:"MPS Hand Tracking augments each sequence with offline hand pose estimates, adding left and right hand tracking results wherever they are detected."}),"\n",(0,o.jsx)(e.h3,{id:"hand-tracking-outputs",children:"Hand Tracking Outputs"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"21 landmarks"})," per detected hand expressed in the device coordinate frame"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Wrist-to-device transform"})," providing the full 6DoF wrist pose"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Palm and wrist normals"})," for reasoning about hand orientation"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Confidence scores"})," indicating tracking quality for each hand"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"loading-hand-tracking-results",children:"Loading Hand Tracking Results"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'print("=== MPS - Hand Tracking ===")\n\nhand_tracking_results_file = os.path.join(\n    mps_folder_path, "hand_tracking", "hand_tracking_results.csv"\n)\nhand_tracking_results = mps.hand_tracking.read_hand_tracking_results(\n    hand_tracking_results_file\n)\n\nif hand_tracking_results:\n    if len(hand_tracking_results) > 10:\n        sample = hand_tracking_results[10] # get stable hand tracking result, since first hand tracking result in example vrs might be empty\n    else:\n        sample = hand_tracking_results[0]\n    sample_ts_us = int(sample.tracking_timestamp.total_seconds() * 1e6)\n    print(f"Sample tracking timestamp: {sample_ts_us} us")\n    print(f"Total number of hand tracking results: {len(hand_tracking_results)}")\n\n    for handedness, hand in (("Left", sample.left_hand), ("Right", sample.right_hand)):\n        if hand is None:\n            print(f"  {handedness} hand: not available in this sample")\n            continue\n\n        landmarks = hand.landmark_positions_device\n        landmark_count = len(landmarks) if landmarks is not None else 0\n        wrist_position = hand.get_wrist_position_device()\n        palm_position = hand.get_palm_position_device()\n        print(f"  {handedness} hand confidence: {hand.confidence:.2f}")\n        print(f"  {handedness} hand landmark count: {landmark_count}")\n        print(f"  {handedness} wrist position (device frame): {wrist_position}")\n        print(f"  {handedness} palm position (device frame): {palm_position}")\nelse:\n    print("hand_tracking_results is empty.")\n'})}),"\n",(0,o.jsx)(e.h2,{id:"visualizing-mps-results",children:"Visualizing MPS Results"}),"\n",(0,o.jsx)(e.p,{children:"In the following snippets, we combine the MPS SLAM outputs and hand tracking results to build an interactive 3D visualization in Rerun."}),"\n",(0,o.jsx)(e.h3,{id:"color-mapping-helper-function",children:"Color Mapping Helper Function"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import numpy as np\n\n# A helper coloring function\ndef color_from_zdepth(z_depth_m: float) -> np.ndarray:\n    """\n    Map z-depth (meters, along the camera\'s forward axis) to a bright Viridis-like RGB color.\n    - If z_depth_m <= 0 (point is behind the camera), return black [0, 0, 0].\n    - Near (0.2 m) -> yellow, Far (5.0 m) -> purple.\n    Returns an array of shape (3,) with dtype=uint8.\n    """\n    if not np.isfinite(z_depth_m) or z_depth_m <= 0.0:\n        return np.array([0, 0, 0], dtype=np.uint8)\n\n    NEAR_METERS, FAR_METERS = 0.2, 5.0\n\n    # Normalize to [0,1], then flip so near \u2192 bright (yellow), far \u2192 dark (purple)\n    clamped = min(max(float(z_depth_m), NEAR_METERS), FAR_METERS)\n    normalized_position = (clamped - NEAR_METERS) / (FAR_METERS - NEAR_METERS + 1e-12)\n    gradient_position = 1.0 - normalized_position\n\n    # Viridis-like anchor colors: purple \u2192 blue \u2192 teal \u2192 green \u2192 yellow\n    color_stops = [\n        (68, 1, 84),\n        (59, 82, 139),\n        (33, 145, 140),\n        (94, 201, 98),\n        (253, 231, 37),\n    ]\n\n    # Locate segment and blend between its endpoints\n    segment_count = len(color_stops) - 1\n    continuous_index = gradient_position * segment_count\n    lower_segment_index = int(continuous_index)\n\n    if lower_segment_index >= segment_count:\n        red, green, blue = color_stops[-1]\n    else:\n        segment_fraction = continuous_index - lower_segment_index\n        r0, g0, b0 = color_stops[lower_segment_index]\n        r1, g1, b1 = color_stops[lower_segment_index + 1]\n        red   = r0 + segment_fraction * (r1 - r0)\n        green = g0 + segment_fraction * (g1 - g0)\n        blue  = b0 + segment_fraction * (b1 - b0)\n\n    return np.array([int(red), int(green), int(blue)], dtype=np.uint8)\n'})}),"\n",(0,o.jsx)(e.h3,{id:"preparing-slam-data-for-visualization",children:"Preparing SLAM Data for Visualization"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'from collections import defaultdict\nfrom itertools import islice\nimport numpy as np\n\nprint("=== Preparing MPS SLAM results for visualization ===")\n\n# Check if we have valid SLAM data to visualize\nif not closed_loop_trajectory or not semidense_points:\n    raise RuntimeError("Warning: This tutorial requires valid MPS SLAM data to run.")\n\n# -----------\n# Prepare Trajectory data\n# -----------\n# Select a short segment of trajectory (e.g., first 5000 samples, subsampled by 50)\nsegment_length = min(50000, len(closed_loop_trajectory))\ntrajectory_segment = closed_loop_trajectory[:segment_length:50]\ntimestamp_to_pose = {\n    pose.tracking_timestamp: pose for pose in trajectory_segment\n}\nprint(f"Finished preparing a trajectory of length {len(trajectory_segment)}... ")\n\n# -----------\n# Prepare Semidense point data\n# -----------\n# Filter the semidense point cloud by confidence and limit max point count, and extract the point positions\nfiltered_semidense_point_cloud_data = filter_points_from_confidence(semidense_points)\npoints_positions = np.array(\n    [\n        point.position_world for point in filtered_semidense_point_cloud_data\n    ]\n)\nprint(f"Finished preparing filtered semidense points cloud of {len(filtered_semidense_point_cloud_data)} points... ")\n\n# -----------\n# Prepare Semidense observation data\n# -----------\n# Based on RGB observations, create a per-timestamp point position list, and color them according to distance from the RGB camera\npoint_uid_to_position = {\n    point.uid: np.array(point.position_world) for point in filtered_semidense_point_cloud_data\n}\n\n# A helper function that creates an easier-to-query mapping to obtain observations according to timestamps\nslam_1_serial = vrs_data_provider.get_device_calibration().get_camera_calib("slam-front-left").get_serial_number()\ntimestamp_to_point_positions = defaultdict(list)  # t_ns -> [position, position, ...]\ntimestamp_to_point_colors = defaultdict(list)  # t_ns -> [color, color, ...]\n\nfor obs in semidense_observations:\n    # Only add observations for SLAM_1 camera, and if the timestamp is in the chosen trajectory segment\n    if (\n        obs.camera_serial == slam_1_serial\n        and obs.frame_capture_timestamp in timestamp_to_pose\n        and obs.point_uid in point_uid_to_position\n    ):\n        # Insert point position\n        obs_timestamp = obs.frame_capture_timestamp\n        point_position = point_uid_to_position[obs.point_uid]\n        timestamp_to_point_positions[obs_timestamp].append(point_position)\n\n        # Insert point color\n        T_world_device = timestamp_to_pose[obs_timestamp].transform_world_device\n        point_in_device = T_world_device.inverse() @ point_position\n        point_z_depth = point_in_device.squeeze()[2]\n        point_color = color_from_zdepth(point_z_depth)\n        timestamp_to_point_colors[obs_timestamp].append(point_color)\n\nprint("Finished preparing semidense points observations: ")\nfor timestamp, points in islice(timestamp_to_point_positions.items(), 5):\n    print(f"\\t timestamp {int(timestamp.total_seconds() * 1e9)} ns has {len(points)} observed points in slam-front-left view. ")\nprint("\\t ...")\n'})}),"\n",(0,o.jsx)(e.h3,{id:"prepare-hand-tracking-data",children:"Prepare Hand Tracking Data"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'print("=== Preparing MPS Hand Tracking data ===")\n\nhand_tracking_results_segment: list[mps.hand_tracking.HandTrackingResult] = []\n\nif not hand_tracking_results:\n    print("No hand tracking results loaded; visualization will skip hands.")\nelse:\n    if trajectory_segment:\n        segment_start_ns = int(\n            trajectory_segment[0].tracking_timestamp.total_seconds() * 1e9\n        )\n        segment_end_ns = int(\n            trajectory_segment[-1].tracking_timestamp.total_seconds() * 1e9\n        )\n\n        hand_tracking_results_segment = [\n            result\n            for result in hand_tracking_results\n            if segment_start_ns\n            <= int(result.tracking_timestamp.total_seconds() * 1e9)\n            <= segment_end_ns\n        ]\n\n    if hand_tracking_results_segment:\n        print(\n            "Finished preparing hand tracking results: "\n            f"{len(hand_tracking_results_segment)} samples overlap with the trajectory segment."\n        )\n    else:\n        hand_tracking_results_segment = hand_tracking_results\n        print(\n            "Hand tracking results do not overlap with the selected trajectory segment; "\n            "visualization will reuse the nearest available hand pose."\n        )\n'})}),"\n",(0,o.jsx)(e.h3,{id:"3d-visualization-with-rerun",children:"3D Visualization with Rerun"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import rerun as rr\nimport numpy as np\nfrom projectaria_tools.utils.rerun_helpers import (\n    AriaGlassesOutline,\n    ToTransform3D,\n    ToBox3D,\n    create_hand_skeleton_from_landmarks,\n)\nfrom projectaria_tools.core.mps.utils import (\n    filter_points_from_confidence,\n    get_nearest_pose,\n)\n\nHAND_COLORS = {\n    "left": np.array([102, 204, 255], dtype=np.uint8),\n    "right": np.array([255, 153, 102], dtype=np.uint8),\n}\nHAND_LANDMARK_RADIUS = 1e-2\nHAND_KEYPOINT_RADIUS = 1.5e-2\nHAND_SKELETON_RADIUS = 7e-3\n\n\ndef log_hand_tracking_result(hand_result):\n    rr.log("world/device/hand-tracking", rr.Clear.recursive())\n    if hand_result is None:\n        return\n\n    for label in ("left", "right"):\n        hand = getattr(hand_result, f"{label}_hand")\n        if hand is None or hand.landmark_positions_device is None:\n            continue\n\n        landmarks = np.array(hand.landmark_positions_device, dtype=np.float32)\n        if landmarks.size == 0:\n            continue\n\n        color = HAND_COLORS[label]\n        colors = np.repeat(color[np.newaxis, :], landmarks.shape[0], axis=0)\n\n        rr.log(\n            f"world/device/hand-tracking/{label}/landmarks",\n            rr.Points3D(\n                positions=landmarks,\n                colors=colors,\n                radii=HAND_LANDMARK_RADIUS,\n            ),\n        )\n\n        skeleton = create_hand_skeleton_from_landmarks(landmarks)\n        if skeleton:\n            rr.log(\n                f"world/device/hand-tracking/{label}/skeleton",\n                rr.LineStrips3D(\n                    skeleton,\n                    colors=[HAND_COLORS[label].tolist()] * len(skeleton),\n                    radii=HAND_SKELETON_RADIUS,\n                ),\n            )\n\n        wrist = hand.get_wrist_position_device()\n        palm = hand.get_palm_position_device()\n        keypoints = [\n            np.array(position, dtype=np.float32)\n            for position in (wrist, palm)\n            if position is not None\n        ]\n        if keypoints:\n            rr.log(\n                f"world/device/hand-tracking/{label}/keypoints",\n                rr.Points3D(\n                    positions=keypoints,\n                    colors=np.repeat(color[np.newaxis, :], len(keypoints), axis=0),\n                    radii=HAND_KEYPOINT_RADIUS,\n                ),\n            )\n\n\nprint("=== Visualizing MPS Results in 3D ===")\n\n# Initialize Rerun\nrr.init("MPS Visualization")\n\n# Set up the 3D scene\nrr.log("world", rr.ViewCoordinates.RIGHT_HAND_Z_UP, static=True)\n\n# Log point cloud\nrr.log(\n    "world/semidense_points",\n    rr.Points3D(\n        positions=points_positions,\n        colors=[255, 255, 255, 125],\n        radii=0.001\n    ),\n    static=True\n)\n\n# Aria glass outline for visualization purpose\ndevice_calib = vrs_data_provider.get_device_calibration()\naria_glasses_point_outline = AriaGlassesOutline(\n    device_calib, use_cad_calib=True\n)\n\n# Plot Closed loop trajectory\nclosed_loop_traj_cached_full = []\nobservation_points_cached = None\nobservation_colors_cached = None\nhand_tracking_timestamps_ns = [\n    int(result.tracking_timestamp.total_seconds() * 1e9)\n    for result in hand_tracking_results_segment\n]\nhand_tracking_index = 0\nlatest_hand_tracking_result = None\n\nfor closed_loop_pose in trajectory_segment:\n    capture_timestamp_ns = int(closed_loop_pose.tracking_timestamp.total_seconds() * 1e9)\n    rr.set_time_nanos("device_time", capture_timestamp_ns)\n\n    if hand_tracking_results_segment:\n        while (\n            hand_tracking_index < len(hand_tracking_results_segment)\n            and hand_tracking_timestamps_ns[hand_tracking_index] <= capture_timestamp_ns\n        ):\n            latest_hand_tracking_result = hand_tracking_results_segment[hand_tracking_index]\n            hand_tracking_index += 1\n    else:\n        latest_hand_tracking_result = None\n\n    T_world_device = closed_loop_pose.transform_world_device\n\n    # Log device pose as a coordinate frame\n    rr.log(\n        "world/device",\n        ToTransform3D(\n            T_world_device,\n            axis_length=0.05,\n        ),\n    )\n\n    log_hand_tracking_result(latest_hand_tracking_result)\n\n    # Plot Aria glass outline\n    rr.log(\n        "world/device/glasses_outline",\n        rr.LineStrips3D(\n            aria_glasses_point_outline,\n            colors=[150,200,40],\n            radii=5e-3,\n        ),\n    )\n\n    # Plot gravity direction vector\n    rr.log(\n        "world/vio_gravity",\n        rr.Arrows3D(\n            origins=[T_world_device.translation()[0]],\n            vectors=[\n                closed_loop_pose.gravity_world * 1e-2\n            ],  # length converted from 9.8 meter -> 10 cm\n            colors=[101,67,33],\n            radii=5e-3,\n        ),\n        static=False,\n    )\n\n    # Update cached results for observations. Cache is needed because observation has a much lower freq than high-freq trajectory.\n    if closed_loop_pose.tracking_timestamp in timestamp_to_point_positions.keys():\n        observation_points_cached = timestamp_to_point_positions[closed_loop_pose.tracking_timestamp]\n        observation_colors_cached = timestamp_to_point_colors[closed_loop_pose.tracking_timestamp]\n    if observation_points_cached is not None:\n        rr.log(\n            "world/semidense_observations",\n            rr.Points3D(\n                positions=observation_points_cached,\n                colors=observation_colors_cached,\n                radii=0.01\n            ),\n            static=False\n        )\n\n    # Plot the entire VIO trajectory that are cached so far\n    closed_loop_traj_cached_full.append(T_world_device.translation()[0])\n    rr.log(\n        "world/vio_trajectory",\n        rr.LineStrips3D(\n            closed_loop_traj_cached_full,\n            colors=[173, 216, 255],\n            radii=5e-3,\n        ),\n        static=False,\n    )\n\nrr.notebook_show()\n'})}),"\n",(0,o.jsx)(e.h2,{id:"understanding-mps-data-structures",children:"Understanding MPS Data Structures"}),"\n",(0,o.jsx)(e.h3,{id:"trajectory-data-types",children:"Trajectory Data Types"}),"\n",(0,o.jsx)(e.h4,{id:"closedlooptrajectorypose",children:"ClosedLoopTrajectoryPose"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"tracking_timestamp"}),": Device timestamp when pose was computed"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"utc_timestamp"}),": UTC timestamp"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"transform_world_device"}),": 6DOF pose in world coordinate frame"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"device_linear_velocity_device"}),": Linear velocity in device frame"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"angular_velocity_device"}),": Angular velocity in device frame"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"quality_score"}),": Pose estimation quality (higher = better)"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"gravity_world"}),": Gravity vector in world frame"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"graph_uid"}),": Unique identifier for the pose graph"]}),"\n"]}),"\n",(0,o.jsx)(e.h4,{id:"openlooptrajectorypose",children:"OpenLoopTrajectoryPose"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"tracking_timestamp"}),": Device timestamp when pose was computed"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"utc_timestamp"}),": UTC timestamp"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"transform_odometry_device"}),": 6DOF pose in odometry coordinate frame"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"device_linear_velocity_odometry"}),": Linear velocity in odometry frame"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"angular_velocity_device"}),": Angular velocity in device frame"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"quality_score"}),": Pose estimation quality (higher = better)"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"gravity_odometry"}),": Gravity vector in odometry frame"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"session_uid"}),": Unique identifier for the session"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"point-cloud-data-types",children:"Point Cloud Data Types"}),"\n",(0,o.jsx)(e.h4,{id:"globalpointposition",children:"GlobalPointPosition"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"uid"}),": Unique identifier for the 3D point"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"graph_uid"}),": Identifier linking point to pose graph"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"position_world"}),": 3D position in world coordinate frame"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"inverse_distance_std"}),": Inverse distance standard deviation (quality metric)"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"distance_std"}),": Distance standard deviation (quality metric)"]}),"\n"]}),"\n",(0,o.jsx)(e.h4,{id:"pointobservation",children:"PointObservation"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"point_uid"}),": Links observation to 3D point"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"frame_capture_timestamp"}),": When the observation was captured"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"camera_serial"}),": Serial number of the observing camera"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"uv"}),": 2D pixel coordinates of the observation"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"mps-vs-on-device-comparisons",children:"MPS vs On-Device Comparisons"}),"\n",(0,o.jsx)(e.h3,{id:"key-differences",children:"Key Differences"}),"\n",(0,o.jsxs)(e.table,{children:[(0,o.jsx)(e.thead,{children:(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.th,{children:"Aspect"}),(0,o.jsx)(e.th,{children:"On-Device (VIO/SLAM)"}),(0,o.jsx)(e.th,{children:"MPS SLAM"})]})}),(0,o.jsxs)(e.tbody,{children:[(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.td,{children:(0,o.jsx)(e.strong,{children:"Processing"})}),(0,o.jsx)(e.td,{children:"Real-time during recording"}),(0,o.jsx)(e.td,{children:"Cloud-based post-processing"})]}),(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.td,{children:(0,o.jsx)(e.strong,{children:"Accuracy"})}),(0,o.jsx)(e.td,{children:"Good for real-time use"}),(0,o.jsx)(e.td,{children:"Higher accuracy with global optimization"})]}),(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.td,{children:(0,o.jsx)(e.strong,{children:"Frequency"})}),(0,o.jsx)(e.td,{children:"20Hz (VIO), 800Hz (high-freq)"}),(0,o.jsx)(e.td,{children:"1kHz (both open/closed loop)"})]}),(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.td,{children:(0,o.jsx)(e.strong,{children:"Drift"})}),(0,o.jsx)(e.td,{children:"Accumulates over time"}),(0,o.jsx)(e.td,{children:"Minimized with loop closure"})]}),(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.td,{children:(0,o.jsx)(e.strong,{children:"Point Cloud"})}),(0,o.jsx)(e.td,{children:"Not available"}),(0,o.jsx)(e.td,{children:"Dense semi-dense reconstructions"})]}),(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.td,{children:(0,o.jsx)(e.strong,{children:"Coordinate Frame"})}),(0,o.jsx)(e.td,{children:"Odometry frame"}),(0,o.jsx)(e.td,{children:"Global world frame"})]})]})]}),"\n",(0,o.jsx)(e.h3,{id:"use-cases",children:"Use Cases"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"On-Device Data"}),": Real-time applications, live feedback, immediate processing"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"MPS Data"}),": High-quality reconstruction, research analysis, detailed mapping"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(e.p,{children:"This tutorial covered the essential aspects of working with MPS data:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Trajectory Access"}),": Loading both open loop and closed loop trajectories"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Point Cloud Data"}),": Accessing semi-dense 3D reconstructions and observations"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Data Filtering"}),": Using confidence thresholds to improve point cloud quality"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Hand Tracking"}),": Loading offline MPS hand tracking results and inspecting landmarks, wrist poses, and confidences"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"3D Visualization"}),": Creating combined visualizations with trajectories, point clouds, and hand tracking results"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Data Structures"}),": Understanding the comprehensive MPS data formats"]}),"\n"]}),"\n",(0,o.jsx)(e.p,{children:"MPS provides high-quality, globally consistent 3D reconstructions that are ideal for:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Research Applications"}),": Detailed spatial analysis and mapping"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"3D Reconstruction"}),": High-fidelity environmental modeling"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Motion Analysis"}),": Accurate trajectory analysis without drift"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Multi-modal Studies"}),": Combining precise 3D data with sensor information"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Benchmarking"}),": Comparing against ground truth for algorithm development"]}),"\n"]})]})}function _(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(p,{...n})}):p(n)}},69470(n,e,i){i.d(e,{A:()=>o});i(96540);var t=i(74848);const o=({notebookUrl:n,colabUrl:e,colabDisabled:i=!1})=>{const o=()=>(0,t.jsx)("svg",{style:{marginRight:"8px",width:"16px",height:"16px"},fill:"currentColor",viewBox:"0 0 24 24",children:(0,t.jsx)("path",{d:"M16.9414 4.9757a7.033 7.033 0 0 0-4.9308 2.0324 7.033 7.033 0 0 0-.1232 9.8068l2.395-2.395a3.6455 3.6455 0 0 1 5.1497-5.1478l2.397-2.3989a7.033 7.033 0 0 0-4.8877-1.9175zM7.07 4.9855a7.033 7.033 0 0 0-4.8878 1.9175l2.395 2.3989a3.6434 3.6434 0 0 1 5.1497 5.1478l2.395 2.395a7.033 7.033 0 0 0-.1232-9.8068A7.033 7.033 0 0 0 7.07 4.9855zm15.0191 2.5923l-2.395 2.395a3.6455 3.6455 0 0 1-5.1497 5.1497l-2.395 2.395a7.033 7.033 0 0 0 9.9397-.123z"})});return(0,t.jsxs)("div",{style:{display:"flex",gap:"10px",marginBottom:"20px",flexWrap:"wrap"},children:[(0,t.jsxs)("a",{href:n,target:"_blank",rel:"noopener noreferrer",style:{display:"inline-flex",alignItems:"center",padding:"8px 16px",backgroundColor:"#f6f8fa",border:"1px solid #d0d7de",borderRadius:"6px",textDecoration:"none",color:"#24292f",fontSize:"14px",fontWeight:"500",transition:"background-color 0.2s"},onMouseEnter:n=>{n.target.style.backgroundColor="#f3f4f6"},onMouseLeave:n=>{n.target.style.backgroundColor="#f6f8fa"},children:[(0,t.jsx)("svg",{style:{marginRight:"8px",width:"16px",height:"16px"},fill:"currentColor",viewBox:"0 0 16 16",children:(0,t.jsx)("path",{d:"M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z"})}),"View Notebook on GitHub"]}),!i&&e?(0,t.jsxs)("a",{href:e,target:"_blank",rel:"noopener noreferrer",style:{display:"inline-flex",alignItems:"center",padding:"8px 16px",backgroundColor:"#f9ab00",border:"1px solid #d0d7de",borderRadius:"6px",textDecoration:"none",color:"#fff",fontSize:"14px",fontWeight:"500",transition:"background-color 0.2s"},onMouseEnter:n=>{n.target.style.backgroundColor="#e8a500"},onMouseLeave:n=>{n.target.style.backgroundColor="#f9ab00"},children:[(0,t.jsx)(o,{}),"Run in Google Colab"]}):(0,t.jsxs)("button",{onClick:()=>{alert("Google Colab integration is not available yet. Please download the notebook and run it locally.")},style:{display:"inline-flex",alignItems:"center",padding:"8px 16px",backgroundColor:"#f6f8fa",border:"1px solid #d0d7de",borderRadius:"6px",color:"#656d76",fontSize:"14px",fontWeight:"500",cursor:"not-allowed",transition:"background-color 0.2s"},children:[(0,t.jsx)(o,{}),"Colab (Coming Soon)"]})]})}},28453(n,e,i){i.d(e,{R:()=>r,x:()=>a});var t=i(96540);const o={},s=t.createContext(o);function r(n){const e=t.useContext(s);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function a(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:r(n.components),t.createElement(s.Provider,{value:e},n.children)}}}]);