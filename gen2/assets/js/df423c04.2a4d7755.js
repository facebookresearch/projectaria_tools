"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[2425],{49887(e,i,n){n.r(i),n.d(i,{assets:()=>l,contentTitle:()=>d,default:()=>h,frontMatter:()=>o,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"dataset/pilot/tutorials/vrs_loading","title":"Aria Gen2 Pilot Dataset Tutorial - VRS Data Loading","description":"<TutorialButtons","source":"@site/docs-research-tools/dataset/pilot/tutorials/vrs_loading.mdx","sourceDirName":"dataset/pilot/tutorials","slug":"/dataset/pilot/tutorials/vrs_loading","permalink":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/vrs_loading","draft":false,"unlisted":false,"editUrl":"https://www.internalfb.com/code/fbsource/arvr/projects/ariane/aria_research_kit/projectaria_tools/website/docs-research-tools/dataset/pilot/tutorials/vrs_loading.mdx","tags":[],"version":"current","sidebarPosition":0,"frontMatter":{"sidebar_position":0,"title":"Aria Gen2 Pilot Dataset Tutorial - VRS Data Loading"},"sidebar":"researchToolsSidebar","previous":{"title":"Download and Usage","permalink":"/projectaria_tools/gen2/research-tools/dataset/pilot/download"},"next":{"title":"Aria Gen2 Pilot Dataset Tutorial - MPS Data Loading","permalink":"/projectaria_tools/gen2/research-tools/dataset/pilot/tutorials/mps_loading"}}');var a=n(74848),r=n(28453),s=n(69470);const o={sidebar_position:0,title:"Aria Gen2 Pilot Dataset Tutorial - VRS Data Loading"},d=void 0,l={},c=[{value:"What You&#39;ll Learn",id:"what-youll-learn",level:2},{value:"Import Required Libraries",id:"import-required-libraries",level:2},{value:"Initialize Data Provider",id:"initialize-data-provider",level:2},{value:"Check Available Data Modalities",id:"check-available-data-modalities",level:2},{value:"Core Data Types",id:"core-data-types",level:3},{value:"Algorithm Outputs",id:"algorithm-outputs",level:3},{value:"VRS - Explore Available Streams",id:"vrs---explore-available-streams",level:2},{value:"Stream Discovery and Navigation",id:"stream-discovery-and-navigation",level:3},{value:"VRS - Device Calibration",id:"vrs---device-calibration",level:3},{value:"Sensor Data Query APIs",id:"sensor-data-query-apis",level:2},{value:"Query by Index",id:"query-by-index",level:3},{value:"Query by Timestamp: TimeDomain and TimeQueryOptions",id:"query-by-timestamp-timedomain-and-timequeryoptions",level:3},{value:"TimeDomain and TimeQueryOptions",id:"timedomain-and-timequeryoptions",level:4},{value:"TimeDomain Options",id:"timedomain-options",level:4},{value:"TimeQueryOptions",id:"timequeryoptions",level:4},{value:"On-Device Eye Tracking and Hand Tracking",id:"on-device-eye-tracking-and-hand-tracking",level:2},{value:"Eye Tracking Data",id:"eye-tracking-data",level:3},{value:"EyeGaze Data Structure",id:"eyegaze-data-structure",level:4},{value:"EyeGaze API Reference",id:"eyegaze-api-reference",level:4},{value:"Eye Tracking Visualization",id:"eye-tracking-visualization",level:4},{value:"Hand Tracking Data",id:"hand-tracking-data",level:3},{value:"HandTracking Data Structure",id:"handtracking-data-structure",level:4},{value:"HandTracking Coordinate System",id:"handtracking-coordinate-system",level:4},{value:"HandTracking API Reference",id:"handtracking-api-reference",level:4},{value:"Interpolated Hand Tracking Results",id:"interpolated-hand-tracking-results",level:4},{value:"Hand Tracking Visualization",id:"hand-tracking-visualization",level:4},{value:"On-Device VIO (Visual Inertial Odometry)",id:"on-device-vio-visual-inertial-odometry",level:2},{value:"VIO Data Structure",id:"vio-data-structure",level:3},{value:"Data Type: <code>FrontendOutput</code>",id:"data-type-frontendoutput",level:4},{value:"VIO High Frequency Data Structure",id:"vio-high-frequency-data-structure",level:3},{value:"Data Type: <code>OpenLoopTrajectoryPose</code>",id:"data-type-openlooptrajectorypose",level:4},{value:"VIO API Reference",id:"vio-api-reference",level:3},{value:"VIO Trajectory Visualization",id:"vio-trajectory-visualization",level:3},{value:"VIO High Frequency Visualization",id:"vio-high-frequency-visualization",level:3}];function _(e){const i={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(s.A,{notebookUrl:"https://github.com/facebookresearch/projectaria_gen2_pilot_dataset/blob/main/examples/tutorial_1_vrs_data_loading.ipynb",colabDisabled:!0}),"\n",(0,a.jsxs)(i.p,{children:["This tutorial demonstrates how to load and visualize VRS data from the Aria Gen2 Pilot Dataset using the ",(0,a.jsx)(i.code,{children:"AriaGen2PilotDataProvider"}),"."]}),"\n",(0,a.jsx)(i.h2,{id:"what-youll-learn",children:"What You'll Learn"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Initialize the AriaGen2PilotDataProvider."}),"\n",(0,a.jsx)(i.li,{children:"Discover available sensor data streams."}),"\n",(0,a.jsx)(i.li,{children:"Query sensor data by index and timestamp."}),"\n",(0,a.jsx)(i.li,{children:"Understand time domains and query options for temporal synchronization."}),"\n",(0,a.jsx)(i.li,{children:"Understand on device machine perception data stream: eye gaze, hand tracking, vio and vio high frequency."}),"\n"]}),"\n",(0,a.jsx)(i.h2,{id:"import-required-libraries",children:"Import Required Libraries"}),"\n",(0,a.jsx)(i.p,{children:"The following libraries are required for this tutorial:"}),"\n",(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-python",children:"# Standard library imports\nimport numpy as np\nimport os\nfrom pathlib import Path\nfrom datetime import timedelta\n\n# Project Aria Tools imports\nfrom projectaria_tools.core.stream_id import StreamId\nfrom projectaria_tools.core.sensor_data import TimeDomain, TimeQueryOptions, VioStatus, TrackingQuality\nfrom projectaria_tools.core.mps import get_unit_vector_from_yaw_pitch\nfrom projectaria_tools.utils.rerun_helpers import (\n    create_hand_skeleton_from_landmarks,\n    AriaGlassesOutline,\n    ToTransform3D\n)\n\n# Aria Gen2 Pilot Dataset imports\nfrom aria_gen2_pilot_dataset import AriaGen2PilotDataProvider\n\n# Visualization library\nimport rerun as rr\n"})}),"\n",(0,a.jsx)(i.h2,{id:"initialize-data-provider",children:"Initialize Data Provider"}),"\n",(0,a.jsxs)(i.p,{children:["The ",(0,a.jsx)(i.code,{children:"AriaGen2PilotDataProvider"})," is the main interface for accessing data from the Aria Gen2 Pilot Dataset. It provides methods to query sensor data, discover available streams, and access device calibration information."]}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"\u26a0\ufe0f Important:"})," Update the ",(0,a.jsx)(i.code,{children:"sequence_path"})," below to point to your downloaded Aria Gen2 Pilot Dataset sequence folder."]}),"\n",(0,a.jsx)(i.h1,{id:"replace-with-the-actual-path-to-your-downloaded-sequence-folder",children:"Replace with the actual path to your downloaded sequence folder"}),"\n",(0,a.jsx)(i.p,{children:'sequence_path = "path/to/your/sequence_folder"'}),"\n",(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-python",children:"# Initialize the data provider\npilot_data_provider = AriaGen2PilotDataProvider(sequence_path)\n"})}),"\n",(0,a.jsx)(i.h2,{id:"check-available-data-modalities",children:"Check Available Data Modalities"}),"\n",(0,a.jsx)(i.p,{children:"Each Aria Gen2 Pilot dataset sequence contains the following data modalities:"}),"\n",(0,a.jsx)(i.h3,{id:"core-data-types",children:"Core Data Types"}),"\n",(0,a.jsxs)(i.ol,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"VRS"})," - Raw sensor data"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"MPS (Machine Perception Service)"})," - SLAM and hand tracking data."]}),"\n"]}),"\n",(0,a.jsx)(i.h3,{id:"algorithm-outputs",children:"Algorithm Outputs"}),"\n",(0,a.jsxs)(i.ol,{children:["\n",(0,a.jsx)(i.li,{children:(0,a.jsx)(i.strong,{children:"Foundation Stereo"})}),"\n",(0,a.jsx)(i.li,{children:(0,a.jsx)(i.strong,{children:"Heart Rate"})}),"\n",(0,a.jsx)(i.li,{children:(0,a.jsx)(i.strong,{children:"Hand-Object Interaction"})}),"\n",(0,a.jsx)(i.li,{children:(0,a.jsx)(i.strong,{children:"Diarization"})}),"\n",(0,a.jsx)(i.li,{children:(0,a.jsx)(i.strong,{children:"Egocentric Voxel Lifting"})}),"\n"]}),"\n",(0,a.jsxs)(i.p,{children:["Please refer to the algorithm introduction ",(0,a.jsx)(i.a,{href:"https://facebookresearch.github.io/projectaria_tools/gen2/research-tools/dataset/pilot/content#additional-perception-algorithms",children:"here"}),". For detailed instructions on loading and visualizing the algorithm output, see [tutorial_3_algorithm_data_loading.ipynb](TODO: add link)."]}),"\n",(0,a.jsx)(i.p,{children:(0,a.jsx)(i.strong,{children:"Important Notes:"})}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"VRS data is mandatory"})," for creating a pilot data provider"]}),"\n",(0,a.jsx)(i.li,{children:"All other modalities are optional and depend on the specific sequence"}),"\n",(0,a.jsx)(i.li,{children:"The availability of optional modalities varies between sequences"}),"\n"]}),"\n",(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-python",children:"# Check what data types are available in this sequence\nprint(\"Data Modalities Available in This Sequence:\")\nprint(\"=\" * 60)\nprint(f\"Raw VRS Sensors:          \u2705 (Mandatory)\")\nprint(f\"MPS Algorithms:           {'\u2705' if pilot_data_provider.has_mps_data() else '\u274c'}\")\nprint(f\"Foundation Stereo:        {'\u2705' if pilot_data_provider.has_stereo_depth_data() else '\u274c'}\")\nprint(f\"Heart Rate Monitoring:    {'\u2705' if pilot_data_provider.has_heart_rate_data() else '\u274c'}\")\nprint(f\"Hand-Object Interaction:  {'\u2705' if pilot_data_provider.has_hand_object_interaction_data() else '\u274c'}\")\nprint(f\"Diarization:              {'\u2705' if pilot_data_provider.has_diarization_data() else '\u274c'}\")\nprint(f\"Egocentric Voxel Lifting: {'\u2705' if pilot_data_provider.has_egocentric_voxel_lifting_data() else '\u274c'}\")\nprint(\"=\" * 60)\n"})}),"\n",(0,a.jsx)(i.h2,{id:"vrs---explore-available-streams",children:"VRS - Explore Available Streams"}),"\n",(0,a.jsx)(i.h3,{id:"stream-discovery-and-navigation",children:"Stream Discovery and Navigation"}),"\n",(0,a.jsxs)(i.p,{children:["A VRS file contains multiple ",(0,a.jsx)(i.strong,{children:"streams"}),", each storing data from a specific sensor or on-device algorithm result."]}),"\n",(0,a.jsxs)(i.p,{children:["Each VRS stream is identified by a unique ",(0,a.jsx)(i.strong,{children:(0,a.jsx)(i.code,{children:"StreamId"})})," (e.g. ",(0,a.jsx)(i.code,{children:"1201-1"}),"), consisting ",(0,a.jsx)(i.strong,{children:"of"})," ",(0,a.jsx)(i.code,{children:"RecordableTypeId"})," (sensor type, e.g. ",(0,a.jsx)(i.code,{children:"1201"}),", standing for \u201cSLAM camera\u201d), and an ",(0,a.jsx)(i.code,{children:"instance_id"})," (for multiple sensors of the same type, e.g. ",(0,a.jsx)(i.code,{children:"-1"}),", standing for \u201cinstance #1 of this sensor type\u201d). Below are some common ",(0,a.jsx)(i.code,{children:"RecordableTypeId"})," in Aria recordings. Full definitions of all Recordable Types are given in the ",(0,a.jsx)(i.a,{href:"https://facebookresearch.github.io/projectaria_tools/",children:"Project Aria Tools documentation"}),", or refer to ",(0,a.jsx)(i.a,{href:"https://github.com/facebookresearch/vrs/blob/main/vrs/StreamId.h#L49",children:"the `StreamId.h` file in the VRS repo"}),"."]}),"\n",(0,a.jsxs)(i.table,{children:[(0,a.jsx)(i.thead,{children:(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.th,{style:{textAlign:"left"},children:"RecordableTypeId"}),(0,a.jsx)(i.th,{style:{textAlign:"left"},children:"Description"})]})}),(0,a.jsxs)(i.tbody,{children:[(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{style:{textAlign:"left"},children:"214"}),(0,a.jsx)(i.td,{style:{textAlign:"left"},children:"RGB camera stream"})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{style:{textAlign:"left"},children:"1201"}),(0,a.jsx)(i.td,{style:{textAlign:"left"},children:"SLAM camera stream"})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{style:{textAlign:"left"},children:"211"}),(0,a.jsx)(i.td,{style:{textAlign:"left"},children:"EyeTracking camera stream"})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{style:{textAlign:"left"},children:"1202"}),(0,a.jsx)(i.td,{style:{textAlign:"left"},children:"IMU sensor stream"})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{style:{textAlign:"left"},children:"231"}),(0,a.jsx)(i.td,{style:{textAlign:"left"},children:"Audio sensor stream"})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{style:{textAlign:"left"},children:"373"}),(0,a.jsx)(i.td,{style:{textAlign:"left"},children:"EyeGaze data stream from on-device EyeTracking algorithm"})]})]})]}),"\n",(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-python",children:'# Get all available streams\nall_streams = pilot_data_provider.get_vrs_all_streams()\nprint(f"Found {len(all_streams)} streams in the VRS file:")\n\n# Print out each stream ID and their corresponding sensor label\nfor stream_id in all_streams:\n    label = pilot_data_provider.get_vrs_label_from_stream_id(stream_id)\n    print(f" --- Data stream {stream_id}\'s label is: {label}")\n'})}),"\n",(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-python",children:'# Find a specific stream\'s StreamId by sensor label\nprint("Seeking RGB data stream...")\nrgb_stream_id = pilot_data_provider.get_vrs_stream_id_from_label("camera-rgb")\nif rgb_stream_id is not None:\n    print(f"Found camera-rgb stream in VRS file: {rgb_stream_id}")\nelse:\n    print("Cannot find camera-rgb stream in VRS file.")\n'})}),"\n",(0,a.jsx)(i.h3,{id:"vrs---device-calibration",children:"VRS - Device Calibration"}),"\n",(0,a.jsx)(i.p,{children:"Device calibration information is essential for accurate sensor data processing and 3D reconstruction. The calibration data includes camera intrinsics, extrinsics, and sensor mounting information."}),"\n",(0,a.jsxs)(i.p,{children:["For detailed usage of device calibration, please refer to the ",(0,a.jsx)(i.a,{href:"https://facebookresearch.github.io/projectaria_tools/",children:"Project Aria Tools documentation"}),"."]}),"\n",(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-python",children:"# Get device calibration information\ndevice_calibration = pilot_data_provider.get_vrs_device_calibration()\n"})}),"\n",(0,a.jsx)(i.h2,{id:"sensor-data-query-apis",children:"Sensor Data Query APIs"}),"\n",(0,a.jsx)(i.p,{children:"The AriaGen2PilotDataProvider offers two main approaches for querying sensor data:"}),"\n",(0,a.jsx)(i.h3,{id:"query-by-index",children:"Query by Index"}),"\n",(0,a.jsx)(i.p,{children:"The query-by-index API allows you to retrieve the k-th data sample from a specific stream using the following syntax:"}),"\n",(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-python",children:"get_vrs_\\{SENSOR\\}_data_by_index(stream_id, index)\n"})}),"\n",(0,a.jsxs)(i.p,{children:["where ",(0,a.jsx)(i.code,{children:"\\{SENSOR\\}"})," can be replaced by any sensor data type available in Aria VRS. See ",(0,a.jsx)(i.a,{href:"https://github.com/facebookresearch/projectaria_tools/blob/main/core/pilot_data_provider/VrsDataProvider.h#L377",children:"the full list of supported sensors"}),"."]}),"\n",(0,a.jsx)(i.p,{children:"This API is commonly used for:"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Sequential processing (iterating through all frames of a single stream)"}),"\n",(0,a.jsx)(i.li,{children:"When you know the exact frame number you want to query within a specific stream"}),"\n"]}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Important Note:"}),"\nThe indices in each stream are independent and not correlated across different sensor streams. For example, the i-th RGB image does not necessarily correspond to the i-th SLAM image. This is because different sensors may operate at different frequencies or have missing frames, so their data streams are not synchronized by index."]}),"\n",(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-python",children:'# Initialize Rerun for visualization\nrr.init("rerun_viz_query_by_index")\n\n# Get number of samples in the RGB stream\nnum_samples = pilot_data_provider.get_vrs_num_data(rgb_stream_id)\nprint(f"RGB stream has a total of {num_samples} frames\\n")\n\n# Access frames sequentially and plot the first few frames\nfirst_few = min(10, num_samples)\nprint(f"Printing the capture timestamps from the first {first_few} frames")\nfor i in range(first_few):  # First 10 frames\n    image_data, image_record = pilot_data_provider.get_vrs_image_data_by_index(\n        rgb_stream_id, i\n    )\n\n    # Access image properties\n    print(f"Frame {i}: timestamp = {image_record.capture_timestamp_ns}")\n\n    # Process and visualize image data\n    if image_data.is_valid():\n        rr.set_time_nanos("device_time", image_record.capture_timestamp_ns)\n        rr.log("camera_rgb", rr.Image(image_data.to_numpy_array()))\n\n# Display the visualization\nrr.notebook_show()\n'})}),"\n",(0,a.jsx)(i.h3,{id:"query-by-timestamp-timedomain-and-timequeryoptions",children:"Query by Timestamp: TimeDomain and TimeQueryOptions"}),"\n",(0,a.jsxs)(i.p,{children:["A key feature of Aria devices is the ability to capture time-synchronized, multi-modal sensor data. To help you access this data with precise temporal control, ",(0,a.jsx)(i.code,{children:"projectaria_tools"})," provides a comprehensive suite of time-based APIs."]}),"\n",(0,a.jsxs)(i.p,{children:["The most commonly used is the ",(0,a.jsx)(i.strong,{children:"timestamp-based query"}),":"]}),"\n",(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-python",children:"get_\\{SENSOR\\}_by_time_ns(stream_id, time_ns, time_domain=None, time_query_options=None)\n"})}),"\n",(0,a.jsxs)(i.p,{children:["where ",(0,a.jsx)(i.code,{children:"\\{SENSOR\\}"})," can be replaced by any sensor data type available in Aria VRS. See here for ",(0,a.jsx)(i.a,{href:"https://github.com/facebookresearch/projectaria_tools/blob/main/core/pilot_data_provider/VrsDataProvider.h#L377",children:"a full list of supported {SENSOR}"})]}),"\n",(0,a.jsx)(i.p,{children:"This API is often used to:"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Synchronize data across multiple sensor streams"}),"\n",(0,a.jsx)(i.li,{children:"Fetch sensor data at specific timestamps"}),"\n",(0,a.jsx)(i.li,{children:"Perform temporal analysis"}),"\n"]}),"\n",(0,a.jsx)(i.h4,{id:"timedomain-and-timequeryoptions",children:"TimeDomain and TimeQueryOptions"}),"\n",(0,a.jsx)(i.p,{children:"When querying sensor data by timestamp, two important concepts are:"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"TimeDomain"}),": Specifies the time reference for your query."]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"TimeQueryOptions"}),": Controls how the API selects data relative to your requested timestamp."]}),"\n"]}),"\n",(0,a.jsx)(i.p,{children:"Below are all available options for each:"}),"\n",(0,a.jsx)(i.h4,{id:"timedomain-options",children:"TimeDomain Options"}),"\n",(0,a.jsxs)(i.table,{children:[(0,a.jsx)(i.thead,{children:(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.th,{children:"Name"}),(0,a.jsx)(i.th,{children:"Description"}),(0,a.jsx)(i.th,{children:"Typical Use Case"})]})}),(0,a.jsxs)(i.tbody,{children:[(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{children:"RECORD_TIME"}),(0,a.jsx)(i.td,{children:"Timestamp stored directly in the VRS index. Fast access, but time domain may vary."}),(0,a.jsx)(i.td,{children:"Quick access, not recommended for sync."})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{children:"DEVICE_TIME"}),(0,a.jsx)(i.td,{children:"Accurate device capture time. All sensors on the same Aria device share this domain."}),(0,a.jsx)(i.td,{children:(0,a.jsx)(i.strong,{children:"Recommended for single-device data."})})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{children:"HOST_TIME"}),(0,a.jsx)(i.td,{children:"Arrival time in the host computer\u2019s domain. May not be accurate."}),(0,a.jsx)(i.td,{children:"Debugging, host-side analysis."})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{children:"SubGhz"}),(0,a.jsx)(i.td,{children:"TimeSync server\u2019s domain using SubGhz signals, accurate for multi-device capture."}),(0,a.jsx)(i.td,{children:"Multi-device synchronization."})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{children:"Utc"}),(0,a.jsx)(i.td,{children:"UTC time domain, only seconds-level accuracy."}),(0,a.jsx)(i.td,{children:"Coarse, global time reference."})]})]})]}),"\n",(0,a.jsx)(i.h4,{id:"timequeryoptions",children:"TimeQueryOptions"}),"\n",(0,a.jsxs)(i.table,{children:[(0,a.jsx)(i.thead,{children:(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.th,{children:"Name"}),(0,a.jsx)(i.th,{children:"Description"})]})}),(0,a.jsxs)(i.tbody,{children:[(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{children:"Before"}),(0,a.jsxs)(i.td,{children:["Returns the last valid data with ",(0,a.jsx)(i.code,{children:"timestamp <= t_query"}),"."]})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{children:"After"}),(0,a.jsxs)(i.td,{children:["Returns the first valid data with ",(0,a.jsx)(i.code,{children:"timestamp >= t_query"}),"."]})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{children:"Closest"}),(0,a.jsxs)(i.td,{children:["Returns the data sample closest to ",(0,a.jsx)(i.code,{children:"t_query"}),". If two are equally close, returns the one ",(0,a.jsx)(i.strong,{children:"before"})," the query."]})]})]})]}),"\n",(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-python",children:'from projectaria_tools.core.sensor_data import TimeDomain, TimeQueryOptions\n\n# Initialize Rerun for visualization\nrr.init("rerun_viz_query_by_timestamp")\n\n# Get time bounds for RGB images\nfirst_timestamp_ns = pilot_data_provider.get_vrs_timestamps_ns(rgb_stream_id, TimeDomain.DEVICE_TIME)[0]\nlast_timestamp_ns = pilot_data_provider.get_vrs_timestamps_ns(rgb_stream_id, TimeDomain.DEVICE_TIME)[-1]\n\n# Query specific timestamp\ntarget_time_ns = first_timestamp_ns + 1000000000  # 1 second later\nimage_data, image_record = pilot_data_provider.get_vrs_image_data_by_time_ns(\n    rgb_stream_id,\n    target_time_ns,\n    TimeDomain.DEVICE_TIME,\n    TimeQueryOptions.CLOSEST\n)\n\nactual_time_ns = image_record.capture_timestamp_ns\nprint(f"Requested RGB data that is closest to: {target_time_ns} ns, Got closest sample at: {actual_time_ns} ns")\n\n# Plot RGB and SLAM images at approximately 1 Hz\ncamera_label_list = ["camera-rgb", "slam-front-left", "slam-front-right", "slam-side-left", "slam-side-right"]\ncamera_stream_ids = [pilot_data_provider.get_vrs_stream_id_from_label(camera_label) for camera_label in camera_label_list]\n\nquery_timestamp_ns = first_timestamp_ns\nfor _ in range(10):\n    for label, stream_id in zip(camera_label_list, camera_stream_ids):\n        # Query each camera\'s data according to query timestamp\n        image_data, image_record = pilot_data_provider.get_vrs_image_data_by_time_ns(\n            stream_id,\n            query_timestamp_ns,\n            TimeDomain.DEVICE_TIME,\n            TimeQueryOptions.CLOSEST)\n        # Note: the actual timestamp of the image data is stored within image_record.\n        # It can be different from the query_time.\n        capture_time_ns = image_record.capture_timestamp_ns\n\n        # Visualize with Rerun\n        if image_data.is_valid():\n            rr.set_time_nanos("device_time", capture_time_ns)\n            rr.log(label, rr.Image(image_data.to_numpy_array()))\n\n    query_timestamp_ns = query_timestamp_ns + int(1e9)  # 1 second\n\n# Display the visualization\nrr.notebook_show()\n'})}),"\n",(0,a.jsx)(i.h2,{id:"on-device-eye-tracking-and-hand-tracking",children:"On-Device Eye Tracking and Hand Tracking"}),"\n",(0,a.jsx)(i.p,{children:"Aria Gen2 glasses feature on-device machine perception algorithms that run during recording. This section covers how to access and visualize eye tracking and hand tracking data from VRS files."}),"\n",(0,a.jsx)(i.h3,{id:"eye-tracking-data",children:"Eye Tracking Data"}),"\n",(0,a.jsx)(i.p,{children:"Eye tracking data provides information about where the user is looking, including gaze direction, depth estimation, and eye movement patterns."}),"\n",(0,a.jsx)(i.h4,{id:"eyegaze-data-structure",children:"EyeGaze Data Structure"}),"\n",(0,a.jsxs)(i.p,{children:["The EyeGaze data type represents on-device eye tracking results. ",(0,a.jsxs)(i.strong,{children:["Importantly, it directly reuses ",(0,a.jsx)(i.a,{href:"https://github.com/facebookresearch/projectaria_tools/blob/main/core/mps/EyeGaze.h",children:"the EyeGaze data structure"})," from MPS (Machine Perception Services)"]}),", providing guaranteed compatibility across VRS and MPS."]}),"\n",(0,a.jsx)(i.p,{children:(0,a.jsxs)(i.strong,{children:["Key ",(0,a.jsx)(i.code,{children:"EyeGaze"})," fields:"]})}),"\n",(0,a.jsxs)(i.table,{children:[(0,a.jsx)(i.thead,{children:(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.th,{style:{textAlign:"left"},children:"Field Name"}),(0,a.jsx)(i.th,{style:{textAlign:"left"},children:"Description"})]})}),(0,a.jsxs)(i.tbody,{children:[(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{style:{textAlign:"left"},children:(0,a.jsx)(i.code,{children:"session_uid"})}),(0,a.jsx)(i.td,{style:{textAlign:"left"},children:"Unique ID for the eyetracking session"})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{style:{textAlign:"left"},children:(0,a.jsx)(i.code,{children:"tracking_timestamp"})}),(0,a.jsx)(i.td,{style:{textAlign:"left"},children:"Timestamp of the eye tracking camera frame in device time domain, in us."})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{style:{textAlign:"left"},children:(0,a.jsx)(i.code,{children:"vergence.t[x,y,z]_[left,right]_eye"})}),(0,a.jsx)(i.td,{style:{textAlign:"left"},children:"Translation for each eye origin in CPF frame"})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsxs)(i.td,{style:{textAlign:"left"},children:[(0,a.jsx)(i.code,{children:"yaw"}),",",(0,a.jsx)(i.code,{children:"vergence.[left,right]_yaw"})]}),(0,a.jsx)(i.td,{style:{textAlign:"left"},children:"Eye gaze yaw angle (horizontal) in radians in CPF frame"})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsxs)(i.td,{style:{textAlign:"left"},children:[(0,a.jsx)(i.code,{children:"pitch"}),",",(0,a.jsx)(i.code,{children:"vergence.[left,right]_pitch"}),"(Gen2-only)"]}),(0,a.jsx)(i.td,{style:{textAlign:"left"},children:"Eye gaze pitch angle (vertical) in radians in CPF frame. The left and right pitch are assumed to be the same in Aria-Gen1."})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{style:{textAlign:"left"},children:(0,a.jsx)(i.code,{children:"depth"})}),(0,a.jsx)(i.td,{style:{textAlign:"left"},children:"Depth in meters of the 3D eye gaze point in CPF frame (0 = unavailable)"})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsxs)(i.td,{style:{textAlign:"left"},children:[(0,a.jsx)(i.code,{children:"yaw_low"}),",",(0,a.jsx)(i.code,{children:"yaw_high"}),",",(0,a.jsx)(i.code,{children:"pitch_low"}),",",(0,a.jsx)(i.code,{children:"pitch_high"})]}),(0,a.jsx)(i.td,{style:{textAlign:"left"},children:"Confidence interval bounds for yaw and pitch angle"})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{style:{textAlign:"left"},children:(0,a.jsx)(i.strong,{children:"Aria-Gen2 specific fields"})}),(0,a.jsx)(i.td,{style:{textAlign:"left"}})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{style:{textAlign:"left"},children:(0,a.jsx)(i.code,{children:"combined_gaze_origin_in_cpf"})}),(0,a.jsx)(i.td,{style:{textAlign:"left"},children:"Combined gaze origin in CPF frame (Gen2 only)"})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{style:{textAlign:"left"},children:(0,a.jsx)(i.code,{children:"spatial_gaze_point_in_cpf"})}),(0,a.jsx)(i.td,{style:{textAlign:"left"},children:"3D spatial gaze point in CPF frame"})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{style:{textAlign:"left"},children:(0,a.jsx)(i.code,{children:"vergence.[left,right]_entrance_pupil_position_meter"})}),(0,a.jsx)(i.td,{style:{textAlign:"left"},children:"Entrance pupil positions for each eye"})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{style:{textAlign:"left"},children:(0,a.jsx)(i.code,{children:"vergence.[left,right]_pupil_diameter_meter"})}),(0,a.jsx)(i.td,{style:{textAlign:"left"},children:"Entrance pupil diameter for each eye"})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{style:{textAlign:"left"},children:(0,a.jsx)(i.code,{children:"vergence.[left,right]_blink"})}),(0,a.jsx)(i.td,{style:{textAlign:"left"},children:"Blink detection for left and right eyes"})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{style:{textAlign:"left"},children:(0,a.jsx)(i.code,{children:"*_valid"})}),(0,a.jsx)(i.td,{style:{textAlign:"left"},children:"Boolean flags to indicating if the corresponding data field in EyeGaze is valid"})]})]})]}),"\n",(0,a.jsx)(i.h4,{id:"eyegaze-api-reference",children:"EyeGaze API Reference"}),"\n",(0,a.jsxs)(i.p,{children:["In ",(0,a.jsx)(i.code,{children:"AriaGen2PilotDataProvider"}),", EyeGaze is treated the same way as any other sensor data, and shares similar query APIs:"]}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.code,{children:"get_vrs_eye_gaze_data_by_index(stream_id, index)"}),": Query by index."]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.code,{children:"get_vrs_eye_gaze_data_by_time_ns(stream_id, timestamp, time_domain, query_options)"}),": Query by timestamp."]}),"\n"]}),"\n",(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-python",children:'# First, let\'s check if eye tracking data is available\neyegaze_label = "eyegaze"\neyegaze_stream_id = pilot_data_provider.get_vrs_stream_id_from_label(eyegaze_label)\n\nif eyegaze_stream_id is None:\n    print(f"\u274c {eyegaze_label} data stream does not exist in this VRS file.")\n    print("Please use a VRS file that contains valid eyegaze data for this tutorial.")\nelse:\n    print(f"\u2705 Found {eyegaze_label} data stream: {eyegaze_stream_id}")\n\n    # Get number of eye tracking samples\n    num_eyegaze_samples = pilot_data_provider.get_vrs_num_data(eyegaze_stream_id)\n    print(f"Total eye tracking samples: {num_eyegaze_samples}")\n\n    # Sample a few eye tracking data points\n    print("\\n=== EyeGaze Data Sample ===")\n    selected_index = min(5, num_eyegaze_samples - 1)\n    eyegaze_data = pilot_data_provider.get_vrs_eye_gaze_data_by_index(eyegaze_stream_id, selected_index)\n\n    if eyegaze_data is not None:\n        # Eyegaze timestamp is in format of datetime.deltatime in microseconds, convert it to integer\n        from datetime import timedelta\n        eyegaze_timestamp_ns = (eyegaze_data.tracking_timestamp // timedelta(microseconds=1)) * 1000\n        print(f"Sample {selected_index}:")\n        print(f"\\tTracking timestamp: {eyegaze_timestamp_ns}")\n\n        # Check if combined gaze is valid, if so, print out the gaze direction\n        print(f"\\tCombined gaze valid: {eyegaze_data.combined_gaze_valid}")\n        if eyegaze_data.combined_gaze_valid:\n            print(f"\\tYaw: {eyegaze_data.yaw:.3f} rad")\n            print(f"\\tPitch: {eyegaze_data.pitch:.3f} rad")\n            print(f"\\tDepth: {eyegaze_data.depth:.3f} m")\n\n            # Convert gaze direction to unit vector\n            from projectaria_tools.core.mps import get_unit_vector_from_yaw_pitch\n            gaze_direction_in_unit_vec = get_unit_vector_from_yaw_pitch(eyegaze_data.yaw, eyegaze_data.pitch)\n            print(f"\\tGaze direction in unit vec [xyz]: {gaze_direction_in_unit_vec}")\n\n        # Check if spatial gaze point is valid\n        print(f"\\tSpatial gaze point valid: {eyegaze_data.spatial_gaze_point_valid}")\n        if eyegaze_data.spatial_gaze_point_valid:\n            print(f"\\tSpatial gaze point in CPF: {eyegaze_data.spatial_gaze_point_in_cpf}")\n    else:\n        print("No eye tracking data available at the selected index.")\n'})}),"\n",(0,a.jsx)(i.h4,{id:"eye-tracking-visualization",children:"Eye Tracking Visualization"}),"\n",(0,a.jsx)(i.p,{children:"To visualize eye tracking results in camera images, you need to project the gaze data into the camera coordinate system using device calibration."}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Important Coordinate System Note:"}),"\nAll eye tracking results in Aria are stored in a reference coordinate system called ",(0,a.jsxs)(i.strong,{children:["Central Pupil Frame (",(0,a.jsx)(i.code,{children:"CPF"}),")"]}),", which is approximately the center of the user's two eye positions. This ",(0,a.jsx)(i.code,{children:"CPF"})," frame is ",(0,a.jsxs)(i.strong,{children:["DIFFERENT from the ",(0,a.jsx)(i.code,{children:"Device"})," frame"]})," in device calibration, where the latter is essentially the ",(0,a.jsx)(i.code,{children:"slam-front-left"})," camera. To transform between ",(0,a.jsx)(i.code,{children:"CPF"})," and ",(0,a.jsx)(i.code,{children:"Device"}),", we use the device calibration API:"]}),"\n",(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-python",children:"device_calibration.get_transform_device_cpf()\n"})}),"\n",(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-python",children:'# Eye tracking visualization helper function\ndef plot_eyegaze_in_camera(eyegaze_data, camera_label, camera_calib, T_device_cpf):\n    """\n    A helper function to plot eyegaze\'s spatial gaze point into a camera image\n    """\n    # Skip if eyegaze data is invalid\n    if not (eyegaze_data.spatial_gaze_point_valid and eyegaze_data.combined_gaze_valid):\n        return\n\n    # First, transform spatial gaze point from CPF -> Device -> Camera frame\n    spatial_gaze_point_in_cpf = eyegaze_data.spatial_gaze_point_in_cpf\n    spatial_gaze_point_in_device = T_device_cpf @ spatial_gaze_point_in_cpf\n    spatial_gaze_point_in_camera = (\n        camera_calib.get_transform_device_camera().inverse()\n        @ spatial_gaze_point_in_device\n    )\n\n    # Project into camera and plot 2D gaze location\n    maybe_pixel = camera_calib.project(spatial_gaze_point_in_camera)\n    if maybe_pixel is not None:\n        rr.log(\n            f"{camera_label}",\n            rr.Points2D(\n                positions=[maybe_pixel],\n                colors=[255, 64, 255],\n                radii = [30.0]\n            ),\n        )\n\n# Visualize eye tracking in camera images (if eye tracking data is available)\nif eyegaze_stream_id is not None:\n    print("\\n=== Visualizing on-device eye tracking in camera images ===")\n\n    # Get device calibration and CPF transform\n    device_calib = pilot_data_provider.get_vrs_device_calibration()\n    T_device_cpf = device_calib.get_transform_device_cpf()\n\n    # Get RGB camera stream\n    rgb_camera_label = "camera-rgb"\n    rgb_stream_id = pilot_data_provider.get_vrs_stream_id_from_label(rgb_camera_label)\n    rgb_camera_calib = device_calib.get_camera_calib(rgb_camera_label)\n\n    # Initialize Rerun for visualization\n    rr.init("rerun_viz_et_in_cameras")\n    rr.notebook_show()\n\n    # Get time bounds and sample data\n    first_timestamp_ns = pilot_data_provider.get_vrs_timestamps_ns(rgb_stream_id, TimeDomain.DEVICE_TIME)[0]\n    last_timestamp_ns = pilot_data_provider.get_vrs_timestamps_ns(rgb_stream_id, TimeDomain.DEVICE_TIME)[-1]\n\n    # Sample a few frames for visualization\n    sample_timestamps = []\n    for i in range(0, min(10, pilot_data_provider.get_vrs_num_data(rgb_stream_id)), 2):\n        rgb_data, rgb_record = pilot_data_provider.get_vrs_image_data_by_index(rgb_stream_id, i)\n        sample_timestamps.append(rgb_record.capture_timestamp_ns)\n\n    # Visualize RGB images with eye tracking overlay\n    for timestamp_ns in sample_timestamps:\n        # Get RGB image\n        rgb_data, rgb_record = pilot_data_provider.get_vrs_image_data_by_time_ns(\n            rgb_stream_id, timestamp_ns, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST\n        )\n\n        if rgb_data.is_valid():\n            # Visualize the RGB image\n            rr.set_time_nanos("device_time", rgb_record.capture_timestamp_ns)\n            rr.log(rgb_camera_label, rr.Image(rgb_data.to_numpy_array()))\n\n            # Get eye tracking data for this timestamp\n            eyegaze_data = pilot_data_provider.get_vrs_eye_gaze_data_by_time_ns(\n                eyegaze_stream_id, timestamp_ns, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST\n            )\n\n            if eyegaze_data is not None:\n                # Plot eye tracking overlay\n                plot_eyegaze_in_camera(\n                    eyegaze_data=eyegaze_data,\n                    camera_label=rgb_camera_label,\n                    camera_calib=rgb_camera_calib,\n                    T_device_cpf=T_device_cpf\n                )\nelse:\n    print("Skipping eye tracking visualization - no eye tracking data available.")\n'})}),"\n",(0,a.jsx)(i.h3,{id:"hand-tracking-data",children:"Hand Tracking Data"}),"\n",(0,a.jsx)(i.p,{children:"Hand tracking data provides comprehensive 3D hand pose information including landmark positions, confidence scores, and hand gestures."}),"\n",(0,a.jsx)(i.h4,{id:"handtracking-data-structure",children:"HandTracking Data Structure"}),"\n",(0,a.jsxs)(i.p,{children:["HandTracking data contains comprehensive 3D hand pose information. ",(0,a.jsxs)(i.strong,{children:["Importantly, it directly reuses the ",(0,a.jsx)(i.a,{href:"https://github.com/facebookresearch/projectaria_tools/blob/main/core/mps/HandTracking.h",children:"HandTrackingResults data structure"})," from MPS (Machine Perception Services)"]}),", providing guaranteed compatibility across VRS and MPS."]}),"\n",(0,a.jsx)(i.p,{children:(0,a.jsxs)(i.strong,{children:["Key Fields in ",(0,a.jsx)(i.code,{children:"HandTrackingResults"}),":"]})}),"\n",(0,a.jsxs)(i.table,{children:[(0,a.jsx)(i.thead,{children:(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.th,{children:"Field Name"}),(0,a.jsx)(i.th,{children:"Description"})]})}),(0,a.jsxs)(i.tbody,{children:[(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{children:(0,a.jsx)(i.code,{children:"tracking_timestamp"})}),(0,a.jsx)(i.td,{children:"Timestamp of the hand-tracking estimate in the device time domain."})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{children:(0,a.jsx)(i.code,{children:"left_hand"})}),(0,a.jsxs)(i.td,{children:["Left-hand pose, or ",(0,a.jsx)(i.code,{children:"None"})," if no valid pose is found for the timestamp."]})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{children:(0,a.jsx)(i.code,{children:"right_hand"})}),(0,a.jsxs)(i.td,{children:["Right-hand pose, or ",(0,a.jsx)(i.code,{children:"None"})," if no valid pose is found for the timestamp."]})]})]})]}),"\n",(0,a.jsx)(i.p,{children:(0,a.jsx)(i.strong,{children:"Single Hand fields (left or right):"})}),"\n",(0,a.jsxs)(i.table,{children:[(0,a.jsx)(i.thead,{children:(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.th,{children:"Field Name"}),(0,a.jsx)(i.th,{children:"Description"})]})}),(0,a.jsxs)(i.tbody,{children:[(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{children:(0,a.jsx)(i.code,{children:"confidence"})}),(0,a.jsx)(i.td,{children:"Tracking confidence score for this hand."})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{children:(0,a.jsx)(i.code,{children:"landmark_positions_device"})}),(0,a.jsxs)(i.td,{children:["List of 21 hand-landmark positions in the device frame (3D points). See the ",(0,a.jsx)(i.a,{href:"https://facebookresearch.github.io/projectaria_tools/docs/data_formats/mps/hand_tracking#hand_tracking_resultscsv",children:"wiki page"})," for landmark definitions."]})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{children:(0,a.jsx)(i.code,{children:"transform_device_wrist"})}),(0,a.jsxs)(i.td,{children:["Full SE3 transform of the wrist in the ",(0,a.jsx)(i.code,{children:"Device"})," frame."]})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{children:(0,a.jsx)(i.code,{children:"wrist_and_palm_normal_device"})}),(0,a.jsxs)(i.td,{children:["Normal vectors for the wrist and palm joints in the ",(0,a.jsx)(i.code,{children:"Device"})," frame."]})]})]})]}),"\n",(0,a.jsx)(i.h4,{id:"handtracking-coordinate-system",children:"HandTracking Coordinate System"}),"\n",(0,a.jsxs)(i.p,{children:["All HandTracking results in Aria are stored in the ",(0,a.jsx)(i.code,{children:"Device"})," coordinate frame, which is the same as device calibration. This makes it easier to work with compared to eye tracking data."]}),"\n",(0,a.jsx)(i.h4,{id:"handtracking-api-reference",children:"HandTracking API Reference"}),"\n",(0,a.jsxs)(i.p,{children:["In ",(0,a.jsx)(i.code,{children:"AriaGen2PilotDataProvider"}),", HandTracking shares similar query APIs:"]}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.code,{children:"get_vrs_hand_pose_data_by_index(stream_id, index)"}),": Query by index."]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.code,{children:"get_vrs_hand_pose_data_by_time_ns(stream_id, timestamp, time_domain, query_options)"}),": Query by timestamp."]}),"\n"]}),"\n",(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-python",children:'# Check if hand tracking data is available\nhandtracking_label = "handtracking"\nhandtracking_stream_id = pilot_data_provider.get_vrs_stream_id_from_label(handtracking_label)\n\nif handtracking_stream_id is None:\n    print(f"\u274c {handtracking_label} data stream does not exist in this VRS file.")\n    print("Please use a VRS file that contains valid handtracking data for this tutorial.")\nelse:\n    print(f"\u2705 Found {handtracking_label} data stream: {handtracking_stream_id}")\n\n    # Get number of hand tracking samples\n    num_handtracking_samples = pilot_data_provider.get_vrs_num_data(handtracking_stream_id)\n    print(f"Total hand tracking samples: {num_handtracking_samples}")\n\n    # Sample a few hand tracking data points\n    print("\\n=== HandTracking Data Sample ===")\n    selected_index = min(5, num_handtracking_samples - 1)\n    hand_data = pilot_data_provider.get_vrs_hand_pose_data_by_index(handtracking_stream_id, selected_index)\n\n    if hand_data is not None:\n        print(f"Sample {selected_index}:")\n        print(f"\\tTracking timestamp: {hand_data.tracking_timestamp}")\n\n        # Print the content of left and right hand if valid\n        if hand_data.left_hand is not None:\n            print("\\tLeft hand detected")\n            print(f"\\t\\tConfidence: {hand_data.left_hand.confidence:.3f}")\n            print(f"\\t\\tLandmarks shape: {len(hand_data.left_hand.landmark_positions_device)}")\n            print(f"\\t\\tWrist location: {hand_data.left_hand.get_wrist_position_device()}")\n        else:\n            print("\\tLeft hand: Not detected")\n\n        if hand_data.right_hand is not None:\n            print("\\tRight hand detected")\n            print(f"\\t\\tConfidence: {hand_data.right_hand.confidence:.3f}")\n            print(f"\\t\\tLandmarks shape: {len(hand_data.right_hand.landmark_positions_device)}")\n            print(f"\\t\\tWrist location: {hand_data.right_hand.get_wrist_position_device()}")\n        else:\n            print("\\tRight hand: Not detected")\n    else:\n        print("No hand tracking data available at the selected index.")\n'})}),"\n",(0,a.jsx)(i.h4,{id:"interpolated-hand-tracking-results",children:"Interpolated Hand Tracking Results"}),"\n",(0,a.jsx)(i.p,{children:(0,a.jsx)(i.strong,{children:"Context:"})}),"\n",(0,a.jsxs)(i.p,{children:["In Aria-Gen2 glasses, ",(0,a.jsx)(i.strong,{children:"the on-device hand-tracking data are calculated from the SLAM cameras, not RGB cameras"}),".\nIn the meantime, the SLAM cameras and RGB camera often run at different sampling frequencies, and their triggering are not aligned either.\nThis causes the hand tracking result's timestamp to often ",(0,a.jsx)(i.strong,{children:"not"})," line up with that of RGB camera, causing additional challenges in accurately visualizing handtracking results in RGB images."]}),"\n",(0,a.jsx)(i.p,{children:(0,a.jsx)(i.strong,{children:"API to query interpolated handtracking results"})}),"\n",(0,a.jsxs)(i.p,{children:["To resolve this, ",(0,a.jsx)(i.code,{children:"AriaGen2PilotDataProvider"})," enables a special query API for handtracking results:"]}),"\n",(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-python",children:"get_vrs_interpolated_hand_pose_data(stream_id, timestamp_ns, time_domain)\n"})}),"\n",(0,a.jsxs)(i.p,{children:["which will return an interpolated hand tracking ",(0,a.jsx)(i.strong,{children:"result"}),", given any timestamp within valid timestamps of the VRS file."]}),"\n",(0,a.jsx)(i.p,{children:(0,a.jsx)(i.strong,{children:"Handtracking Interpolation Implementation"})}),"\n",(0,a.jsxs)(i.ol,{children:["\n",(0,a.jsx)(i.li,{children:"Find the 2 nearest hand-tracking results before and after the target timestamp."}),"\n",(0,a.jsxs)(i.li,{children:["If the 2 hand-tracking results time delta is larger than 100 ms, interpolation is considered unreliable \u2192 return ",(0,a.jsx)(i.code,{children:"None"}),"."]}),"\n",(0,a.jsxs)(i.li,{children:["Otherwise, interpolate each hand separately:\na. For the left or right hand, perform interpolation ",(0,a.jsx)(i.strong,{children:'only if both the "before" and "after" samples contain a valid result for that hand'}),".\nb. If either sample is missing, the interpolated result for that hand will be ",(0,a.jsx)(i.code,{children:"None"}),"."]}),"\n",(0,a.jsxs)(i.li,{children:["Single-hand interpolation is calculated as:\na. Apply linear interpolation on the 3D hand landmark positions.\nb. Apply SE3 interpolation on ",(0,a.jsx)(i.code,{children:"T_Device_Wrist"})," 3D pose.\nc. Re-calculate the wrist and palm normal vectors.\nd. Take the ",(0,a.jsx)(i.code,{children:"min"})," of confidence values."]}),"\n"]}),"\n",(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-python",children:'# Demonstrate interpolated hand tracking (if hand tracking data is available)\nif handtracking_stream_id is not None:\n    print("\\n=== Demonstrating query interpolated hand tracking results ===")\n\n    # Get SLAM and RGB camera streams for comparison\n    slam_stream_id = pilot_data_provider.get_vrs_stream_id_from_label("slam-front-left")\n    rgb_stream_id = pilot_data_provider.get_vrs_stream_id_from_label("camera-rgb")\n\n    if slam_stream_id is not None and rgb_stream_id is not None:\n        # Retrieve a SLAM frame, use its timestamp as query\n        slam_sample_index = min(10, pilot_data_provider.get_vrs_num_data(slam_stream_id) - 1)\n        slam_data, slam_record = pilot_data_provider.get_vrs_image_data_by_index(slam_stream_id, slam_sample_index)\n        slam_timestamp_ns = slam_record.capture_timestamp_ns\n\n        # Retrieve the closest RGB frame\n        rgb_data, rgb_record = pilot_data_provider.get_vrs_image_data_by_time_ns(\n            rgb_stream_id, slam_timestamp_ns, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST\n        )\n        rgb_timestamp_ns = rgb_record.capture_timestamp_ns\n\n        # Retrieve the closest hand tracking data sample\n        raw_ht_data = pilot_data_provider.get_vrs_hand_pose_data_by_time_ns(\n            handtracking_stream_id, slam_timestamp_ns, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST\n        )\n\n        if raw_ht_data is not None:\n            from datetime import timedelta\n            raw_ht_timestamp_ns = (raw_ht_data.tracking_timestamp // timedelta(microseconds=1)) * 1000\n\n            # Check if hand tracking aligns with RGB or SLAM data\n            print(f"SLAM timestamp: {slam_timestamp_ns}")\n            print(f"RGB timestamp:  {rgb_timestamp_ns}")\n            print(f"Hand tracking timestamp:   {raw_ht_timestamp_ns}")\n            print(f"Hand tracking-SLAM time diff: {abs(raw_ht_timestamp_ns - slam_timestamp_ns) / 1e6:.2f} ms")\n            print(f"Hand tracking-RGB time diff: {abs(raw_ht_timestamp_ns - rgb_timestamp_ns) / 1e6:.2f} ms")\n\n            # Now, query interpolated hand tracking data sample using RGB timestamp\n            interpolated_ht_data = pilot_data_provider.get_vrs_interpolated_hand_pose_data(\n                handtracking_stream_id, rgb_timestamp_ns, TimeDomain.DEVICE_TIME\n            )\n\n            # Check that interpolated hand tracking now aligns with RGB data\n            if interpolated_ht_data is not None:\n                interpolated_ht_timestamp_ns = (interpolated_ht_data.tracking_timestamp // timedelta(microseconds=1)) * 1000\n                print(f"Interpolated hand tracking timestamp: {interpolated_ht_timestamp_ns}")\n                print(f"Interpolated hand tracking-RGB time diff: {abs(interpolated_ht_timestamp_ns - rgb_timestamp_ns) / 1e6:.2f} ms")\n            else:\n                print("Interpolated hand tracking data is None - interpolation failed")\n        else:\n            print("No raw hand tracking data found for comparison")\n    else:\n        print("Required camera streams not found for interpolation demonstration")\nelse:\n    print("Skipping hand tracking interpolation demonstration - no hand tracking data available.")\n'})}),"\n",(0,a.jsx)(i.h4,{id:"hand-tracking-visualization",children:"Hand Tracking Visualization"}),"\n",(0,a.jsx)(i.p,{children:"To visualize hand tracking results in camera images, you need to project the hand landmarks and skeleton into the camera coordinate system using device calibration. Since hand tracking data is already in the Device frame, the transformation is more straightforward than eye tracking."}),"\n",(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-python",children:'# Hand tracking visualization helper functions\ndef plot_single_hand_in_camera(hand_joints_in_device, camera_label, camera_calib, hand_label):\n    """\n    A helper function to plot a single hand data in 2D camera view\n    """\n    # Setting different marker plot sizes for RGB and SLAM since they have different resolutions\n    plot_ratio = 3.0 if camera_label == "camera-rgb" else 1.0\n    marker_color = [255,64,0] if hand_label == "left" else [255, 255, 0]\n\n    # Project into camera frame, and also create line segments\n    hand_joints_in_camera = []\n    for pt_in_device in hand_joints_in_device:\n        pt_in_camera = (\n            camera_calib.get_transform_device_camera().inverse() @ pt_in_device\n        )\n        pixel = camera_calib.project(pt_in_camera)\n        hand_joints_in_camera.append(pixel)\n\n    # Create hand skeleton in 2D image space\n    from projectaria_tools.utils.rerun_helpers import create_hand_skeleton_from_landmarks\n    hand_skeleton = create_hand_skeleton_from_landmarks(hand_joints_in_camera)\n\n    # Remove "None" markers from hand joints in camera. This is intentionally done AFTER the hand skeleton creation\n    hand_joints_in_camera = list(\n        filter(lambda x: x is not None, hand_joints_in_camera)\n    )\n\n    rr.log(\n        f"{camera_label}/{hand_label}/landmarks",\n        rr.Points2D(\n            positions=hand_joints_in_camera,\n            colors= marker_color,\n            radii= [3.0 * plot_ratio]\n        ),\n    )\n    rr.log(\n        f"{camera_label}/{hand_label}/skeleton",\n        rr.LineStrips2D(\n            hand_skeleton,\n            colors=[0, 255, 0],\n            radii= [0.5 * plot_ratio],\n        ),\n    )\n\ndef plot_handpose_in_camera(hand_pose, camera_label, camera_calib):\n    """\n    A helper function to plot hand tracking results into a camera image\n    """\n    # Plot both hands\n    if hand_pose.left_hand is not None:\n        plot_single_hand_in_camera(\n            hand_joints_in_device=hand_pose.left_hand.landmark_positions_device,\n            camera_label=camera_label,\n            camera_calib = camera_calib,\n            hand_label="left")\n    if hand_pose.right_hand is not None:\n        plot_single_hand_in_camera(\n            hand_joints_in_device=hand_pose.right_hand.landmark_positions_device,\n            camera_label=camera_label,\n            camera_calib = camera_calib,\n            hand_label="right")\n\n# Visualize hand tracking in camera images (if hand tracking data is available)\nif handtracking_stream_id is not None:\n    print("\\n=== Visualizing on-device hand tracking in camera images ===")\n\n    # Get device calibration\n    device_calib = pilot_data_provider.get_vrs_device_calibration()\n\n    # Get RGB camera stream\n    rgb_camera_label = "camera-rgb"\n    rgb_stream_id = pilot_data_provider.get_vrs_stream_id_from_label(rgb_camera_label)\n    rgb_camera_calib = device_calib.get_camera_calib(rgb_camera_label)\n\n    # Initialize Rerun for visualization\n    rr.init("rerun_viz_ht_in_cameras")\n    rr.notebook_show()\n\n    # Get time bounds and sample data\n    first_timestamp_ns = pilot_data_provider.get_vrs_timestamps_ns(rgb_stream_id, TimeDomain.DEVICE_TIME)[0]\n\n    # Sample a few frames for visualization\n    sample_timestamps = []\n    for i in range(0, min(10, pilot_data_provider.get_vrs_num_data(rgb_stream_id)), 2):\n        rgb_data, rgb_record = pilot_data_provider.get_vrs_image_data_by_index(rgb_stream_id, i)\n        sample_timestamps.append(rgb_record.capture_timestamp_ns)\n\n    # Visualize RGB images with hand tracking overlay\n    for timestamp_ns in sample_timestamps:\n        # Get RGB image\n        rgb_data, rgb_record = pilot_data_provider.get_vrs_image_data_by_time_ns(\n            rgb_stream_id, timestamp_ns, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST\n        )\n\n        if rgb_data.is_valid():\n            # Visualize the RGB image\n            rr.set_time_nanos("device_time", rgb_record.capture_timestamp_ns)\n            rr.log(rgb_camera_label, rr.Image(rgb_data.to_numpy_array()))\n\n            # Query interpolated hand tracking data for this timestamp\n            interpolated_hand_pose = pilot_data_provider.get_vrs_interpolated_hand_pose_data(\n                handtracking_stream_id, timestamp_ns, TimeDomain.DEVICE_TIME\n            )\n\n            if interpolated_hand_pose is not None:\n                # Plot hand tracking overlay\n                plot_handpose_in_camera(\n                    hand_pose=interpolated_hand_pose,\n                    camera_label=rgb_camera_label,\n                    camera_calib=rgb_camera_calib\n                )\nelse:\n    print("Skipping hand tracking visualization - no hand tracking data available.")\n'})}),"\n",(0,a.jsx)(i.h2,{id:"on-device-vio-visual-inertial-odometry",children:"On-Device VIO (Visual Inertial Odometry)"}),"\n",(0,a.jsx)(i.p,{children:"VIO (Visual Inertial Odometry) combines camera images and IMU (Inertial Measurement Unit) data to estimate device pose and motion in real-time. VIO tracks the device's position, orientation, and velocity by performing visual tracking, IMU integration, sensor fusion, etc, making it the foundation for spatial tracking and understanding."}),"\n",(0,a.jsx)(i.p,{children:"In Aria-Gen2 devices, the VIO algorithm runs on-device to produce 2 types of tracking results as part of the VRS file: VIO and VIO High Frequency."}),"\n",(0,a.jsx)(i.h3,{id:"vio-data-structure",children:"VIO Data Structure"}),"\n",(0,a.jsxs)(i.h4,{id:"data-type-frontendoutput",children:["Data Type: ",(0,a.jsx)(i.code,{children:"FrontendOutput"})]}),"\n",(0,a.jsx)(i.p,{children:"This is a new data type introduced to store the results from the VIO system, containing the following fields:"}),"\n",(0,a.jsxs)(i.table,{children:[(0,a.jsx)(i.thead,{children:(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.th,{children:"Field Name"}),(0,a.jsx)(i.th,{children:"Description"})]})}),(0,a.jsxs)(i.tbody,{children:[(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{children:(0,a.jsx)(i.code,{children:"frontend_session_uid"})}),(0,a.jsx)(i.td,{children:"Session identifier (resets on VIO restart)"})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{children:(0,a.jsx)(i.code,{children:"frame_id"})}),(0,a.jsx)(i.td,{children:"Frame set identifier"})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{children:(0,a.jsx)(i.code,{children:"capture_timestamp_ns"})}),(0,a.jsx)(i.td,{children:"Center capture time in nanoseconds"})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{children:(0,a.jsx)(i.code,{children:"unix_timestamp_ns"})}),(0,a.jsx)(i.td,{children:"Unix timestamp in nanoseconds"})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{children:(0,a.jsx)(i.code,{children:"status"})}),(0,a.jsx)(i.td,{children:"VIO status (VALID/INVALID)"})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{children:(0,a.jsx)(i.code,{children:"pose_quality"})}),(0,a.jsx)(i.td,{children:"Pose quality (GOOD/BAD/UNKNOWN)"})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{children:(0,a.jsx)(i.code,{children:"visual_tracking_quality"})}),(0,a.jsx)(i.td,{children:"Visual-only tracking quality"})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{children:(0,a.jsx)(i.code,{children:"online_calib"})}),(0,a.jsx)(i.td,{children:"Online calibration estimates for SLAM cameras and IMUs"})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{children:(0,a.jsx)(i.code,{children:"gravity_in_odometry"})}),(0,a.jsx)(i.td,{children:"Gravity vector in odometry frame"})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{children:(0,a.jsx)(i.code,{children:"transform_odometry_bodyimu"})}),(0,a.jsx)(i.td,{children:"Body IMU's pose in odometry reference frame"})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{children:(0,a.jsx)(i.code,{children:"transform_bodyimu_device"})}),(0,a.jsx)(i.td,{children:"Transform from body IMU to device frame"})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{children:(0,a.jsx)(i.code,{children:"linear_velocity_in_odometry"})}),(0,a.jsx)(i.td,{children:"Linear velocity in odometry frame in m/s"})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{children:(0,a.jsx)(i.code,{children:"angular_velocity_in_bodyimu"})}),(0,a.jsx)(i.td,{children:"Angular velocity in body IMU frame in rad/s"})]})]})]}),"\n",(0,a.jsxs)(i.p,{children:["Here, ",(0,a.jsx)(i.strong,{children:"body IMU"})," is the IMU that is picked as the reference for motion tracking. For Aria-Gen2's on-device VIO algorithm, this is often ",(0,a.jsx)(i.code,{children:"imu-left"}),"."]}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Important Note"}),": Always check ",(0,a.jsx)(i.code,{children:"status == VioStatus.VALID"})," and ",(0,a.jsx)(i.code,{children:"pose_quality == TrackingQuality.GOOD"})," for VIO data validity!"]}),"\n",(0,a.jsx)(i.h3,{id:"vio-high-frequency-data-structure",children:"VIO High Frequency Data Structure"}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"VIO High Frequency"})," results are generated directly from the on-device VIO results by performing IMU integration between VIO poses, hence ",(0,a.jsx)(i.strong,{children:"providing"})," a much higher data rate at approximately ",(0,a.jsx)(i.strong,{children:"800Hz"}),"."]}),"\n",(0,a.jsxs)(i.h4,{id:"data-type-openlooptrajectorypose",children:["Data Type: ",(0,a.jsx)(i.code,{children:"OpenLoopTrajectoryPose"})]}),"\n",(0,a.jsxs)(i.p,{children:["The ",(0,a.jsx)(i.strong,{children:"VioHighFrequency"})," stream ",(0,a.jsx)(i.strong,{children:"re-uses"})," the ",(0,a.jsx)(i.code,{children:"OpenLoopTrajectoryPose"})," data structure ",(0,a.jsx)(i.a,{href:"https://github.com/facebookresearch/projectaria_tools/blob/main/core/mps/Trajectory.h",children:"defined in MPS"}),"."]}),"\n",(0,a.jsxs)(i.table,{children:[(0,a.jsx)(i.thead,{children:(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.th,{children:"Field Name"}),(0,a.jsx)(i.th,{children:"Description"})]})}),(0,a.jsxs)(i.tbody,{children:[(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{children:(0,a.jsx)(i.code,{children:"tracking_timestamp"})}),(0,a.jsx)(i.td,{children:"Timestamp in device time domain, in microseconds"})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{children:(0,a.jsx)(i.code,{children:"transform_odometry_device"})}),(0,a.jsx)(i.td,{children:"Transformation from device to odometry coordinate frame, represented as a SE3 instance."})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{children:(0,a.jsx)(i.code,{children:"device_linear_velocity_odometry"})}),(0,a.jsx)(i.td,{children:"Translational velocity of device in odometry frame, in m/s"})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{children:(0,a.jsx)(i.code,{children:"angular_velocity_device"})}),(0,a.jsx)(i.td,{children:"Angular velocity of device in device frame, in rad/s"})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{children:(0,a.jsx)(i.code,{children:"quality_score"})}),(0,a.jsx)(i.td,{children:"Quality of pose estimation (higher = better)"})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{children:(0,a.jsx)(i.code,{children:"gravity_odometry"})}),(0,a.jsx)(i.td,{children:"Earth gravity vector in odometry frame"})]}),(0,a.jsxs)(i.tr,{children:[(0,a.jsx)(i.td,{children:(0,a.jsx)(i.code,{children:"session_uid"})}),(0,a.jsx)(i.td,{children:"Unique identifier for VIO tracking session"})]})]})]}),"\n",(0,a.jsxs)(i.p,{children:[(0,a.jsx)(i.strong,{children:"Important Note"}),": Due to the high frequency nature of this data (~800Hz), consider subsampling for visualization to maintain performance."]}),"\n",(0,a.jsx)(i.h3,{id:"vio-api-reference",children:"VIO API Reference"}),"\n",(0,a.jsxs)(i.p,{children:["In ",(0,a.jsx)(i.code,{children:"AriaGen2PilotDataProvider"}),", VIO data shares similar query APIs:"]}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.code,{children:"get_vrs_vio_data_by_index(stream_id, index)"}),": Query VIO data by index."]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.code,{children:"get_vrs_vio_data_by_time_ns(stream_id, timestamp, time_domain, query_options)"}),": Query VIO data by timestamp."]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.code,{children:"get_vrs_vio_high_freq_data_by_index(stream_id, index)"}),": Query VIO high frequency data by index."]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.code,{children:"get_vrs_vio_high_freq_data_by_time_ns(stream_id, timestamp, time_domain, query_options)"}),": Query VIO high frequency data by timestamp."]}),"\n"]}),"\n",(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-python",children:'# Check if VIO data is available\nvio_label = "vio"\nvio_stream_id = pilot_data_provider.get_vrs_stream_id_from_label(vio_label)\n\nvio_high_freq_label = "vio_high_frequency"\nvio_high_freq_stream_id = pilot_data_provider.get_vrs_stream_id_from_label(vio_high_freq_label)\n\nprint("=== VIO Data Availability ===")\nif vio_stream_id is None:\n    print(f"\u274c {vio_label} data stream does not exist in this VRS file.")\nelse:\n    print(f"\u2705 Found {vio_label} data stream: {vio_stream_id}")\n\nif vio_high_freq_stream_id is None:\n    print(f"\u274c {vio_high_freq_label} data stream does not exist in this VRS file.")\nelse:\n    print(f"\u2705 Found {vio_high_freq_label} data stream: {vio_high_freq_stream_id}")\n\n# Sample VIO data (if available)\nif vio_stream_id is not None:\n    print("\\n=== VIO Data Sample ===")\n\n    # Find the first valid VIO data sample\n    num_vio_samples = pilot_data_provider.get_vrs_num_data(vio_stream_id)\n    first_valid_index = None\n\n    for idx in range(100, 100 + min(10, num_vio_samples)):\n        vio_data = pilot_data_provider.get_vrs_vio_data_by_index(vio_stream_id, idx)\n        if vio_data is not None:\n            # Check if VIO data is valid (we\'ll import the status enums)\n            from projectaria_tools.core.sensor_data import VioStatus, TrackingQuality\n            if (vio_data.status == VioStatus.VALID and vio_data.pose_quality == TrackingQuality.GOOD):\n                first_valid_index = idx\n                break\n\n    if first_valid_index is not None:\n        vio_data = pilot_data_provider.get_vrs_vio_data_by_index(vio_stream_id, first_valid_index)\n        print("=" * 50)\n        print(f"First VALID VIO Data Sample (Index: {first_valid_index})")\n        print("=" * 50)\n\n        # Session Information\n        print(f"Session UID: {vio_data.frontend_session_uid}")\n        print(f"Frame ID: {vio_data.frame_id}")\n\n        # Timestamps\n        print(f"Capture Time: {vio_data.capture_timestamp_ns} ns")\n        print(f"Unix Time: {vio_data.unix_timestamp_ns} ns")\n\n        # Quality Status\n        print(f"Status: {vio_data.status}")\n        print(f"Pose Quality: {vio_data.pose_quality}")\n        print(f"Visual Quality: {vio_data.visual_tracking_quality}")\n\n        # Transforms\n        print(f"Transform Odometry \u2192 Body IMU:\\n{vio_data.transform_odometry_bodyimu.to_matrix()}")\n        print(f"Transform Body IMU \u2192 Device:\\n{vio_data.transform_bodyimu_device.to_matrix()}")\n\n        # Motion\n        print(f"Linear Velocity: {vio_data.linear_velocity_in_odometry}")\n        print(f"Angular Velocity: {vio_data.angular_velocity_in_bodyimu}")\n        print(f"Gravity Vector: {vio_data.gravity_in_odometry}")\n    else:\n        print("\u26a0\ufe0f  No valid VIO sample found")\n\n# Sample VIO High Frequency data (if available)\nif vio_high_freq_stream_id is not None:\n    print("\\n=== VIO High-Frequency Data Sample ===")\n\n    # Find the first VIO high_frequency data sample with high quality value\n    num_vio_high_freq_samples = pilot_data_provider.get_vrs_num_data(vio_high_freq_stream_id)\n    first_valid_index = None\n\n    for idx in range(min(10, num_vio_high_freq_samples)):\n        vio_high_freq_data = pilot_data_provider.get_vrs_vio_high_freq_data_by_index(vio_high_freq_stream_id, idx)\n        if vio_high_freq_data is not None and vio_high_freq_data.quality_score > 0.5:\n            first_valid_index = idx\n            break\n\n    if first_valid_index is not None:\n        vio_high_freq_data = pilot_data_provider.get_vrs_vio_high_freq_data_by_index(vio_high_freq_stream_id, first_valid_index)\n        print("=" * 50)\n        print(f"First VIO High Freq Data Sample with good quality score (Index: {first_valid_index})")\n        print("=" * 50)\n\n        # Timestamps, convert timedelta to nanoseconds\n        capture_timestamp_ns = int(vio_high_freq_data.tracking_timestamp.total_seconds() * 1e9)\n\n        # Session Information\n        print(f"Session UID: {vio_high_freq_data.session_uid}")\n\n        # Timestamps\n        print(f"Tracking Time: {capture_timestamp_ns} ns")\n\n        # Quality\n        print(f"Quality Score: {vio_high_freq_data.quality_score:.3f}")\n\n        # Transform\n        print(f"Transform Odometry \u2192 Device:\\n{vio_high_freq_data.transform_odometry_device.to_matrix()}")\n\n        # Motion\n        print(f"Linear Velocity: {vio_high_freq_data.device_linear_velocity_odometry}")\n        print(f"Angular Velocity: {vio_high_freq_data.angular_velocity_device}")\n        print(f"Gravity Vector: {vio_high_freq_data.gravity_odometry}")\n    else:\n        print("\u26a0\ufe0f  No valid VIO high frequency sample found")\n'})}),"\n",(0,a.jsx)(i.h3,{id:"vio-trajectory-visualization",children:"VIO Trajectory Visualization"}),"\n",(0,a.jsx)(i.p,{children:"The following code demonstrates how to visualize a VIO trajectory in a 3D view, showing the device's movement through space over time."}),"\n",(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-python",children:'# VIO trajectory visualization using sequential data access (matching Tutorial 5 approach)\nif vio_stream_id is not None:\n    print("\\n=== Visualizing on-device VIO trajectory in 3D view ===")\n\n    # Initialize Rerun for 3D visualization\n    rr.init("rerun_viz_vio_trajectory")\n    rr.notebook_show()\n\n    # Get device calibration for glasses outline\n    device_calib = pilot_data_provider.get_vrs_device_calibration()\n\n    # Get time bounds for VIO data\n    first_timestamp_ns = pilot_data_provider.get_vrs_timestamps_ns(vio_stream_id, TimeDomain.DEVICE_TIME)[0]\n    last_timestamp_ns = pilot_data_provider.get_vrs_timestamps_ns(vio_stream_id, TimeDomain.DEVICE_TIME)[-1]\n\n    # Play for only 3 seconds\n    total_length_ns = last_timestamp_ns - first_timestamp_ns\n    skip_begin_ns = int(15 * 1e9)  # Skip 15 seconds\n    duration_ns = int(3 * 1e9)     # 3 seconds\n    skip_end_ns = max(total_length_ns - skip_begin_ns - duration_ns, 0)\n\n    start_time_ns = first_timestamp_ns + skip_begin_ns\n    end_time_ns = start_time_ns + duration_ns\n\n    print(f"Visualizing VIO trajectory from {start_time_ns} to {end_time_ns} ns")\n\n    # Plot VIO trajectory in 3D view using sequential approach\n    # Need to keep a cache to store already-loaded trajectory\n    vio_traj_cached_full = []\n    valid_vio_count = 0\n\n    # Get VIO timestamps in the time window\n    all_vio_timestamps = pilot_data_provider.get_vrs_timestamps_ns(vio_stream_id, TimeDomain.DEVICE_TIME)\n    window_timestamps = [ts for ts in all_vio_timestamps if start_time_ns <= ts <= end_time_ns]\n\n    print(f"Processing {len(window_timestamps)} VIO samples in time window")\n\n    for timestamp_ns in window_timestamps:\n        # Query VIO data by timestamp\n        vio_data = pilot_data_provider.get_vrs_vio_data_by_time_ns(\n            vio_stream_id, timestamp_ns, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST\n        )\n\n        if vio_data is not None:\n            # Check VIO data validity, only plot for valid data\n            if (vio_data.status == VioStatus.VALID and vio_data.pose_quality == TrackingQuality.GOOD):\n                # Set timestamp\n                rr.set_time_nanos("device_time", vio_data.capture_timestamp_ns)\n\n                # Set and plot the Device pose for the current timestamp, as a RGB axis\n                T_World_Device = (\n                    vio_data.transform_odometry_bodyimu @ vio_data.transform_bodyimu_device\n                )\n\n                # Plot device pose as coordinate frame\n                rr.log(\n                    "world/device",\n                    ToTransform3D(\n                        T_World_Device,\n                        axis_length=0.05,\n                    ),\n                )\n\n                # Also plot Aria glass outline for visualization\n                aria_glasses_point_outline = AriaGlassesOutline(\n                    device_calib, use_cad_calib=True\n                )\n                rr.log(\n                    "world/device/glasses_outline",\n                    rr.LineStrips3D(\n                        aria_glasses_point_outline,\n                        colors=[200,200,200],\n                        radii=5e-4,\n                    ),\n                )\n\n                # Plot gravity direction vector\n                rr.log(\n                    "world/vio_gravity",\n                    rr.Arrows3D(\n                        origins=[T_World_Device.translation()[0]],\n                        vectors=[\n                            vio_data.gravity_in_odometry * 1e-2\n                        ],  # length converted from 9.8 meter -> 10 cm\n                        colors=[101,67,33],\n                        radii=1.5e-3,\n                    ),\n                    static=False,\n                )\n\n                # Plot VIO trajectory that are cached so far\n                vio_traj_cached_full.append(T_World_Device.translation()[0])\n                rr.log(\n                    "world/vio_trajectory",\n                    rr.LineStrips3D(\n                        vio_traj_cached_full,\n                        colors=[173, 216, 255],\n                        radii=1.5e-3,\n                    ),\n                    static=False,\n                )\n                valid_vio_count += 1\n            else:\n                print(f"VIO data is invalid for timestamp {timestamp_ns}")\n\n    print(f"Visualized {valid_vio_count} valid VIO poses out of {len(window_timestamps)} samples")\n\nelse:\n    print("Skipping VIO trajectory visualization - no VIO data available.")\n'})}),"\n",(0,a.jsx)(i.h3,{id:"vio-high-frequency-visualization",children:"VIO High Frequency Visualization"}),"\n",(0,a.jsx)(i.p,{children:"VIO High Frequency data provides much higher temporal resolution (~800Hz) compared to regular VIO data. This section demonstrates how to visualize the high-frequency trajectory data."}),"\n",(0,a.jsx)(i.pre,{children:(0,a.jsx)(i.code,{className:"language-python",children:'# VIO High Frequency trajectory visualization (if available)\nif vio_high_freq_stream_id is not None:\n    print("\\n=== Visualizing VIO High Frequency trajectory ===")\n\n    # Initialize Rerun for high frequency visualization\n    rr.init("rerun_viz_vio_high_freq_trajectory")\n    rr.notebook_show()\n\n    # Get device calibration for glasses outline\n    device_calib = pilot_data_provider.get_vrs_device_calibration()\n\n    # Get time bounds for VIO high frequency data\n    first_timestamp_ns = pilot_data_provider.get_vrs_timestamps_ns(vio_high_freq_stream_id, TimeDomain.DEVICE_TIME)[0]\n    last_timestamp_ns = pilot_data_provider.get_vrs_timestamps_ns(vio_high_freq_stream_id, TimeDomain.DEVICE_TIME)[-1]\n\n    # Play for only 2 seconds (shorter duration due to high frequency)\n    total_length_ns = last_timestamp_ns - first_timestamp_ns\n    skip_begin_ns = int(15 * 1e9)  # Skip 15 seconds\n    duration_ns = int(2 * 1e9)     # 2 seconds\n    skip_end_ns = max(total_length_ns - skip_begin_ns - duration_ns, 0)\n\n    start_time_ns = first_timestamp_ns + skip_begin_ns\n    end_time_ns = start_time_ns + duration_ns\n\n    print(f"Visualizing VIO High Frequency trajectory from {start_time_ns} to {end_time_ns} ns")\n\n    # Get VIO high frequency timestamps in the time window\n    all_vio_hf_timestamps = pilot_data_provider.get_vrs_timestamps_ns(vio_high_freq_stream_id, TimeDomain.DEVICE_TIME)\n    window_timestamps = [ts for ts in all_vio_hf_timestamps if start_time_ns <= ts <= end_time_ns]\n\n    # Subsample for performance (every 10th sample due to high frequency)\n    subsampled_timestamps = window_timestamps[::10]\n\n    print(f"Processing {len(subsampled_timestamps)} VIO High Frequency samples (subsampled from {len(window_timestamps)})")\n\n    # Plot VIO High Frequency trajectory in 3D view\n    vio_hf_traj_cached_full = []\n    valid_vio_hf_count = 0\n\n    for timestamp_ns in subsampled_timestamps:\n        # Query VIO high frequency data by timestamp\n        vio_hf_data = pilot_data_provider.get_vrs_vio_high_freq_data_by_time_ns(\n            vio_high_freq_stream_id, timestamp_ns, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST\n        )\n\n        if vio_hf_data is not None and vio_hf_data.quality_score > 0.5:\n            # Set timestamp\n            rr.set_time_nanos("device_time", int(vio_hf_data.tracking_timestamp.total_seconds() * 1e9))\n\n            # Plot device pose as coordinate frame\n            T_World_Device = vio_hf_data.transform_odometry_device\n            rr.log(\n                "world/device",\n                ToTransform3D(\n                    T_World_Device,\n                    axis_length=0.03,  # Smaller axis for high frequency data\n                ),\n            )\n\n            # Also plot Aria glass outline for visualization\n            aria_glasses_point_outline = AriaGlassesOutline(\n                device_calib, use_cad_calib=True\n            )\n            rr.log(\n                "world/device/glasses_outline",\n                rr.LineStrips3D(\n                    aria_glasses_point_outline,\n                    colors=[150,150,150],  # Slightly different color for high frequency\n                    radii=3e-4,\n                ),\n            )\n\n            # Plot gravity direction vector\n            rr.log(\n                "world/vio_hf_gravity",\n                rr.Arrows3D(\n                    origins=[T_World_Device.translation()[0]],\n                    vectors=[\n                        vio_hf_data.gravity_odometry * 1e-2\n                    ],  # length converted from 9.8 meter -> 10 cm\n                    colors=[67,101,33],  # Different color for high frequency\n                    radii=1e-3,\n                ),\n                static=False,\n            )\n\n            # Plot VIO High Frequency trajectory that are cached so far\n            vio_hf_traj_cached_full.append(T_World_Device.translation()[0])\n            rr.log(\n                "world/vio_hf_trajectory",\n                rr.LineStrips3D(\n                    vio_hf_traj_cached_full,\n                    colors=[255, 173, 216],  # Different color for high frequency\n                    radii=1e-3,\n                ),\n                static=False,\n            )\n\n            valid_vio_hf_count += 1\n\n    print(f"Visualized {valid_vio_hf_count} valid VIO High Frequency poses out of {len(subsampled_timestamps)} samples")\n\nelse:\n    print("Skipping VIO High Frequency trajectory visualization - no VIO High Frequency data available.")\n'})})]})}function h(e={}){const{wrapper:i}={...(0,r.R)(),...e.components};return i?(0,a.jsx)(i,{...e,children:(0,a.jsx)(_,{...e})}):_(e)}},69470(e,i,n){n.d(i,{A:()=>a});n(96540);var t=n(74848);const a=({notebookUrl:e,colabUrl:i,colabDisabled:n=!1})=>{const a=()=>(0,t.jsx)("svg",{style:{marginRight:"8px",width:"16px",height:"16px"},fill:"currentColor",viewBox:"0 0 24 24",children:(0,t.jsx)("path",{d:"M16.9414 4.9757a7.033 7.033 0 0 0-4.9308 2.0324 7.033 7.033 0 0 0-.1232 9.8068l2.395-2.395a3.6455 3.6455 0 0 1 5.1497-5.1478l2.397-2.3989a7.033 7.033 0 0 0-4.8877-1.9175zM7.07 4.9855a7.033 7.033 0 0 0-4.8878 1.9175l2.395 2.3989a3.6434 3.6434 0 0 1 5.1497 5.1478l2.395 2.395a7.033 7.033 0 0 0-.1232-9.8068A7.033 7.033 0 0 0 7.07 4.9855zm15.0191 2.5923l-2.395 2.395a3.6455 3.6455 0 0 1-5.1497 5.1497l-2.395 2.395a7.033 7.033 0 0 0 9.9397-.123z"})});return(0,t.jsxs)("div",{style:{display:"flex",gap:"10px",marginBottom:"20px",flexWrap:"wrap"},children:[(0,t.jsxs)("a",{href:e,target:"_blank",rel:"noopener noreferrer",style:{display:"inline-flex",alignItems:"center",padding:"8px 16px",backgroundColor:"#f6f8fa",border:"1px solid #d0d7de",borderRadius:"6px",textDecoration:"none",color:"#24292f",fontSize:"14px",fontWeight:"500",transition:"background-color 0.2s"},onMouseEnter:e=>{e.target.style.backgroundColor="#f3f4f6"},onMouseLeave:e=>{e.target.style.backgroundColor="#f6f8fa"},children:[(0,t.jsx)("svg",{style:{marginRight:"8px",width:"16px",height:"16px"},fill:"currentColor",viewBox:"0 0 16 16",children:(0,t.jsx)("path",{d:"M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z"})}),"View Notebook on GitHub"]}),!n&&i?(0,t.jsxs)("a",{href:i,target:"_blank",rel:"noopener noreferrer",style:{display:"inline-flex",alignItems:"center",padding:"8px 16px",backgroundColor:"#f9ab00",border:"1px solid #d0d7de",borderRadius:"6px",textDecoration:"none",color:"#fff",fontSize:"14px",fontWeight:"500",transition:"background-color 0.2s"},onMouseEnter:e=>{e.target.style.backgroundColor="#e8a500"},onMouseLeave:e=>{e.target.style.backgroundColor="#f9ab00"},children:[(0,t.jsx)(a,{}),"Run in Google Colab"]}):(0,t.jsxs)("button",{onClick:()=>{alert("Google Colab integration is not available yet. Please download the notebook and run it locally.")},style:{display:"inline-flex",alignItems:"center",padding:"8px 16px",backgroundColor:"#f6f8fa",border:"1px solid #d0d7de",borderRadius:"6px",color:"#656d76",fontSize:"14px",fontWeight:"500",cursor:"not-allowed",transition:"background-color 0.2s"},children:[(0,t.jsx)(a,{}),"Colab (Coming Soon)"]})]})}},28453(e,i,n){n.d(i,{R:()=>s,x:()=>o});var t=n(96540);const a={},r=t.createContext(a);function s(e){const i=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function o(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),t.createElement(r.Provider,{value:i},e.children)}}}]);