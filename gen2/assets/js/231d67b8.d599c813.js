"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[3586],{28453:(e,t,n)=>{n.d(t,{R:()=>s,x:()=>o});var i=n(96540);const a={},r=i.createContext(a);function s(e){const t=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),i.createElement(r.Provider,{value:t},e.children)}},33265:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>o,default:()=>h,frontMatter:()=>s,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"dataset/pilot/content","title":"Content","description":"The Aria Gen 2 Pilot Dataset is a multi-participant, egocentric dataset collected using Aria Gen 2 glasses with four participants (a primary wearer and three co-participants) simultaneously recording a variety of daily activities, resulting in rich, time-synchronized multimodal data. The dataset is structured to demonstrate the Gen 2 device capability and potential applications in computer vision, multimodal learning, robotics, and contextual AI.","source":"@site/docs-research-tools/dataset/pilot/content.mdx","sourceDirName":"dataset/pilot","slug":"/dataset/pilot/content","permalink":"/projectaria_tools/gen2/research-tools/dataset/pilot/content","draft":false,"unlisted":false,"editUrl":"https://www.internalfb.com/code/fbsource/arvr/projects/ariane/aria_research_kit/projectaria_tools/website/docs-research-tools/dataset/pilot/content.mdx","tags":[],"version":"current","sidebarPosition":0,"frontMatter":{"sidebar_position":0,"title":"Content"},"sidebar":"researchToolsSidebar","previous":{"title":"Advanced Installation In Pixi","permalink":"/projectaria_tools/gen2/research-tools/projectariatools/advanced-installation-pixi"},"next":{"title":"Format","permalink":"/projectaria_tools/gen2/research-tools/dataset/pilot/format"}}');var a=n(74848),r=n(28453);const s={sidebar_position:0,title:"Content"},o="Aria Gen 2 Pilot Dataset",c={},d=[{value:"Dataset Content",id:"dataset-content",level:2},{value:"Additional Perception Algorithms",id:"additional-perception-algorithms",level:3},{value:"Resources",id:"resources",level:2},{value:"Citation",id:"citation",level:2}];function l(e){const t={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.header,{children:(0,a.jsx)(t.h1,{id:"aria-gen-2-pilot-dataset",children:"Aria Gen 2 Pilot Dataset"})}),"\n",(0,a.jsx)(t.p,{children:"The Aria Gen 2 Pilot Dataset is a multi-participant, egocentric dataset collected using Aria Gen 2 glasses with four participants (a primary wearer and three co-participants) simultaneously recording a variety of daily activities, resulting in rich, time-synchronized multimodal data. The dataset is structured to demonstrate the Gen 2 device capability and potential applications in computer vision, multimodal learning, robotics, and contextual AI."}),"\n",(0,a.jsx)(t.hr,{}),"\n",(0,a.jsx)(t.h2,{id:"dataset-content",children:"Dataset Content"}),"\n",(0,a.jsx)(t.p,{children:"The Aria Gen 2 pilot dataset comprises four primary data content types:"}),"\n",(0,a.jsxs)(t.ol,{children:["\n",(0,a.jsxs)(t.li,{children:["raw sensor streams acquired directly from Aria Gen 2 devices and recorded by ",(0,a.jsx)(t.a,{href:"/technical-specs/device/profile",children:"Profile 8"})]}),"\n",(0,a.jsx)(t.li,{children:"real-time machine perception outputs generated on-device via embedded algorithms during data collection"}),"\n",(0,a.jsxs)(t.li,{children:["offline machine perception results produced by ",(0,a.jsx)(t.a,{href:"/ark/mps/start",children:"Machine Perception Services (MPS)"})," during post-processing; and"]}),"\n",(0,a.jsx)(t.li,{children:"outputs from additional offline perception algorithms. See below for details."}),"\n"]}),"\n",(0,a.jsx)(t.p,{children:"Content (1) and (2) are obtained natively from the device, whereas (3) and (4) are derived through post-hoc processing."}),"\n",(0,a.jsx)(t.h3,{id:"additional-perception-algorithms",children:"Additional Perception Algorithms"}),"\n",(0,a.jsxs)(t.table,{children:[(0,a.jsx)(t.thead,{children:(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.th,{children:"Algorithm"}),(0,a.jsx)(t.th,{children:"Description"}),(0,a.jsx)(t.th,{children:"Output"})]})}),(0,a.jsxs)(t.tbody,{children:[(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:(0,a.jsx)(t.strong,{children:"Directional Automatic Speech Recognition (ASR)"})}),(0,a.jsx)(t.td,{children:"Distinguishes between wearer and others, generating timestamped transcripts for all sequences. Enables analysis of conversational dynamics and social context."}),(0,a.jsx)(t.td,{children:"Timestamped transcripts of speech."})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:(0,a.jsx)(t.strong,{children:"Heart Rate Estimation"})}),(0,a.jsx)(t.td,{children:"Uses PPG sensors to estimate continuous heart rate, reflecting physical activity and physiological state. Coverage for over 95% of recording duration."}),(0,a.jsx)(t.td,{children:"Timestamped heart rate in beats per minute."})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:(0,a.jsx)(t.strong,{children:"Hand-Object Interaction Recognition"})}),(0,a.jsx)(t.td,{children:"Segments left/right hands and interacted objects, enabling analysis of manipulation patterns and object usage."}),(0,a.jsx)(t.td,{children:"Segmentation masks for hands and objects per RGB image."})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:(0,a.jsx)(t.a,{href:"https://facebookresearch.github.io/projectaria_tools/docs/open_models/evl",children:(0,a.jsx)(t.strong,{children:"3D Object Detection (Egocentric Voxel Lifting)"})})}),(0,a.jsx)(t.td,{children:"Detects 2D and 3D bounding boxes for objects in indoor scenes using multi-camera data. Supports spatial understanding and scene reconstruction."}),(0,a.jsx)(t.td,{children:"2D and 3D bounding boxes with class prediction."})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:(0,a.jsx)(t.strong,{children:"Depth Estimation (Foundation Stereo)"})}),(0,a.jsx)(t.td,{children:"Generates depth maps from overlapping CV cameras, enabling research in 3D scene understanding and object localization."}),(0,a.jsx)(t.td,{children:"Depth images, rectified CV images, and corresponding camera intrinsics/extrinsics."})]})]})]}),"\n",(0,a.jsx)(t.hr,{}),"\n",(0,a.jsx)(t.h2,{id:"resources",children:"Resources"}),"\n",(0,a.jsx)(t.p,{children:"For more information about the Aria Gen 2 Pilot Dataset:"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:["\ud83d\udcc4 ",(0,a.jsxs)(t.a,{href:"https://arxiv.org/abs/2510.16134",children:[(0,a.jsx)(t.strong,{children:"ArXiv Paper"}),": https://arxiv.org/abs/2510.16134"]})]}),"\n",(0,a.jsxs)(t.li,{children:["\ud83c\udf10 ",(0,a.jsxs)(t.a,{href:"https://www.projectaria.com/datasets/gen2pilot/",children:[(0,a.jsx)(t.strong,{children:"Project Website"}),": https://www.projectaria.com/datasets/gen2pilot/"]})]}),"\n",(0,a.jsxs)(t.li,{children:["\ud83d\udcbb ",(0,a.jsxs)(t.a,{href:"https://github.com/facebookresearch/projectaria_gen2_pilot_dataset",children:[(0,a.jsx)(t.strong,{children:"GitHub Repository"}),": https://github.com/facebookresearch/projectaria_gen2_pilot_dataset"]})]}),"\n",(0,a.jsxs)(t.li,{children:["\ud83d\udd0d ",(0,a.jsxs)(t.a,{href:"https://explorer.projectaria.com/gen2pilot",children:[(0,a.jsx)(t.strong,{children:"Dataset Explorer"}),": https://explorer.projectaria.com/gen2pilot"]})]}),"\n"]}),"\n",(0,a.jsx)(t.hr,{}),"\n",(0,a.jsx)(t.h2,{id:"citation",children:"Citation"}),"\n",(0,a.jsx)(t.p,{children:"If you use the Aria Gen 2 Pilot Dataset in your research, please cite the following:"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-bibtex",children:"@misc{kong2025ariagen2pilot,\n    title         ={Aria Gen 2 Pilot Dataset},\n    author        ={Chen Kong and James Fort and Aria Kang and Jonathan Wittmer and Simon Green and Tianwei Shen and Yipu Zhao and Cheng Peng and Gustavo Solaira and Andrew Berkovich and Nikhil Raina and Vijay Baiyya and Evgeniy Oleinik and Eric Huang and Fan Zhang and Julian Straub and Mark Schwesinger and Luis Pesqueira and Xiaqing Pan and Jakob Julian Engel and Carl Ren and Mingfei Yan and Richard Newcombe},\n    year          ={2025},\n    eprint        ={2510.16134},\n    archivePrefix ={arXiv},\n    primaryClass  ={cs.CV},\n    url           ={https://arxiv.org/abs/2510.16134},\n}\n"})})]})}function h(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(l,{...e})}):l(e)}}}]);