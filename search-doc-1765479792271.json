[{"title":"Aria Research Kit","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/about_ARK","content":"","keywords":"","version":"Next"},{"title":"Just received Project Aria glasses?​","type":1,"pageTitle":"Aria Research Kit","url":"/projectaria_tools/docs/ARK/about_ARK#just-received-project-aria-glasses","content":" The Quickstart Guide shows how to set up your glasses and make recordingsThe Glasses User Manual provides a range of information, including how to factory reset your glasses  ","version":"Next","tagName":"h2"},{"title":"Need to record data?​","type":1,"pageTitle":"Aria Research Kit","url":"/projectaria_tools/docs/ARK/about_ARK#need-to-record-data","content":" Follow the steps in the Quickstart Guide to set up your glasses, make and download recordings.  ","version":"Next","tagName":"h2"},{"title":"Additional recording capabilities​","type":1,"pageTitle":"Aria Research Kit","url":"/projectaria_tools/docs/ARK/about_ARK#additional-recording-capabilities","content":" If you wish to get personalized Eye Gaze MPS outputs, perform In-Session Eye Gaze Calibration at the start of each recordingIf you wish to create time synchronized recordings between multiple Aria glasses, use the Client SDK to enable TICSync recordings  ","version":"Next","tagName":"h3"},{"title":"Processing and using the data​","type":1,"pageTitle":"Aria Research Kit","url":"/projectaria_tools/docs/ARK/about_ARK#processing-and-using-the-data","content":" Use Aria Studio or the MPS CLI to request MPS derived data outputsUse Project Aria Tools to: Make VRS data easier to consume by downstream applications without needing to convert to another file formatVisualize the dataExport VRS files to MP4, to facilitate reviews  ","version":"Next","tagName":"h3"},{"title":"Need to stream data?​","type":1,"pageTitle":"Aria Research Kit","url":"/projectaria_tools/docs/ARK/about_ARK#need-to-stream-data","content":" Set up your glasses as outlined in the Quickstart Guide and then install the Client SDK. ","version":"Next","tagName":"h2"},{"title":"Aria Studio","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/aria_studio","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Aria Studio","url":"/projectaria_tools/docs/ARK/aria_studio#overview","content":" Aria Studio is part of the Aria Research Kit. The application will launch in your default web browser and provides the ability to:  Examine preview thumbnails and metadata of Aria recordings (on device or stored locally)Transfer recordings from the device to the local computer or delete recordings stored on the device or local computerVisualize recordings We show visualizations of all the sensor streams except for audio. Audio outputs are not supported at this time. Group recordings for multi-sequence processing jobs Grouped recordings are a special feature unlocked by Aria Studio. Users can group VRS recordings from anywhere in their directory for organizational purposes, or to request Multi-SLAM outputs.Groups are stored on your local machine and associated with your Aria user account Request and manage Machine Perception Services (MPS)  Aria Studio can be installed on:  x64 Linux distributions of: Fedora 36, 37, 38Ubuntu jammy (22.04 LTS) and focal (20.04 LTS) Mac ARM-based (M1) with MacOS 11 (Big Sur) or newer Note: Current versions of Aria Studio are not supported on Intel-based Mac machines, though version 1.0.3 is available for download. x64 Windows Intel machines running Windows 11 or newer  Aria Studio is supported on Google Chrome.  Just received Aria glasses? Go to the Quickstart Guide for info on how to start using them.  ","version":"Next","tagName":"h2"},{"title":"Machine Perception Services (MPS) capabilities​","type":1,"pageTitle":"Aria Studio","url":"/projectaria_tools/docs/ARK/aria_studio#machine-perception-services-mps-capabilities","content":" Request MPS processing for recordings SLAM, Eye Gaze and Hand Tracking are available for individual recordingsMulti-SLAM is available for grouped recordings View MPS request history and status of current requestsAccess MPS CLI capabilities through a graphical user interface, such as: Resumable uploadsMPS outputs automatically downloading (as long as Aria Studio is running)The ability to submit multiple MPS requests at the same time  Like the MPS CLI, you can exercise granular control over settings like uploads and download speeds by customizing $HOME/.projectaria/mps.ini. Go to the MPS CLI Guide for details about all the settings.  ","version":"Next","tagName":"h2"},{"title":"Install Aria Studio with one click​","type":1,"pageTitle":"Aria Studio","url":"/projectaria_tools/docs/ARK/aria_studio#install-aria-studio-with-one-click","content":" Install the Aria Studio 1.1.x package appropriate for your computer platform. Double-click on the downloaded file and follow the prompts to complete the installation.  Mac ARM 1.1.1: Download the Package for Mac ARMWindows 1.1.1: Download the Package for WindowsLinux 1.1.2: Download the Package for Linux  ","version":"Next","tagName":"h2"},{"title":"Previous versions​","type":1,"pageTitle":"Aria Studio","url":"/projectaria_tools/docs/ARK/aria_studio#previous-versions","content":" The table below includes links to previous versions of Aria Studio across multiple platforms. Note that if you are installing version 1.0.3, you will need to allow the installation of unsigned builds.  Version\tPlatform\tDownload link1.1.1\tLinux\tAria_Studio-1.1.1-Linux.appimage 1.1.0\tMac ARM\tAria_Studio-1.1.0-arm64.dmg 1.1.0\tWindows\tAria_Studio-1.1.0-windows.exe 1.1.0\tLinux\tAria_Studio-1.1.0-Linux.appimage 1.0.3\tMac ARM\tAria_Studio-1.0.3-arm64.pkg 1.0.3\tMac Intel\tAria_Studio-1.0.3-Intel.pkg 1.0.3\tWindows\tAria_Studio-1.0.3-windows.exe 1.0.3\tLinux\tAria_Studio-1.0.3-Linux.AppImage  Note: Version 1.0.3 is an unsigned build​  You will need to allow the installation of unsigned builds if you want to install version 1.0.3 in this way. We don't recommend enabling unsigned builds for any other versions.  macOS  Double-click the downloaded PKG and select Done.Go to System Settings &gt; Privacy &amp; Security, scroll to the bottom, and select Open Anyway to enable the installation.  Windows  Double-click on the downloaded EXE file.When the &quot;Windows protected your PC&quot; dialog appears, select More info.Select Run anyway.  ","version":"Next","tagName":"h3"},{"title":"Install Aria Studio via PyPI​","type":1,"pageTitle":"Aria Studio","url":"/projectaria_tools/docs/ARK/aria_studio#install-aria-studio-via-pypi","content":" note If you're going to install Aria Studio via PyPI, you'll want Python 3.11 or above. To check which version of Python 3 you have, use python3 --version. Get the latest version from the Python 3 download page.  To install Aria Studio via PyPI, use the following command:  python3 -m pip install aria_studio --no-cache-dir   ","version":"Next","tagName":"h2"},{"title":"Optional: Create a virtual environment​","type":1,"pageTitle":"Aria Studio","url":"/projectaria_tools/docs/ARK/aria_studio#optional-create-a-virtual-environment","content":" When using PyPI's pip command, it's a best practice to use a virtual environment. This will keep all the modules in one folder and avoid breaking anything in your local environment. To install Aria Studio in a virtual environment, use the following steps:  ","version":"Next","tagName":"h3"},{"title":"Linux and macOS​","type":1,"pageTitle":"Aria Studio","url":"/projectaria_tools/docs/ARK/aria_studio#linux-and-macos","content":" rm -rf $HOME/venv/aria_studio python3 -m venv $HOME/venv/aria_studio source $HOME/venv/aria_studio/bin/activate python3 -m pip install aria_studio --no-cache-dir   ","version":"Next","tagName":"h3"},{"title":"Windows​","type":1,"pageTitle":"Aria Studio","url":"/projectaria_tools/docs/ARK/aria_studio#windows","content":" remove-Item -Path $HOME/venv/aria_studio -recurse -Force py -3 -m venv $HOME/venv/aria_studio &amp; $HOME\\venv\\aria_studio\\Scripts\\Activate.ps1 py -3 -m pip install aria_studio --no-cache-dir   ","version":"Next","tagName":"h3"},{"title":"Run Aria Studio​","type":1,"pageTitle":"Aria Studio","url":"/projectaria_tools/docs/ARK/aria_studio#run-aria-studio","content":" ","version":"Next","tagName":"h2"},{"title":"Run Aria Studio from your desktop (single-click installation)​","type":1,"pageTitle":"Aria Studio","url":"/projectaria_tools/docs/ARK/aria_studio#run-aria-studio-from-your-desktop-single-click-installation","content":" If you installed Aria Studio used single-click installation, you can run Aria Studio like you would any other application on your desktop.  ","version":"Next","tagName":"h3"},{"title":"Run Aria Studio using Python (PyPI installation)​","type":1,"pageTitle":"Aria Studio","url":"/projectaria_tools/docs/ARK/aria_studio#run-aria-studio-using-python-pypi-installation","content":" aria_studio   Aria Studio will then open in your web browser.  Closing the browser won't quit the application If you close the browser, Aria Studio will continue to run in the background, which can be helpful when running MPS requests. Use Ctrl + C in the command window to quit the application.  To return to your installation of Aria Studio at any time, restart the virtual environment using the following commands:  source $HOME/aria_studio/bin/activate aria_studio   ","version":"Next","tagName":"h3"},{"title":"Using Aria Studio​","type":1,"pageTitle":"Aria Studio","url":"/projectaria_tools/docs/ARK/aria_studio#using-aria-studio","content":" On the main page you’ll see options to view recordings on your glasses, recordings on your computer, grouped recordings and past MPS requests.  You’ll also be able to see the status of any glasses plugged into your computer.  ","version":"Next","tagName":"h2"},{"title":"Recordings on glasses​","type":1,"pageTitle":"Aria Studio","url":"/projectaria_tools/docs/ARK/aria_studio#recordings-on-glasses","content":" Plug your Aria glasses into your computerSelect Recordings on glasses If you do not see a connected device, try: Changing portsPlugging the glasses in more directly if you’re using a USB hubIf your glasses battery is flat, wait 10 minutes for them to chargeIf you’re on Linux you may need to fix driver issues You’ll see a thumbnail preview of the recording, plus metadata for each recording  From this page you can see video previews of each recording, download recordings and delete recordings.  When recordings are downloaded, they’ll also download a JSON file containing the file’s metadata. Metadata can also be extracted from the VRS file, but the JSON file provides a text editor readable file with recording profile, name, OS version the recording was made with, etc.  ","version":"Next","tagName":"h2"},{"title":"Recordings on computer​","type":1,"pageTitle":"Aria Studio","url":"/projectaria_tools/docs/ARK/aria_studio#recordings-on-computer","content":" From “Recordings on computer” you can request MPS, add recordings to a group or visualize the recordings. Select the info icon (or file information in Actions) for recording metadata or status of MPS requests associated with that file.  When you request MPS, the output results will be automatically downloaded to an MPS folder in the requesting VRS file’s directory.    ","version":"Next","tagName":"h2"},{"title":"Groups​","type":1,"pageTitle":"Aria Studio","url":"/projectaria_tools/docs/ARK/aria_studio#groups","content":" In the “Groups” page you can create new groups, examine recordings within a group or request Multi-SLAM MPS processing on a group of recordings. VRS files are not duplicated when they are grouped. So one file can be in multiple groups, without taking up extra storage space.  Go to “Recordings on computer” to add recordings to a group. For a Multi-SLAM request to be successful, all recordings in the group need to have used recording profiles that collect SLAM camera and IMU data.  ","version":"Next","tagName":"h2"},{"title":"Creating and using groups​","type":1,"pageTitle":"Aria Studio","url":"/projectaria_tools/docs/ARK/aria_studio#creating-and-using-groups","content":" Go to Groups and select Create a new groupProvide a group name The group name needs to be a valid directory name, as this will be the name of the folder where outputs are automatically downloaded to Select a directory This directory will define where the group folder for outputs will be savedVRS recordings will not be added to this directoryThe outputs will include a JSON file with the file paths of all the VRS source files Go to Recordings on computer to add VRS files to the group You can select multiple VRS files at the same time Return to Groups Select the info icon to view group or individual recording dataSelect Request MPS to request Multi-SLAM MPS  ","version":"Next","tagName":"h3"},{"title":"Past MPS requests​","type":1,"pageTitle":"Aria Studio","url":"/projectaria_tools/docs/ARK/aria_studio#past-mps-requests","content":" Past MPS requests is a page that logs the status of current and previous MPS requests.  ","version":"Next","tagName":"h2"},{"title":"Troubleshooting/FAQ​","type":1,"pageTitle":"Aria Studio","url":"/projectaria_tools/docs/ARK/aria_studio#troubleshootingfaq","content":" ","version":"Next","tagName":"h2"},{"title":"My MPS request didn’t automatically download!​","type":1,"pageTitle":"Aria Studio","url":"/projectaria_tools/docs/ARK/aria_studio#my-mps-request-didnt-automatically-download","content":" MPS requests can only be downloaded when Aria Studio is open. Open Aria Studio to check the status of MPS requests. You’ll see “Successfully Downloaded” as the status.  ","version":"Next","tagName":"h3"},{"title":"What happens if I quit Aria Studio while files are uploading?​","type":1,"pageTitle":"Aria Studio","url":"/projectaria_tools/docs/ARK/aria_studio#what-happens-if-i-quit-aria-studio-while-files-are-uploading","content":" Next time you open Aria Studio the uploads should resume. Once files have successfully uploaded you’ll see “Processing” as the status.  ","version":"Next","tagName":"h3"},{"title":"What are the different statuses and steps when submitting MPS requests?​","type":1,"pageTitle":"Aria Studio","url":"/projectaria_tools/docs/ARK/aria_studio#what-are-the-different-statuses-and-steps-when-submitting-mps-requests","content":" Go to the MPS CLI Guide for details about how the back end works.  ","version":"Next","tagName":"h3"},{"title":"How do I quit Aria Studio?​","type":1,"pageTitle":"Aria Studio","url":"/projectaria_tools/docs/ARK/aria_studio#how-do-i-quit-aria-studio","content":" Press Ctrl + C in the original console where aria_studio was started.  ","version":"Next","tagName":"h3"},{"title":"Unable to login (on Mac)​","type":1,"pageTitle":"Aria Studio","url":"/projectaria_tools/docs/ARK/aria_studio#unable-to-login-on-mac","content":" Go to MPS Troubleshooting - Authentication issue on macOS if you encounter this issue.  ","version":"Next","tagName":"h3"},{"title":"Where are my groups?​","type":1,"pageTitle":"Aria Studio","url":"/projectaria_tools/docs/ARK/aria_studio#where-are-my-groups","content":" Groups are stored locally and associated with a specific user. So if you’ve logged into Aria Studio using a different account name, you won’t see the groups associated with another user. ","version":"Next","tagName":"h3"},{"title":"ARK SW Downloads and Updates","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/ark_downloads","content":"","keywords":"","version":"Next"},{"title":"Mobile Companion App​","type":1,"pageTitle":"ARK SW Downloads and Updates","url":"/projectaria_tools/docs/ARK/ark_downloads#mobile-companion-app","content":" The current version of the Companion App is v245. Go to ARK Software Release Notes to learn more about the latest features.  iOSAndroid iOS​ Requirements​ If you're using iOS, your mobile device will need: OS version must be iOS 14 or above(Optional) TrueDepth camera (iPhone X or later) needed for eye-tracking calibration Download and Install​ The Aria Mobile Companion app is available on iOS as a beta app through TestFlight. On your phone, scan this QR code to open the Aria Testflight invitation link and download the app. To update: Open the TestFlight appNext to the Aria app, select Update    ","version":"Next","tagName":"h2"},{"title":"Aria Studio​","type":1,"pageTitle":"ARK SW Downloads and Updates","url":"/projectaria_tools/docs/ARK/ark_downloads#aria-studio","content":" Aria Studio is available on:  x64 Linux distributions of: Fedora 36, 37, 38Ubuntu Jammy (22.04 LTS) and Focal (20.04 LTS) Mac Intel or Mac ARM-based (M1) with MacOS 11 (Big Sur) or newerx64 Windows Intel machines running Windows 11 or newer  You can install it through either one-click installation or via PyPI (pip instiallation). Go to the Aria Studio guide to get started.    ","version":"Next","tagName":"h2"},{"title":"Client SDK with CLI​","type":1,"pageTitle":"ARK SW Downloads and Updates","url":"/projectaria_tools/docs/ARK/ark_downloads#client-sdk-with-cli","content":" Aria Client SDK with CLI is available on:  x64 Linux distributions of: Fedora 36, 37, 38Ubuntu Jammy (22.04 LTS) Mac Intel or Mac ARM-based (M1) with MacOS 11 (Big Sur) or newer  It is installed via PyPI (pip instiallation). Go to the Aria Client SDK setup guide to get started.  ","version":"Next","tagName":"h2"},{"title":"Are these apps open source?​","type":1,"pageTitle":"ARK SW Downloads and Updates","url":"/projectaria_tools/docs/ARK/ark_downloads#are-these-apps-open-source","content":" The Mobile Companion app, Aria Studio, and Client SDK are Meta Licensed Materials and are licensed by Meta to research partners via organizational or individual research agreements. Go to projectaria.com to find out how to become a research partner.  This software is not part of Project Aria Tools, so it won’t be downloaded when you install Project Aria Tools Data Utilities, which is open source and licensed under Apache 2.0. ","version":"Next","tagName":"h2"},{"title":"Project Aria Glasses Quickstart","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/ARK_quickstart","content":"","keywords":"","version":"Next"},{"title":"Get set up​","type":1,"pageTitle":"Project Aria Glasses Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#get-set-up","content":" Once you've been approved and can receive your Aria glasses you will get the following emails:  Welcome to Aria Contains your account details, use these for signing into the Mobile app [Academic Partners only] Join Project Aria Academic Partner Announcements, Feedback &amp; Support Follow the prompts to join Aria's research community space where researchers and engineers at Meta and all our Academic partners can connect, ask questions, share ideas and provide support.See How to join the Academic Partner Workplace group for further instructions.  ","version":"Next","tagName":"h2"},{"title":"Get connected​","type":1,"pageTitle":"Project Aria Glasses Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#get-connected","content":" Before your glasses arrive we encourage you to join the Academic Partners Workplace group. It's a great place to get the latest announcements, provide feedback, ask questions. Unboxing videos are very welcome!  ","version":"Next","tagName":"h2"},{"title":"Get to know your glasses​","type":1,"pageTitle":"Project Aria Glasses Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#get-to-know-your-glasses","content":" About the Aria Research Kit Includes hardware requirements for using ARK apps Technical Specifications Go to the Tech Spec part of the wiki to find out about Aria capabilities, recording profiles etc Glasses Manual Information about Project Aria glasses buttons, powering on and off, the privacy switch, how to do a factory reset, LED states, etc Recording Profiles Information and guidance about recording profiles and how to create Custom Profiles Aria Cable Clip Project Aria can provide a cable to securely connect the charge cable to the glasses. This can be helpful if using a battery pack while recording     ","version":"Next","tagName":"h2"},{"title":"Update your glasses​","type":1,"pageTitle":"Project Aria Glasses Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#update-your-glasses","content":" You'll need to update your Aria glasses using the Mobile companion app before you can use them.  ","version":"Next","tagName":"h2"},{"title":"Install the Mobile companion app​","type":1,"pageTitle":"Project Aria Glasses Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#install-the-mobile-companion-app","content":" Follow the instructions in the ARK SW Downloads and Updates page to download and install the app (this is where you'll download updates as well).  ","version":"Next","tagName":"h3"},{"title":"Pair your glasses​","type":1,"pageTitle":"Project Aria Glasses Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#pair-your-glasses","content":" Plug your glasses into power using the provided cableSign into the app using your provided user name and passwordFollow the prompts to pair your glasses Your glasses are automatically updated when you first pair themGo to the Mobile app page for further information  Further updates will be queued automatically when you use your glasses via the Mobile Companion App. If you need to manually check for updates, select Device Settings and then select Check for OS Updates.  ","version":"Next","tagName":"h3"},{"title":"To view Device Settings​","type":1,"pageTitle":"Project Aria Glasses Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#to-view-device-settings","content":" Tap the Paired Glasses info card.    ","version":"Next","tagName":"h3"},{"title":"Set your Default Recording Profile​","type":1,"pageTitle":"Project Aria Glasses Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#set-your-default-recording-profile","content":" You can set the default recording profile via the mobile app. Once set, this is the default profile used when recording or streaming data.  When you select New Recording in the Mobile Companion app, tap Recording profile to view sensor settings or to select a different recording profile. Recording profile options include a custom profile where you can alter which sensors are used and how.  You can change the Default Recording Profile when selecting recording profiles or from Device Settings.  ","version":"Next","tagName":"h2"},{"title":"Install Project Aria Tools (optional)​","type":1,"pageTitle":"Project Aria Glasses Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#install-project-aria-tools-optional","content":" The MPS CLI is the preferred way to request Machine Perception Services on your data. Install Project Aria Tools via virtual environment to be able to use the MPS CLI. You can only request Multi-SLAM MPS via the CLI.  ","version":"Next","tagName":"h2"},{"title":"Install Client SDK and CLI (optional)​","type":1,"pageTitle":"Project Aria Glasses Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#install-client-sdk-and-cli-optional","content":" Once you've set up your glasses with the Mobile Companion app, researchers can use the Client SDK or CLI to directly interact with Aria glasses. This is particularly helpful for streaming and subscribing to sensor data.  Go to the Client SDK and CLI section for how to install the SDK and explore code samples.  ","version":"Next","tagName":"h2"},{"title":"Record Data​","type":1,"pageTitle":"Project Aria Glasses Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#record-data","content":" It's best to start and stop recording using the Capture button, Mobile Companion app, the CLI or SDK.  ","version":"Next","tagName":"h2"},{"title":"General recording principles​","type":1,"pageTitle":"Project Aria Glasses Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#general-recording-principles","content":" If you start recording with one method, you can stop recording using a different methodPlease allow a few seconds before the recording starts. You'll know recording has started when the Recording LEDs turn onThe recording does not stop until the Recoding LED turns off Because the Aria glasses need to finish indexing before the recording stops, may be a small delay between initiating the stop and the recording stopping The longer your glasses record for, the longer it takes a recording to stop This is because the larger the VRS file, the more time it takes for a recording to finish indexing. Go to the Aria Glasses User Manual to see images of all the buttons as well as LED placement  ","version":"Next","tagName":"h3"},{"title":"Press the Capture button​","type":1,"pageTitle":"Project Aria Glasses Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#press-the-capture-button","content":" The capture button is located on the top right of your glasses. The capture button will use the default recording profile.  Press the capture button to start recordingPress the capture button to stop and save the recording  tip Engaging the privacy switch instead of the Capture button will stop the recording and discard your recording. Discarded recordings cannot be retrieved.  ","version":"Next","tagName":"h3"},{"title":"Mobile Companion app​","type":1,"pageTitle":"Project Aria Glasses Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#mobile-companion-app","content":" Select New Recording Session in the Aria Dashboard to start recordingSelect Stop Recording in the app Go to the Mobile Companion App page for more information  You can alter the Name and Notes of a recording by going to the Recordings menu, selecting a recording and then selecting Edit.  ","version":"Next","tagName":"h3"},{"title":"Download data​","type":1,"pageTitle":"Project Aria Glasses Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#download-data","content":" Aria recordings are stored as a VRS file and an accompanying JSON file that includes the recording's metadata. Metadata includes:  The name and description of the recording as shown in the Mobile Companion AppThe recording profile usedWhat version of the Mobile Companion App was used to create the recording  Aria recordings are directly accessible from the glasses' storage. There are three ways you can download data to your local machine using:  MTPADB commands  We recommend using MTP for a faster download experience.  ","version":"Next","tagName":"h2"},{"title":"Use MTP via File Explorer​","type":1,"pageTitle":"Project Aria Glasses Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#use-mtp-via-file-explorer","content":" Windows &amp; Linux​  When you plug in your Aria glasses to your computer, you can navigate to it as if it were any other USB external storage device.  Plug your Aria glasses into your computer, using the supplied cable Please allow a few minutes for your glasses to be detected Your glasses will appear in your directory as an external drive called “Aria”Select Aria and then go to Internal Storage &gt; recordingIn this folder you will see the .vrs file and .json file that stores the .vrs files' metadata You'll also see a thumbnails folder, which contains the thumbnails that are used to provide previews of your content in the Mobile app Copy the data to local storage  MacOS​  MTP is not provided natively on MacOS, but there are lightweight tools that you can use, such as OpenMTP.  Download and install OpenMTPConnect your Aria glasses to your computerOpen OpenMTPDrag &amp; drop Aria recordings from Aria's internal storage  ","version":"Next","tagName":"h3"},{"title":"Use ADB​","type":1,"pageTitle":"Project Aria Glasses Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#use-adb","content":" Android Debug Bridge (adb) is a command line tool that can be used with Aria glasses.  To download all your data:  adb pull /sdcard/recording /home/unixname/MyVRSFolder   To download a single VRS file  adb pull /sdcard/recording/myVrsFile.vrs /home/unixname/MyVRSFolder/   To download a single metadata file  adb pull /sdcard/recording/myVrsFile.json /home/unixname/MyVRSFolder/   ","version":"Next","tagName":"h3"},{"title":"Streaming​","type":1,"pageTitle":"Project Aria Glasses Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#streaming","content":" Install the Client SDK to stream Aria data.  ","version":"Next","tagName":"h2"},{"title":"Access basic visualization​","type":1,"pageTitle":"Project Aria Glasses Quickstart","url":"/projectaria_tools/docs/ARK/ARK_quickstart#access-basic-visualization","content":" When streaming data, we recommend only using Profiles 12 and 18, which are optimized for streaming. ","version":"Next","tagName":"h3"},{"title":"Project Aria Frame Sizes","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/frame_sizing","content":"","keywords":"","version":"Next"},{"title":"How to find out your Aria Frame Size​","type":1,"pageTitle":"Project Aria Frame Sizes","url":"/projectaria_tools/docs/ARK/frame_sizing#how-to-find-out-your-aria-frame-size","content":" ","version":"Next","tagName":"h2"},{"title":"Option 1 - Order Head Sizing Kit​","type":1,"pageTitle":"Project Aria Frame Sizes","url":"/projectaria_tools/docs/ARK/frame_sizing#option-1---order-head-sizing-kit","content":" Ask your Project Aria contact to order you a Head Sizing kit.  ","version":"Next","tagName":"h3"},{"title":"Option 2 - DIY Head Sizing​","type":1,"pageTitle":"Project Aria Frame Sizes","url":"/projectaria_tools/docs/ARK/frame_sizing#option-2---diy-head-sizing","content":" Place your head snugly between two objects and then measure the difference.     If it's under 152mm, order a Small. If it's 152mm or more order a Large  ","version":"Next","tagName":"h3"},{"title":"Option 3 - Check the frame size of existing glasses/sunglasses​","type":1,"pageTitle":"Project Aria Frame Sizes","url":"/projectaria_tools/docs/ARK/frame_sizing#option-3---check-the-frame-size-of-existing-glassessunglasses","content":" This may be less accurate than other methods, given normal glasses have stiffer arms that don't fit snugly around your head. But it can get you going faster if you don't want to do DIY Frame Sizing.  Find a pair of glasses or sunglasses that fit you comfortablyMeasure the distance between the two armsIf it's under 152mm, order a Small. If it's 152mm or more order a Large ","version":"Next","tagName":"h3"},{"title":"Project Aria Glasses Cable Clip","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/glasses_manual/cable_clip","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Project Aria Glasses Cable Clip","url":"/projectaria_tools/docs/ARK/glasses_manual/cable_clip#overview","content":" A glasses cable clip is available to hold the Project Aria glasses cable onto the glasses. This can be useful if connecting glasses to a battery pack while recording.  If you need this accessory, please contact AriaOps@meta.com.  ","version":"Next","tagName":"h2"},{"title":"How to use​","type":1,"pageTitle":"Project Aria Glasses Cable Clip","url":"/projectaria_tools/docs/ARK/glasses_manual/cable_clip#how-to-use","content":" Unlatch the clipSlide the clip onto the Aria glasses’ armPlace the charger onto the magnetic portClose the clip using the rubber band     ","version":"Next","tagName":"h2"},{"title":"Aria Glasses Fit and Comfort","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/glasses_manual/fit_and_comfort","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Aria Glasses Fit and Comfort","url":"/projectaria_tools/docs/ARK/glasses_manual/fit_and_comfort#overview","content":" There are a few adjustments you can make to your Project Aria glasses to get a more comfortable fit.  Your browser does not support the video tag. Video of a person adjusting Project Aria glasses temple arm tips and nosepads.  ","version":"Next","tagName":"h2"},{"title":"Ideal Fit​","type":1,"pageTitle":"Aria Glasses Fit and Comfort","url":"/projectaria_tools/docs/ARK/glasses_manual/fit_and_comfort#ideal-fit","content":" Proper frame width: The device should match the overall width of your head. When too small, the device will be compressing on the sides of your head creating marks. When too large, the temple arms will feel too loose and not secure.Proper nose pad fit: The device should rest comfortably on your nose without slipping down or causing marks. The Nose pad bracket can be adjusted for proper angle and pad placement.Proper temple arm fit: Temple arms reach all the way behind your ears and flex adjusted to fit.  Your frames should align evenly with your eyes, no higher than your eyebrows. For eye-tracking, ensure that your eyes (pupils) are approximately located in between the dotted lines as shown in the picture.    ","version":"Next","tagName":"h2"},{"title":"Nose Pad Adjustments​","type":1,"pageTitle":"Aria Glasses Fit and Comfort","url":"/projectaria_tools/docs/ARK/glasses_manual/fit_and_comfort#nose-pad-adjustments","content":" ","version":"Next","tagName":"h2"},{"title":"The nose pads feel too tight or pinching on the nose?​","type":1,"pageTitle":"Aria Glasses Fit and Comfort","url":"/projectaria_tools/docs/ARK/glasses_manual/fit_and_comfort#the-nose-pads-feel-too-tight-or-pinching-on-the-nose","content":" Solution: Move pads further apart  Securing the frame eye wire with your hands, use your thumbs to spread the pad apart carefully. Make small incremental adjustments until desired fit is achieved.Using your fingers, you can make small incremental adjustments on the pad angle by moving/turning the pad arm carefully.  An ideal fit is achieved by making sure the full surface of the soft nose pad is evenly placed on each side of your nose bridge.  Your browser does not support the video tag. Video of the nosepads of Aria glasses being gently pushed outwards.  ","version":"Next","tagName":"h3"},{"title":"Nose pads are sitting too low on the nose, or feel loose?​","type":1,"pageTitle":"Aria Glasses Fit and Comfort","url":"/projectaria_tools/docs/ARK/glasses_manual/fit_and_comfort#nose-pads-are-sitting-too-low-on-the-nose-or-feel-loose","content":" Solution: Move pads closer to each other  Use your thumbs to push the nose pads closer together until they fit snugly against either side of your nose, being careful to not damage the Device. Make small incremental adjustments until desired fit is achieved.Using your fingers, you can make small incremental adjustments on the pad angle by moving/turning the pad arm carefully.An ideal fit is achieved by making sure the full surface of the soft nose pad is evenly placed on each side of your nose bridge.  Your browser does not support the video tag. Video of the nosepads of Aria glasses being gently pushed inwards.  ","version":"Next","tagName":"h3"},{"title":"Flexible Temple Arm Tips​","type":1,"pageTitle":"Aria Glasses Fit and Comfort","url":"/projectaria_tools/docs/ARK/glasses_manual/fit_and_comfort#flexible-temple-arm-tips","content":" IMPORTANT! The flexible temple arm tips are restricted to horizontal inward/outward movement only. DO NOT move them vertically up or down.  The ends of Project Aria Device arms are flexible, allowing you to make small adjustments to increase wearing comfort. Using your hands, you can bend the tips inward to make it tighter behind the ears or outward to make it looser.  Your browser does not support the video tag. Video of the tips of Project Aria glasses being bent inwards and outwards.  ","version":"Next","tagName":"h2"},{"title":"Ear Hooks​","type":1,"pageTitle":"Aria Glasses Fit and Comfort","url":"/projectaria_tools/docs/ARK/glasses_manual/fit_and_comfort#ear-hooks","content":" You can install Ear Hooks at the end of each temple arm tip, for a more secure and stable grip behind the ears. ","version":"Next","tagName":"h2"},{"title":"Project Aria Glasses User Manual","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual","content":"","keywords":"","version":"Next"},{"title":"Proper Handling and Cleaning​","type":1,"pageTitle":"Project Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#proper-handling-and-cleaning","content":" Handle the Glasses with care &amp; avoid damage. The Glasses are sensitive electronic equipment and should be handled with care.Do not try to bend the temples arms. Attempting to fold them like normal glasses will damage them.Do not drop, strike, or shake your Glasses excessively. Do not use the Glasses if they are damaged.The Glasses are not intended for use as safety glasses or eye protection.Do not use with other head mounted displays, such as virtual reality headsets or other glasses.Storage. Store the Glasses in a clean, dry, temperature-controlled environment. Do not leave in an unattended vehicle where temperatures may reach hot or cold extremes.Water &amp; liquids. Your Glasses are resistant to water splashes but are not designed for submersion in water or extended exposure to water or other liquids. If water exposure occurs, dry your Glasses thoroughly and clear the charging areas of residue or other debris.Cleaning. Clean using a microfiber cloth or a lens cleaning wipe. Do not soak, rinse, or submerge.  ","version":"Next","tagName":"h2"},{"title":"You can't fold the glasses, but you can adjust the tips of the temple arms​","type":1,"pageTitle":"Project Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#you-cant-fold-the-glasses-but-you-can-adjust-the-tips-of-the-temple-arms","content":" Your browser does not support the video tag. Video of temple arm tips being adjusted.  Go to the Fit and Comfort page for more tips about adjusting your glasses.  ","version":"Next","tagName":"h3"},{"title":"Detailed Cleaning Instructions​","type":1,"pageTitle":"Project Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#detailed-cleaning-instructions","content":" ","version":"Next","tagName":"h2"},{"title":"To sanitize​","type":1,"pageTitle":"Project Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#to-sanitize","content":" Use 70% pre moistened alcohol lens wipes with a light touch.  ","version":"Next","tagName":"h3"},{"title":"To clean​","type":1,"pageTitle":"Project Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#to-clean","content":" Clean the eyeglass lenses with a dry microfiber cloth or with a traditional lens cleaner available at any optical store.  Try your best to only use a microfiber cloth for cleaning. Wash the microfiber cloth with soap and water at least weekly and air dry to ensure cleanliness of cloth. Do not use any paper products such as paper towels or Kleenex as it will damage the anti-reflective coating on the lenses. If a microfiber cloth is not available, you can use a 100% soft cotton cloth to wipe your lenses.  Be mindful not to get the camera lenses and sensors wet as much as possible. It's best to spray the cloth first (soap+water mixture ok to use too) to dampen then wipe the lenses with it.  ","version":"Next","tagName":"h3"},{"title":"Project Aria Glasses​","type":1,"pageTitle":"Project Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#project-aria-glasses","content":"    ","version":"Next","tagName":"h2"},{"title":"Charging​","type":1,"pageTitle":"Project Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#charging","content":" Your Aria glasses must be connected to a charger to upload data. It connects to USB via its magnetic connector on the right arm. If you see the LED flashing red on the inside of the right temple arm, your battery is depleted.    If you would like to charge your glasses while using them, you can use a cable clip to secure the cable to the glasses and plug the charger into a portable battery.  ","version":"Next","tagName":"h3"},{"title":"Powering On​","type":1,"pageTitle":"Project Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#powering-on","content":" Connect your glasses to the charger to automatically turn them on.  or  Hold the power button for 3-5 seconds.Continue holding the button when you see the LED flash green once. You can release the button once you see the LED turn solid blue.  Your browser does not support the video tag. Video of Project Aria glasses being turned on.  ","version":"Next","tagName":"h3"},{"title":"Powering Off​","type":1,"pageTitle":"Project Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#powering-off","content":" Hold the power button for 5 seconds and release It may take several seconds for the LED to turn off.  Your browser does not support the video tag. Video of Project Aria glasses being turned off.  ","version":"Next","tagName":"h3"},{"title":"Privacy Switch​","type":1,"pageTitle":"Project Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#privacy-switch","content":" The Privacy Switch, located underneath the right arm, toggles privacy mode on/off. Privacy mode is enabled when the switch is toggled toward the crossed out camera (i.e., &quot;back&quot; position relative to the wearer).  When the privacy switch is engaged, the Aria glasses cannot record data.Any active recordings will be stopped and deleted when the switch is engaged.  Your browser does not support the video tag. Video of privacy switch being toggled on and off.  ","version":"Next","tagName":"h3"},{"title":"Capture Button​","type":1,"pageTitle":"Project Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#capture-button","content":" Located over the right frame.Can be used to start and stop an active recording.The capture and privacy switch are used in combination if you need to factory reset your device.  Please note, it may take several seconds for your recording to start or stop. The larger the recording, the longer it will take to finish indexing and stop the recording.    ","version":"Next","tagName":"h3"},{"title":"Proximity Sensor​","type":1,"pageTitle":"Project Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#proximity-sensor","content":" Senses when the Aria glasses are being worn.    ","version":"Next","tagName":"h3"},{"title":"LED States​","type":1,"pageTitle":"Project Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#led-states","content":" ","version":"Next","tagName":"h2"},{"title":"Right Arm LED​","type":1,"pageTitle":"Project Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#right-arm-led","content":" Low Battery: Flashing RedRecovery Mode: Alternating Blue and Red (may look violet because of the alternating speed)Powering On: Flashing Green oncePower On: Solid BluePower Off: OffCharge Loop: Solid Red (when the battery is too low to boot successfully)Data Uploading: Pulsing Blue 3 times, repeating this pattern (reminder, your device must be connected to power to upload data)    ","version":"Next","tagName":"h3"},{"title":"User Facing LED​","type":1,"pageTitle":"Project Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#user-facing-led","content":" Recording in Progress: Solid WhiteThermal Mitigation Reached: Solid Red, may appear Orange (when the Device temperature is too high)  ","version":"Next","tagName":"h3"},{"title":"Bystander Facing LED​","type":1,"pageTitle":"Project Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#bystander-facing-led","content":" Recording in Progress: Solid White    ","version":"Next","tagName":"h3"},{"title":"Power Cycle/Reboot​","type":1,"pageTitle":"Project Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#power-cyclereboot","content":" If they are connected to a charge cable, unplug your glassesHold down the power button down for about 10 seconds, until the nearby LED flashes green once and returns to solid blue  ","version":"Next","tagName":"h3"},{"title":"Factory Reset​","type":1,"pageTitle":"Project Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#factory-reset","content":" caution This will delete any recordings on your glasses.  Engage the Privacy Switch (i.e. in the recordings disabled position).Tap (press but do not hold) the power button and the capture button at the same time.The device will reboot. After a while, you will see the LED near the power button flashing purple.  Your browser does not support the video tag. Video of Project Aria glasses being factory reset.  ","version":"Next","tagName":"h2"},{"title":"Device ID​","type":1,"pageTitle":"Project Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#device-id","content":" Aria glasses have a unique device_ID. Unlike the serial number, the device_ID is reset every time the device is a factory reset or the glasses are unpaired.  Aria glasses must go through a factory reset before they can be paired to a new user, so the device_ID will always be associated with a single user account (although an individual user may have multiple devices associated with their account).  Note: disconnecting your glasses does not trigger a factory reset, only unpairing them so that a new user may use the glasses. The serial number persists and is tied to the device, like any other product.  You can find the Device ID for your glasses in the Mobile Companion App Device Settings.  ","version":"Next","tagName":"h2"},{"title":"To view Device Settings​","type":1,"pageTitle":"Project Aria Glasses User Manual","url":"/projectaria_tools/docs/ARK/glasses_manual/glasses_user_manual#to-view-device-settings","content":" Tap the Paired Glasses info card.    Health and Safety Information  In addition to the Health and Safety information provided with your welcome kit, you can also read Aria Glasses health and safety information in the Mobile companion app:  Open the Aria companion app on your phoneSelect ProfileSelect Health &amp; Safety ","version":"Next","tagName":"h3"},{"title":"Pair Additional Glasses and Pairing Troubleshooting","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/glasses_manual/pair_glasses","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Pair Additional Glasses and Pairing Troubleshooting","url":"/projectaria_tools/docs/ARK/glasses_manual/pair_glasses#overview","content":" This page provides information about how to pair additional Project Aria glasses, as well as troubleshooting if your device does not pair.  ","version":"Next","tagName":"h2"},{"title":"Terminology​","type":1,"pageTitle":"Pair Additional Glasses and Pairing Troubleshooting","url":"/projectaria_tools/docs/ARK/glasses_manual/pair_glasses#terminology","content":" Within this page, we refer to device pairing/forgetting and device connecting/disconnecting. These terms are not interchangable, and have specific implications.  ","version":"Next","tagName":"h2"},{"title":"Pairing/Unpairing​","type":1,"pageTitle":"Pair Additional Glasses and Pairing Troubleshooting","url":"/projectaria_tools/docs/ARK/glasses_manual/pair_glasses#pairingunpairing","content":" Pair(ed): connecting a device to the Mobile Companion App for the first time.Unpair(ed): the process of unpairing a device from the Mobile Companion App. This will delete all data stored locally on the device.  ","version":"Next","tagName":"h3"},{"title":"Connecting/Disconnecting​","type":1,"pageTitle":"Pair Additional Glasses and Pairing Troubleshooting","url":"/projectaria_tools/docs/ARK/glasses_manual/pair_glasses#connectingdisconnecting","content":" Connect(ed): the Mobile Companion App has active control over the device.Disconnect(ed): the Mobile Companion App no longer has active control over the device.  Multiple devices can be paired with the one account, but only one can be connected to the Mobile Companion app at a time.  ","version":"Next","tagName":"h3"},{"title":"Adding Glasses​","type":1,"pageTitle":"Pair Additional Glasses and Pairing Troubleshooting","url":"/projectaria_tools/docs/ARK/glasses_manual/pair_glasses#adding-glasses","content":" Plug your glasses into power, using the provided cable Your device will automatically power on when you plug it in Make sure the privacy switch is in the &quot;off&quot; or &quot;forward&quot; position (i.e., away from the crossed-out camera icon)Select the Add Glasses button on the Aria dashboard. The app will then start looking for nearby Aria glassesFollow the the typical app and glasses set up flow to finish pairing your new device and update the OS  ","version":"Next","tagName":"h2"},{"title":"Unpairing Aria glasses​","type":1,"pageTitle":"Pair Additional Glasses and Pairing Troubleshooting","url":"/projectaria_tools/docs/ARK/glasses_manual/pair_glasses#unpairing-aria-glasses","content":" Go to Device Settings Tap the Paired Glasses info card on the Dashboard Select Unpair Glasses  ","version":"Next","tagName":"h2"},{"title":"Glasses Not Pairing?​","type":1,"pageTitle":"Pair Additional Glasses and Pairing Troubleshooting","url":"/projectaria_tools/docs/ARK/glasses_manual/pair_glasses#glasses-not-pairing","content":" ","version":"Next","tagName":"h2"},{"title":"If there are no error messages​","type":1,"pageTitle":"Pair Additional Glasses and Pairing Troubleshooting","url":"/projectaria_tools/docs/ARK/glasses_manual/pair_glasses#if-there-are-no-error-messages","content":" Kill/Force Stop the appRestart the Mobile Companion AppTry to pair your device again  ","version":"Next","tagName":"h3"},{"title":"Error message, device is owned by someone else​","type":1,"pageTitle":"Pair Additional Glasses and Pairing Troubleshooting","url":"/projectaria_tools/docs/ARK/glasses_manual/pair_glasses#error-message-device-is-owned-by-someone-else","content":" Aria glasses can only be associated with one user account. Factory reset your glasses to enable them to be paired with this account.  Turn on the Privacy Switch (i.e., in the recordings disabled position).Tap (press but do not hold) the power button and the capture button at the same time. The Aria Glasses User Manual contains a video of how to factory reset your glasses The device will reboot. After a while, you will see the LED near the power button flashing purple.Select Add Glasses in the Mobile Compaion App's dashboard and follow the normal pairing flow. ","version":"Next","tagName":"h3"},{"title":"Project Aria Machine Perception Services","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/mps","content":"","keywords":"","version":"Next"},{"title":"Current MPS offerings​","type":1,"pageTitle":"Project Aria Machine Perception Services","url":"/projectaria_tools/docs/ARK/mps#current-mps-offerings","content":" The following MPS can be requested, as long as the data has been recorded with a compatible Recording Profile. Go to the Recording Profiles for information about each profile.  MPS offerings are grouped into SLAM, Eye Gaze and Hand Tracking services.  ","version":"Next","tagName":"h2"},{"title":"SLAM services​","type":1,"pageTitle":"Project Aria Machine Perception Services","url":"/projectaria_tools/docs/ARK/mps#slam-services","content":" To get these outputs the recording profile must have SLAM cameras + IMU enabled.  ","version":"Next","tagName":"h2"},{"title":"6DoF trajectory​","type":1,"pageTitle":"Project Aria Machine Perception Services","url":"/projectaria_tools/docs/ARK/mps#6dof-trajectory","content":" MPS provides two types of high frequency (1kHz) trajectories:  Open loop trajectory - local odometry estimation from visual-inertial odometry (VIO)Closed loop trajectory - created via batch optimization, using multi-sensors' input (SLAM, IMU, barometer, Wi-Fi and GPS), fully optimized and provides poses in a consistent frame of reference.  ","version":"Next","tagName":"h3"},{"title":"Semi-dense point cloud​","type":1,"pageTitle":"Project Aria Machine Perception Services","url":"/projectaria_tools/docs/ARK/mps#semi-dense-point-cloud","content":" Semi-dense point cloud data supports researchers who need static scene 3D reconstructions, reliable 2D images tracks or a representative visualization of the environment.  ","version":"Next","tagName":"h3"},{"title":"Online sensor calibration​","type":1,"pageTitle":"Project Aria Machine Perception Services","url":"/projectaria_tools/docs/ARK/mps#online-sensor-calibration","content":" The time-varying intrinsic and extrinsic calibrations of cameras and IMUs are estimated at the frequency of the SLAM (mono scene) cameras by our multi-sensor state estimation pipeline.  ","version":"Next","tagName":"h3"},{"title":"Multi-SLAM​","type":1,"pageTitle":"Project Aria Machine Perception Services","url":"/projectaria_tools/docs/ARK/mps#multi-slam","content":" Multi-SLAM can be requested on two or more recordings. It creates all of the above SLAM output, in a shared co-ordinate frame.  Multi-SLAM can only be requested using the MPS CLI SDK.  ","version":"Next","tagName":"h3"},{"title":"Eye Gaze services​","type":1,"pageTitle":"Project Aria Machine Perception Services","url":"/projectaria_tools/docs/ARK/mps#eye-gaze-services","content":" Eye Gaze data is generated using Aria's Eye Tracking (ET) camera images to estimate the direction the user is looking. These outputs can be generated for any recording that had ET cameras enabled.  In March 2024, we updated our eye gaze model to support depth estimation. We do this by providing left and right eye gaze directions (yaw values) along with the depth at which these gaze directions intersect (translation values).  If you have made a recording with In-Session Eye Gaze Calibration, you will receive a second .csv file with calibrated eye gaze outputs.  ","version":"Next","tagName":"h2"},{"title":"Hand Tracking​","type":1,"pageTitle":"Project Aria Machine Perception Services","url":"/projectaria_tools/docs/ARK/mps#hand-tracking","content":" To compute hand tracking outputs, the recording profile must have SLAM cameras enabled.  Hand Tracking data is created by using SLAM camera images to estimate the hand movement of the wearer. The 21 landmarks (incl. wrist and palm) are given in the device frame in meters.  ","version":"Next","tagName":"h2"},{"title":"About MPS Data Loader APIs​","type":1,"pageTitle":"Project Aria Machine Perception Services","url":"/projectaria_tools/docs/ARK/mps#about-mps-data-loader-apis","content":" Please refer to our MPS data loader APIs (C++ and Python support) to load the MPS outputs into your application. The visualization guide shows how to visualize all the MPS outputs.  ","version":"Next","tagName":"h2"},{"title":"Questions & Feedback​","type":1,"pageTitle":"Project Aria Machine Perception Services","url":"/projectaria_tools/docs/ARK/mps#questions--feedback","content":" If you have feedback you'd like to provide, be it overall trends and experiences or where we can improve, we'd love to hear from you. Go to our Support page for different ways to get in touch. ","version":"Next","tagName":"h2"},{"title":"MPS Hand Tracking Algorithm Performance Benchmark","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/mps/benchmark","content":"","keywords":"","version":"Next"},{"title":"Benchmark Dataset​","type":1,"pageTitle":"MPS Hand Tracking Algorithm Performance Benchmark","url":"/projectaria_tools/docs/ARK/mps/benchmark#benchmark-dataset","content":" 3D evaluation is conducted on the HOT3D Aria split, a publicly available dataset for hand-object interaction research.  Dataset: HOT3DTotal Duration: 400 minutes of Aria Gen1 recordingsNumber of Participants: 19Ground Truth Source: Motion Capture (MoCap)  ","version":"Next","tagName":"h2"},{"title":"Evaluation Metrics​","type":1,"pageTitle":"MPS Hand Tracking Algorithm Performance Benchmark","url":"/projectaria_tools/docs/ARK/mps/benchmark#evaluation-metrics","content":" We report the following metrics (lower is better for both):  Metric\tDescriptionLTR (Lose Track Ratio)\tThe ratio of frames where the hand is not tracked successfully. MKPE (Mean Keypoint Error)\tThe mean 3D distance error over all hand keypoints and all frames, measured in millimeters.  ","version":"Next","tagName":"h2"},{"title":"Results​","type":1,"pageTitle":"MPS Hand Tracking Algorithm Performance Benchmark","url":"/projectaria_tools/docs/ARK/mps/benchmark#results","content":" MPS HT Version\tLTR (%) ↓\tMKPE (mm) ↓MPS HT 2.0.0\t6.31\t51.97 MPS HT 3.1.1\t3.94\t22.80  ","version":"Next","tagName":"h2"},{"title":"About HOT3D Dataset​","type":1,"pageTitle":"MPS Hand Tracking Algorithm Performance Benchmark","url":"/projectaria_tools/docs/ARK/mps/benchmark#about-hot3d-dataset","content":" HOT3D is a large-scale dataset for egocentric hand and object tracking in 3D. The Aria split of HOT3D provides diverse hand-object interaction scenarios captured with Aria Gen1 glasses, making it an ideal benchmark for evaluating hand tracking algorithms in real-world conditions.  For more information about the HOT3D dataset, visit the Project Aria Datasets page. ","version":"Next","tagName":"h2"},{"title":"Project Aria Mobile Companion App","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/mobile_companion_app","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Project Aria Mobile Companion App","url":"/projectaria_tools/docs/ARK/mobile_companion_app#overview","content":" The Project Aria Mobile Companion App, provides the ability to interact &amp; record with your Aria glasses via Bluetooth. This section covers:  Getting Started (Update Your Glasses)RecordingMobile Companion App Screens  These instructions are only useful if you have access to Aria glasses. Go to projectaria.com to find out how to become a research partner and gain access to the Aria Research Kit (ARK).  ","version":"Next","tagName":"h2"},{"title":"Mobile Companion App features include:​","type":1,"pageTitle":"Project Aria Mobile Companion App","url":"/projectaria_tools/docs/ARK/mobile_companion_app#mobile-companion-app-features-include","content":" Fully wirelessCheck your glasses status (temperature, GPS, privacy switch etc..)Handle and select between multiple paired Aria glassesUpdate Aria glasses to the latest OS buildSelect a recording profile and start recording directly from the mobile appSet default recording profileCreate a custom recording profileData quality signals while recordingIn-session Eye Gaze CalibrationAccept, store and delete security certificates to enable the Client SDK and CLI  ","version":"Next","tagName":"h2"},{"title":"Getting Started (Update Your Glasses)​","type":1,"pageTitle":"Project Aria Mobile Companion App","url":"/projectaria_tools/docs/ARK/mobile_companion_app#getting-started-update-your-glasses","content":" ","version":"Next","tagName":"h2"},{"title":"Download and Install​","type":1,"pageTitle":"Project Aria Mobile Companion App","url":"/projectaria_tools/docs/ARK/mobile_companion_app#download-and-install","content":" Follow the instructions in the ARK SW Downloads and Updates page to download and install the app (this is where you'll download updates as well).  ","version":"Next","tagName":"h3"},{"title":"Sign in and pair​","type":1,"pageTitle":"Project Aria Mobile Companion App","url":"/projectaria_tools/docs/ARK/mobile_companion_app#sign-in-and-pair","content":" Plug your Project Aria glasses into their charger This will automatically turn your glasses Make sure the Privacy Switch is not engaged The Privacy Switch should be pushed forwards, towards the lenses Open the Companion app for the first time and log in with your email address (or, if you have a test account created before 4/10/2025, with your alias + @tfbnw.net) When launching the Companion App for the first time, you'll need to grant it certain permissions to work correctly, such as location services.Follow the prompts to agree to Project Aria Research Community Guidelines and read the Health and Safety informationSelect Get Started to begin setting up your glasses.The app will begin to look for nearby Project Aria glasses. When your glasses are discovered, they will be listed at the top of the screen alongside its serial number.After selecting your glasses, the Companion App will begin pairing with it.Once pairing completes, the app will ask to name your Aria glasses.Join a Wi-Fi network, your glasses must be plugged into a charger to complete the setup process.Once connected to Wi-Fi, the glasses will look for updates and update your glasses' OSOnce you have completed setup, you’ll see the Mobile Companion App Dashboard page  Go to Pair Additional Devices and Pairing Troubleshooting for more information about pairing your glasses.  ","version":"Next","tagName":"h3"},{"title":"Companion App login changes​","type":1,"pageTitle":"Project Aria Mobile Companion App","url":"/projectaria_tools/docs/ARK/mobile_companion_app#companion-app-login-changes","content":" (Updated as of 4/10/2025)  If you had an Aria test account before 4/10/2025, you previously used your alias to log in. Starting from Companion App version 210, you will use an email to log in: the email is simply your alias + “@tfbnw.net”. For example, if your alias was projectaria123, you’ll enter projectaria123@tfbnw.net as your email.  These changes to login credentials only apply to the Companion App. Aria Studio and the MPS CLI are not affected.  Open the Companion App.Select Log in with Meta.Select Continue to sign in using meta.com.Select Log into another account.Select Continue with email.Enter your @tfbnw.net email (as discussed above), then select Next to continue.Enter the password, then select Log in.In the new window, select Continue as.  ","version":"Next","tagName":"h3"},{"title":"Set Default Recording Profile​","type":1,"pageTitle":"Project Aria Mobile Companion App","url":"/projectaria_tools/docs/ARK/mobile_companion_app#set-default-recording-profile","content":" The default recording profile determines determines which recording profile the New Recordings page starts with, as well as the profile used when initiating recording via the capture button.  In the Mobile Companion App, you can change the Default Recording Profile at any time via Device Settings.  ","version":"Next","tagName":"h3"},{"title":"To view Device Settings​","type":1,"pageTitle":"Project Aria Mobile Companion App","url":"/projectaria_tools/docs/ARK/mobile_companion_app#to-view-device-settings","content":" Tap the Paired Glasses info card.    ","version":"Next","tagName":"h3"},{"title":"Update OS​","type":1,"pageTitle":"Project Aria Mobile Companion App","url":"/projectaria_tools/docs/ARK/mobile_companion_app#update-os","content":" Your Aria glasses's OS will also automatically update when plugged into power and connected to Wi-Fi with an internet connection.  To manually update Aria glasses OS:  In the Mobile Companion App, select Device SettingsScroll down to view your Aria Device's OS VersionSelect Check for UpdatesOnce your glasses have finished updating, they will reboot and the update will be complete.  ","version":"Next","tagName":"h3"},{"title":"Recording​","type":1,"pageTitle":"Project Aria Mobile Companion App","url":"/projectaria_tools/docs/ARK/mobile_companion_app#recording","content":" ","version":"Next","tagName":"h2"},{"title":"To start recording​","type":1,"pageTitle":"Project Aria Mobile Companion App","url":"/projectaria_tools/docs/ARK/mobile_companion_app#to-start-recording","content":" Select New Recording Session on the Aria Dashboard.You have the option to configure your recording session before it begins: Name (optional): This will define the name of your VRS file. If you do not provide a name a random alphanumeric name will be givenNotes (optional): Notes are appended to your recordings ID. You can input a short or long text string in this field. Notes is a value that is provided in the vrs.json file associated with your recording.Sensors Used: Select Sensors Used to see details of the recording profile. From the Sensors Used menu, select Profile to choose different recording profiles to record with.If you’ve selected a default recording profile, that profile will be automatically populated, otherwise you will need to select a recording profile. Select Begin new recording While you're recording the Sensor Status field will let you know if there are any data quality issues Perform in-session Eye Gaze Calibration (optional) You only need to do this if you want Eye Gaze MPS with personalized dataGo To Eye Gaze Calibration for more information End your recording by selecting Complete Recording in the app or use the capture button on the top right side of the glasses  Once you've completed a recording, you'll return to the New Recording screen, pre-populated with your previous details.  For information about the different recording profiles available, and how to create a custom recording profile, go to (Recording Profiles)[/tech_spec/recording_profiles.mdx].  ","version":"Next","tagName":"h3"},{"title":"Mobile Companion App Screens​","type":1,"pageTitle":"Project Aria Mobile Companion App","url":"/projectaria_tools/docs/ARK/mobile_companion_app#mobile-companion-app-screens","content":" ","version":"Next","tagName":"h2"},{"title":"Dashboard​","type":1,"pageTitle":"Project Aria Mobile Companion App","url":"/projectaria_tools/docs/ARK/mobile_companion_app#dashboard","content":" The Aria Dashboard has several interactive elements:  Tap New Recording Session to make a recordingTap Add to pair additional devices with your Aria glasses One account can have multiple Aria glasses associated it, but each pair of glasses can only be associated with one accountMultiple glasses can be paired, but only one pair of glasses can be connected at a time Tap Switch to toggle between multiple paired Aria glasses or pair with new glasses This will replace Add if more than one set of glasses is paired Tap the glasses info card to go to Device SettingsTap the Bluetooth icon to disconnect your glassesTap the Wi-Fi icon to connect to different Wi-Fi network, forget a network or see the glasses' IP address    ","version":"Next","tagName":"h3"},{"title":"Aria Device Settings​","type":1,"pageTitle":"Project Aria Mobile Companion App","url":"/projectaria_tools/docs/ARK/mobile_companion_app#aria-device-settings","content":" Device Settings can be accessed by tapping the Paired Glasses info card on the main dashboard.    The Device Settings page includes:  Bluetooth, Wi-Fi, Battery, GPS and Temperature statusOS version and ability to check for OS updatesRemaining storage spaceMAC AddressSerial numberDevice IDCheck Device Mode (it should say Partner)Change the default recording profile  ","version":"Next","tagName":"h3"},{"title":"Recordings menu​","type":1,"pageTitle":"Project Aria Mobile Companion App","url":"/projectaria_tools/docs/ARK/mobile_companion_app#recordings-menu","content":" In the Recordings menu, you'll be able to see recordings that are currently on your Aria glasses.  Select a recording in the Recording menu to edit your recording's name and notes (notes will be stored in the vrs.json file) and see a range of details including:  Recording durationRecording profile usedUp to 10 thumbnail images from your recording  If you have not given your recording a name (such as when you initiate recording via the capture button) the VRS file will have a random alphanumeric name when it is downloaded.  To name or edit the name of a VRS recording on your glasses:  Go to the recordings tab of the Mobile appSelect a recordingSelect Edit on the top right corner  ","version":"Next","tagName":"h3"},{"title":"Profile menu​","type":1,"pageTitle":"Project Aria Mobile Companion App","url":"/projectaria_tools/docs/ARK/mobile_companion_app#profile-menu","content":" The Profile menu shows the settings and profile information for the Mobile Companion app, not your Aria glasses. You'll be able to see the App version, for example, but not the Aria glasses' OS.  The Advanced Settings menu includes Clear Local Data. This can be helpful if you encounter issues that restarting your Mobile App does not resolve. Aria recordings are stored on the glasses, not on the phone, so clearing local data will not delete any of your recordings. ","version":"Next","tagName":"h3"},{"title":"MPS Data Lifecycle","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/mps/mps_processing","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"MPS Data Lifecycle","url":"/projectaria_tools/docs/ARK/mps/mps_processing#overview","content":" Researchers can upload data collected by Project Aria glasses to Meta for cloud-based Machine Perception Services (MPS) processing.  This page provides information about how all Aria sequences submitted to Meta for MPS are processed, handled and stored.  Go to Machine Perception Services to find out more about the dataGo to How to Request MPS for how to get your data processed  ","version":"Next","tagName":"h2"},{"title":"How sequences are processed​","type":1,"pageTitle":"MPS Data Lifecycle","url":"/projectaria_tools/docs/ARK/mps/mps_processing#how-sequences-are-processed","content":" Raw Aria sequences (VRS files) are uploaded to secure cloud storage via Aria StudioThe data is only uploaded to Meta servers to serve MPS requests and is deleted after 30 daysThe MPS output is saved in the cloud User account that requested the MPS gets the token necessary to access MPS outputsThis derived data is persisted in the cloud Raw data is deleted from the cloud Meta’s data management processes mandate that this raw data cannot be stored for more than 30 days  Figure 1: MPS Processing Lifecycle  ","version":"Next","tagName":"h2"},{"title":"Data storage and use​","type":1,"pageTitle":"MPS Data Lifecycle","url":"/projectaria_tools/docs/ARK/mps/mps_processing#data-storage-and-use","content":" Partner data is only used to serve MPS requests. Partner data is not available to Meta researchers or Meta’s affiliates.Raw partner data (VRS files) is stored for no more than 30 days.The whole process is automated and only engineers in the core MPS team can access the pipeline under explicit permission from the customer.All MPS output (trajectories, semi-dense point clouds, gaze vectors etc.) continue to be stored in secure cloud storage, so that users can re-download the data at any time. MPS output is not available to Meta researchers or Meta’s affiliates. Only the user account that requested the MPS output gets the token necessary to download the derived data The MPS pipeline generates statistics about how the algorithms are performing as well as the console logs from processing. These aggregated statistics are used by the Project Aria MPS team to help improve our offerings. ","version":"Next","tagName":"h2"},{"title":"MPS Troubleshooting","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/mps/mps_troubleshooting","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"MPS Troubleshooting","url":"/projectaria_tools/docs/ARK/mps/mps_troubleshooting#overview","content":" This page provides troubleshooting support for issues relating to Project Aria's Machine Perception Services (MPS). If you can't find a solution for your problem on this page, go to the Support page for how to contact our team.  This page covers:  General MPS questionsMPS CLI and Aria Studio specific troubleshooting and error codes  ","version":"Next","tagName":"h2"},{"title":"General MPS questions​","type":1,"pageTitle":"MPS Troubleshooting","url":"/projectaria_tools/docs/ARK/mps/mps_troubleshooting#general-mps-questions","content":" ","version":"Next","tagName":"h2"},{"title":"How do I learn more about MPS data?​","type":1,"pageTitle":"MPS Troubleshooting","url":"/projectaria_tools/docs/ARK/mps/mps_troubleshooting#how-do-i-learn-more-about-mps-data","content":" The MPS Data Formats section of the wiki provides more information about each MPS output and links to visualizers you can use with this data.  ","version":"Next","tagName":"h3"},{"title":"How do I find out which recording profile was used?​","type":1,"pageTitle":"MPS Troubleshooting","url":"/projectaria_tools/docs/ARK/mps/mps_troubleshooting#how-do-i-find-out-which-recording-profile-was-used","content":" Recording profile information is stored in the .vrs.json file that contains metadata about that recording. Open the file with any text editor and search for the recording_profile value.  ","version":"Next","tagName":"h3"},{"title":"MPS CLI and Aria Studio specific issues​","type":1,"pageTitle":"MPS Troubleshooting","url":"/projectaria_tools/docs/ARK/mps/mps_troubleshooting#mps-cli-and-aria-studio-specific-issues","content":" ","version":"Next","tagName":"h2"},{"title":"How do I request support?​","type":1,"pageTitle":"MPS Troubleshooting","url":"/projectaria_tools/docs/ARK/mps/mps_troubleshooting#how-do-i-request-support","content":" When requesting support, please include:  Log file MPS CLI logs File path is shown on the bottom left side of the MPS CLI UIStored in /tmp/logs/projectaria/mps/ by default Aria Studio logs Stored in /tmp/logs/projectaria/aria_studio.log by default Supplemental debugging files, if available. These will be stored in output folders: Files that include vrs_health_check, such as: vrs_health_check.jsonvrs_health_check_slam.json summary.json Screenshot showing the error codes  Please don’t send us any raw data Raw data is generally not necessary, but if it is, User Support will send you instructions for where to upload it.  ","version":"Next","tagName":"h3"},{"title":"Authentication issue on macOS​","type":1,"pageTitle":"MPS Troubleshooting","url":"/projectaria_tools/docs/ARK/mps/mps_troubleshooting#authentication-issue-on-macos","content":" If you encounter a permissions error while trying to sign in to the MPS CLI, try the following command (updated with your Python version):  ln -s /etc/ssl/* /Library/Frameworks/Python.framework/Versions/3.9/etc/openssl   ","version":"Next","tagName":"h3"},{"title":"Terminal display issues in macOS​","type":1,"pageTitle":"MPS Troubleshooting","url":"/projectaria_tools/docs/ARK/mps/mps_troubleshooting#terminal-display-issues-in-macos","content":" Some users have reported that the MPS CLI UI has display issues in Terminal. Using a different terminal app may help. Why doesn't Textual look good on macOS provides further information about this issue.  ","version":"Next","tagName":"h3"},{"title":"MPS CLI and Aria Studio error codes​","type":1,"pageTitle":"MPS Troubleshooting","url":"/projectaria_tools/docs/ARK/mps/mps_troubleshooting#mps-cli-and-aria-studio-error-codes","content":" ","version":"Next","tagName":"h3"},{"title":"Error codes​","type":1,"pageTitle":"MPS Troubleshooting","url":"/projectaria_tools/docs/ARK/mps/mps_troubleshooting#error-codes","content":" Error Code\tDescription\tAll 1xx codes are local client errors 100\tMulti-Recording processing error. Another recording in the group had a failure, so the processing was halted. We need the full group of recordings to successfully process and generate multi-slam MPS output. The recording that fails will show its own error code, processing then stops and and sets the other recordings to this error code. 101\tSomething failed on your computer. When you see this error, we suggest inspecting the local log for this run. If you need to reach out to support, please include this log file. 102\tThere were health check failures during processing. Check the vrs_health_check.json and vrs_health_check_slam.json for more information about what went wrong. 103\tMulti-Recording processing error. Duplicate file detected. Duplicate recordings are not allowed for multi-SLAM processing. 104\tMPS CLI failed to encrypt the file. Check error logs for more information. 105\tInput-output mismatch error. This error happens when you try to run multi-SLAM processing but pass an output directory that already contains output or intermediate artifacts from a different multi-SLAM job. 106\tThe output directory used for multi-SLAM processing is not empty. Non-empty output directory is only allowed if the output directory contains output or artifacts from a previous run of the same group of recordings. 108\tThe server failed to query the status of the request. Retrying should usually fix this issue. 109\tError during computing the hash id of the file. Check error logs for more information. 110\tError when checking existing outputs on the disk. Check error logs for more information. 111\tError when checking previously submitted MPS requests for a file. Check error logs for more information. 112\tError when checking if the recording was previously uploaded. Check error logs for more information. 113\tError uploading the VRS file. Check error logs for more information. 114\tError submitting MPS request to the server. Check error logs for more information. Retrying should fix the issue. 115\tError when checking the processing status of the MPS request. Check error logs for more information. 116\tError when downloading the MPS results. Check error logs for more information. All other error codes\tServer side failure. Reach out to support with the error code and log file. ","version":"Next","tagName":"h3"},{"title":"Project Aria In-Session Eye Gaze Calibration","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/mps/eye_gaze_calibration","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Project Aria In-Session Eye Gaze Calibration","url":"/projectaria_tools/docs/ARK/mps/eye_gaze_calibration#overview","content":" This page provides an overview of what In-Session Eye Gaze Calibration does, and how to collect In-Session Eye Gaze Calibration with Project Aria glasses.  Not required to request Eye Gaze MPS In-Session Eye Gaze Calibration provides an additional personalized Eye Gaze output, but is not required for requesting Eye Gaze MPS.  ","version":"Next","tagName":"h2"},{"title":"What In-Session Eye Gaze Calibration does​","type":1,"pageTitle":"Project Aria In-Session Eye Gaze Calibration","url":"/projectaria_tools/docs/ARK/mps/eye_gaze_calibration#what-in-session-eye-gaze-calibration-does","content":" In-Session Eye Gaze Calibration enables researchers to improve the eye gaze estimations in Eye Gaze MPS outputs, enabling researchers to more accurately determine where wearers are looking during the recordings.  When you request Eye Gaze Machine Perception Services (MPS) and the file has an in-session Eye Gaze Calibration as part of the VRS file, you will receive two outputs:  general_eye_gaze.csv - based on the standard eye gaze configurationpersonalized_eye_gaze.csv - personalized eye gaze data based on the calibration data collected in the recording  Every person is unique in terms of how they move their eyes and look at objects, so the personalized_eye_gaze estimation is expected to be more accurate for the individual.  Further resources:  Eye Gaze Data FormatsHow to Request MPSRecording ProfilesMachine Perception Services Tutorial - includes sample output from a recording where good in-session Eye Gaze Calibration data was collected  note Eye gaze calibration is not the same as Aria device calibration. For information about Aria device calibration, go to Project Aria Device Calibration.  ","version":"Next","tagName":"h2"},{"title":"HW & SW Requirements​","type":1,"pageTitle":"Project Aria In-Session Eye Gaze Calibration","url":"/projectaria_tools/docs/ARK/mps/eye_gaze_calibration#hw--sw-requirements","content":" ","version":"Next","tagName":"h2"},{"title":"Android​","type":1,"pageTitle":"Project Aria In-Session Eye Gaze Calibration","url":"/projectaria_tools/docs/ARK/mps/eye_gaze_calibration#android","content":" Hardware​  ARCore Depth API (https://developers.google.com/ar/devices) support  Software​  Google Play Services for AR (https://play.google.com/store/apps/details?id=com.google.ar.core) For phones that have ARCore Depth API capabilities, it's normally installed by defaultYou'll see the error message &quot;Please check that Google Play Services for AR is installed&quot; if you don't have ARCore Depth API installed.  ","version":"Next","tagName":"h3"},{"title":"iOS​","type":1,"pageTitle":"Project Aria In-Session Eye Gaze Calibration","url":"/projectaria_tools/docs/ARK/mps/eye_gaze_calibration#ios","content":" Hardware​  TrueDepth camera (iPhone X or later)  Software​  iOS 14 or later  ","version":"Next","tagName":"h3"},{"title":"How to Collect In-Session Eye Gaze Calibration​","type":1,"pageTitle":"Project Aria In-Session Eye Gaze Calibration","url":"/projectaria_tools/docs/ARK/mps/eye_gaze_calibration#how-to-collect-in-session-eye-gaze-calibration","content":" In-Session Eye Gaze Calibration can only be initiated via the Mobile Companion App. Follow these steps for any recording where you may wish to generate Calibrated Eye Gaze MPS.  In the Mobile Companion app, create a new recording using a profile that includes ET and RGB cameras (such as Profile 15 or 25)Once your recording has started, close the recording window Select X on the top left of the screen Go to Device Settings Tap the Paired Glasses info card on the Dashboard Select Eye Tracking CalibrationConfirm that you’d like to run it during the current recording sessionFollow the prompts to calibrate your glasses  ","version":"Next","tagName":"h2"},{"title":"To view Device Settings​","type":1,"pageTitle":"Project Aria In-Session Eye Gaze Calibration","url":"/projectaria_tools/docs/ARK/mps/eye_gaze_calibration#to-view-device-settings","content":" Tap the paired glasses info card.    ","version":"Next","tagName":"h3"},{"title":"Multiple Users Within the One Recording​","type":1,"pageTitle":"Project Aria In-Session Eye Gaze Calibration","url":"/projectaria_tools/docs/ARK/mps/eye_gaze_calibration#multiple-users-within-the-one-recording","content":" It’s possible for multiple users to do an in-session calibration within the one recording.  When a new user gets the glasses, the first thing they should do is the in-session Eye Tracking Calibration.  See Eye Gaze Data Format for how multiple users are tracked.  ","version":"Next","tagName":"h3"},{"title":"Eye Gaze Calibration tips​","type":1,"pageTitle":"Project Aria In-Session Eye Gaze Calibration","url":"/projectaria_tools/docs/ARK/mps/eye_gaze_calibration#eye-gaze-calibration-tips","content":" ","version":"Next","tagName":"h2"},{"title":"Things to avoid​","type":1,"pageTitle":"Project Aria In-Session Eye Gaze Calibration","url":"/projectaria_tools/docs/ARK/mps/eye_gaze_calibration#things-to-avoid","content":" ❌ Do not wear a face covering during eye calibration.  ❌ Choose an area with ample and even lighting; do not face a bright light, window or reflective surface.  ❌ Do not set your phone screen brightness too high compared to your surroundings.  ❌ Do not fully extend your arm(s) during eye calibration. Your elbows should be bent so that the phone is roughly 1 ft (30 cm) away from your face.  ","version":"Next","tagName":"h3"},{"title":"Things to do​","type":1,"pageTitle":"Project Aria In-Session Eye Gaze Calibration","url":"/projectaria_tools/docs/ARK/mps/eye_gaze_calibration#things-to-do","content":" ✅ The phone should be held straight in front of your face, so that you shouldn't look up or down to see the screen. Hold the phone plumb (90 degrees) vertically to the ground.    ✅ The &quot;Leveler&quot; stage appears if the position of your phone isn't within specifications for the calibration process. Adjust the phone in front of you and its distance by bending your elbow until the smaller, black circle turns into a green disk with a check mark.  ✅ Once the &quot;Leveler&quot; stage is successfully completed, do your best to keep your phone in exactly the same position throughout the full eye calibration process. If your phone is moved to a position no longer suited to calibrate your device, the app will return to the &quot;Leveler&quot; stage.  ✅ During eye calibration stages 1 to 10, move your nose towards the direction indicated by the arrow. If you're only following the direction with your gaze without moving your head, the calibration stage will time out and fail. However, please make sure to keep your eyes fixed on the number within the dot the whole time.   ","version":"Next","tagName":"h3"},{"title":"How to request MPS","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/mps/request_mps","content":"How to request MPS Cloud based Machine Perception Services (MPS) are available to Project Aria research partners to generate SLAM, Multi-Slam, Eye Gaze and Hand Tracking derived data outputs. Partner data is only used to serve MPS requests. Partner data is not available to Meta researchers or Meta’s affiliates. Go to MPS Data Processing for more details about how data is processed. The preferred ways to request MPS are: MPS CLI Installed as part of the pip installation version of Project Aria Tools Aria Studio A web-based application that builds on features of the MPS CLI","keywords":"","version":"Next"},{"title":"MPS CLI","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"MPS CLI","url":"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli#overview","content":" The Project Aria MPS Command Line Interface (MPS CLI) is part of Project Aria Tools (available on MacOS and Linux) that enables users to request Machine Perception Services (MPS). Aria Studio is a web-based application that builds on Aria MPS CLI capabilities.  MPS CLI features:  Request SLAM, Eye Gaze and Hand Tracking dataRequest multi-sequence SLAM dataAuto run health checks prior to upload Recordings will not be uploaded if they are not valid for any of the MPS services requested Resumable uploads Uploads, interrupted for any reason, can be resumed within 24 hrsParticularly useful for large VRS files Concurrent processing CLI takes a list of directories/files as input and recursively finds all .vrs files to process concurrentlyThe number of concurrent processes for each stage can be modified in Settings Automatically downloads outputs once processing is completeRecordings are processed once If the output directory already exists and contains the MPS results, processing is skippedIf the recording was processed in the past, the results will be downloaded without submitting the recording for processing again (unless --force is passed as an input) Uploaded data is stored for 30 days Additional MPS can be requested without needing to upload againData can be reprocessed without needing to upload again CLI can be integrated into your automated workflows This includes ensuring that all commands and processes work without using the UISee the Command Line Reference for more details  MPS CLI documentation:  Getting StartedMPS CLI Guide  Further resources:  Aria StudioAbout Project Aria Machine Perception ServicesMPS Data FormatsVisualize MPS Using PythonVisualize MPS Using C++ ","version":"Next","tagName":"h2"},{"title":"MPS Service Versions","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/mps/mps_versioning","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"MPS Service Versions","url":"/projectaria_tools/docs/ARK/mps/mps_versioning#overview","content":" The versioning system for MPS services adheres to the following format:  Major Number: Indicates significant changes that are not backward compatible. This number is incremented when there are major updates or changes in the API that could affect existing functionalities.Minor Number: Represents minor updates or improvements that are backward compatible. This number is incremented when additional features or minor improvements are introduced.Bugfix Number: Used for small fixes and patches to address issues without affecting the overall functionality. This number is incremented when bugs are fixed.  ","version":"Next","tagName":"h2"},{"title":"Current MPS Services and Versions:​","type":1,"pageTitle":"MPS Service Versions","url":"/projectaria_tools/docs/ARK/mps/mps_versioning#current-mps-services-and-versions","content":" ","version":"Next","tagName":"h2"},{"title":"Eye Gaze Service​","type":1,"pageTitle":"MPS Service Versions","url":"/projectaria_tools/docs/ARK/mps/mps_versioning#eye-gaze-service","content":" Current version: 3.1.0  Description: Project Aria's MPS Eye Gaze service provides general and personalized eye gaze data. The latest version was released in March 2024 and introduced eye gaze depth estimation.  ","version":"Next","tagName":"h3"},{"title":"SLAM (Simultaneous Localization and Mapping) Service​","type":1,"pageTitle":"MPS Service Versions","url":"/projectaria_tools/docs/ARK/mps/mps_versioning#slam-simultaneous-localization-and-mapping-service","content":" Current version: 1.1.0  Description: Project Aria's MPS SLAM service provides the trajectory in 6DoF, a semi-dense point cloud, and the online sensor calibration. The latest version 1.1.0 was released in September 2024 and introduced RGB camera online calibration along with accuracy and robustness improvements.  ","version":"Next","tagName":"h3"},{"title":"Hand Tracking Service​","type":1,"pageTitle":"MPS Service Versions","url":"/projectaria_tools/docs/ARK/mps/mps_versioning#hand-tracking-service","content":" Current version: 3.1.1  Description: Project Aria's MPS Hand Tracking service provides the 21 landmark positions, a 6DoF transfrom from the hand frame to the device frame, and wrist and palm normal vectors.  ","version":"Next","tagName":"h3"},{"title":"Summary​","type":1,"pageTitle":"MPS Service Versions","url":"/projectaria_tools/docs/ARK/mps/mps_versioning#summary","content":" This versioning system ensures that each MPS service is consistently updated and maintained, providing reliable and efficient performance for Project Aria glasses. ","version":"Next","tagName":"h3"},{"title":"Project Aria Client SDK","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/sdk","content":"","keywords":"","version":"Next"},{"title":"Client SDK features​","type":1,"pageTitle":"Project Aria Client SDK","url":"/projectaria_tools/docs/ARK/sdk#client-sdk-features","content":" After connecting your Project Aria glasses and PC via USB or WiFi, you will be able to:  Retrieve device information, status and sensor calibration dataStart and stop recordingConfigure, start and stop streaming from the glassesSubscribe to streaming data from the glassesCreate time synchronized recordings between multiple devices using TICSync  ","version":"Next","tagName":"h3"},{"title":"Project Aria CLI​","type":1,"pageTitle":"Project Aria Client SDK","url":"/projectaria_tools/docs/ARK/sdk#project-aria-cli","content":" In addition to APIs and code samples, the SDK also provides the Aria CLI, which allows you to control Project Aria glasses and manage their data without touching any code.  ","version":"Next","tagName":"h3"},{"title":"This documentation covers:​","type":1,"pageTitle":"Project Aria Client SDK","url":"/projectaria_tools/docs/ARK/sdk#this-documentation-covers","content":" Setup Guide PrerequisitesDownload and install the Client SDKPair your Aria glasses to the computerExtract SDK samples Code samples to demonstrate using the SDK to control Project Aria glasses and build you own real-time applications. Connection: connect an Aria device to a computer, fetch the device information and status.Recording: start and stop a recording via USB and Wi-Fi.Streaming Subscription: subscribe to and unsubscribe from a streaming deviceStreaming and Visualizing All Live Sensor Data: programmatically start and stop a streaming session, add callbacks to visualize and manipulate the streamed dataStreaming Undistorted RGB Image Using Calibration: programmatically start and stop a streaming session, access sensor calibration and undistort an RGB live streamTICSync Time Synchronization Core Concepts Sensor Profiles: access information about Aria recording profiles via the CLI or SDKStreaming Internals: deeper dives into how data is streamedAbout TICSync API ReferencesCLI documentationSDK Troubleshooting  If you run into any issues, please check out the Troubleshooting page or contact us through one of our Support channels. ","version":"Next","tagName":"h3"},{"title":"Getting Started With the MPS CLI","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli_getting_started","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Getting Started With the MPS CLI","url":"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli_getting_started#overview","content":" The Project Aria Machine Perceptions Services Command Line Interface (MPS CLI) is a command line tool used to request and receive Machine Perception Services. This page provides basic information to get you started with the MPS CLI, go to the MPS CLI Guide for more details.  The MPS inputs can be a file or directory, and multiple inputs can be listed in a single command.  The MPS CLI has two modes:  Single Process each recording individuallyOutput is always saved next to the input VRS fileThe most common way to request MPS Multi Process multiple recordings to generate multi-sequence SLAM dataUser must provide a directory for the outputs  Non-UI options available This tutorial uses the the MPS CLI UI, but all processes can also work can without using the UI and can be integrated into automated workflows. See the Command Line Reference in the User Guide for more details.  ","version":"Next","tagName":"h2"},{"title":"Getting Started​","type":1,"pageTitle":"Getting Started With the MPS CLI","url":"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli_getting_started#getting-started","content":" ","version":"Next","tagName":"h2"},{"title":"Install MPS CLI​","type":1,"pageTitle":"Getting Started With the MPS CLI","url":"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli_getting_started#install-mps-cli","content":" Please follow this table to install the MPS CLI:  Platform\tInstallation Steps Mac/Linux\tPlease follow the latest MPS CLI installation guide. Windows\tWindows support for the new MPS CLI package is coming soon. Until then, please continue to use pip installation version of Project Aria Tools.  ","version":"Next","tagName":"h3"},{"title":"Download the MPS CLI sample dataset​","type":1,"pageTitle":"Getting Started With the MPS CLI","url":"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli_getting_started#download-the-mps-cli-sample-dataset","content":" To try out the following commands on VRS files:  Download the sample files: Sample 1 - single VRS fileSample 2 - single VRS file Move them to a directory called Example in your downloads directory  info You may also wish to use your own recordings.  ","version":"Next","tagName":"h3"},{"title":"Request MPS for all VRS files in the “Example” directory and it’s subdirectories:​","type":1,"pageTitle":"Getting Started With the MPS CLI","url":"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli_getting_started#request-mps-for-all-vrs-files-in-the-example-directory-and-its-subdirectories","content":" aria_mps single -i ~/Downloads/Example/   You'll be prompted to enter your username and password. Use the Project Aria credentials you use to sign into the Mobile Companion app.    Once the request has been processed, the MPS output will be downloaded next to the original VRS file. In this example, a recording in the Example directory called recording1.vrs was used to generate MPS.   └── Example folder ├── mps_recording1_vrs │ ├── eye_gaze │ │ ├── general_eye_gaze.csv │ │ └── summary.json │ ├── slam │ │ ├── closed_loop_trajectory.csv │ │ ├── online_calibration.jsonl │ │ ├── open_loop_trajectory.csv │ │ ├── semidense_observations.csv.gz │ │ ├── semidense_points.csv.gz │ │ └── summary.json │ ├── hand_tracking │ │ ├── wrist_and_palm_poses.csv │ │ └── summary.json │ ├── vrs_health_check.json │ └── vrs_health_check_slam.json └── recording1.vrs   Go to MPS Data Format Basics for more details about the folder structure.  ","version":"Next","tagName":"h3"},{"title":"Exit the MPS CLI​","type":1,"pageTitle":"Getting Started With the MPS CLI","url":"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli_getting_started#exit-the-mps-cli","content":" To quit the MPS CLI, press CTRL + Q. The CLI will ask for confirmation before quitting.  If you quit the request tool while the files are uploading the uploads will stop.If you resubmit the request the uploads will resume where they left off, progress won’t be lost.  If you quit the request tool once the files have been uploaded, the MPS processes will continue. Once processing is complete, and the request tool is open, MPS files will be automatically downloaded to your VRS files’ location.  ","version":"Next","tagName":"h3"},{"title":"Multi Sequence MPS Requests​","type":1,"pageTitle":"Getting Started With the MPS CLI","url":"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli_getting_started#multi-sequence-mps-requests","content":" When you request MPS using multi mode, MPS will process a group of recordings together to generate Multi-SLAM MPS. Multi-SLAM MPS creates SLAM MPS outputs in a shared co-ordinate frame. Once the request has been processed, the MPS output will be downloaded to the directory you defined.  aria_mps multi -i ~/Downloads/Example/ -o ~/Documents/multi_slam_output     ","version":"Next","tagName":"h3"},{"title":"Working with MPS data​","type":1,"pageTitle":"Getting Started With the MPS CLI","url":"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli_getting_started#working-with-mps-data","content":" You may find the following resources helpful when working with MPS data:  MPS Data FormatsVisualize MPS Using PythonVisualize MPS Using C++ ","version":"Next","tagName":"h2"},{"title":"Aria CLI","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/sdk/cli","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Aria CLI","url":"/projectaria_tools/docs/ARK/sdk/cli#overview","content":" This page provides an overview of Aria CLI features and a few useful commands, go to the Command Reference page for a full list.  ","version":"Next","tagName":"h2"},{"title":"About the CLI​","type":1,"pageTitle":"Aria CLI","url":"/projectaria_tools/docs/ARK/sdk/cli#about-the-cli","content":" Aria CLI is a simple command line binary available on Linux and Mac which allows users to control their Project Aria glasses via CLI commands.  The CLI currently provides the ability to:  Pair the glasses via USB or Wi-FiConnect to the glasses via USB or Wi-FiRetrieve the device status and informationControl Aria recording capabilitiesControl Aria streaming capabilities  The CLI does not enable you to download Aria data. Go to the Downloads section of the Project Aria Glasses Quickstart Guide for ways to download Aria data.  caution To be able to use these commands, follow the setup instructions to install the CLI, and pair your glasses.  ","version":"Next","tagName":"h2"},{"title":"Help​","type":1,"pageTitle":"Aria CLI","url":"/projectaria_tools/docs/ARK/sdk/cli#help","content":" aria --help   will show you available options and subcommands.  ","version":"Next","tagName":"h2"},{"title":"Connecting​","type":1,"pageTitle":"Aria CLI","url":"/projectaria_tools/docs/ARK/sdk/cli#connecting","content":" You can connect to Aria glasses via Wi-Fi or USB.  Wi-Fi: you will need to specify the option --device-ip Pair your Aria glasses with the Mobile Companion app to get their IP address by tapping Wi-Fi in the DashboardGo to the Mobile Companion app page for screenshots and further instructions USB: you don't need to manually input the device IP when it is plugged in via USB, the CLI will automatically connect Use the option --serial, if more than one device is plugged in via USB ","version":"Next","tagName":"h2"},{"title":"Project Aria Machine Perception Services CLI Guide","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli_guide","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Project Aria Machine Perception Services CLI Guide","url":"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli_guide#overview","content":" The Project Aria MPS CLI Guide provides detailed information about how to use this tool. The guide contain:  About the MPS CLIHow the MPS CLI works Go to MPS Data Lifecycle for more details about how sequences are processed on the server MPS CLI SettingsMPS CLI Command Line Reference    ","version":"Next","tagName":"h2"},{"title":"Further resources:​","type":1,"pageTitle":"Project Aria Machine Perception Services CLI Guide","url":"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli_guide#further-resources","content":" Getting Started With the MPS CLI for a quick tutorial showing basic commands using sample data.MPS Troubleshooting and Error Codes  ","version":"Next","tagName":"h3"},{"title":"About​","type":1,"pageTitle":"Project Aria Machine Perception Services CLI Guide","url":"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli_guide#about","content":" The Project Aria MPS Command Line Interface (MPS CLI), part of Project Aria Tools, is the preferred way to request Machine Perception Services (MPS).  The MPS CLI has two modes:  Single Process each recording individuallyThe input can be a file and/or directory, so you can batch process multiple recordings with a single commandOutput is always saved next to the input fileThe most common way to request MPS Multi Process multiple recordings to generate multi-sequence SLAM dataUser must provide a directory for the outputs     Go to MPS CLI Overview for a full breakdown of all the available features.  ","version":"Next","tagName":"h2"},{"title":"How the MPS CLI works​","type":1,"pageTitle":"Project Aria Machine Perception Services CLI Guide","url":"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli_guide#how-the-mps-cli-works","content":" The MPS CLI enables you to upload VRS files from your computer to the Meta servers for processing. The outputs are then saved to your local directory. The MPS CLI will try to process multiple recordings concurrently. The concurrency for various stages can be controlled via Settings.    Once you submit your request, the MPS CLI for the selected mode will open and show the status of your requests. See the Getting Started or the Command Line Reference below for how to submit a request. Once authenticated, the request tool checks with the server to see if this recording was previously processed. We use unique IDs (Hashing stage) to check if this is a new or a previously known recording.If it is a known recording, we skip processing and directly download the results (Download Results stage) or show the error code.If this is a new recording, we run health checks on the recording (HealthCheck stage), to minimize the chances that it will fail during processing. While this check catches obvious errors, like gaps in data and ensures presence of the right sensor streams, but server side processing may still fail.If the health check passes, the recording is encrypted on your machine (Encrypting stage).After encryption, the recordings are uploaded to the MPS servers (Uploading stage) for processing. Uploads are resumable.Interrupted uploads can be resumed within 24 hours. Data is processed on MPS servers (Processing stage). The MPS CLI periodically checks the MPS request's status on the server.It is safe to close the MPS Request tool once the data is processing. When the MPS CLI is reopened, it will check the status of your data and progress to Downloading if it is ready.  If you get an error code The server re-attempts processing multiple times before it stops and provides an error message Check Error Codes to see what the error was.We encourage you to send a bug report with log files to Aria User Support if it is not an error code 1xx. By default, logs are stored in /tmp/logs/projectaria/mps/.  Once the processing is complete, and the tool is open, outputs are automatically downloaded (Downloading stage).Recordings in the MPS CLI UI will show the Success status once the outputs been successfully downloaded.  ","version":"Next","tagName":"h2"},{"title":"Logs​","type":1,"pageTitle":"Project Aria Machine Perception Services CLI Guide","url":"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli_guide#logs","content":" Each run will write the console logs to a log file on the local drive. Since the MPS CLI can run concurrently on multiple recordings, these logs are useful for debugging purposes. The logs are named by the current time when the request was initiated via CLI.  By default, logs are stored in /tmp/logs/projectaria/mps/. The location can be modified in settings.     ","version":"Next","tagName":"h3"},{"title":"CLI Settings​","type":1,"pageTitle":"Project Aria Machine Perception Services CLI Guide","url":"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli_guide#cli-settings","content":" Project Aria MPS CLI settings can be customized via the mps.ini file. This file is located in the $HOME/.projectaria/mps.ini  Setting\tDescription\tDefault Value General settings log_dir\tWhere log files are saved for each run. The filename is the timestamp from when the request tool started running.\t/tmp/logs/projectaria/mps/ status_check_interval\tHow long the MPS CLI waits to check the status of data during the Processing stage.\t30 secs HASH concurrent_hashes\tMaximum number of files that can be concurrently hashed\t4 chunk_size\tChunk size to use for hashing\t10MB Encryption chunk_size\tChunk size to use for encryption\t50MB concurrent_encryptions\tMaximum number of files that can be concurrently encrypted\t4 delete_encrypted_files\tWhether to delete the encrypted files after upload is done. If you set this to false local disk usage will double, due to an encrypted copy of each file.\tTrue. Health Check concurrent_health_checks\tMaximum number of VRS file healthchecks that can be run concurrently\t2 Uploads backoff\tThe exponential back off factor for retries during failed uploads. The wait time between successive retries will increase with this factor.\t1.5 interval\tBase delay between retries.\t20 secs retries\tMaximum number of retries before giving up.\t10 concurrent_uploads\tMaximum number of concurrent uploads.\t4 max_chunk_size\tMaximum chunk size that can be used during uploads.\t100 MB min_chunk_size\tThe minimum upload chunk size.\t5 MB smoothing_window_size\tSize of the smoothing window to adjust the chunk size. This value defines the number of uploaded chunks that will be used to determine the next chunk size.\t10 target_chunk_upload_secs\tTarget time to upload a single chunk. If the chunks in a smoothing window take longer, we reduce the chunk size. If it takes less time, we increase the chunk size.\t3 secs GraphQL (Query the MPS backend for MPS Status) backoff\tThis the exponential back off factor for retries for failed queries. The wait time between successive retries will increase with this factor\t1.5 interval\tBase delay between retries\t4 secs retries\tMaximum number of retries before giving up\t3 Download backoff\tThis the exponential back off factor for retries during failed downloads. The wait time between successive retries will increase with this factor.\t1.5 interval\tBase delay between retries\t20 secs retries\tMaximum number of retries before giving up\t10 chunk_size\tThe chunk size to use for downloads\t10MB concurrent_downloads\tNumber of concurrent downloads\t10 delete_zip\tThe server will send the results in a zip file. This flag controls whether to delete the zip file after extraction or not\tTrue     ","version":"Next","tagName":"h2"},{"title":"Command line reference​","type":1,"pageTitle":"Project Aria Machine Perception Services CLI Guide","url":"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli_guide#command-line-reference","content":" The MPS CLI has two distinct modes: single (process each recording individually) and multi (SLAM outputs for multiple recordings in a shared co-ordinate frame).  aria_mps single &lt;options&gt;   or  aria_mps multi &lt;options&gt;   ","version":"Next","tagName":"h2"},{"title":"Help​","type":1,"pageTitle":"Project Aria Machine Perception Services CLI Guide","url":"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli_guide#help","content":" To see the available options and subcommands, use:  --help   or  -h   ","version":"Next","tagName":"h3"},{"title":"Authentication​","type":1,"pageTitle":"Project Aria Machine Perception Services CLI Guide","url":"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli_guide#authentication","content":" Log in​  The first time you use the MPS CLI, you’ll be prompted to enter your username and password. Use the Project Aria credentials you use to sign into the Mobile Companion app. You can also supply the username and password via CLI input. This option is more suited for running MPS as part of a batch script or other automated workflows.  The authentication token gets saved in the $HOME/.projectaria directory.  To log in using the CLI  -u USERNAME -p PASSWORD   or  --username USERNAME --password PASSWORD   Token storage​  The login token is saved onto your machine by default.  If you request MPS using --no-ui , you'll have the option to pass --no-save-token. This means the token won't be saved. Once processing is complete the MPS CLI will also logout and invalidate any existing tokens.  Log out​  Use the following command to log out the authentication token and delete it from the system. Next time you run the CLI, it will ask for username and password again.  aria_mps logout   ","version":"Next","tagName":"h3"},{"title":"Request commands for any mode​","type":1,"pageTitle":"Project Aria Machine Perception Services CLI Guide","url":"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli_guide#request-commands-for-any-mode","content":" These options are shared between both modes.  Define input path​  Provides the path for a directory or file that will be uploaded for processing. Where a directory is provided, all subdirectories will be scanned for VRS files. At least one input file must be provided. There is no limit of how many files or folders can be included in a single request.  -i INPUT   or  --input INPUT   Force the provided files to be reprocessed​  Force the server to reprocess all of the provided files, regardless of their current state on the server.  --force   Automatically retry processing if it fails​  By default the MPS server will retry processing data multiple times before generating a failure code. By adding this flag requests automatically retries again if the processing fails. This command is generally only worth using if you’ve done some debugging to warrant it.  --retry-failed   note If you retry 30 days after the recording was uploaded, you'll also need to re-upload the data.  Don’t show the UI​  Instead of the MPS CLI UI, you’ll see the raw outputs and processes in the command line.  --no-ui   ","version":"Next","tagName":"h3"},{"title":"Single Recording mode​","type":1,"pageTitle":"Project Aria Machine Perception Services CLI Guide","url":"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli_guide#single-recording-mode","content":" Select the MPS you wish to generate​  By default, Eye Gaze and SLAM MPS are generated. Use the features option to select a single service, which can speed up processing.  --features {EYE_GAZE,SLAM}   ","version":"Next","tagName":"h3"},{"title":"Multi-Recording mode​","type":1,"pageTitle":"Project Aria Machine Perception Services CLI Guide","url":"/projectaria_tools/docs/ARK/mps/request_mps/mps_cli_guide#multi-recording-mode","content":" Define the output directory​  Define the output directory where the results will be stored. Setting the output path is required. The output directory must be empty before processing starts.  -o OUTPUT_DIR   Or  --output OUTPUT_DIR  ","version":"Next","tagName":"h3"},{"title":"Aria Client SDK API Reference","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/sdk/api_reference","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#overview","content":" This page provides a list of Project Aria Client SDK's Python APIs. There is one global function in this API. Classes appear next in alphabetical order, then enums in alphabetical order.  ","version":"Next","tagName":"h2"},{"title":"Global Functions & Methods​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#global-functions--methods","content":" See Level in the Enums section.  Function\tType\tDescriptionset_log_level(level: Level)\tNone\tSets the SDK logging verbosity.  ","version":"Next","tagName":"h2"},{"title":"Classes​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#classes","content":" Generally, classes in the API maintain state about configurations and processes like streaming and recording. Concerns about security, networking, synchronization, and physical connections are all managed through classes described in this section.  Some classes have properties and methods, some classes have only properties. Properties can be read-only or read-write. Methods are primarily for side-effect, with return type None, with a few exceptions.  ","version":"Next","tagName":"h2"},{"title":"aria.sdk.BaseStreamingClientObserver​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkbasestreamingclientobserver","content":" Method\tType\tDescriptionon_streaming_client_failure(reason: ErrorCode, message: str)\tNone\tRetrieve streaming failure. on_image_received(image_and_record: projectaria_tools.core.sensor_data.ImageDataAndRecord)\tNone\tRetrieve image data streamed from rgb, slam1, slam2 or eye-tracking camera sensors. on_audio_received(audio_and_record: projectaria_tools.core.sensor_data.AudioDataAndRecord)\tNone\tRetrieve audio data streamed from microphone sensors. on_magneto_received(magneto_data: projectaria_tools.core.sensor_data.MotionData)\tNone\tRetrieve magnetometer data streamed from magnetometer sensor. on_baro_received(baro_data: projectaria_tools.core.sensor_data.BarometerData)\tNone\tRetrieve barometer data streamed from barometer sensor. on_imu_received(motion_data: List[projectaria_tools.core.sensor_data.MotionData], imu_idx: int)\tNone\tRetrieve imu data streamed from IMU1 and IMU2 sensors. Use imu_idx to determine the IMU.  ","version":"Next","tagName":"h3"},{"title":"aria.sdk.DdsRpcEnabledStatus​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkddsrpcenabledstatus","content":" Property\tType\tDescriptionstate\tDdsRpcState\t(read-only) State information about DDS RPC (https://www.omg.org/spec/DDS-RPC/) str\tSharedSessionId\t(read-only) Shared Session uuid for synchronized recordings (see TimeSyncMode)  ","version":"Next","tagName":"h3"},{"title":"aria.sdk.Device​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkdevice","content":" Methods​  Method\tType\tDescriptionset_dds_rpc_enabled(enabled: bool, interface: StreamingInterface)\tNone\tEnable or Disable a StreamingInterface to DDS RPC. dds_rpc_new_session_id()\tstr\tReturn a new session uuid for DDS RPC  Properties​  Property\tType\tDescriptionrecording_manager\tRecordingManager\t(read-only) Recording capabilities streaming_manager\tStreamingManager\t(read-only) Streaming capabilities info\tDeviceInfo\t(read-only) Device information such as device name and serial number status\tDeviceStatus\t(read-only) Device status such as battery level and device temperature factory_calibration_json\tstr\t(read-only) Device factory calibration as JSON string wifi_manager\tWifiManager\t(read-only) Aria Wi-Fi capabilities dds_rpc_enabled_status\tDdsRpcEnabledStatus\t(read-only) Status of DDS RPC for the device  ","version":"Next","tagName":"h3"},{"title":"aria.sdk.DeviceClient​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkdeviceclient","content":" Methods​  Method\tType\tDescriptioncreate()\tDeviceClient\tCreate DeviceClient instance. authenticate()\tNone\tAuthenticate your client using the specified config. connect()\tDevice\tConnect to device via Wifi or via USB using the specified config. If setting both IP address and ADB path, ADB path is ignored. disconnect(device: Device)\tNone\tDisconnect Device instance from this client. is_connected(device: Device)\tbool\tChecks whether connection with device is established. set_client_config(config: DeviceClientConfig)\tNone\tSets the config used to authenticate and connect this device as a client.  Properties​  Property\tType\tDescriptionusb_devices\tList[Tuple[str, str]]\t(read-only) List of currently connected USB devices active_connections\tList[Device]\t(read-only) Current active connected devices  ","version":"Next","tagName":"h3"},{"title":"aria.sdk.DeviceClientConfig​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkdeviceclientconfig","content":" Property\tType\tDescriptionip_v4_address\tstr\t(read-write) IP v4 address to use for connecting to the device via Wi-Fi device_serial\tstr\t(read-write) Device serial number used when connecting to the device via USB (only necessary if multiple devices are plugged in) adb_path\tstr\t(read-write) Specify your own custom version of ADB. reconnection_attempts\tint\t(read-write) Number of reconnection attempts before time out; defaults to 2  ","version":"Next","tagName":"h3"},{"title":"aria.sdk.DeviceInfo​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkdeviceinfo","content":" Property\tType\tDescriptionboard\tstr\t(read-only) Device board name bootloader\tstr\t(read-only) Bootloader version brand\tstr\t(read-only) Device brand name device\tstr\t(read-only) Manufacturer's name for the device host\tstr\t(read-only) Id string for the host id\tstr\t(read-only) Id string for the device manufacturer\tstr\t(read-only) Manufacturer name model\tstr\t(read-only) Model name product\tstr\t(read-only) Product name serial\tstr\t(read-only) Serial number time\tint\t(read-only) OS build timestamp type\tstr\t(read-only) Type of the device user\tstr\t(read-only) User name registered for the device  ","version":"Next","tagName":"h3"},{"title":"aria.sdk.DeviceStatus​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkdevicestatus","content":" Property\tType\tDescriptionbattery_level\tint\t(read-only) Battery level charger_connected\tbool\t(read-only) USB charger cable state charging\tbool\t(read-only) Charging state wifi_enabled\tbool\t(read-only) WiFi activation state wifi_configured\tbool\t(read-only) WiFi configuration state wifi_connected\tbool\t(read-only) WiFi connection state wifi_ip_address\tstr\t(read-only) WiFi IP address wifi_device_name\tstr\t(read-only) WiFi device name wifi_ssid\tstr\t(read-only) WiFi SSID name logged_in\tbool\t(read-only) Companion App user login state developer_mode\tbool\t(read-only) Developer mode state adb_enabled\tbool\t(read-only) ADB state thermal_mitigation_triggered\tbool\t(read-only) Indicate max level temperature has been reached triggering throttling skin_temp_celsius\tfloat\t(read-only) Device temperature default_recording_profile\tstr\t(read-only) Default recording profile used when pressing the top right HW button is_recording_allowed\tbool\t(read-only) Recording capability state device_mode\tDeviceMode\t(read-only) Device mode  ","version":"Next","tagName":"h3"},{"title":"aria.sdk.Error​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkerror","content":" Property\tType\tDescriptioncode\tint\t(read-only) Error code (not the ErrorCode enum) message\tstr\t(read-only) Human-readable error message  ","version":"Next","tagName":"h3"},{"title":"aria.sdk.RecordingConfig​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkrecordingconfig","content":" Property\tType\tDescriptionprofile_name\tstr\t(read-write) Sensor's profile name for recording time_sync_mode\tTimeSyncMode\t(read-write) Timesync mode for recording timecode_trigger\tbool\t(read-write) True if this device triggers timecode synchronization rgb_isp_tuning_version\tint\t(read-write) RGB ISP tuning version  ","version":"Next","tagName":"h3"},{"title":"aria.sdk.RecordingManager​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkrecordingmanager","content":" Methods​  Method\tType\tDescriptionstart_recording()\tNone\tStart recording. stop_recording()\tNone\tStop recording. sensors_calibration()\tstr\tRetrieve the device calibration computed from the sensor profile used to record.  Properties​  Property\tType\tDescriptionrecording_state\tRecordingState\t(read-only) Fetch the current recording state. recording_config\tRecordingConfig\t(read-write) Fetch or set recording parameters such as the sensor's profile. recording_profiles\tList[str]\t(read-only) Fetch a list of profile names that can be applied to a recording. ticsync_status\tTicSyncStatus\t(read-only) Fetch TICSync status for this recording session.  ","version":"Next","tagName":"h3"},{"title":"aria.sdk.StreamingClient​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkstreamingclient","content":" Methods​  Method\tType\tDescriptioncreate()\tStreamingClient\tCreate StreamingClient instance. subscribe(StreamingClient)\tNone\tSubscribe the specified streaming client to data streamed from Aria. unsubscribe()\tNone\tUnsubscribe from data streamed from Aria. is_subscribed()\tbool\tReturn True if the client is subscribed to data streamed from Aria. set_streaming_client_observer(observer: StreamingClientObserver)\tNone\tSet the observer to subscribe to the data streamed from Aria.  Property​  Property\tType\tDescriptionsubscription_config\tStreamingSubscriptionConfig\t(read-write) Access the streaming subscription config for this streaming session.  ","version":"Next","tagName":"h3"},{"title":"aria.sdk.StreamingConfig​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkstreamingconfig","content":" Property\tType\tDescriptionsecurity_options\tStreamingSecurityOptions\t(read-write) Security options used to start streaming topic_prefix\tstr\t(read-write) A unique string to prefix all streamed data messages profile_name\tstr\t(read-write) Sensors' profile name used to start streaming streaming_interface\tStreamingInterface\t(read-write) Network interface used to start streaming time_sync_mode\tTimeSyncMode\t(read-write) The timesync mode for this streaming session timecode_trigger\tbool\t(read-write) True if this device triggers timecode synchronization rgb_isp_tuning_version\tint\t(read-write) RGB ISP tuning version  ","version":"Next","tagName":"h3"},{"title":"aria.sdk.StreamingManager​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkstreamingmanager","content":" Methods​  Method\tType\tDescriptionstart_streaming()\tNone\tStart streaming. stop_streaming()\tNone\tStop streaming. sensors_calibration()\tstr\tRetrieve the device calibration computed from the sensors profile used to stream.  Properties​  Property\tType\tDescriptionstreaming_config\tStreamingConfig\t(read-write) Used to configure streaming parameters related to network interface, security, and sensor's profile. streaming_state\tStreamingState\t(read-only) Determine current streaming state. ticsync_status\tTicSyncStatus\t(read-only) TICSync status for this streaming session streaming_client\tStreamingClient\t(read-only) Return the StreamingClient object for this streaming session.  ","version":"Next","tagName":"h3"},{"title":"aria.sdk.StreamingSecurityOptions​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkstreamingsecurityoptions","content":" Property\tType\tDescriptionuse_ephemeral_certs\tbool\t(read-write) Use ephemeral certs instead of persistent ones. local_certs_root_path\tstr\t(read-write) Local directory path where streaming certificates are stored  ","version":"Next","tagName":"h3"},{"title":"aria.sdk.StreamingSubscriptionConfig​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkstreamingsubscriptionconfig","content":" Property\tType\tDescriptionsecurity_options\tStreamingSecurityOptions\t(read-write) Security options used to subscribe to an existing live secure stream subscriber_data_type\tStreamingDataType\t(read-write) Data types to subscribe to message_queue_size\tint\t(read-write) Size for the message queue; A shorter queue size may be useful if the processing callback is always slow and you wish to process more recent data. subscriber_name\tstr\t(read-write) Subscriber name subscriber_topic_prefix\tstr\t(read-write) The topic used to prefix the existing live stream   ","version":"Next","tagName":"h3"},{"title":"aria.sdk.TicSyncStatus​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkticsyncstatus","content":" Property\tType\tDescriptionserver_id\tstr\t(read-write) The uuid for the device if the device is a TICSync server client_id\tstr\t(read-write) The uuid for the device if the device is a TICSync client connection_quality\tConnectionQuality\t(read-write) The connection quality for this TICSync session synchronization_stability\tSynchronizationStability\t(read-write) The synchronization stability for this TICSync session  ","version":"Next","tagName":"h3"},{"title":"aria.sdk.WifiHotSpotStatus​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkwifihotspotstatus","content":" Property\tType\tDescriptionenabled\tbool\t(read_write) whether the device has enabled its Wi-Fi hotspot ssid\tstr\t(read_write) the ssid of the hotspot for connecting devices passphrase\tstr\t(read_write) the passphrase for connecting devices  ","version":"Next","tagName":"h3"},{"title":"aria.sdk.WifiManager​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkwifimanager","content":" Methods​  Method\tType\tDescriptionscan_wifi()\tNone\tScan Wi-Fi networks nearby. connect_wifi(ssid: str, password: str, auth: WifiAuthentication, hidden: bool, username: str, disable_other_networks: bool, skip_internet_check: bool)\tNone\tConnect to Wi-Fi network with the given ssid, credentials, and options. forget_wifi(ssid: str)\tNone\tForget the Wi-Fi network with the given ssid. set_device_hotspot_status(enabled: bool, band5GHz: bool = False, country_code: str = &quot;US&quot;)\tNone\tTurn on or off the Wi-Fi hotspot, defaulting to 2.4 GHz and country code &quot;US&quot;. keep_wifi_on(keep_on: bool)\tNone\tTurn on or off the keep_on attribute of the Wi-Fi manager.  Properties​  Property\tType\tDescriptiondevice_hotspot_status\tWifiHotSpotStatus\t(read-only) An enabled flag, an ssid, and a passphrase wifi_status\tWifiStatus\t(read-only) Information about available Wi-Fi networks  ","version":"Next","tagName":"h3"},{"title":"aria.sdk.WifiNetwork​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkwifinetwork","content":" Property\tType\tDescriptionssid\tstr\t(read-write) the ssid of the Wi-Fi network auth\tList[WifiAuthentication]\t(read-write) authentication methods offered by the Wi-Fi network signal\tOptional[int]\t(read-write) signal strength in dBm (usually -30 to -100)  ","version":"Next","tagName":"h3"},{"title":"aria.sdk.WifiScanResult​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkwifiscanresult","content":" Property\tType\tDescriptionnetworks\tList[WifiNetwork]\t(read-write) Networks in range device_mac_address\tstr\t(read-write) This device's MAC address in standard format  ","version":"Next","tagName":"h3"},{"title":"aria.sdk.WifiStatus​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkwifistatus","content":" Property\tType\tDescriptionenabled\tbool\t(read-write) Whether the device has enabled Wi-Fi networking network\tWifiNetwork\t(read-write) Information about the Wi-Fi network reachability\tOptional[ReachabilityStatus]\t(read-write) Reachability information about the Wi-Fi connection known_networks\tList[WifiNetwork]\t(read-write) Known Wi-Fi networks within range of this device  ","version":"Next","tagName":"h3"},{"title":"Enums​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#enums","content":" Enums specify choices or outcomes for various properties and methods in the classes such as modes, states, statuses, and types.  ","version":"Next","tagName":"h2"},{"title":"aria.sdk.CameraId​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkcameraid","content":" Name\tDescriptionSlam1\tSlam camera 1 sensor Slam2\tSlam camera 2 sensor Rgb\tRgb camera sensor EyeTrack\tEye tracking camera sensors Invalid\tUnknown camera sensor  ","version":"Next","tagName":"h3"},{"title":"aria.sdk.ConnectionQuality​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkconnectionquality","content":" Name\tDescriptionUnknown\tNo connection-quality information ia available for this TICSync session. Good\tThe connection quality is good for this TICSync session. Medium\tThe connection quality is acceptable for this TICSync session. Bad\tThe connection quality is bad for this TICSync session.  ","version":"Next","tagName":"h3"},{"title":"aria.sdk.DdsRpcState​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkddsrpcstate","content":" Name\tDescriptionOn\tDDS RPC is on for the current connection Off\tDDS RPC is off for the current connection  ","version":"Next","tagName":"h3"},{"title":"aria.sdk.ErrorCode​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkerrorcode","content":" Name\tDescriptionConnectionLost\tConnection has been lost. Adb\tAn error occurred in the interface to Android. UnknownError\tAn unknown error occurred. BadRequest\tA bad request was made. UnsupportedMethod\tAn unsupported method was called. BatteryTooLow\tDevice battery is too low to continue. BadArgument\tAn invalid argument was passed to a method in the API. AuthenticationFailure\tAuthentication failed for a device. TimedOut\tA request timed out waiting for a response. AlreadyInProgress\tThe requested operation is already in progress. InternalError\tAn internal error was encountered. WifiNoNetwork\tNo Wi-Fi network can be found in range of the device. WifiInvalidAuth\tAuthentication failed on the Wi-Fi network. BadAccessToken\tA bad access token was passed. DeviceWifiError\tThe device encountered a Wi-Fi error. DeviceBleError\tAn error occurred with BLE (Bluetooth Low Energy). WifiNoInternet\tNo Internet connection is available on this Wi-Fi connection. WifiAuthTimeout\tWi-Fi authentication or authorization failed. WifiIpConfigFail\tIP configuration for Wi-Fi failed. OperationNotAllowed\tThe requested operation is not allowed. FailedToStartRecording\tThe recording failed to start. FailedToStopRecording\tThe recording failed to stop. RecordingInProgress\tRecording is already in progress. NoRecordingInProgress\tRecording is not in progress. InvalidRecordingProfile\tThe recording profile is invalid. FailedToListRecordings\tStored recordings could not be listed. FwUpdateInProgress\tPort-forwarding update is in progress. PrivacySwitchOn\tThe hardware privacy switch is in the ON position on the device. RecordingNotFound\tThe requested recording could not be found. GpsRfcalInvalid\tGPS RF calibration is invalid. InvalidDeviceCalibration\tThe device calibration is invalid. LowBatteryLevel\tThe device's battery level is critically low (warning). LowDiskSpace\tThe device's storage space is low (warning). FailedToStartStreaming\tStreaming from or to the device failed to start. FailedToStopStreaming\tStreaming from or to the device failed to stop. UploadAlreadyStarted\tUploading to the device has already started. UploadEmptyRecording\tAn attempt was made to upload an empty recording. UploadEmptyMetadata\tAn attempt was made to upload empty metadata. UploadInvalidMetadata\tAn attempt was made to upload invalid metadata. WifiConnectionFailed\tWi-Fi connection attempt failed. TimecodeNotReceived\tAn expected timecode synchronization message was not received. NtpTimeSyncFailed\tSynchronization by NTP failed. TicSyncWifiConfigFailed\tTICSync Wi-Fi configuration failed. TicSyncStatusFailed\tTICSync status check failed. InvalidStreamingInterface\tAn invalid StreamingInterface enum was specified. UsbRndisConnectionFailed\tThe RNDIS specification failed for a USB connection. UsbNcmConnectionFailed\tThe NCM specification failed for a USB connection. PhoneGpsUpdateFailed\tThe device failed to update GPS. DdsDiscoveryFailed\tDDS Discovery failed. SharedSessionIdNotReceived\tThe shared session id for synchronized recordings was not received. ThumbnailNotFound\tNo thumbnail was found for the specified recording. DefaultRecordingProfileNotSet\tNo default recording profile has been set. StreamingCertsInstallFailed\tFailed to install certificates for streaming. StreamingCertsUninstallFailed\tFailed to un-install certificates for streaming. StreamingCertsNotFound\tStreaming certificates were not found. DdsRpcSetEnabledFailed\tFailed to set DDS RPC. DdsRpcGetSessionIdFailed\tFailed to fetch a session id for DDS RPC. DdsRpcUpdateSessionIdFailed\tFailed o update the session id for DDS RPC. WifiScanConfigFailed\tThe Wi-Fi scanning configuration failed. StreamingNotAllowed\tStreaming is not allowed. StreamingSetupFailed\tStreaming setup failed. NewFilenameExists\tA file with the specified name already exists on the device. FileNotFound\tA file with the specified name could not be found on the device. InvalidFilename\tThe specified filename is invalid on the device. TlsClientCertsError\tThe TLS certificate on the device is invalid.  ","version":"Next","tagName":"h3"},{"title":"aria.sdk.Level​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdklevel","content":" Name\tDescriptionDisabled\tDisable all logs. Error\tPrint only error logs. Warning\tPrint warning and error logs. Info\tPrint info, warning, and error logs (default). Debug\tPrint debug, info, warning, and error logs. Trace\tPrint all logs.  ","version":"Next","tagName":"h3"},{"title":"aria.sdk.ReachabilityStatus​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkreachabilitystatus","content":" Name\tDescriptionOk\tThe Wi-Fi network has full reachability. OculusServerUnreachable\tThe Oculus server is unreachable through this Wi-Fi network. InternetUnreachable\tThe internet is unreachable through this Wi-Fi network. RouterUnreachable\tThe router is unreachable for this Wi-Fi network.  ","version":"Next","tagName":"h3"},{"title":"aria.sdk.RecordingState​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkrecordingstate","content":" Name\tDescriptionUnknown\tRecording state not known NotStarted\tRecording not started Started\tRecording stopped Streaming\tRecording in progress Stopped\tRecording stopped  ","version":"Next","tagName":"h3"},{"title":"aria.sdk.StreamingDataType​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkstreamingdatatype","content":" Name\tDescriptionUnknown\tUnknown streaming data type Rgb\tRgb sensor data Slam\tSlam1 and Slam2 sensors data EyeTrack\tEye tracking sensors data Audio\tMicrophones data Imu\tImu sensors data Magneto\tMagnetometer sensor data Baro\tBarometer sensor data TimeSync\tTimesync data topic  ","version":"Next","tagName":"h3"},{"title":"aria.sdk.StreamingInterface​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkstreaminginterface","content":" Name\tDescriptionWifiStation\tStream through Wi-Fi router Usb\tStream through USB cable  ","version":"Next","tagName":"h3"},{"title":"aria.sdk.StreamingStability​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkstreamingstability","content":" Name\tDescriptionWarmingUp\tTICSync is still warming up. Stable\tTICSync has stabilized.  ","version":"Next","tagName":"h3"},{"title":"aria.sdk.StreamingState​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkstreamingstate","content":" Name\tDescriptionUnknown\tStreaming state unknown NotStarted\tStreaming not started Started\tStreaming started Streaming\tStreaming in progress Stopped\tStreaming stopped  ","version":"Next","tagName":"h3"},{"title":"aria.sdk.TimeSyncMode​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdktimesyncmode","content":" Name\tDescriptionNotEnabled\tTimesync is not enabled for this recording. Timecode\tTimecode timesync is enabled for this recording. Ntp\tNTP timesync is enable for this recording. TicSyncClient\tTICSync is enabled for this recording and this device is a TICSync client. TicSyncServer\tTICSync is enabled for this recording and this device is a TICSync server.  ","version":"Next","tagName":"h3"},{"title":"aria.sdk.WifiAuthentication​","type":1,"pageTitle":"Aria Client SDK API Reference","url":"/projectaria_tools/docs/ARK/sdk/api_reference#ariasdkwifiauthentication","content":" Name\tDescriptionNone\tNo authentication Eap\tEAP authentication Wpa\tWPA authentication Wep\tWEP authentication ","version":"Next","tagName":"h3"},{"title":"About TICSync","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/sdk/concepts/about_ticsync","content":"","keywords":"","version":"Next"},{"title":"What is TICSync​","type":1,"pageTitle":"About TICSync","url":"/projectaria_tools/docs/ARK/sdk/concepts/about_ticsync#what-is-ticsync","content":" TICSync is an extremely efficient algorithm for learning the mapping between distributed clocks, which typically achieves better than millisecond accuracy within just a few seconds. It works by estimating clock offset and skew between itself (the host) and a device (Aria leader or server).  The host obtains time triplets from two-way communication (hostRequestTime, deviceTime, hostReceiveTime) – and gets a first-order (offset, skew) estimate that converges quickly and is usually stable after a couple dozen samples. The samples get added to the estimator so that estimates keep improving over time and converge very quickly.  The accuracy of this method, using Project Aria glasses with the Client SDK, has been tested to be better than 1ms on average after approximately 45 seconds warm-up.    Figure 1: True Time Diagram, Alastair Harrison and Paul Newman, TICSync: Knowing when things happened, IEEE International Conference on Robotics and Automation, 2011  The TICSync: Knowing when things happened paper describes in more detail how the algorithm works, provides a mathematical analysis and results with real experiments.  ","version":"Next","tagName":"h2"},{"title":"How Project Aria uses TICSync​","type":1,"pageTitle":"About TICSync","url":"/projectaria_tools/docs/ARK/sdk/concepts/about_ticsync#how-project-aria-uses-ticsync","content":" TICSync can be used to capture time-synchronized data between multiple Project Aria glasses (and potentially other devices) on the one Wi-Fi network via TICSync. TICSync recordings are initiated via the Project Aria Client SDK. Using the Aria hotspot feature, one pair of glasses (server) acts as a Wi-Fi access point that forms a network between all glasses. Time requests from all other glasses go to the server/leader device, creating a synchronized time reference.  Further resources:  How to Create Time Synchronized Recordings with Multiple Aria GlassesTICSync Code Snippet ","version":"Next","tagName":"h2"},{"title":"Streaming Internals","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/sdk/concepts/streaming_internals","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Streaming Internals","url":"/projectaria_tools/docs/ARK/sdk/concepts/streaming_internals#overview","content":" This page provides details about how Project Aria data is streamed using the Client SDK. This is followed by how to stream and subscribe to Aria glasses’ sensor streams.  ","version":"Next","tagName":"h2"},{"title":"Concepts​","type":1,"pageTitle":"Streaming Internals","url":"/projectaria_tools/docs/ARK/sdk/concepts/streaming_internals#concepts","content":" ","version":"Next","tagName":"h2"},{"title":"Introduction​","type":1,"pageTitle":"Streaming Internals","url":"/projectaria_tools/docs/ARK/sdk/concepts/streaming_internals#introduction","content":" Project Aria glasses can be configured to stream sensor data over a network to one or more devices running the Client SDK. You can stream data from the glasses over Wi-Fi or USB.  There are two aspects to streaming; configuring the glasses to stream data and subscribing to the data. You may configure the device for streaming via the SDK, the CLI or by changing the default recording set by the Companion App. During configuration, you can select a profile that determines which sensors stream data and at what rate and resolution. Once streaming has started, the glasses will start capturing data according to the set profile, but no data will be transmitted over the network until a computer running the SDK subscribes to the data.  A computer running the SDK may then subscribe to all or a subset of the sensor streams from the glasses. The device or process subscribing does not need to be the same as the device that configured the sensors or started the streaming.  ","version":"Next","tagName":"h3"},{"title":"Streaming security​","type":1,"pageTitle":"Streaming Internals","url":"/projectaria_tools/docs/ARK/sdk/concepts/streaming_internals#streaming-security","content":" The sensor data streams are secured via a certificate pair. A subscribing device must have a valid certificate matching the one installed on the glasses to successfully subscribe to the sensor streams.  note Streaming certificates are not the same as the certificates used to pair the SDK with the glasses.  You may use ephemeral or persistent streaming certificates.  Persistent certificates​  Persistent certificates are generated once and are valid for a year. They may be used to subscribe to streaming data from several different computers. Persistent certificates may be generated and installed on the glasses via the CLI using:  aria streaming install-certs   Future streaming sessions will use these certificates by default. The certificates may be used on another machine by copying the certificates from the following directory to the same directory on another machine.  ~/.aria/streaming-certs/persistent   Ephemeral certificates​  Ephemeral certificates are generated on the fly for each streaming session and are only valid for that streaming session. These are most useful when you are configuring streaming and subscribing from the same machine.  To start streaming with ephemeral certificates from the CLI use  aria streaming start *--use-ephemeral-certs*   ","version":"Next","tagName":"h3"},{"title":"Streaming interfaces​","type":1,"pageTitle":"Streaming Internals","url":"/projectaria_tools/docs/ARK/sdk/concepts/streaming_internals#streaming-interfaces","content":" Data may be streamed over Wi-Fi or a USB network interface.  Streaming over W-Fi​  Streaming over Wi-Fi provides more flexibility in terms of mobility, but the bandwidth and latency is dependent on the strength of the Wi-Fi connection and contention from other network users.  Streaming over USB​  Streaming over USB gives higher bandwidth and a more stable connection. Since the glasses are powered over USB the duration of the streaming session is not limited by the battery.   ","version":"Next","tagName":"h3"},{"title":"Streaming backend​","type":1,"pageTitle":"Streaming Internals","url":"/projectaria_tools/docs/ARK/sdk/concepts/streaming_internals#streaming-backend","content":" We use eProsima implementation of Fast DDS to support streaming between glasses and the SDK. DDS enables automatic discovery of glasses over the network.  To support streaming, the following requirements must be met on the network:  Multicast UDP must be supported across the networkUDP ports in the range 7000-8000 should be open on your machine  Each sensor stream is published over a DDS topic. If multiple Aria glasses are present on the network, a separate topic prefix should be used for each pair of glasses.  ","version":"Next","tagName":"h3"},{"title":"Starting streaming​","type":1,"pageTitle":"Streaming Internals","url":"/projectaria_tools/docs/ARK/sdk/concepts/streaming_internals#startingstreaming","content":" ","version":"Next","tagName":"h2"},{"title":"With the CLI​","type":1,"pageTitle":"Streaming Internals","url":"/projectaria_tools/docs/ARK/sdk/concepts/streaming_internals#with-the-cli","content":" The Subscribe to Aria Streaming Data code sample shows how to initiate streaming using the CLI over USB or Wi-Fi.  ","version":"Next","tagName":"h3"},{"title":"With the SDK​","type":1,"pageTitle":"Streaming Internals","url":"/projectaria_tools/docs/ARK/sdk/concepts/streaming_internals#with-the-sdk","content":" Setting up the glasses for streaming is configured via StreamingManager. There you can configure which sensors should be streamed by setting the profile and configure the security settings for subscribing to the data.  Go to the Streaming code example for how to start streaming using the SDK.  ","version":"Next","tagName":"h3"},{"title":"Subscribing to data with the SDK​","type":1,"pageTitle":"Streaming Internals","url":"/projectaria_tools/docs/ARK/sdk/concepts/streaming_internals#subscribing-to-data-with-the-sdk","content":" Go to Subscribe to Aria Streaming Data code samples for a code walkthrough of the streaming subscription setup and configuration steps described below.  ","version":"Next","tagName":"h2"},{"title":"Setting up the streaming client​","type":1,"pageTitle":"Streaming Internals","url":"/projectaria_tools/docs/ARK/sdk/concepts/streaming_internals#setting-up-the-streaming-client","content":" You can subscribe to data from the glasses using a StreamingClient object. If you have started streaming from the SDK you can obtain a preconfigured StreamingClient from StreamingManager using streamingManager.streamingClient().  If you set up streaming from another CLI or another machine, you will need to set the StreamingSubscriptionConfig appropriately.  You may control which sensor streams you subscribe to using the subscriber_data_type configuration option. If you are only using a subset of the streams, this can significantly reduce the bandwidth required for stable streaming and reduce the power consumption of the glasses, extending the length of the streaming session.   ","version":"Next","tagName":"h3"},{"title":"Message queue size​","type":1,"pageTitle":"Streaming Internals","url":"/projectaria_tools/docs/ARK/sdk/concepts/streaming_internals#message-queue-size","content":" For each data type you may set a custom queue size, controlling how many messages will be queued if the message callback is slow. The best setting for this value is application-dependent. If the processing time is bursty and it is important not to drop messages you may wish to set a long queue length. Conversely if processing is known to be slow and you are only interested in the most recent data (for example visualizing the image stream) you may wish to set the queue depth to 1.  ","version":"Next","tagName":"h3"},{"title":"Adding an observer to receive data​","type":1,"pageTitle":"Streaming Internals","url":"/projectaria_tools/docs/ARK/sdk/concepts/streaming_internals#adding-an-observer-to-receive-data","content":" Streaming data is provided via callbacks on an observer object you provide to the streaming client. Please be aware that the callbacks will not be invoked from the main thread and may be invoked from a different thread from message to message. If the callback runs slowly, subsequent data on that topic will be queued up to the queue size you have configured. If the queue is full, older data will be dropped. ","version":"Next","tagName":"h3"},{"title":"Project Aria Client SDK Code Samples","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/sdk/samples","content":"Project Aria Client SDK Code Samples This section provides code samples and walkthroughs for using the Aria Client SDK to interact with the Project Aria glasses. Make sure you have finished the Setup Guide before progressing to code samples. If you follow the guide, code samples are run from ~/projectaria_client_sdk_samples: cd ~/projectaria_client_sdk_samples Except for Time Synchronization TICSync code samples, which are run from ~/projectaria_client_sdk_samples/ticsync. cd ~/projectaria_client_sdk_samples/ticsync We recommend starting with device_connect.py to validate that your Aria glasses have connected successfully. Connection Connect to the Aria glasses using USB and Wi-FiFetch device information, such as the device serial numberFetch device status, such as battery level or Wi-Fi SSID. Recording Start and stop recording on an Aria device via USB and Wi-FiSet a recording profile for a recording Streaming Subscription Subscribe to and unsubscribe from a streaming sessionVisualize the live stream Streaming and Visualizing All Live Sensor Data Start and stop streaming from an Aria device to a computer via USB and Wi-FiAdd callbacks to visualize and manipulate the streamed data Streaming Undistorted RGB Image Using Calibration Start and stop streaming from an Aria device to a computer via USB and Wi-FiAccess sensor calibration and undistort an RGB live stream TICSync Time Synchronization Requires at least two Project Aria glassesDeeper dive and more granular control of features than commands provided in How to Create Time Synchronized Recordings with Multiple Aria Glasses","keywords":"","version":"Next"},{"title":"Aria CLI Command References","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/sdk/cli/api_reference","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Aria CLI Command References","url":"/projectaria_tools/docs/ARK/sdk/cli/api_reference#overview","content":" This page provides all of the commands available when using the Project Aria CLI (installed as part of the Client SDK). You can also use ADB to interact with Project Aria glasses.  For any command with [OPTIONS], use --help or -h to find out about the options and subcommands available.  ","version":"Next","tagName":"h2"},{"title":"Authentication​","type":1,"pageTitle":"Aria CLI Command References","url":"/projectaria_tools/docs/ARK/sdk/cli/api_reference#authentication","content":" Manage device pairing with external client over Wi-Fi and USB  aria auth [OPTIONS] SUBCOMMAND   ","version":"Next","tagName":"h2"},{"title":"pair​","type":1,"pageTitle":"Aria CLI Command References","url":"/projectaria_tools/docs/ARK/sdk/cli/api_reference#pair","content":" Pair device with client  aria auth pair   ","version":"Next","tagName":"h3"},{"title":"unpair​","type":1,"pageTitle":"Aria CLI Command References","url":"/projectaria_tools/docs/ARK/sdk/cli/api_reference#unpair","content":" Unpair device from client  aria auth unpair   ","version":"Next","tagName":"h3"},{"title":"Device​","type":1,"pageTitle":"Aria CLI Command References","url":"/projectaria_tools/docs/ARK/sdk/cli/api_reference#device","content":" Control device.  aria device [OPTIONS] SUBCOMMAND   ","version":"Next","tagName":"h2"},{"title":"list​","type":1,"pageTitle":"Aria CLI Command References","url":"/projectaria_tools/docs/ARK/sdk/cli/api_reference#list","content":" List connected USB devices.  aria device list   ","version":"Next","tagName":"h3"},{"title":"info​","type":1,"pageTitle":"Aria CLI Command References","url":"/projectaria_tools/docs/ARK/sdk/cli/api_reference#info","content":" Get device info.  aria device info   ","version":"Next","tagName":"h3"},{"title":"status​","type":1,"pageTitle":"Aria CLI Command References","url":"/projectaria_tools/docs/ARK/sdk/cli/api_reference#status","content":" Get device status.  aria device status   ","version":"Next","tagName":"h3"},{"title":"default-profile​","type":1,"pageTitle":"Aria CLI Command References","url":"/projectaria_tools/docs/ARK/sdk/cli/api_reference#default-profile","content":" Manage default profile.  aria device default-profile [OPTIONS] SUBCOMMAND   Subcommands include​  --status   Get default profile status.  --set   Set default profile.  ","version":"Next","tagName":"h3"},{"title":"Recording​","type":1,"pageTitle":"Aria CLI Command References","url":"/projectaria_tools/docs/ARK/sdk/cli/api_reference#recording","content":" Control device recording.  aria recording [OPTIONS] SUBCOMMAND   ","version":"Next","tagName":"h2"},{"title":"start​","type":1,"pageTitle":"Aria CLI Command References","url":"/projectaria_tools/docs/ARK/sdk/cli/api_reference#start","content":" Start recording.  aria recording start [OPTIONS]   Options​  --profile (string)   Recording profile name.  ","version":"Next","tagName":"h3"},{"title":"stop​","type":1,"pageTitle":"Aria CLI Command References","url":"/projectaria_tools/docs/ARK/sdk/cli/api_reference#stop","content":" Stop recording.  aria recording stop   tip Go to the Downloads section of the Project Aria Glasses Quickstart Guide for ways to download Aria data.  ","version":"Next","tagName":"h3"},{"title":"profiles​","type":1,"pageTitle":"Aria CLI Command References","url":"/projectaria_tools/docs/ARK/sdk/cli/api_reference#profiles","content":" List recording profiles.  aria recording profiles [OPTIONS]   Options​  --save-json (string)   Save profiles as JSON files.  --json-dir (string)   Location of directory where profiles will be saved as JSON files.  ","version":"Next","tagName":"h3"},{"title":"Streaming​","type":1,"pageTitle":"Aria CLI Command References","url":"/projectaria_tools/docs/ARK/sdk/cli/api_reference#streaming","content":" Control device streaming.  aria streaming [OPTIONS] SUBCOMMAND   ","version":"Next","tagName":"h2"},{"title":"start​","type":1,"pageTitle":"Aria CLI Command References","url":"/projectaria_tools/docs/ARK/sdk/cli/api_reference#start-1","content":" Start streaming  aria streaming start [OPTIONS]   Options​  --profile (string)   Streaming profile name.  --topic-prefix (string)   Streaming topic prefix.  --interface (string)   Streaming interface name. Values can be:[usb] - Streaming over USB. [wifi] - Streaming over Wi-Fi network. [hotspot] - Streaming over Aria Wi-Fi hotspot.  --use-ephemeral-certs (flag)   Use ephemeral streaming certs on both device and local host (see aria streaming install-certs --help)  --local-certs-dir (string)   Local streaming certificates directory. Use to specify the local directory where local streaming certificates are installed on local host.  ","version":"Next","tagName":"h3"},{"title":"Stop​","type":1,"pageTitle":"Aria CLI Command References","url":"/projectaria_tools/docs/ARK/sdk/cli/api_reference#stop-1","content":" Stop streaming.  aria streaming stop [OPTIONS]   ","version":"Next","tagName":"h3"},{"title":"Options​","type":1,"pageTitle":"Aria CLI Command References","url":"/projectaria_tools/docs/ARK/sdk/cli/api_reference#options-3","content":" --remove-ephemeral-certs (flag)   Remove ephemeral streaming certs from local host.  Use aria streaming uninstall-certs --help for commands to uninstall ephemeral certs on Aria glasses and local host.  info Ephemeral certificates should automatically expire and be removed when streaming stops.  --local-certs-dir (string)   Local streaming certificates directory.  Use to specify the local directory where local streaming certificates are installed on local host.  ","version":"Next","tagName":"h3"},{"title":"profiles​","type":1,"pageTitle":"Aria CLI Command References","url":"/projectaria_tools/docs/ARK/sdk/cli/api_reference#profiles-1","content":" List streaming profiles.  aria streaming profiles [OPTIONS]   Options​  --save-json (flag)   Save profiles as JSON files.  --json-dir (string)   Location of directory where profiles will be saved as JSON files.  ","version":"Next","tagName":"h3"},{"title":"install-certs​","type":1,"pageTitle":"Aria CLI Command References","url":"/projectaria_tools/docs/ARK/sdk/cli/api_reference#install-certs","content":" Installs streaming certificates on Aria glasses and on the local host.  aria streaming install-certs [OPTIONS]   Options​  --local-certs-dir (string)   Local streaming certificates directory. Use to specify the local directory where local streaming certificates are installed on local host.  ","version":"Next","tagName":"h3"},{"title":"uninstall-certs​","type":1,"pageTitle":"Aria CLI Command References","url":"/projectaria_tools/docs/ARK/sdk/cli/api_reference#uninstall-certs","content":" Removes streaming certificates on Aria glasses and on the local host.  aria streaming uninstall-certs [OPTIONS]   Options​  --local-certs-dir (string)   Local streaming certificates directory. Use to specify the local directory where local streaming certificates are installed on local host. ","version":"Next","tagName":"h3"},{"title":"Connect to the glasses and retrieve device status","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/sdk/samples/device_connection","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Connect to the glasses and retrieve device status","url":"/projectaria_tools/docs/ARK/sdk/samples/device_connection#overview","content":" The device_connect example shows how to connect to your Project Aria device using the Client SDK and retrieve:  Device status, such as such as battery level, Wi-Fi SSID or Wi-Fi IP addressDevice information, such as the device serial number or device model  ","version":"Next","tagName":"h2"},{"title":"Running the sample​","type":1,"pageTitle":"Connect to the glasses and retrieve device status","url":"/projectaria_tools/docs/ARK/sdk/samples/device_connection#running-the-sample","content":" In your terminal, from the samples directory, run:  python -m device_connect   You should then see:  [AriaSdk:DeviceControllerImpl][INFO]: Connecting to device &lt;serial_number&gt; using ADB [AriaSdk:DeviceClientImpl][INFO]: Connection established with device &lt;serial_number&gt; Aria Device Status: battery level 100, wifi ssid &lt;xxxxxxxx&gt; , wifi ip &lt;192.168.xx.xx&gt;, mode DeviceMode.Partner Aria Device Info: model Aria, serial &lt;serial_number&gt; Aria Device Connected, disconnecting   ","version":"Next","tagName":"h2"},{"title":"Code walkthrough​","type":1,"pageTitle":"Connect to the glasses and retrieve device status","url":"/projectaria_tools/docs/ARK/sdk/samples/device_connection#code-walkthrough","content":" 1. Create and configure a Device Client​  DeviceClient allow you to connect to Project Aria glasses over Wi-Fi or USB.  device_client = aria.DeviceClient()   By default, DeviceClient connects to Aria glasses over USB. To connect to glasses over Wi-Fi, configure the DeviceClient by creating a DeviceClientConfig, setting ip_v4_address and setting the config.  client_config = aria.DeviceClientConfig() if args.ip_address: client_config.ip_v4_address = args.ip_address device_client.set_client_config(client_config)   info Get your Aria glasses' IP address from the Mobile Companion App by tapping Wi-Fi on the Dashboard.  2. Connect to a Device​  Connect to the Aria glasses and retrieve a Device instance. An Exception will be thrown if the connection is not successful.  device = device_client.connect()   3. Fetch device status and information​  We can then obtain the device status and information:  status = device.status print( &quot;Aria Device Status: battery level {0}, wifi ssid {1}, wifi ip {2}, mode {3}&quot;.format( status.battery_level, status.wifi_ssid, status.wifi_ip_address, status.device_mode, ) )   Check out the full status list.  info = device.info print( &quot;Aria Device Info: model {}, serial {}, manufacturer {}&quot;.format( info.model, info.serial, info.manufacturer ) )   Check out the full information list.  4. Disconnect​  Once all operations have been completed, you can disconnect from your glasses to release any held resources.  device_client.disconnect(device)  ","version":"Next","tagName":"h3"},{"title":"Make a Recording Using the Client SDK","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/sdk/samples/device_recording","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Make a Recording Using the Client SDK","url":"/projectaria_tools/docs/ARK/sdk/samples/device_recording#overview","content":" This device_record example shows how to set a recording profile, start and stop recording using the Project Aria Client SDK.  ","version":"Next","tagName":"h2"},{"title":"Running the sample​","type":1,"pageTitle":"Make a Recording Using the Client SDK","url":"/projectaria_tools/docs/ARK/sdk/samples/device_recording#running-the-sample","content":" ","version":"Next","tagName":"h2"},{"title":"Example 1: Start and stop recording over USB​","type":1,"pageTitle":"Make a Recording Using the Client SDK","url":"/projectaria_tools/docs/ARK/sdk/samples/device_recording#example-1-start-and-stop-recording-over-usb","content":" Use the SDK to establish a USB connection, start a 10 second recording using the default recording profile, then stop the recording.  Plug your Aria glasses into your computer, using the provided USB cableFrom the samples directory in Terminal, run:  python -m device_record   The recording LED will show on your Aria glassesAfter 10 seconds the recording will stop and be stored in your Aria glasses View the recording metadata in the Recordings tab of the Mobile Companion appRun adb shell ls -l /sdcard/recording and you should see your newly recorded fileRun adb pull /sdcard/recording/myVrsFile.vrs myLocalFolder/ to download the VRS fileRun adb pull /sdcard/recording/myVrsFile.json myLocalFolder/ to download the VRS metadataVRS files can be visualized using the Aria Viewer  tip Go to the Downloads section of the Project Aria Glasses Quickstart Guide for more ways to download Aria data.  ","version":"Next","tagName":"h3"},{"title":"Example 2: Start and stop recording using a custom sensor profile​","type":1,"pageTitle":"Make a Recording Using the Client SDK","url":"/projectaria_tools/docs/ARK/sdk/samples/device_recording#example-2-start-and-stop-recording-using-a-custom-sensor-profile","content":" Use Project Aria Recording Profiles to choose among different sensor configurations. To set a specific profile, run:  python -m device_record --profile profile15   ","version":"Next","tagName":"h3"},{"title":"Example 3: Start and stop recording over Wi-Fi​","type":1,"pageTitle":"Make a Recording Using the Client SDK","url":"/projectaria_tools/docs/ARK/sdk/samples/device_recording#example-3-start-and-stop-recording-over-wi-fi","content":" Recording over Wi-Fi is similar with Example 1, with a few extra steps.  Connect your Aria glasses and computer to the same compatible Wi-Fi network: Check Requirements for details about compatible routers Open the Mobile Companion app and tap Wi-Fi on the Dashboard to see your glasses' IP addressFrom the samples directory in Terminal, run:  python -m device_record --device-ip &lt;Glasses IP&gt;   Make sure you replace &lt;Glasses IP&gt; with the IP address you got from the Mobile Companion app  The recording LED will show on your Aria glassesAfter 10 seconds the recording will stop and be stored in your Aria glasses. Follow the same step in Example 1 to pull the data to your computer  ","version":"Next","tagName":"h3"},{"title":"Code walkthrough​","type":1,"pageTitle":"Make a Recording Using the Client SDK","url":"/projectaria_tools/docs/ARK/sdk/samples/device_recording#code-walkthrough","content":" 1. Retrieve RecordingManager instance​  Use RecordingManager to start and stop a recording and get access to the calibration data.  recording_manager = device.recording_manager   2. Configure recording settings​  Set the profile name for the recording in RecordingConfig or you'll use your glasses' default recording profile. If a default recording profile has not been set, you'll record with profile8. Go to the Sensor Profiles page for supported profiles and their specs.  recording_config = aria.RecordingConfig() # If set, profile_name takes precedence over the default profile recording_config.profile_name = args.profile_name recording_manager.recording_config = recording_config   3. Start recording for specific amount of time​  recording_manager.start_recording() time.sleep(args.recording_duration) recording_manager.stop_recording()   info You can also stop recording using the Capture button on the glasses, Mobile Companion app or CLI.  ","version":"Next","tagName":"h3"},{"title":"Useful links​","type":1,"pageTitle":"Make a Recording Using the Client SDK","url":"/projectaria_tools/docs/ARK/sdk/samples/device_recording#useful-links","content":" Glasses Quickstart Guide: explore other ways to make recordings. ","version":"Next","tagName":"h2"},{"title":"Access Sensor Profiles Using the CLI or SDK","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/sdk/concepts/sdk_sensor_profiles","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Access Sensor Profiles Using the CLI or SDK","url":"/projectaria_tools/docs/ARK/sdk/concepts/sdk_sensor_profiles#overview","content":" Project Aria glasses have multiple recording profiles, so that you can choose what sensors record or stream data, as well as the settings those sensors use. This page shows how to use the Client SDK or CLI to download profile information and set which profile is used.  If a value is not set, Aria glasses will use the default recording profile. If a default recording profile is not set, it will use profile8 when making recordings and profile18 when streaming data.  Go to the Recording Profiles page for more information about possible sensor configurations. You can also view all available recording profile details when you select Create a Recording in the Mobile Companion app. In addition to sensor setup, recording profile information can provide insights into how battery and thermal levels can be affected when streaming or recording.  ","version":"Next","tagName":"h2"},{"title":"Retrieve list of available sensor profiles​","type":1,"pageTitle":"Access Sensor Profiles Using the CLI or SDK","url":"/projectaria_tools/docs/ARK/sdk/concepts/sdk_sensor_profiles#retrieve-list-of-available-sensor-profiles","content":" The CLI or SDK can be used to retrieve the available sensor profiles for recording or streaming stored on your Aria glasses. To make sure you have access to all possible profiles, make sure your Aria glasses OS are up to date.  The available profiles for recording or streaming are the same, but profile12 and profile18 were created specifically for streaming data.  ","version":"Next","tagName":"h2"},{"title":"Save streaming/recording profiles​","type":1,"pageTitle":"Access Sensor Profiles Using the CLI or SDK","url":"/projectaria_tools/docs/ARK/sdk/concepts/sdk_sensor_profiles#save-streamingrecording-profiles","content":" The following commands export all profiles as JSON files. There is one JSON file per recording, profile12.json etc.  SDKCLI import aria.sdk as aria device_client = aria.DeviceClient() device = device_client.connect() # sensor profiles for recording recording_manager = device.recording_manager recording_profiles = recording_manager.recording_profiles print(*recording_profiles) # sensor profiles for streaming streaming_manager = device.streaming_manager streaming_profiles = streaming_manager.streaming_profiles print(*streaming_profiles)   ","version":"Next","tagName":"h3"},{"title":"Example output​","type":1,"pageTitle":"Access Sensor Profiles Using the CLI or SDK","url":"/projectaria_tools/docs/ARK/sdk/concepts/sdk_sensor_profiles#example-output","content":" Each JSON file will show which sensors are used and how each sensor is configured.  profile12.json will contain the following:  { &quot;name&quot;: &quot;profile12&quot;, &quot;description&quot;: &quot;Streaming mode with JPEG (RGB 10fps 2MP, SLAM 10fps, ET 10fps QVGA, JPEG; Audio, GPS, Wi-Fi and BLE off)&quot;, &quot;imu1&quot;: { &quot;enabled&quot;: true, &quot;dataRateHz&quot;: 1000 }, &quot;imu2&quot;: { &quot;enabled&quot;: true, &quot;dataRateHz&quot;: 800 }, &quot;magnetometer&quot;: { &quot;enabled&quot;: true, &quot;dataRateHz&quot;: 10 }, &quot;barometer&quot;: { &quot;enabled&quot;: true, &quot;dataRateHz&quot;: 50 }, &quot;audio&quot;: { &quot;enabled&quot;: false, &quot;numChannels&quot;: 0, &quot;sampleRateHz&quot;: &quot;0&quot;, &quot;periodSize&quot;: 0 }, &quot;gps&quot;: { &quot;enabled&quot;: false, &quot;dataRateHz&quot;: 0 }, &quot;ble&quot;: { &quot;enabled&quot;: false, &quot;scanDurationMs&quot;: 0 }, &quot;wifi&quot;: { &quot;enabled&quot;: false, &quot;scanDurationMs&quot;: 0, &quot;wifiScanModeActive&quot;: false, &quot;wifiMinDwellTimeMs&quot;: 0, &quot;wifiMaxDwellTimeMs&quot;: 0 }, &quot;slamCameras&quot;: { &quot;enabled&quot;: true, &quot;width&quot;: 640, &quot;height&quot;: 480, &quot;fps&quot;: 10, &quot;autoExposureEnabled&quot;: true, &quot;exposureMinUs&quot;: &quot;0&quot;, &quot;exposureMaxUs&quot;: &quot;0&quot;, &quot;gainMin&quot;: 0, &quot;gainMax&quot;: 0, &quot;exposureUs&quot;: &quot;0&quot;, &quot;gain&quot;: 0, &quot;irLedEnabled&quot;: false, &quot;imageFormat&quot;: &quot;JPEG&quot;, &quot;jpegEncoderType&quot;: &quot;HARDWARE&quot;, &quot;jpegQuality&quot;: 80, &quot;videoEncoderQp&quot;: 0, &quot;videoCodecType&quot;: &quot;H264&quot;, &quot;targetIntensity&quot;: 0 }, &quot;etCamera&quot;: { &quot;enabled&quot;: true, &quot;width&quot;: 320, &quot;height&quot;: 240, &quot;fps&quot;: 10, &quot;autoExposureEnabled&quot;: false, &quot;exposureMinUs&quot;: &quot;0&quot;, &quot;exposureMaxUs&quot;: &quot;0&quot;, &quot;gainMin&quot;: 0, &quot;gainMax&quot;: 0, &quot;exposureUs&quot;: &quot;1000&quot;, &quot;gain&quot;: 1, &quot;irLedEnabled&quot;: true, &quot;imageFormat&quot;: &quot;JPEG&quot;, &quot;jpegEncoderType&quot;: &quot;HARDWARE&quot;, &quot;jpegQuality&quot;: 80, &quot;videoEncoderQp&quot;: 0, &quot;videoCodecType&quot;: &quot;H264&quot;, &quot;targetIntensity&quot;: 0 }, &quot;rgbCamera&quot;: { &quot;enabled&quot;: true, &quot;width&quot;: 1408, &quot;height&quot;: 1408, &quot;fps&quot;: 10, &quot;autoExposureEnabled&quot;: true, &quot;exposureMinUs&quot;: &quot;0&quot;, &quot;exposureMaxUs&quot;: &quot;0&quot;, &quot;gainMin&quot;: 0, &quot;gainMax&quot;: 0, &quot;exposureUs&quot;: &quot;0&quot;, &quot;gain&quot;: 0, &quot;irLedEnabled&quot;: false, &quot;imageFormat&quot;: &quot;JPEG&quot;, &quot;jpegEncoderType&quot;: &quot;HARDWARE&quot;, &quot;jpegQuality&quot;: 80, &quot;videoEncoderQp&quot;: 0, &quot;videoCodecType&quot;: &quot;H264&quot;, &quot;targetIntensity&quot;: 0 }, &quot;attention&quot;: { &quot;enabled&quot;: false } }   ","version":"Next","tagName":"h3"},{"title":"Use a recording/sensor profile​","type":1,"pageTitle":"Access Sensor Profiles Using the CLI or SDK","url":"/projectaria_tools/docs/ARK/sdk/concepts/sdk_sensor_profiles#use-a-recordingsensor-profile","content":" Once retrieved a profile can be used to start recording or streaming.  In this example, we use the sensor profile profile12 obtained from the previous step to start streaming:  SDKCLI streaming_manager = device.streaming_manager streaming_config = aria.RecordingConfig() streaming_config.profile_name = &quot;profile12&quot; streaming_manager.streaming_config = streaming_config streaming_manager.start_streaming()  ","version":"Next","tagName":"h2"},{"title":"Subscribe to Aria Streaming Data","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/sdk/samples/streaming_subscribe","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Subscribe to Aria Streaming Data","url":"/projectaria_tools/docs/ARK/sdk/samples/streaming_subscribe#overview","content":" This streaming_subscribe example shows how to subscribe to and unsubscribe from a streaming session as well as visualize the live stream, using the Project Aria Client SDK.  ","version":"Next","tagName":"h2"},{"title":"Stream and subscribe examples​","type":1,"pageTitle":"Subscribe to Aria Streaming Data","url":"/projectaria_tools/docs/ARK/sdk/samples/streaming_subscribe#stream-and-subscribe-examples","content":" ","version":"Next","tagName":"h2"},{"title":"Example 1: Stream and subscribe over USB​","type":1,"pageTitle":"Subscribe to Aria Streaming Data","url":"/projectaria_tools/docs/ARK/sdk/samples/streaming_subscribe#example-1-stream-and-subscribe-over-usb","content":" In this example, the CLI is used to initiate streaming and the Client SDK is used to subscribe to the stream. To find out how to start streaming using the SDK, go to Streaming Sensor Data.  Plug your Aria glasses into your computer, using the provided cableFrom the samples directory in Terminal, run:  aria streaming start --interface usb --use-ephemeral-certs   Wait for the stream to start then run:  python -m streaming_subscribe   You should then see:    There are several ways you can stop streaming:  Press q or ESC to quit the appUse Ctrl-C to terminate in terminalPress the Capture button on your glasses  ","version":"Next","tagName":"h3"},{"title":"Example 2: Using Wi-Fi​","type":1,"pageTitle":"Subscribe to Aria Streaming Data","url":"/projectaria_tools/docs/ARK/sdk/samples/streaming_subscribe#example-2-using-wi-fi","content":" To use Wi-Fi to initiate streaming or to stream data, alter the aria streaming start --interface usb --use-ephemeral-certs command.  To stream data over Wi-Fi, use --interface wifiTo initiate streaming over Wi-Fi, add --device-ip &lt;glasses IP&gt; Open the Mobile Companion app and tap Wi-Fi on the Dashboard to see your glasses' IP address  ","version":"Next","tagName":"h3"},{"title":"Example 3: Stream and subscribe in hotspot mode​","type":1,"pageTitle":"Subscribe to Aria Streaming Data","url":"/projectaria_tools/docs/ARK/sdk/samples/streaming_subscribe#example-3-stream-and-subscribe-in-hotspot-mode","content":" In this example, the CLI is used to initiate streaming and the Client SDK is used to subscribe to the stream.  From the samples directory in Terminal, run:  aria streaming start --interface hotspot --use-ephemeral-certs   Wait for the stream to start: once it does, it should display the details of the hotspot connection. Go to the Wi-Fi settings on your computer and connect to the glasses' hotspot connection using the printed SSID and passphrase information. Run the below command to subscribe to the streamed data:  python -m streaming_subscribe --update_iptables   There are several ways you can stop streaming:  Use Ctrl-C to terminate in Terminal.Press the Capture button on your glasses.  ","version":"Next","tagName":"h3"},{"title":"Code walkthrough​","type":1,"pageTitle":"Subscribe to Aria Streaming Data","url":"/projectaria_tools/docs/ARK/sdk/samples/streaming_subscribe#code-walkthrough","content":" 1. Configure the subscription​  Use subscriber_data_type attribute to set the type of data the client subscribes to.  config = streaming_client.subscription_config config.subscriber_data_type = ( aria.StreamingDataType.Rgb | aria.StreamingDataType.Slam )   2. Set message queue size​  The message queue size determines how many recent frames will be retained. A smaller queue size is utilized to process only the most recent data.  config.message_queue_size[aria.StreamingDataType.Rgb] = 1 config.message_queue_size[aria.StreamingDataType.Slam] = 1   3. Set streaming security options​  Security options are set to use ephemeral certificates through a StreamingSecurityOptions instance. Go to the Streaming Internals page for various aspects of streaming security and how certificates are used.  options = aria.StreamingSecurityOptions() options.use_ephemeral_certs = True config.security_options = options   4. Create an StreamingClient observer and attach it​  Find more description of observer in the streaming code sample  class StreamingClientObserver: def __init__(self): self.images = {} def on_image_received(self, image: np.array, record: ImageDataRecord): self.images[record.camera_id] = image observer = StreamingClientObserver() streaming_client.set_streaming_client_observer(observer)   5. Start subscribing and listen to the live stream​  The client begins listening for incoming streaming data from the subscribed data types.  streaming_client.subscribe()   6. Visualize the live stream​  RGB and SLAM images are visualized in separate windows using OpenCV. The images are processed and displayed the streaming stops or the application quit. We rotate the image and stack the SLAM images so that they are shown in a single window.  while not quit_keypress(): # Render the RGB image if aria.CameraId.Rgb in observer.images: rgb_image = np.rot90(observer.images[aria.CameraId.Rgb], -1) rgb_image = cv2.cvtColor(rgb_image, cv2.COLOR_BGR2RGB) cv2.imshow(rgb_window, rgb_image) del observer.images[aria.CameraId.Rgb] # Stack and display the SLAM images if ( aria.CameraId.Slam1 in observer.images and aria.CameraId.Slam2 in observer.images ): slam1_image = np.rot90(observer.images[aria.CameraId.Slam1], -1) slam2_image = np.rot90(observer.images[aria.CameraId.Slam2], -1) cv2.imshow(slam_window, np.hstack((slam1_image, slam2_image))) del observer.images[aria.CameraId.Slam1] del observer.images[aria.CameraId.Slam2]   note Cameras on Aria glasses are installed sideways. The visualizer rotates the raw image data for a more natural view.  ","version":"Next","tagName":"h3"},{"title":"7. Unsubscribe from the stream and free resources​","type":1,"pageTitle":"Subscribe to Aria Streaming Data","url":"/projectaria_tools/docs/ARK/sdk/samples/streaming_subscribe#7-unsubscribe-from-the-stream-and-free-resources","content":" Unsubscribing stops the client from listening to streaming data and cleans up resources.  streaming_client.unsubscribe()  ","version":"Next","tagName":"h3"},{"title":"Streaming Undistorted RGB Image Using Calibration","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/sdk/samples/undistort_rgb_image","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Streaming Undistorted RGB Image Using Calibration","url":"/projectaria_tools/docs/ARK/sdk/samples/undistort_rgb_image#overview","content":" This page shows how to run the code sample undistort_rgb_image to:  Access a Project Aria Tools type device calibration objectUse core data utilities in projectaria_tools to undistort streamed camera data  ","version":"Next","tagName":"h2"},{"title":"Run undistort_rgb_image​","type":1,"pageTitle":"Streaming Undistorted RGB Image Using Calibration","url":"/projectaria_tools/docs/ARK/sdk/samples/undistort_rgb_image#run-undistort_rgb_image","content":" Plug your Aria glasses into your computer, using the provided USB cableFrom the samples directory in Terminal, run:  python -m undistort_rgb_image --interface usb --update_iptables   info Use --interface wifi to stream over Wi-FI    ","version":"Next","tagName":"h2"},{"title":"Code walkthrough​","type":1,"pageTitle":"Streaming Undistorted RGB Image Using Calibration","url":"/projectaria_tools/docs/ARK/sdk/samples/undistort_rgb_image#code-walkthrough","content":" The code walkthrough for undistort_rgb_image.py is similar to device_stream.py, but has 2 key differences:  1. Access sensor calibration​  Once the sensors have been configured, the recording manager can provide the sensor calibration data for those settings.  sensors_calib_json = streaming_manager.sensors_calibration()   2. Use Project Aria Tools for calibration operations​  A Project Aria Tools type device calibration object can then be retrieved by using the device_calibration_from_json_string function:  from projectaria_tools.core.calibration import ( device_calibration_from_json_string, distort_by_calibration, get_linear_camera_calibration, ) sensors_calib = device_calibration_from_json_string(sensors_calib_json) rgb_calib = sensors_calib.get_camera_calib(&quot;camera-rgb&quot;)   Get a linear camera calibration object to be used in RGB image undistortion:  dst_calib = get_linear_camera_calibration(512, 512, 150, &quot;camera-rgb&quot;)   To find out more about how to use sensor calibration, go to the Accessing Sensor Calibration page.  3. Undistort and visualize the live RGB image stream​  Unlike device_stream.py that uses custom streaming client observer class, undistort_rgb_image.py uses a simple streaming client observer class to define callbacks:  class StreamingClientObserver: def __init__(self): self.rgb_image = None def on_image_received(self, image: np.array, record: ImageDataRecord): self.rgb_image = image   Undistort the RGB image using distort_by_calibration and visualize it in a while loop. The camera RGB image and the undistorted RGB image are visualized in separate windows using OpenCV. The images are processed and displayed the streaming stops or the application quit.  while not (quit_keypress() or ctrl_c): if observer.rgb_image is not None: rgb_image = cv2.cvtColor(observer.rgb_image, cv2.COLOR_BGR2RGB) cv2.imshow(rgb_window, np.rot90(rgb_image, -1)) # Apply the undistortion correction undistorted_rgb_image = distort_by_calibration( rgb_image, dst_calib, rgb_calib ) # Show the undistorted image cv2.imshow(undistorted_window, np.rot90(undistorted_rgb_image, -1)) observer.rgb_image = None   note Cameras on Aria glasses are installed sideways. The visualizer rotates the raw image data for a more natural view. ","version":"Next","tagName":"h3"},{"title":"Streaming and Visualizing All Live Sensor Data","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/sdk/samples/device_stream","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Streaming and Visualizing All Live Sensor Data","url":"/projectaria_tools/docs/ARK/sdk/samples/device_stream#overview","content":" This page shows how to run the code sample device_stream to:  Start and stop streaming data from Aria glasses over USB and Wi-FiVisualize the sensor streams from Aria's RGB, Mono Scene (SLAM), ET cameras and IMU sensors  note These code samples require Python 3.9+ because of a fastplotlib dependency  ","version":"Next","tagName":"h2"},{"title":"Run device_stream​","type":1,"pageTitle":"Streaming and Visualizing All Live Sensor Data","url":"/projectaria_tools/docs/ARK/sdk/samples/device_stream#run-device_stream","content":" ","version":"Next","tagName":"h2"},{"title":"Example 1: Start and stop streaming over USB​","type":1,"pageTitle":"Streaming and Visualizing All Live Sensor Data","url":"/projectaria_tools/docs/ARK/sdk/samples/device_stream#example-1-start-and-stop-streaming-over-usb","content":" Plug your Aria glasses into your computer, using the provided USB cableFrom the samples directory in Terminal, run:  python -m device_stream --interface usb --update_iptables   You should then see:    note Cameras on Aria glasses are installed sideways. The visualizer rotates the raw image data for a more natural view.  There are several ways you can stop streaming:  Press q or ESC to quit the appUse Ctrl-C to terminate in terminalPress the Capture button on your glasses  ","version":"Next","tagName":"h3"},{"title":"Example 2: Start and stop streaming over Wi-Fi​","type":1,"pageTitle":"Streaming and Visualizing All Live Sensor Data","url":"/projectaria_tools/docs/ARK/sdk/samples/device_stream#example-2-start-and-stop-streaming-over-wi-fi","content":" Streaming over Wi-Fi is similar, with a few extra steps.  Connect your Aria Glasses and Computer to the same Wi-Fi compatible Wi-Fi network: Check Requirements for details about compatible routers Open the Mobile Companion app and tap Wi-Fi on the Dashboard to see your glasses' IP addressFrom the samples directory in Terminal, run:  python -m device_stream --interface wifi --device-ip &lt;Glasses IP&gt; --update_iptables   Make sure you replace &lt;Glasses IP&gt; with the IP address you got from the Mobile Companion app  If you're on MacOS and lose internet connection, run aria-doctor in a separate terminal.  Stop streaming using any of the methods listed in Example 1  ","version":"Next","tagName":"h3"},{"title":"Code walkthrough​","type":1,"pageTitle":"Streaming and Visualizing All Live Sensor Data","url":"/projectaria_tools/docs/ARK/sdk/samples/device_stream#code-walkthrough","content":" 1. Get StreamingManager instance​  Use StreamingManager to start and stop streaming.  streaming_manager = device.streaming_manager   2. Configure Streaming Settings (optional)​  Specify the profile name for the streaming in StreamingConfig, otherwise you'll use your glasses' default recording profile. If a default recording profile has not been set, you'll stream using profile18. Go to the Sensor Profiles page for supported profiles and their specs.  streaming_config = StreamingConfig() streaming_config.profile_name = args.profile_name   Use ephemeral streaming certificates to protect your streamed data from being eavesdropped by other computer without certificate. This needs to be set to true when no persistent certificates were pre-installed. Go to the Streaming Internals page on the various aspects of streaming security and how certificates are used.  streaming_config.security_options.use_ephemeral_certs = True streaming_manager.streaming_config = streaming_config   3. Start Streaming​  Start streaming using all the previously set configurations.  streaming_manager.start_streaming()   4. Write callbacks for each sensor data stream​  Create an observer class derived from BaseStreamingClientObserver defined in visualizer.py. Find the complex example named AriaVisualizerStreamingClientObserver in visualizer.py, which is used in this sample app to fetch and visualize data from each sensor.  Find a simpler example in undistort_rgb_image.py to fetch RGB data from the stream:  class StreamingClientObserver: def __init__(self): self.rgb_image = None def on_image_received(self, image: np.array, record: ImageDataRecord): self.rgb_image = image   5. Register callbacks in the streaming client​  Once your observer class is defined, you will need to register it with a stream client object.  # get streaming client from the streaming manager streaming_client = streaming_manager.streaming_client # create custom visualizer and streaming client observer class instances aria_visualizer = AriaVisualizer() aria_visualizer_streaming_client_observer = AriaVisualizerStreamingClientObserver(aria_visualizer) # register callbacks streaming_client.set_streaming_client_observer(aria_visualizer_streaming_client_observer)   6. Visualize live stream​  Subscribe to the live stream data and enter the rendering loop to visualize the streaming data until the window is closed.  streaming_client.subscribe() aria_visualizer.render_loop()   7. Stop the stream &amp; free resources​  Once you've finished streaming, unsubscribe from StreamingClient instance, so that any held resources can be released, stop streaming and disconnect the device.  streaming_client.unsubscribe() streaming_manager.stop_streaming() device_client.disconnect(device)  ","version":"Next","tagName":"h3"},{"title":"TICSync Code Snippet","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/sdk/samples/ticsync_sample","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"TICSync Code Snippet","url":"/projectaria_tools/docs/ARK/sdk/samples/ticsync_sample#overview","content":" These code samples show how to use the Client SDK to create time synchronized recordings between multiple Project Aria glasses using TICSync.  ticsync_recording.py - create TICSync time synchronized recordings with the option to set: Which device is server, which are clientsWhat recording profile each device uses ticsync_cleanup.py - Stop recording (if necessary) and return glasses to their normal recording stateticsync_file_manager.py - list and download recordings made with shared session ids  This page goes through examples using each of the scripts, followed by a ticsync_recording.py code walkthrough.  ","version":"Next","tagName":"h2"},{"title":"Running the sample​","type":1,"pageTitle":"TICSync Code Snippet","url":"/projectaria_tools/docs/ARK/sdk/samples/ticsync_sample#running-the-sample","content":" ","version":"Next","tagName":"h2"},{"title":"Example 1: Create TICSync time synchronized recordings, download them and visualize the data​","type":1,"pageTitle":"TICSync Code Snippet","url":"/projectaria_tools/docs/ARK/sdk/samples/ticsync_sample#example-1-create-ticsync-time-synchronized-recordings-download-them-and-visualize-the-data","content":" For an end to end guide from how to create TICSync recordings to how to consume the data, go to Time Synchronized Recordings with Multiple Aria Glasses. This guide uses ticsync_recording.py in its simplest form, where the user does not need to set the server/client details or individual recording profiles.  ","version":"Next","tagName":"h3"},{"title":"Example 2: Create TICSync time synchronized recordings with granular control over details.​","type":1,"pageTitle":"TICSync Code Snippet","url":"/projectaria_tools/docs/ARK/sdk/samples/ticsync_sample#example-2-create-ticsync-time-synchronized-recordings-with-granular-control-over-details","content":" For TICSync to work, all devices need to be on the one Wi-Fi network and a leader device needs to create a synchronized time reference. By default, ticsync.py will select one device to be the Server device (Wi-Fi hotspot and leader device) and all the other devices will be client devices. You can, however, manually configure which device is the server or alter the script to use different Wi-Fi hotspots or leader devices.  Make the recording​  Plug all glasses into your computerGo to the TICSync sample code directory  cd ~/projectaria_client_sdk_samples/ticsync   Get the serial number for all connected devices by running aria device --listUse the Client SDK to run ticsync.py to: Set one pair of glasses to be the server (Wi-Fi hotspot and using Fast DDS to be the leader device)Set the other glasses to be client devices connecting to the serverSet what recording profiles will be usedStart recording with TICSync protocols  In this example, three devices will be making a time synchronized recording and each uses a different recording profile.  python -m ticsync_recording --server &lt;server_serial&gt; profile4 --client &lt;client1_serial&gt; profile16 --client &lt;client2_serial&gt; profile24   Some recording profiles won't work Recording profiles 0, 2, 10, 19, 25 and 27 have wifiScanModeActive enabled, so they can't be used to create TICSync recordings.  In the command line you’ll see: Server device has started a Wi-Fi hotspot and Client devices have joined the hotspotAll devices have started recording You should also have visual confirmation from the recording LED on each of the glasses Script prompting that it is waiting for devices to be ready for time synchronized data collection.Script prompting that devices are ready for time synchronized data collection. Unplug the glasses (if you wish) and record your activity It may take ~40 seconds for the ticsync algorithm to reach stabilization between all devices. The TICSync Jupyter notebook tutorial includes how to assess how long it takes for your specific setup and recording profile. Stop recording: Press the Capture button on the top right of the Aria glasses and then return glasses to their normal state by plugging them in and using ‘ticsync_cleanup’Plug the glasses back into your computer and use ticsync_cleanup  Run cleanup based on number of devices plugged in:  python -m ticsync_cleanup --total_num_devices 3   Run clean up by specifying devices:  python -m ticsync_cleanup --server &lt;server_serial&gt; --clients &lt;client_serial1&gt; &lt;client_serial2&gt;   Example output:  -------- Plug in all devices to your computer again for TicSync cleanup -------- -------- Then press Enter to start TicSync cleanup -------- [('1WM281623D3490', 'Aria'), ('1WM391623D5689', 'Aria'), ('1WM999999D0000', 'Aria')] [AriaSdk:DeviceControllerImpl][INFO]: Connecting to device 1WM281623D3490 over USB [AriaSdk:DeviceControllerImpl][INFO]: Connecting to device 1WM391623D5689 over USB [AriaSdk:DeviceControllerImpl][INFO]: Connecting to device 1WM999999D0000 over USB Detected server serial 1WM281623D3490 Detected client serials ['1WM391623D5689', '1WM999999D0000'] [AriaSdk:DeviceControllerImpl][INFO]: Connecting to device 1WM391623D5689 over USB [AriaSdk:DeviceControllerImpl][INFO]: Connecting to device 1WM999999D0000 over USB [AriaSdk:DeviceControllerImpl][INFO]: Connecting to device 1WM281623D3490 over USB -------- All devices reconnected, please keep all devices plugged in. Performing cleanup -------- DDS RPC enabled, disabling it -------- Successfully performed cleanup. Exiting --------   ","version":"Next","tagName":"h3"},{"title":"Example 3: Examine and download recordings​","type":1,"pageTitle":"TICSync Code Snippet","url":"/projectaria_tools/docs/ARK/sdk/samples/ticsync_sample#example-3-examine-and-download-recordings","content":" ticsync_file_manager enables you to view recordings on your Aria glasses that contain TICSync data as well as download the server and client files at the same time.  Plug all Aria glasses into your computer.List VRS recordings grouped by shared session ID. The list is in descending order by date.  python -m ticsync_file_manager --list   Depending on how many client devices were used during recordings, there will be 1 or more client devices information listed per server device:  2024-05-23 21:17:19 Shared Session ID: 7dfe4d16-6f54-47bf-859e-6ee4043234d3 Server Serial: 1WM391623D5689 Server Recording UUID: d8b717ce-ef48-40cd-bcc8-56cd9fe14319 Client Serial: 1WM999999D0000 Client Recording UUID: 63334502-6125-498e-8f2e-86567c4cfc0e Client Serial: 1WM222222S0000 Client Recording UUID: c0fae751-4ad5-4e95-a21d-f9060edc1112 2024-05-23 21:14:58 Shared Session ID: 4b5e2587-de1a-42f9-a17c-60bd96a8658c Server Serial: 1WM391623D5689 Server Recording UUID: 1204bcb6-71ea-42b9-bfbb-62e11f5c620a Client Serial: 1WM999999D0000 Client Recording UUID: 012e5c7f-c904-4d6f-822e-0b26aedb7684 ...   Using the shared session ID, you can download all the time synchronized recordings for that session, rather than needing to pull individual records. Making a copy of the all the details associated with a shared session ID will be helpful when consuming the data  Download the recordings​  Get the Shared Session ID for the recordings you want to download from ticsync_file_manager --list. Download recordings for that Shared Session ID to a specified output directory:  python -m ticsync_file_manager --download 7dfe4d16-6f54-47bf-859e-6ee4043234d3 --output_dir ticsync_vrs   In this example, the server and client recordings with the shared session ID 7dfe4d16-6f54-47bf-859e-6ee4043234d3 will be downloaded to a newly created ticsync_vrs folder.  ","version":"Next","tagName":"h3"},{"title":"ticsync_recording.py code walkthrough​","type":1,"pageTitle":"TICSync Code Snippet","url":"/projectaria_tools/docs/ARK/sdk/samples/ticsync_sample#ticsync_recordingpy-code-walkthrough","content":" ","version":"Next","tagName":"h2"},{"title":"1. Retrieve serial number and profile names for server and client devices​","type":1,"pageTitle":"TICSync Code Snippet","url":"/projectaria_tools/docs/ARK/sdk/samples/ticsync_sample#1-retrieve-serial-number-and-profile-names-for-server-and-client-devices","content":" A helper function get_device_serial_and_profile_names enables the specified script options to be parsed. If the total number of devices were specified, then it tries to detect the requested number of devices and returns the list. If the device serials and profile names were specified explicitly instead, it simply returns those values.  [server_serial_and_profile_name, client_serial_and_profile_names] = ( get_device_serial_and_profile_names(args, device_client) )   ","version":"Next","tagName":"h3"},{"title":"2. Set up the server device​","type":1,"pageTitle":"TICSync Code Snippet","url":"/projectaria_tools/docs/ARK/sdk/samples/ticsync_sample#2-set-up-the-server-device","content":" Use a DeviceClient instance that was configured by a DeviceClientConfig with the server device serial. Connect to the server device.  server_serial = server_serial_and_profile_name[0] server_profile_name = server_serial_and_profile_name[1] # Set up the server device device_client_config.device_serial = server_serial device_client.set_client_config(device_client_config) # Connect to server device server_device = device_client.connect()   ","version":"Next","tagName":"h3"},{"title":"3. Set up hotspot on the server device​","type":1,"pageTitle":"TICSync Code Snippet","url":"/projectaria_tools/docs/ARK/sdk/samples/ticsync_sample#3-set-up-hotspot-on-the-server-device","content":" Retrieve the WifiManager instance from the server device’s Device instance and set up a hotspot on the server device.  # Retrieve wifi_manager of the server device server_wifi_manager = server_device.wifi_manager # Switch the server device to hotspot mode with a random password server_wifi_manager.set_device_hotspot_status( True, WIFI_HOTSPOT_5GHZ, args.hotspot_country_code )   ","version":"Next","tagName":"h3"},{"title":"4. Set up the DDS RPC on the server device​","type":1,"pageTitle":"TICSync Code Snippet","url":"/projectaria_tools/docs/ARK/sdk/samples/ticsync_sample#4-set-up-the-dds-rpc-on-the-server-device","content":" Use the server device’s Device instance to enable RPC over DDS. If already enabled, retrieve a new DDS RPC session ID.  if server_device.dds_rpc_enabled_status.state == aria.DdsRpcState.Off: print(&quot;DDS RPC is not enabled, enabling it&quot;) server_device.set_dds_rpc_enabled(True, aria.StreamingInterface.WifiSoftAp) else: # Retrieve a new DDS RPC session ID session_id = server_device.dds_rpc_new_session_id() print(&quot;Retrieved a new DDS RPC session ID&quot;, session_id) # Retrieve the server device hotspot status. Will be used to connect the client devices to # the server device hotspot server_wifi_hotspot_status = server_wifi_manager.device_hotspot_status   ","version":"Next","tagName":"h3"},{"title":"5. Retrieve the recording manager of the server device​","type":1,"pageTitle":"TICSync Code Snippet","url":"/projectaria_tools/docs/ARK/sdk/samples/ticsync_sample#5-retrieve-the-recording-manager-of-the-server-device","content":" Configure server device’s RecordingManager by using a RecordingConfig instance with the specified profile and the time sync mode TicSyncServer.  # Retrieve recording_manager of the server device server_recording_manager = server_device.recording_manager # Set time sync mode to TicSyncServer using custom recording config recording_config = aria.RecordingConfig() recording_config.profile_name = server_profile_name recording_config.time_sync_mode = aria.TimeSyncMode.TicSyncServer server_recording_manager.recording_config = recording_config   ","version":"Next","tagName":"h3"},{"title":"6. Set up the client devices​","type":1,"pageTitle":"TICSync Code Snippet","url":"/projectaria_tools/docs/ARK/sdk/samples/ticsync_sample#6-set-up-the-client-devices","content":" Reuse the existing DeviceClient instance by resetting the DeviceClientConfig with each client device serial. Connect to the client device.  client_devices = {} client_recording_managers = {} # Set up the client devices for [serial, profile_name] in client_serial_and_profile_names: # Reuse the existing DeviceClient instance by setting a new client config device_client_config.device_serial = serial device_client.set_client_config(device_client_config) # Connect to client device device = device_client.connect()   ","version":"Next","tagName":"h3"},{"title":"7. Connect client devices to the server device hotspot**​","type":1,"pageTitle":"TICSync Code Snippet","url":"/projectaria_tools/docs/ARK/sdk/samples/ticsync_sample#7-connect-client-devices-to-the-server-device-hotspot","content":" Retrieve the WifiManager instance from the client device’s Device instance and connect to the hotspot that was set up on the server device in step 4.  wifi_manager = device.wifi_manager # Connect client device to server device hotspot wifi_status = wifi_manager.wifi_status # Check if the client device is already connected to the server device hotspot if ( wifi_status.enabled is False or wifi_status.network.ssid != server_wifi_hotspot_status.ssid ): # If not, connect client device to server device hotspot wifi_manager.connect_wifi( server_wifi_hotspot_status.ssid, server_wifi_hotspot_status.passphrase, aria.WifiAuthentication.Wpa, False, # hidden &quot;&quot;, # username True, # disable_other_network True, # skip_internet_check ) # Set keep Wi-Fi on as true for the client devices # This keeps the client devices connected to the server Wi-Fi hotspot even when they are disconnected from USB wifi_manager.keep_wifi_on(True)   ","version":"Next","tagName":"h3"},{"title":"8. Start recording on the server device*​","type":1,"pageTitle":"TICSync Code Snippet","url":"/projectaria_tools/docs/ARK/sdk/samples/ticsync_sample#8-start-recording-on-the-server-device","content":" # Start recording on the server device print(f&quot;Starting to record the server device {server_serial} using {profile_name}&quot;) server_recording_manager.start_recording() # Get recording state of the server device server_recording_state = server_recording_manager.recording_state print(f&quot;Recording state of server device {server_serial}: {server_recording_state}&quot;)   ","version":"Next","tagName":"h3"},{"title":"9. Start recording on the client devices​","type":1,"pageTitle":"TICSync Code Snippet","url":"/projectaria_tools/docs/ARK/sdk/samples/ticsync_sample#9-start-recording-on-the-client-devices","content":" # Start recording on the client devices for manager in client_recording_managers.values(): manager.start_recording()   ","version":"Next","tagName":"h3"},{"title":"10. Wait for devices to be ready for data collection​","type":1,"pageTitle":"TICSync Code Snippet","url":"/projectaria_tools/docs/ARK/sdk/samples/ticsync_sample#10-wait-for-devices-to-be-ready-for-data-collection","content":" Use RecordingManager’s TicSync status field to query whether the client devices are ready for data collection.  # Wait for for ticsync convergence def _is_stable(recording_manager): status = recording_manager.tic_sync_status return status.synchronization_stability == aria.SynchronizationStability.Stable print( &quot;-------- Waiting for devices to be ready for time synchronized data collection, this will take around 45 seconds. --------\\n&quot; &quot;-------- Please keep all devices plugged in. ---------&quot; ) while not all( _is_stable(manager) for manager in client_recording_managers.values() ): time.sleep(5) print( &quot;-------- All devices are ready for data collection. You can safely unplug all your glasses from USB ---------&quot; )   Once you’ve finished recording, make sure you run ticsync_cleanup.py with all the glasses plugged in to return them to their normal state. Go to “Time Synchronized Recordings with Multiple Aria Glasses” for more information about downloading and consuming the data. ","version":"Next","tagName":"h3"},{"title":"Project Aria Client SDK and CLI Setup Guide","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/sdk/setup","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Project Aria Client SDK and CLI Setup Guide","url":"/projectaria_tools/docs/ARK/sdk/setup#overview","content":" The page provides instructions about how to get started with the Project Aria Client SDK, covering:  Hardware and software requirementsDownloading and installing the SDK Installing projectaria_client_sdk via pip will also add the Aria CLI to your PATH Running Project Aria Doctor to setup your computer and fix common issuesPairing your Aria GlassesExtracting and exploring the sample apps    ","version":"Next","tagName":"h2"},{"title":"Requirements​","type":1,"pageTitle":"Project Aria Client SDK and CLI Setup Guide","url":"/projectaria_tools/docs/ARK/sdk/setup#requirements","content":" ","version":"Next","tagName":"h2"},{"title":"Hardware​","type":1,"pageTitle":"Project Aria Client SDK and CLI Setup Guide","url":"/projectaria_tools/docs/ARK/sdk/setup#hardware","content":" Project Aria glasses that have: Completed full device setup using the Aria Mobile Companion AppLatest up-to-date OS If you want to stream over Wi-Fi, you'll need a router, such as Asus, Netgear or TP-Link, that has: No firewallSupports Wi-Fi 6 So that the glasses can connect to the 5GHz band when streaming over Wi-Fi  danger The Client SDK does not currently support streaming over corporate, university or public networks. Those networks are protected by many layers of security and firewalls. We recommend using one of the recommended routers listed above to stream over Wi-Fi.  ","version":"Next","tagName":"h3"},{"title":"Platforms​","type":1,"pageTitle":"Project Aria Client SDK and CLI Setup Guide","url":"/projectaria_tools/docs/ARK/sdk/setup#platforms","content":" The codebase is supported on the following platforms:  x64 Linux distributions of: Fedora 36 or newerUbuntu jammy (22.04) or newer Mac Intel or Mac ARM-based (M1) with MacOS 11 (Big Sur) or newer  ","version":"Next","tagName":"h3"},{"title":"Software​","type":1,"pageTitle":"Project Aria Client SDK and CLI Setup Guide","url":"/projectaria_tools/docs/ARK/sdk/setup#software","content":" A version of Python within the range of 3.9 to 3.12 (inclusive) Python 3 download pageTo check which version of Python 3 you have, use python3 --version ADB (optional) In addition to the CLI, you can use ADB to interact with Aria glassesADB is one of the ways that you can download Aria data    ","version":"Next","tagName":"h3"},{"title":"Environment Setup​","type":1,"pageTitle":"Project Aria Client SDK and CLI Setup Guide","url":"/projectaria_tools/docs/ARK/sdk/setup#environment-setup","content":" ","version":"Next","tagName":"h2"},{"title":"Step 1: Install SDK from PyPI​","type":1,"pageTitle":"Project Aria Client SDK and CLI Setup Guide","url":"/projectaria_tools/docs/ARK/sdk/setup#step-1-install-sdk-from-pypi","content":" ","version":"Next","tagName":"h2"},{"title":"Create a virtual environment​","type":1,"pageTitle":"Project Aria Client SDK and CLI Setup Guide","url":"/projectaria_tools/docs/ARK/sdk/setup#create-a-virtual-environment","content":" When using pip, it is best practice to use a virtual environment. This will keep all the modules under one folder and will not break your local environment. Use the following command with your version of Python3.  python3 -m venv ~/venv   ","version":"Next","tagName":"h3"},{"title":"Install the Client SDK and CLI​","type":1,"pageTitle":"Project Aria Client SDK and CLI Setup Guide","url":"/projectaria_tools/docs/ARK/sdk/setup#install-the-client-sdk-and-cli","content":" Install projectaria_client_sdk with pip  source ~/venv/bin/activate python3 -m pip install projectaria_client_sdk --no-cache-dir   Note: If you are using Linux, install version 1.1.0 instead of the latest version:  python3 -m pip install projectaria_client_sdk==1.1.0 --no-cache-dir   ","version":"Next","tagName":"h3"},{"title":"Step 2: Run Project Aria Doctor utility​","type":1,"pageTitle":"Project Aria Client SDK and CLI Setup Guide","url":"/projectaria_tools/docs/ARK/sdk/setup#step-2-run-project-aria-doctor-utility","content":" The Project Aria Doctor utility can help detect and resolve common issues connecting and streaming from the glasses.  Run the utility and follow the prompts to resolve any issues.  aria-doctor   info If you're on MacOS and lose internet connection while streaming, run aria-doctor again.    ","version":"Next","tagName":"h2"},{"title":"Step 3: Pair Aria Glasses with your computer​","type":1,"pageTitle":"Project Aria Client SDK and CLI Setup Guide","url":"/projectaria_tools/docs/ARK/sdk/setup#step-3-pair-aria-glasses-with-your-computer","content":" Pairing your Aria glasses to your computer allows the Client SDK and CLI to control the glasses. A pair of Aria glasses can be paired to multiple computers.  Turn on your Aria glasses and connect it to your computer using the provided USB cableOpen the Mobile Companion app on your phoneOn your computer, run:  aria auth pair   A prompt should then appear in the Mobile app, tap Approve to pair your glasses The hash in the terminal and the app should be the sameView (or revoke) certificates by going to Device SettingsThe Client SDK Certificate will remain valid until you manually revoke it or factory reset your glasses    info At this point, you can now use the Aria CLI to interact with you Aria glasses.    ","version":"Next","tagName":"h2"},{"title":"Step 4: Extract and explore the sample apps​","type":1,"pageTitle":"Project Aria Client SDK and CLI Setup Guide","url":"/projectaria_tools/docs/ARK/sdk/setup#step-4-extract-and-explore-the-sample-apps","content":" Extract the Client SDK code samples (here to your home directory)  python -m aria.extract_sdk_samples --output ~   Navigate to the sample folder  cd ~/projectaria_client_sdk_samples   Install necessary dependencies:  python3 -m pip install -r requirements.txt   All code samples can be run from cd ~/projectaria_client_sdk_samples, with the exception of the Time Synchronization TICSync code samples. To use TICSync, go to cd ~/projectaria_client_sdk_samples/ticsync.  Go to Code Samples to explore Aria Client SDK features.  Go to Streaming Internals to understand how streaming works and how to configure your own streaming setup.  Go to Time Synchronization for how to create recordings with multiple Project Aria glasses.  If you encounter any issues please run aria-doctor in a separate terminal or check out troubleshooting.  info You can check your Aria glasses' recording or streaming status in the Mobile Companion app.  danger The Client SDK does not currently support streaming over corporate, university or public networks. Those networks are protected by many layers of security and firewalls. We recommend using one of the recommended routers listed above to stream over Wi-Fi.  ","version":"Next","tagName":"h2"},{"title":"Useful Links​","type":1,"pageTitle":"Project Aria Client SDK and CLI Setup Guide","url":"/projectaria_tools/docs/ARK/sdk/setup#useful-links","content":" SDK API Reference - full list of APIsCLI Command ReferenceSDK &amp; CLI Troubleshooting ","version":"Next","tagName":"h2"},{"title":"Client SDK & CLI Troubleshooting & Known Issues","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/sdk/sdk_troubleshooting","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Client SDK & CLI Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/sdk/sdk_troubleshooting#overview","content":" This page provides troubleshooting information for using Project Aria Client SDK or CLI. If you cannot find a solution for your problem on this page, go to the Support page for how to contact our team.  ","version":"Next","tagName":"h2"},{"title":"Aria Doctor​","type":1,"pageTitle":"Client SDK & CLI Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/sdk/sdk_troubleshooting#aria-doctor","content":" The Project Aria Doctor utility can help detect and resolve common issues connecting and streaming from the glasses.  Run the utility using the following command and follow the prompts to resolve any issues.  aria-doctor   ","version":"Next","tagName":"h2"},{"title":"Connection and pairing issues​","type":1,"pageTitle":"Client SDK & CLI Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/sdk/sdk_troubleshooting#connection-and-pairing-issues","content":" ","version":"Next","tagName":"h2"},{"title":"Computer can't find Aria Glasses​","type":1,"pageTitle":"Client SDK & CLI Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/sdk/sdk_troubleshooting#computer-cant-find-aria-glasses","content":" It may be that the battery is drained, make sure your Aria Glasses are correctly charging (there should be a blue LED on the right arm) and wait ten minutes.  On Linux, this may be due to USB driver issues. Run adb kill-server &amp;&amp; adb start-server and aria-doctor, then try connecting to Aria glasses again.  ","version":"Next","tagName":"h3"},{"title":"Mobile app doesn't receive authorization​","type":1,"pageTitle":"Client SDK & CLI Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/sdk/sdk_troubleshooting#mobile-app-doesnt-receive-authorization","content":" If you use aria auth pair and don't receive an authorization prompt in the Mobile Companion app, try the following steps.  Make sure the Mobile Companion app is in the foreground and try againRestart the Mobile Companion app and try againRevoke any existing certificates (via Device Settings) and try againMake sure you're using venv as your virtual environment Users have experienced difficulties using the Client SDK in other virtual environments, such as Conda  If you've paired multiple Aria glasses to the one account​  If you've paired multiple Aria glasses to the one account, the wrong glasses may be connected to the app.  Tap Select other on the top right of the dashboard If you see Add glasses instead, only one set of Aria glasses is connected to this account Tap the glasses connected to your computer  If you're not sure which glasses you're using:  Go to Device Info in the Mobile Companion app to find out the serial number of the glasses that are currently connected to the Mobile Companion appTo find out the serial number of the glasses connected to your computer The serial number is printed on the right arm of the glasses, near the privacy switch (go to the Glasses Manual for screenshots of where it is)In Terminal, use adb devices (ADB is part of Android Studio) The device number returned is the serial number  ","version":"Next","tagName":"h3"},{"title":"Streaming or Recording Issues​","type":1,"pageTitle":"Client SDK & CLI Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/sdk/sdk_troubleshooting#streaming-or-recording-issues","content":" ","version":"Next","tagName":"h2"},{"title":"Lost Internet Connection (MacOS)​","type":1,"pageTitle":"Client SDK & CLI Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/sdk/sdk_troubleshooting#lost-internet-connection-macos","content":" If you lose internet connection on MacOS while streaming, run aria-doctor in a separate terminal.  ","version":"Next","tagName":"h3"},{"title":"Can't start streaming/recording. RuntimeError: (9) Failed to read data from socket: Operation canceled​","type":1,"pageTitle":"Client SDK & CLI Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/sdk/sdk_troubleshooting#cant-start-streamingrecording-runtimeerror-9-failed-to-read-data-from-socket-operation-canceled","content":" You may encounter this error message if you:  Tried to start streaming/recording and the Privacy Switch was engagedTurned off the Privacy SwitchImmediately tried to start recording again  The Aria glasses were still switching modes. Please try again.  ","version":"Next","tagName":"h3"},{"title":"Streaming is laggy/ only some streaming visualizations appear/ visualizer is blank​","type":1,"pageTitle":"Client SDK & CLI Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/sdk/sdk_troubleshooting#streaming-is-laggy-only-some-streaming-visualizations-appear-visualizer-is-blank","content":" This issue can occur for several reasons.  Corporate or VPN interference (even if streaming via ADB)​  This issue may occur if you're on a VPN or corporate network. Even if you're only streaming via ADB, some security protocols may interfere.  Aria Glasses and Computer on different Wi-Fi networks​  info This can occur if you know you've set both devices to the same network!  Devices will sometimes preferentially switch back to Wi-Fi connections with stronger signal strength, so you may need to forget a corporate network on the Aria Glasses or computer.  How to adjust Wi-Fi settings via the mobile app​  Open the Aria Mobile Companion AppIn the Paired Glasses section of the Dashboard, select Select Wi-FiSelect your preferred network and follow the prompts to connect You can also forget an existing network from the Wi-Fi menuMake sure it is a non-corporate network that is the same as your computerThe glasses Wi-Fi network is independent of your phone's Wi-Fi network  Resources are tied up​  You may have a previous streaming session running. Follow the device_stream.py or streaming_subscribe.py instructions to stop the stream and free resources.  Linux connection issue​  If streaming on Linux does not show any data, try the following steps to address connection issues:  Make sure that you have run aria-doctorMake sure that the UDP ports used for streaming are not blocked by your machine firewall. You can add an iptable entry to open these ports with the following commands  sudo iptables -A INPUT -p udp -m udp --dport 7000:8000 -j ACCEPT   Ensure that during streaming, USB Ethernet tab in the &quot;Network Settings Window&quot; has Aria selected  ","version":"Next","tagName":"h3"},{"title":"Aria diagnostics​","type":1,"pageTitle":"Client SDK & CLI Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/sdk/sdk_troubleshooting#aria-diagnostics","content":" Running aria-diagnostics in your Python environment will create a diagnostic report file called diagnostics.zip in the same folder you're currently working in. You can send this collection of logs to the most relevant location listed under How do I get support/report issues?, as it's very useful for debugging. Running this command captures the following data:  Hardware informationOS informationUSB informationNetwork informationThe Aria home folder structureThe ADB connected devices listedDevice logs for all connected Aria devicesDevice status and info for all connected Aria devicesAfter attempting streaming with connected Aria devices for 20 seconds, it captures logs for both successful and failed attempts.  NOTE: The exception handling is intentionally broad, as the goal is to avoid crashing in any scenario and capture a subset of essential data even in failure modalities.  ","version":"Next","tagName":"h2"},{"title":"Other Useful Links​","type":1,"pageTitle":"Client SDK & CLI Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/sdk/sdk_troubleshooting#other-useful-links","content":" Aria Glasses User Manual for general information about using your glasses.Aria Research Kit (ARK) Troubleshooting and Known Issues page for general Aria Glasses troubleshooting information. ","version":"Next","tagName":"h2"},{"title":"Time Synchronized Recordings with Multiple Aria Glasses","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/sdk/ticsync","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Time Synchronized Recordings with Multiple Aria Glasses","url":"/projectaria_tools/docs/ARK/sdk/ticsync#overview","content":" Users can capture time-synchronized data between multiple Project Aria glasses on the one Wi-Fi network via TICSync. Using the Aria hotspot feature, one pair of glasses (server) acts as a Wi-Fi access point that forms a network between all glasses. Time requests from all other glasses go to the server/leader device, creating a synchronized time reference.  The accuracy of this method has been tested to be better than 1ms on average after approximately 45 seconds warm-up.  This guide covers how to:  Create TICSync recordingsDownload TICSync recordingsLoad and visualize TICSync recordings  For more details, go to:  TICSync Code Snippet Advanced features and code walkthrough TICSync Technical Details  note Multi-device time synchronization has been verified to work for up to 3 Aria devices at once. You can use more than 3 devices, but it's possible device issues could occur.  ","version":"Next","tagName":"h2"},{"title":"Create TICSync recordings​","type":1,"pageTitle":"Time Synchronized Recordings with Multiple Aria Glasses","url":"/projectaria_tools/docs/ARK/sdk/ticsync#create-ticsync-recordings","content":" ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Time Synchronized Recordings with Multiple Aria Glasses","url":"/projectaria_tools/docs/ARK/sdk/ticsync#prerequisites","content":" All Aria Glasses need to be set up and paired with the Mobile Companion app prior to use This will ensure your glasses have the latest OS and configures them to automatically update when connected to power and a Wi-Fi connectionThe glasses do not need to be paired to the same user accountOn the Mobile Companion app dashboard, tap **Add **or Switch to set up additional glasses Go to How to Pair Additional Glasses and Pairing Troubleshooting for more details Install the Client SDK and pair each set of glasses with your computer (Steps 1-3 in the SDK Setup Guide) All the glasses must be paired to the one computerTap Switch to toggle between different glasses In the Mobile Companion app, you can only see the status for the glasses currently connected to the app To check if your glasses have the certificates necessary to connect Go to Device Settings (tap the glasses info card on the Dashboard)Scroll down to Glasses OS, if you see Revoke Client SDK Certificates, your glasses are paired to a computer  ","version":"Next","tagName":"h3"},{"title":"Make the recording​","type":1,"pageTitle":"Time Synchronized Recordings with Multiple Aria Glasses","url":"/projectaria_tools/docs/ARK/sdk/ticsync#make-the-recording","content":" Plug all glasses into your computerGo to the TICSync sample code directory  cd ~/projectaria_client_sdk_samples/ticsync   Use ticsync_recording.py to start recording  In this example all the glasses will start recording with recording profile 4  python -m ticsync_recording --total_num_devices 3 --profile profile4   The instructions above automatically chose the device with the lowest serial number to be the server and for all the glasses use the same recording profile. For granular control of how the glasses make recordings go to TICSync Code Snippet.  Some recording profiles won't work Recording profiles 0, 2, 10, 19, 25 and 27 have wifiScanModeActive enabled, so they can't be used to create TICSync recordings.  In the command line you should then see: Confirmation that all glasses have started recording You’ll also see the recording LED to start on each of the glasses Confirmation the glasses are ready for time synchronized data collection   &quot;-------- All devices are ready for data collection. You can safely unplug all your glasses from USB ---------&quot;   Unplug the glasses (if you wish) and record your activity It may take ~45 seconds for the ticsync algorithm to reach stabilization between all devices. The ticsync_tutorial below includes how to assess how long it takes for your specific setup and recording profile. Stop recording: Press the Capture button on the top right of the Aria glassesPlug the glasses back into your computer and use ticsync_cleanup  Once you’ve completed the recordings:  Plug all your glasses into the computerUse ticsync_cleanup.py to return the glasses to their normal recording state (no longer creating TICSync recordings). In this example, three devices are plugged in. Run:  python -m ticsync_cleanup --total_num_devices 3   Press Enter to initiate cleanup  Example output:  -------- Plug in all devices to your computer again for TicSync cleanup -------- -------- Then press Enter to start TicSync cleanup -------- [('1WM999999D0000', 'Aria'), ('1WM281623D3490', 'Aria'), ('1WM391623D5689', 'Aria')] [AriaSdk:DeviceControllerImpl][INFO]: Connecting to device 1WM999999D0000 over USB [AriaSdk:DeviceControllerImpl][INFO]: Connecting to device 1WM281623D3490 over USB [AriaSdk:DeviceControllerImpl][INFO]: Connecting to device 1WM391623D5689 over USB Detected server serial 1WM999999D0000 Detected client serials ['1WM281623D3490', '1WM391623D5689'] [AriaSdk:DeviceControllerImpl][INFO]: Connecting to device 1WM281623D3490 over USB [AriaSdk:DeviceControllerImpl][INFO]: Connecting to device 1WM391623D5689 over USB [AriaSdk:DeviceControllerImpl][INFO]: Connecting to device 1WM999999D0000 over USB -------- All devices reconnected, please keep all devices plugged in. Performing cleanup -------- DDS RPC enabled, disabling it -------- Successfully performed cleanup. Exiting --------   ","version":"Next","tagName":"h3"},{"title":"Download the recordings​","type":1,"pageTitle":"Time Synchronized Recordings with Multiple Aria Glasses","url":"/projectaria_tools/docs/ARK/sdk/ticsync#download-the-recordings","content":" ticsync_file_manager enables you to view recordings on your Aria glasses that contain TICSync data as well as download the server and client files at the same time.  Plug all glasses into your computerView what files with TICSync data are available  python -m ticsync_file_manager --list   The Shared Session ID outputs will be listed for each recording session, with a list of devices that were in each session.  Example output:  2024-05-23 21:17:19 Shared Session ID: 7dfe4d16-6f54-47bf-859e-6ee4043234d3 Server Serial: 1WM281623D3490 Server Recording UUID: d8b717ce-ef48-40cd-bcc8-56cd9fe14319 Client Serial: 1WM391623D5689 Client Recording UUID: 63334502-6125-498e-8f2e-86567c4cfc0e Client Serial: 1WM222222S0000 Client Recording UUID: c0fae751-4ad5-4e95-a21d-f9060edc1112 2024-05-23 21:14:58 Shared Session ID: 4b5e2587-de1a-42f9-a17c-60bd96a8658c Server Serial: 1WM281623D3490 Server Recording UUID: 1204bcb6-71ea-42b9-bfbb-62e11f5c620a Client Serial: 1WM391623D5689 Client Recording UUID: 012e5c7f-c904-4d6f-822e-0b26aedb7684   Making a copy of the details of each recording associated with a Shared Session ID will be helpful when consuming the data using the notebook below.  Download VRS files and supporting metadata  Use a Shared Session ID to copy all files from a recording session to a specific folder. In this example all the recordings from session 7dfe4d16-6f54-47bf-859e-6ee4043234d3 will saved to the new folder recording_session_2  python -m ticsync_file_manager --download 7dfe4d16-6f54-47bf-859e-6ee4043234d3 --output_dir recording_session_2   The folder will contain a VRS for each recording with all sensor data, including timestamps from the shared reference clock.    ","version":"Next","tagName":"h2"},{"title":"Load and visualize the recordings​","type":1,"pageTitle":"Time Synchronized Recordings with Multiple Aria Glasses","url":"/projectaria_tools/docs/ARK/sdk/ticsync#load-and-visualize-the-recordings","content":" The ticsync_tutorial.ipynb Jupyter notebook will take you through how to:  Load synchronized sensor data programmaticallyVisualize the dataAssess the accuracy of the time synchronization  ","version":"Next","tagName":"h2"},{"title":"Run Jupyter Notebook on Google Colab​","type":1,"pageTitle":"Time Synchronized Recordings with Multiple Aria Glasses","url":"/projectaria_tools/docs/ARK/sdk/ticsync#run-jupyter-notebook-on-google-colab","content":" Use the following link to run the Python notebook in an installation free playground:  Project Aria TICSync Tutorial  ","version":"Next","tagName":"h3"},{"title":"Build and run the Jupyter Notebook locally​","type":1,"pageTitle":"Time Synchronized Recordings with Multiple Aria Glasses","url":"/projectaria_tools/docs/ARK/sdk/ticsync#build-and-run-the-jupyter-notebook-locally","content":" Download and install the Python version of Project Aria ToolsRun the notebook  cd $HOME/Documents/projectaria_sandbox jupyter notebook projectaria_tools/core/examples/ticsync_tutorial.ipynb   ","version":"Next","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"Time Synchronized Recordings with Multiple Aria Glasses","url":"/projectaria_tools/docs/ARK/sdk/ticsync#troubleshooting","content":" ","version":"Next","tagName":"h2"},{"title":"General​","type":1,"pageTitle":"Time Synchronized Recordings with Multiple Aria Glasses","url":"/projectaria_tools/docs/ARK/sdk/ticsync#general","content":" Go to Client SDK and CLI Troubleshooting if you encounter issues creating the recordings or setting up the SDKGo to Project Aria Tools Troubleshooting if you encounter issues creating the Jupyter notebook  ","version":"Next","tagName":"h3"},{"title":"Recordings keep failing​","type":1,"pageTitle":"Time Synchronized Recordings with Multiple Aria Glasses","url":"/projectaria_tools/docs/ARK/sdk/ticsync#recordings-keep-failing","content":" If you try to initiate recording and it fails, it may be because the glasses are in a bad state. Try rebooting the glasses.  If they are connected to a charge cable, unplug your glassesHold down the power button down for about 10 seconds, until the nearby LED flashes green once and returns to solid blue ","version":"Next","tagName":"h3"},{"title":"ARK Troubleshooting & Known Issues","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/troubleshooting/troubleshooting_issues","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"ARK Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/troubleshooting/troubleshooting_issues#overview","content":" This page provides troubleshooting information for issues you may encounter while using the Aria Research Kit (ARK).  It covers:  Device and Recording Issues  Further resources​  Glasses Manual Information about LED States, button configuration, how to factory reset and power cycle your device. MPS Troubleshooting and Error Codes For MPS CLI. Client SDK &amp; CLI Troubleshooting &amp; Known Issues  If you need further support, have feedback or feature requests, go to our Support page.  ","version":"Next","tagName":"h2"},{"title":"Device and Recording Issues​","type":1,"pageTitle":"ARK Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/troubleshooting/troubleshooting_issues#device-and-recording-issues","content":" ","version":"Next","tagName":"h2"},{"title":"I can't start recording​","type":1,"pageTitle":"ARK Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/troubleshooting/troubleshooting_issues#i-cant-start-recording","content":" Check that the privacy switch is not engaged. The switch should be pushed forward (towards the lenses of your device) for recording to be possible.    ","version":"Next","tagName":"h3"},{"title":"Where's my recording?​","type":1,"pageTitle":"ARK Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/troubleshooting/troubleshooting_issues#wheres-my-recording","content":" It is possible you may have accidentally discarded your recording. Do not use the privacy switch to stop recording, as it prevents recording and deletes any current recording.  ","version":"Next","tagName":"h3"},{"title":"Stop recording does not work/ has a long delay​","type":1,"pageTitle":"ARK Troubleshooting & Known Issues","url":"/projectaria_tools/docs/ARK/troubleshooting/troubleshooting_issues#stop-recording-does-not-work-has-a-long-delay","content":" The longer a device records for, the longer it takes for a recording to stop. This is because the larger the VRS file, the longer it takes a recording to finish indexing.  note Recording has not fully stopped until the Recording LEDs have turned off ","version":"Next","tagName":"h3"},{"title":"How to Update Your Aria Glasses' OS","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/troubleshooting/update_glasses_os","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"How to Update Your Aria Glasses' OS","url":"/projectaria_tools/docs/ARK/troubleshooting/update_glasses_os#overview","content":" This page is for Project Aria glasses users that wish to manually update their device's Operating System. Normally, your glasses' OS will automatically update when it is plugged into power and connected to Wi-Fi.  ","version":"Next","tagName":"h2"},{"title":"To Identify Your OS Build​","type":1,"pageTitle":"How to Update Your Aria Glasses' OS","url":"/projectaria_tools/docs/ARK/troubleshooting/update_glasses_os#to-identify-your-os-build","content":" In the Mobile Companion App Dashboard, tap the Paired Glasses info card to go to Device Settings    Scroll down to view your Aria Glasses OS Version  ","version":"Next","tagName":"h3"},{"title":"To Update the OS​","type":1,"pageTitle":"How to Update Your Aria Glasses' OS","url":"/projectaria_tools/docs/ARK/troubleshooting/update_glasses_os#to-update-the-os","content":" Plug in your glasses into power and ensure they are connected to Wi-Fi.In the Mobile Companion App, select Device SettingsScroll down to view the OS VersionSelect Check for UpdatesOnce your glasses have finished updating, it will reboot your glasses, and the update will be complete  ","version":"Next","tagName":"h3"},{"title":"If you can't update your glasses​","type":1,"pageTitle":"How to Update Your Aria Glasses' OS","url":"/projectaria_tools/docs/ARK/troubleshooting/update_glasses_os#if-you-cant-update-your-glasses","content":" Upload any recordings on your glasses Uploads may take longer than usual if your OS is substantially out of date Factory reset your glasses (this will delete any recordings on your Aria glasses)Pair your glasses with the app On the top right of the dashboard, select Add glassesAs part of the pairing process, your glasses OS will be updated ","version":"Next","tagName":"h3"},{"title":"Project Aria Research Kit Release Notes","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/sw_release_notes","content":"","keywords":"","version":"Next"},{"title":"ARK v1.15 is Now Available!​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#ark-v115-is-now-available","content":" We are excited to announce the release of Aria Research Kit v1.15 today, containing improvements for both user experience and technical performance. This update adds robust debug sharing in Aria Studio and adds a settings page for customization of machine perception job submission. A new version of the companion app enables users to quickly report issues using the “rage-shake” feature. This release also brings significant improvements to MPS Hand Tracking accuracy.  ","version":"Next","tagName":"h2"},{"title":"Key Features​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#key-features","content":" Aria Studio​  MPS Job Debug Sharing: Users can now easily create and submit MPS job failure reports directly within Aria Studio. Attach screenshots, log files, and grant permission for the Aria Team to provide technical support—all from the new interface. You can also track the history and real-time status of your support cases, streamlining communication and accelerating issue resolution. Settings Page: A new GUI settings page in Aria Studio allows you to customize MPS settings with intuitive, accessible controls. This enhancement makes it simpler to adjust relevant options, improving usability and efficiency when debugging or configuring your environment.  Download and Installation Details: Aria Studio one-click Installation;PyPI installation  Aria Companion App v238​  Rage-shake Bug Reporting: Quickly report abnormal behavior by simply shaking your phone! The new rage-shake feature instantly captures the issue and connects you to Aria Support for faster problem resolution.  Download and Installation Details: Mobile Companion App installation  Improved MPS Hand Tracking​  We are releasing MPS Hand Tracking v3.0.0 on October 20th. This version reduces loss of hand tracking and reduces the mean keypoint estimation error on our test datasets, resulting in more reliable and precise hand tracking results. All Aria Research Kit users will be migrated to this new version at the time of release. If anyone has a need to continue using the existing version for dataset consistency, please reach out immediately so we can keep you allowlisted for the current version of hand tracking until you are ready for migration. This is a server-side update that does not require any software installation or updates.  ","version":"Next","tagName":"h3"},{"title":"Questions, Comments, Concerns?​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#questions-comments-concerns","content":" If you encounter any problems with Project Aria software please contact our support team through one of the following channels:  NEW! &quot;Rage shake&quot; in the companion app to report a bug directly to the team. NEW! Report MPS Job failures directly in Aria Studio. Post to Project Aria Tools issues on GitHub Post to Academic Partners Feedback and Support workplace group - discussion, feedback or user support Post to Project Aria Discord - best for discussion, feedback, or user support Email us directly at AriaOps@meta.com  Thank you for being part of the Project Aria community. We look forward to your feedback and to seeing what you build with these new capabilities!  ","version":"Next","tagName":"h3"},{"title":"Aria Mobile App v230 is now available​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#aria-mobile-app-v230-is-now-available","content":" Dear Academic Partners,  The Aria Mobile App v230 is now available. Visit the ARK SW Downloads page to download the latest version of the app for Android. For iOS, select Update in the TestFlight app. Keeping your app up to date helps ensure the best performance and compatibility.  ","version":"Next","tagName":"h2"},{"title":"PROJECT ARIA LATEST OS​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#project-aria-latest-os","content":" The latest Project Aria OS build 4975080.310.70 (released April 1, 2025).  ","version":"Next","tagName":"h3"},{"title":"QUESTIONS, CONCERNS, FEEDBACK?​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#questions-concerns-feedback","content":" If you encounter any problems using Project Aria support, please contact us. There are several ways for getting user support.  We encourage you to post issues to GitHub.  Post to Project Aria Tools issues on GitHub Post to Academic Partners Feedback and Support workplace group - discussion, feedback or user support Post to Project Aria Discord - best for discussion, feedback or user support Email AriaOps@meta.com - for feedback or user support  Note: If you are a Research Partner with access to the Aria Research Kit, there are several other support options available, although GitHub is ideal for bug reports and feature requests.  ","version":"Next","tagName":"h3"},{"title":"Aria Mobile App v220 is now available​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#aria-mobile-app-v220-is-now-available","content":" Dear Academic Partners,  The Aria Mobile App v220 is now available. Visit the ARK SW Downloads page to download the latest version of the app for Android. For iOS, select Update in the TestFlight app. Please update the app to access the following features and bug fixes.  ","version":"Next","tagName":"h2"},{"title":"BUG FIXES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#bug-fixes","content":" Various bug fixes improving the overall usage.  ","version":"Next","tagName":"h3"},{"title":"PROJECT ARIA LATEST OS​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#project-aria-latest-os-1","content":" The latest Project Aria OS build 4975080.310.70 (released April 1, 2025).  ","version":"Next","tagName":"h3"},{"title":"QUESTIONS, CONCERNS, FEEDBACK?​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#questions-concerns-feedback-1","content":" If you encounter any problems using Project Aria support, please contact us. There are several ways for getting user support.  We encourage you to post issues to GitHub.  Post to Project Aria Tools issues on GitHub Post to Academic Partners Feedback and Support workplace group - discussion, feedback or user support Post to Project Aria Discord - best for discussion, feedback or user support Email AriaOps@meta.com - for feedback or user support  Note: If you are a Research Partner with access to the Aria Research Kit there are several other support options available, although GitHub is ideal for bug reports and feature requests.  ","version":"Next","tagName":"h3"},{"title":"Announcing Aria Research Kit v1.14 with 21 Landmark Hand Tracking in MPS​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#announcing-aria-research-kit-v114-with-21-landmark-hand-tracking-in-mps","content":" We are pleased to announce the release of Aria Research Kit v1.14 which now makes fully articulated 21 landmark hand joint positions available with Machine Perception Services Hand Tracking output as a new CSV file called hand_tracking_results.csv. Researchers can utilize this long-awaited feature to produce detailed 3D hand pose annotations useful for robotics research, contextual AI, and anywhere else that hand signals may be important.    In addition to 21 landmarks output, we output a six-degree-of-freedom transform at the wrist location to allow for transformation of the landmarks from the hand frame to the device frame, and normal vectors computed at both the wrist and palm landmarks. The transform and normals form a consistent system and the normals also match the normals we’ve computed on previous versions of wrist and palm output.    We encourage hand tracking users to transition to this new output format, where we will be investing in any future maintenance and bug fixing. We will also continue to provide wrist_and_palm_poses output, which can be loaded with prior versions of projectaria_tools, but will not invest in bug fixes or feature additions for this legacy format.  Here's a Notebook and Sample Code (Python &amp; C++) demonstrating how to load and use the landmarks. See Documentation on data format definitions.  Reminder for Aria Desktop App Deprecation: As mentioned in previous release notes, the Aria Desktop App will be fully deprecated on May 30th, 2025, at which point requests to MPS will not be possible and documentation for the tool will be removed. Please migrate over to Aria Studio if you haven’t already.  ","version":"Next","tagName":"h2"},{"title":"Update your Software to Access These Improvements​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#update-your-software-to-access-these-improvements","content":" Aria Companion App​  Aria Mobile Companion App App v215 is now available for installation. Visit to the ARK SW Downloads and Updates to download the latest version for Android. For iOS, select Update in the TestFlight app.  Project Aria Tools​  The latest projectaria-tools 1.6.0 contains tooling necessary to load and visualize 21 landmark hand tracking output. See documentation for download instructions.  Aria Studio v1.1.0​  The latest version of Aria Studi, v1.1.0 is required to submit requests to the MPS service for hand tracking annotations with 21 landmark output. See documentation for instructions on how to install Aria Studio. Note that starting with this version, we no longer support Mac Intel with a one-click installer and Mac Intel users must use pip for Aria Studio installation.  ","version":"Next","tagName":"h3"},{"title":"Questions, comments, concerns?​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#questions-comments-concerns-1","content":" If you encounter any bugs or issues, please contact our support team through one of the following channels:  Post in the Project Aria Discord Join the server | User support channel Post to Workplace Feedback and support group Email us directly at AriaOps@meta.com  ","version":"Next","tagName":"h3"},{"title":"Announcement: Aria Research Kit v1.13 is now available​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#announcement-aria-research-kit-v113-is-now-available","content":" We are excited to announce Aria Research Kit v1.13, featuring color correction and devignetting on videos recorded with the Aria RGB camera and a new ability for Aria to operate as a Wi-Fi hotspot for streaming, eliminating dependency on external networks. Download the latest versions of the Aria Mobile Companion App, the Aria Client SDK, and Project Aria Tools to access these improvements today.  ","version":"Next","tagName":"h2"},{"title":"Color correction and devignetting for the Aria RGB camera​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#color-correction-and-devignetting-for-the-aria-rgb-camera","content":" You may have noticed blueish color distortion in Aria RGB recordings. We’ve now fixed these with consistent gamma curves and standard color temperatures in ARK v1.13. With this update, Aria devices now produce images with more consistent colors, calibrated to a standard color temperature of 5000K. Additionally, a new devignetting API is available in Project Aria Tools to further enhance image quality and accuracy.    ","version":"Next","tagName":"h3"},{"title":"Wi-Fi hotspot streaming​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#wi-fi-hotspot-streaming","content":" A new Wi-Fi hotspot streaming feature allows users to set up a dedicated hotspot using the Aria device, enabling direct and stable streaming without relying on external network infrastructure. Users can enable this feature through the Aria Client SDK or directly within the Aria Companion App.  ","version":"Next","tagName":"h3"},{"title":"Update your software to access these improvements​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#update-your-software-to-access-these-improvements-1","content":" Aria Companion App​  Aria Mobile Companion App App v210 is now available for download. Visit ARK SW Downloads and Updates to download the latest version for Android. For iOS, select Update in the TestFlight app.  Project Aria latest OS​  Project Aria OS build 4975080.310.70 (released April 1, 2025) contains the changes necessary for RGB color correction and devignetting. Update to this build or newer using the Aria Companion App.  Aria Client SDK​  The Aria Client SDK now supports streaming in hotspot mode, diagnostic logging collections for users, support for Python 3.12, RGB color correction, and more. See the ARK download documentation for download instructions.  Project Aria Tools​  The latest projectaria-tools 1.5.8 contains the tooling necessary to support RGB camera color correction and devignetting. See the data utilities overview for download instructions.  ","version":"Next","tagName":"h3"},{"title":"Questions, comments, concerns?​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#questions-comments-concerns-2","content":" If you encounter any bugs or issues, please contact our support team through one of the following channels:  Post in the Project Aria Discord Join the server | User support channel Post to Workplace Feedback and support group Email us directly at AriaOps@meta.com  ","version":"Next","tagName":"h3"},{"title":"Aria Research Kit v1.12​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#aria-research-kit-v112","content":" We are excited to announce updates to the Aria Research Kit with v1.12. Download the latest versions and access new features for Aria Studio, Project Aria Tools, and the Companion App.  ","version":"Next","tagName":"h2"},{"title":"Mobile Companion App​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#mobile-companion-app","content":" Aria Mobile Companion App v200 is now available for download. Visit ARK SW Downloads and Updates to download the latest version for Android. For iOS, select Update in the TestFlight app.  Bug fixes​  Version 200 includes minor bug fixes and refinements to the existing functionality and user experience.  Latest Project Aria OS​  Version 200 maintains the latest Project Aria OS build: 4971771.1550.70 from October 3, 2024.  ","version":"Next","tagName":"h3"},{"title":"Aria Studio​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#aria-studio-1","content":" Aria Studio v1.0.3 includes minor bug fixes and refinements. It is now available via one-click installation and pip-install from PyPi in Mac ARM, Mac Intel, Windows, and Linux.  ","version":"Next","tagName":"h3"},{"title":"Project Aria Tools​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#project-aria-tools-2","content":" We are excited to announce Project Aria Tools v1.5.7! Download or update your installation of Project Aria Tools to get access to the latest updates.  See our GitHub release notes to review all Project Aria Tools updates.  New Advanced Image Utility: Image devignetting​  Devignetting is a digital image processing technique used to correct uneven lighting in images, particularly those captured by cameras with wide-angle lenses or those that have inherent optical vignetting. Vignetting refers to the gradual decrease in brightness toward the edges of an image, often resulting in a darker periphery and a brighter center.  We now provide official guidance on how to apply devignetting with Python and C++. See the notebook and documentation for more information.  ","version":"Next","tagName":"h3"},{"title":"Questions, comments, concerns?​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#questions-comments-concerns-3","content":" If you encounter any bugs or issues, please contact our support team through one of the following channels:  Post in the Project Aria Discord Join the server | User support channel Post to Workplace Feedback and support group Email us directly at AriaOps@meta.com  ","version":"Next","tagName":"h3"},{"title":"Aria Companion App v195 now Available!​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#aria-companion-app-v195-now-available","content":" December 13, 2024  Aria Mobile App v195 is now available for download. For Android, visit the ARK SW Downloads and Updates page and download the latest version. For iOS, select Update in the TestFlight app. Please update the app to access the following features and bug fixes:  ","version":"Next","tagName":"h2"},{"title":"New & Updated Features​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#new--updated-features","content":" A new updates tab dedicated to feature announcements and program updates is now available in-app Our eye tracking calibration feature is now fully compatible with iPhone 16  ","version":"Next","tagName":"h3"},{"title":"Bug Fixes​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#bug-fixes-2","content":" Some bug fixes to improve the overall usage.  ","version":"Next","tagName":"h3"},{"title":"Project Aria Latest OS​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#project-aria-latest-os-3","content":" The latest Project Aria OS build 4971771.1550.70 (October 3, 2024).  ","version":"Next","tagName":"h3"},{"title":"Aria Studio - Please Update to v1.0.2 for Bug Fixes​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#aria-studio---please-update-to-v102-for-bug-fixes","content":" After our v1.0.0/1.0.1 release of Aria Studio on Monday Dec 9th, we identified a bug for users who downloaded Aria Studio with the Windows 1-click installer that caused MPS jobs to fail.  The team worked quickly to resolve the bug, and the fix is now available as part of a new release: Aria Studio 1.0.2. Please update to the latest version (1.0.2) at your earliest convenience.  ","version":"Next","tagName":"h3"},{"title":"Questions, Comments, or Feedback?​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#questions-comments-or-feedback","content":" If you encounter any problems with Project Aria software, please contact our support team through one of the following channels:  Post to Project Aria Tools issues on GitHub - preferred Post to Academic Partners Feedback and Support workplace group - discussion, feedback or user support Post to Project Aria Discord - best for discussion, feedback or user support Email us directly at AriaOps@meta.com  ","version":"Next","tagName":"h3"},{"title":"Announcing Aria Research Kit v1.11!​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#announcing-aria-research-kit-v111","content":" December 9, 2024  ","version":"Next","tagName":"h2"},{"title":"Aria Studio v1.0.1: Now on Windows, plus some other nice features​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#aria-studio-v101-now-on-windows-plus-some-other-nice-features","content":" Aria Studio is a modern desktop application with a browser-based UI that helps you manage recordings from your Aria glasses, access Machine Perception Services, and visualize egocentric data. We released a beta version of Aria Studio at CVPR this year with support for Mac and Linux. Today, we are releasing Aria Studio v1.0.1, which extends our OS support to include Windows. Aria Studio is now available as a downloadable 1-click installer package in addition to the previously available pip install method. Another new feature includes the ability to submit feedback to the Aria team directly in the application and a section where the Aria team will publish news program updates and important information in the home tab.  ","version":"Next","tagName":"h3"},{"title":"Updates for the MPS CLI and the Mobile Companion App​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#updates-for-the-mps-cli-and-the-mobile-companion-app","content":" This release also brings the Aria Machine Perception Services Command Line Interface (MPS CLI) to Windows. Download or update your installation of Project Aria Tools on your Windows machine to use the MPS CLI to submit your recordings to Aria Machine Perception Services. Coming soon, we will release v195 of the Aria Companion App on mobile, featuring a new section on the home tab where you can stay up-to-date with the latest news and updates from the Aria team.  ","version":"Next","tagName":"h3"},{"title":"Deprecation Plan for Aria Desktop App​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#deprecation-plan-for-aria-desktop-app","content":" For those still using the Aria Desktop App: Today, we are announcing a deprecation plan for the Aria Desktop App. We are no longer onboarding new users to the Desktop App and will shut off the ability to request MPS from the Desktop App on May 30th, 2025. Now with support for Windows, Aria Studio fully replaces the functionality of the Desktop App. Please switch over to Aria Studio as soon as possible and reach out to ariaops@meta.com if you have any questions about the transition.  ","version":"Next","tagName":"h3"},{"title":"Feedback and Support​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#feedback-and-support","content":" If you encounter any problems using ARK v1.11, please contact us. There are several ways for getting user support.  Post to Academic Partners Feedback and Support workplace group Post to Project Aria Discord Email AriaOps@meta.com  You can always request a meeting through our ops team if you need more in-depth discussions requiring higher bandwidth.  ","version":"Next","tagName":"h3"},{"title":"Aria Mobile App v185 is now available​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#aria-mobile-app-v185-is-now-available","content":" October 25, 2024  Dear Academic Partners,  The Aria Mobile App v185 is now available. Go to the Downloads page to get the latest version for Android. For iOS, select Update in the TestFlight app. Please Update the app to access the following features and bug fixes.  ","version":"Next","tagName":"h2"},{"title":"BUG FIXES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#bug-fixes-3","content":" Various bug fixes to improve the overall usage.  ","version":"Next","tagName":"h3"},{"title":"PROJECT ARIA LATEST OS​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#project-aria-latest-os-4","content":" The latest Project Aria OS build 4970205.890.70 (June 28, 2024).  ","version":"Next","tagName":"h3"},{"title":"QUESTIONS, CONCERNS, FEEDBACK?​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#questions-concerns-feedback-2","content":" If you encounter any problems using Project Aria support, please contact us. There are several ways for getting user support.  We encourage you to post issues to GitHub, so that they can be tracked.  Post to Project Aria Tools issues on GitHub  If you are a Research Partner with access to the Aria Research Kit there are several other support options available, although GitHub is ideal for bug reports and feature requests.  Post to Academic Partners Feedback and Support workplace group - discussion, feedback or user supportPost to Project Aria Discord - best for discussion, feedback or user supportEmail AriaOps@meta.com - for feedback or user support      ","version":"Next","tagName":"h3"},{"title":"Announcing Aria Research Kit v1.10​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#announcing-aria-research-kit-v110","content":" October 9, 2024  We're announcing the release of Aria Research Kit v1.10. This release adds versioning to all of the Machine Perception Services and includes a major update to make the SLAM service more accurate for downstream tasks that utilize 3D results projected into the RGB camera frame. We are also making it much easier for you to visualize, edit, and prepare your Aria recordings for downstream use such as dataset preparation and model training with PyTorch.  ","version":"Next","tagName":"h2"},{"title":"MPS Versioning​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#mps-versioning","content":" Starting now, we are introducing the concept of versioning to machine perception services. When we make changes to our SLAM, hand tracking, or eye tracking services, you will have information about which version of the service was utilized for your result computation. This will be available directly at the very end of the summary.json file. Versions will have a major, minor, and bugfix number (e.g. SLAM v1.1.0). We will keep our MPS Service Versions page updated as we roll out new versions so that you are aware of the changes we make.  ","version":"Next","tagName":"h3"},{"title":"RGB Online Calibration​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#rgb-online-calibration","content":" Aria excels at enabling computer vision from a 3D perspective. Our SLAM Machine Perception Service provides Aria trajectory and a semi-dense point cloud output that allow computations in a 3D reference frame. Tasks like object detection can be performed in 3D and then be re-projected back into the 2D RGB images. Available as of v1.1.0, we’ve implemented extremely accurate RGB online calibration for all of your downstream tasks that utilize 3D results reprojected into the RGB camera frame. Read the MPS Output Calibration page for more details on how to make use of online calibration.  ","version":"Next","tagName":"h3"},{"title":"New Tools for Working with VRS​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#new-tools-for-working-with-vrs","content":" We know that it takes manual work to prepare Aria recordings for downstream use, so we’ve released some new tools to make things easier. We now provide a lightweight VRS Player App that allows you to easily play the visual and audio sensor streams of a recording from a file menu or by using a simple command. We’ve also released a VRS Command Line Tool that enables easy inspection and editing of VRS files. For example, you can time crop a recording to eliminate an unwanted segment or remove sensor streams that you no longer need in your final dataset after processing through Machine Perception Services. Install both the lightweight VRS Player and the VRS Command Line Tool with the VRS Tools Installer.  ","version":"Next","tagName":"h3"},{"title":"Aria Training and Evaluation Toolkit​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#aria-training-and-evaluation-toolkit","content":" Preparing Aria recordings for training in PyTorch just got easier with Aria Training and Evaluation Kit (ATEK). ATEK offers an efficient solution to convert VRS files to PyTorch compatible formats, simplifying integration and use within deep learning projects. We’ve also pre-processed two popular Aria datasets with ATEK for direct usage in a data store. And lastly, we’ve created standardized evaluation libraries for 3D object detection and surface reconstruction. Read an in-depth blog post on ATEK for more information and see the ATEK GitHub page to install ATEK today.  ","version":"Next","tagName":"h3"},{"title":"Project Aria Now on Huggingface​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#project-aria-now-on-huggingface","content":" Finally, we introduce the Project Aria Huggingface Space, which allows researchers to track their model performance, helping measure, compare, and accelerate AI and ML research within the open community. AI models and datasets, released in the Project Aria Hugging Face space, will be supported by ATEK. Where possible, AI models trained using Project Aria data will also be available with full weights and scripts, making it easy for researchers to discover and benchmark existing models against their own methods.  ","version":"Next","tagName":"h3"},{"title":"Aria Studio Updates​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#aria-studio-updates","content":" We've also made some updates to Aria Studio. We now display a version number and have a mechanism for surfacing updates and other key information that we want to communicate to our users. Today, Aria Studio is installed via pip install, and in doing so you'll automatically get these new features. If you haven't yet tried it, please install Aria Studio today.  ","version":"Next","tagName":"h3"},{"title":"Any Questions?​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#any-questions","content":" If you have any questions regarding this new functionality in Aria Research Kit v1.10 or any issues accessing any of the features, please reach out to our support team at AriaOps@meta.com.      ","version":"Next","tagName":"h3"},{"title":"Aria Mobile App v180 is now available​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#aria-mobile-app-v180-is-now-available","content":" September 17, 2024  Dear Academic Partners,  The Aria Mobile App v180 for Android is now available. Go to the Downloads page to get the latest version for Android. For iOS, select Update in the TestFlight app. Please update the app.  ","version":"Next","tagName":"h2"},{"title":"BUG FIXES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#bug-fixes-4","content":" Various bug fixes to improve the overall usage.  ","version":"Next","tagName":"h3"},{"title":"PROJECT ARIA LATEST OS​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#project-aria-latest-os-5","content":" The latest Project Aria OS build 4970205.890.70 (June 28, 2024).  ","version":"Next","tagName":"h3"},{"title":"QUESTIONS, CONCERNS, FEEDBACK?​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#questions-concerns-feedback-3","content":" Contact Aria User Support by posting here in Academic Partners Feedback and Support workplace group or emailing AriaOps@meta.com.  Additional resources are available at Project Aria Github.      ","version":"Next","tagName":"h3"},{"title":"Announcing Aria Research Kit v1.9​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#announcing-aria-research-kit-v19","content":" We're announcing a new release of Aria Research Kit v1.9, now including Live Preview in the Aria Companion App and updates to our Wrist and Palm tracking MPS output. Read below for more details!  Use our new feature, Live Preview, to visualize the RGB camera, SLAM camera, and eye tracking camera sensor streams directly in the Aria Companion App on your mobile device. Live Preview gives you a tighter feedback loop to ensure that you are recording the right data for your research activities.  With Live Preview, your glasses operate as a Wi-Fi hotspot, streaming the sensor data directly without the need for an external router.  ","version":"Next","tagName":"h2"},{"title":"How to Use Live Preview​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#how-to-use-live-preview","content":" Update your Companion App to v175 (software download page)Update your glasses to the latest OS version using OTA updates.In the Dashboard tab, look for and tap the Live Preview button.  ","version":"Next","tagName":"h3"},{"title":"Wrist and Palm Tracking Updates in MPS​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#wrist-and-palm-tracking-updates-in-mps","content":" We’ve made some exciting updates to our wrist and palm tracking capabilities in our machine perception services (MPS). Starting now, all jobs launched will utilize these new features.  What’s New?​  We’ve tuned our hand tracking algorithm to improve recall for common computer vision research use cases.We are including normal vectors to the wrist and palm landmark output so that you can better understand the orientation of hands in your sequences (see documentation and a notebook to visualize sample data)We provide a demonstration of how to project popular third-party hand tracking models onto our wrist and palm landmark output for tasks that require higher-fidelity hand tracking output (see Jupyter notebook).  What Do I Need to Update to Access It?​  Follow instructions in our Wiki to re-install Aria Studio. This will update all necessary dependencies for you.If you’re only using our CLI directly instead of Aria Studio, you can just update to Projectaria_tools 1.5.6 using pip install projectaria_tools --upgrade. This will be handled automatically if you update or re-install Aria Studio.  Any Questions?​  If you have any issues accessing or using Live Preview or our new hand tracking functionality, please reach out to our support team at AriaOps@meta.com.      ","version":"Next","tagName":"h3"},{"title":"Mobile Companion App v170 and Time Synchronized Recordings Between Multiple Project Aria Devices now available​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#mobile-companion-app-v170-and-time-synchronized-recordings-between-multiple-project-aria-devices-now-available","content":" Dear Academic Partners,  Mobile Companion app V170 and Aria OS 4970205.890.70 are now available. With the release of Aria OS 4970205.890.70, you can now create time synchronized recordings between multiple devices via the Client SDK.  Recent documentation improvements include: Renamed The Project Aria Tools documentation site has been renamed to reflect its broader scope, it’s now called Project Aria Docs. The Data Utilities section has been renamed to Project Aria Tools.Updated Intro page and About ARKNew Project Aria FAQ  ","version":"Next","tagName":"h2"},{"title":"Aria Client SDK (TICSync support and general improvements)​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#aria-client-sdk-ticsync-support-and-general-improvements","content":" Users can now use the Aria Client SDK to capture time-synchronized data between multiple Project Aria glasses (and potentially other devices) on the one Wi-Fi network via TICSync. Using the Aria hotspot feature, one pair of glasses (server) acts as a Wi-Fi access point that forms a network between all glasses. Time requests from all other glasses go to the server/leader device, creating a synchronized time reference.  How to Create Time Synchronized Recordings with Multiple Aria GlassesTICSync Code SnippetTICSync Technical Details  To use TICSync you’ll need to:  Update to the Aria glasses OS 4970205.750.70, June 28 release Project Aria glasses OS should automatically update when plugged into power and connected to Wi-Fi. Go to How to Update Your Aria Glasses’ OS if you want to manually update your glasses. Update to Aria Client SDK V1.1 Project Aria Client SDK and CLI Setup GuideUpdate to the latest version via PyPI  In addition:  The Aria Client SDK is now supported on Python 3.12Wi-Fi connections to Aria glasses have been made more reliable by enabling sticky connection  Known Issues​  The Client SDK is a software developer kit. While we have created code samples to enable a range of features, we have not created fully fledged applications. We encourage people to make this code their own, and to use the code samples as a starting point.  Bug Fixes​  Information in error messages has been improvedReliability of multiple Project Aria devices connected over USB has been improved  ","version":"Next","tagName":"h3"},{"title":"Mobile Companion app​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#mobile-companion-app-1","content":" The Aria Mobile App v170 is now available. Follow the in-app prompt or go to the Downloads page to get the latest version for Android. For iOS, select Update in the TestFlight app. Please update the app to access the latest bug fixes and finalized retheming of all app pages.  ","version":"Next","tagName":"h3"},{"title":"PROJECT ARIA LATEST OS​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#project-aria-latest-os-6","content":" The latest Project Aria OS build 4970205.890.70 (June 28, 2024).  Project Aria glasses OS should automatically update when plugged into power and connected to Wi-Fi. Go to How to Update Your Aria Glasses’ OS if you want to manually update your glasses.  ","version":"Next","tagName":"h3"},{"title":"QUESTIONS, CONCERNS, FEEDBACK?​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#questions-concerns-feedback-4","content":" If you encounter any problems using Project Aria support, please contact us. There are several ways for getting user support.  We encourage you to post issues to GitHub, so that they can be tracked.  Post to Project Aria Tools issues on GitHub  If you are a Research Partner with access to the Aria Research Kit there are several other support options available, although GitHub is ideal for bug reports and feature requests.  Post to Academic Partners Feedback and Support workplace group - discussion, feedback or user supportPost to Project Aria Discord - best for discussion, feedback or user supportEmail AriaOps@meta.com - for feedback or user support  ","version":"Next","tagName":"h3"},{"title":"Announcing Aria Research Kit v1.8 - Aria Studio (in beta today), TICSync (coming soon), improved vrs2mp4 conversion (available today)​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#announcing-aria-research-kit-v18---aria-studio-in-beta-today-ticsync-coming-soon-improved-vrs2mp4-conversion-available-today","content":" Dear Academic Partners: We’re excited to introduce Aria Research Kit V1.8, featuring Aria Studio (in beta today) and TICSync (coming soon). Aria Studio is a brand new browser-based application for managing your devices, recordings, and MPS jobs available today for Mac and Linux via pip install. TICSync will enable multiple Aria devices to be easily time synchronized and will be made available via pip install towards the end of June along with the next device OS update. We've also released improvements in the VRS_to_MP4 converter in Project Aria Tools to solve for audio and video synchronization issues.  ","version":"Next","tagName":"h2"},{"title":"Aria Studio (BETA)​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#aria-studio-beta","content":" Aria Studio is a new browser-based application we are releasing in beta for managing devices, recordings, and machine perception services jobs, designed for cross-platform support, easy update installation, stability, and a smooth user flow for MPS job submission.Aria Studio offers an easy way to visualize both your recordings transferred to your machine and processed MPS results.Aria Studio is based on the existing MPS CLI tool and everything possible in the MPS CLI tool will also be possible via Aria Studio.Aria Studio is currently available on certain distributions of x64 Linux and Mac.Go to the Aria Studio documentation page for installation details.  ","version":"Next","tagName":"h3"},{"title":"Aria TICSync Support​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#aria-ticsync-support","content":" Aria TICSync allows you to run millisecond-scale-accurate time-synchronized recordings for multi-view computer vision applications.You can start time-synchronized recordings for multiple Aria devices via simple CLI commands with devices connected to a host machineYou can disconnect the Aria devices from the host machine and enable WiFi-based synchronization with one Aria device operating as a hotspot.Aria TICSync functionality will be included with Aria SDK v1.1.0 available towards the end of June. Keep an eye out for another post when the installer is ready.A Jupyter Notebook is available today with example code for how to work with and visualize time synchronized recordings for your learning and experimentation.Go to the TICSync documentation page for details.  ","version":"Next","tagName":"h3"},{"title":"Project Aria Tools VRS to MP4 improvements​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#project-aria-tools-vrs-to-mp4-improvements","content":" We've improved timing accuracy during export for audio and video syncing.We updated the translator to include frame timestamps in the metadata.Go to Project Aria Tools 1.5.2 release notes for details.  ","version":"Next","tagName":"h3"},{"title":"Feedback and Support​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#feedback-and-support-1","content":" If you encounter any problems using ARK v1.8, please contact us. There are several ways for getting user support.  Post to Academic Partners Feedback and Support workplace groupPost to Project Aria DiscordEmail AriaOps@meta.comYou can always request a meeting through our ops team if you need more in-depth discussions requiring higher bandwidth.  ","version":"Next","tagName":"h3"},{"title":"Aria Mobile App v165 is now available​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#aria-mobile-app-v165-is-now-available","content":" Dear Academic Partners,  The Aria Mobile App v165 is now available. Go to the Downloads page to get the latest version for Android. For iOS, select Update in the TestFlight app. Please update the app to access the following features and bug fixes.  Project Aria recently released Project Aria Eye Tracking, an open source inference code for the Pre March 2024 Eye Gaze Model used by MPS. This model:  Provides the ability to generate eye gaze data using the old model, if that is more suited to your research use case or if you’re not able to request MPSThis code can be used on downloaded data or when streaming data using the Aria Client SDK  ","version":"Next","tagName":"h2"},{"title":"BUG FIXES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#bug-fixes-6","content":" Various bug fixes to improve the overall usage.  ","version":"Next","tagName":"h3"},{"title":"PROJECT ARIA LATEST OS​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#project-aria-latest-os-7","content":" The latest Project Aria OS build 4968951.840.70 (April 25th, 2024).  ","version":"Next","tagName":"h3"},{"title":"QUESTIONS, CONCERNS, FEEDBACK?​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#questions-concerns-feedback-5","content":" Go to our Support page for ways to get in touch.    ","version":"Next","tagName":"h3"},{"title":"Announcing: New Custom Recording Profile Capabilities​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#announcing-new-custom-recording-profile-capabilities","content":" We’re pleased to announce that Aria Mobile Companion app v160 contains the ability to configure individual sensors when creating custom recordings using the Mobile Companion app.  How to access the update  You’ll be prompted to update the app when you open it, and you can also manually download it.  You’ll need Aria OS version 4968999.0.70 (April 25th, 2024) or later to be able to set custom recording profiles.  How to manually update the OS  ","version":"Next","tagName":"h2"},{"title":"Custom Recording Profile​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#custom-recording-profile","content":" To keep the new Custom Recording Profile feature approachable, and to reduce our testing matrix, we are launching this feature with a limited set of parameters. If people find the feature useful, we are open to unblocking research needs by providing the ability to include additional parameters. Please use one of our support channels to make feature requests.  To start a recording with a custom recording profile:  In the Aria app Dashboard, select New Recording SessionSelect Recording ProfileThe first recording profile in the list will be Custom Profile You may need to scroll up to see this option Select the Custom ProfileSelect Edit parametersAdjust sensor settings Certain combinations, such as RGB Cameras at 2880 x 2880 at 15 or more fps will trigger warnings as they cause elevated thermal loads and reduced battery lifeTap More info on the Recording Profiles Tab to see more details, such as auto exposure  ","version":"Next","tagName":"h3"},{"title":"Bug Fixes​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#bug-fixes-7","content":" An issue where the More info link on the top right of the Recording Profiles page was difficult to see has been resolved.  ","version":"Next","tagName":"h3"},{"title":"Known issues​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#known-issues-1","content":" Custom Recordings is a new feature! If you encounter data quality issues, please help us track down the issue by emailing the repro steps (including sensor configuration) to AriaOps@meta.com.  ","version":"Next","tagName":"h3"},{"title":"QUESTIONS, CONCERNS, FEEDBACK?​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#questions-concerns-feedback-6","content":" Go to our Support page for ways to get in touch.    ","version":"Next","tagName":"h2"},{"title":"Announcing New MPS, Recording Profiles, and Updates to the Companion Apps​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#announcing-new-mps-recording-profiles-and-updates-to-the-companion-apps","content":" March 21 2024  Dear Academic Partners,  This release includes new Machine Perception Services (MPS) capabilities, Wrist and Palm Tracking data and a revised Eye Gaze model that provides depth estimations, as well as four new recording profiles, and an Automatic Speech Recognition speech to text code sample.  ","version":"Next","tagName":"h2"},{"title":"How to access the new features​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#how-to-access-the-new-features","content":" MPS NEW SERVICE: Hand Tracking Request Hand Tracking via the MPS CLI or Desktop app to get Wrist and Palm Tracking data! UPDATED SERVICE: The new Eye Gaze model includes depth estimations Backwards compatibility with Project Aria Tools will be maintained for data generated using the old model Use the MPS CLI to do bulk requests on existing data Raw uploaded data will now be stored for 30 days for MPS processing and reprocessing Desktop app v38 [MacOS and Windows] Download the latest version of the app for the ability to request Hand Tracking MPS via Desktop app, and to get bug fixes Project Aria Tools v1.5 New code sample that generates speech to text outputs from VRS filesInstall via Pip to be able to request Hand Tracking data using the MPS CLI Aria glasses OS 4968297.250.70 Update to the latest OS for four new recording profiles and essential bug fixesPlease make sure your glasses have been updated in the last two months Aria Mobile Companion App v155 Download the latest version of the app to enjoy the updated look and bug fixes Documentation the latest documentation updates including: New MPS Troubleshooting page.New Collaborative Tools Page, did you know you can use Aria data with Nerfstudio? We’ve collaborated with Nerfstudio to enable Gaussian Splatting with their tools.Updated MPS Google Colab tutorials.  ","version":"Next","tagName":"h3"},{"title":"MPS Updates​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#mps-updates","content":" Once you upload data using the MPS CLI, it will be stored for 30 days for additional processing/reprocessing, instead of 24 hours.  New Hand Tracking MPS, Wrist and Palm Tracking​  For this new service, we use Aria's SLAM camera images to estimate the hand movements of the wearer. The wrist and palm poses are given in the device frame in meters. These outputs are automatically generated when doing the default MPS CLI request (or request hand_tracking mode) and are listed as Hand Tracking in the Desktop App.  Install Project Aria Tools vs 1.5 using Pip or install Desktop App v38 to be able to request this new service.  Hand Tracking Data FormatMPS CLIInstall/Update Project Aria Tools Using PipDownload Desktop app  Update to Eye Gaze outputs - depth estimations​  We have introduced a new model for computing eye gaze, which produces left and right eye gaze directions along with the depth at which these gaze directions intersect.  This new model’s output schema adds to general_eye_gaze.csv and personalized_eye_gaze.csv in a way that ensures backward compatibility with Project Aria Tools.  Eye Gaze Data Formats page - schema has been updatedEye Gaze Code Snippets - new page  Known Issues​  The new Eye Gaze model does not generate yaw_rads_cpf outputs This value can be computed using helper functions and is automatically read when parsed with our data utilities.  Bug Fixes​  Minor bug fixes  ","version":"Next","tagName":"h3"},{"title":"Desktop Companion App v38​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#desktop-companion-app-v38","content":" Download v38 on Windows or MacOS to request Hand Tracking MPS and to get bug fixes.  MPS requests using the Desktop app have been slightly restructured, you no longer need to request Point Cloud data as an additional service. The request options are:  SLAM (6DoF Trajectory, Semi-Dense Point Cloud, Online Sensor Calibration)Eye Gaze (General, plus personalized if the VRS file contains In-Session Eye Gaze Calibration)Hand Tracking (Wrist and Palm Tracking)  The Streaming button in the dashboard has been renamed to Preview, to better reflect the capability provided by the Desktop app. Use the Client SDK with CLI to stream data.  Desktop app logs are now stored in ~/.aria/logs/aria_desktop_app_{date}_{time}.log  Please note, the streaming preview available through the Desktop app is optimized for Profile 12.  Known Issues​  If you encounter the file too small error when requesting MPS, it may be because the Desktop app is in a bad state. Restarting the app should resolve this issueSelecting Cancel does not work when downloading MPS is in progress[MacOS] When the app is launched with logging enabled, the Aria icon might not appear in the taskbar, but functions should not be impacted[Windows] The Aria icon does not render correctly when pinned to the taskbar  Bug Fixes​  If starting recording/streaming fails the UI will now refreshIssue where you can’t request MPS unless you interact with tick boxes has been resolvedIssue where the Desktop app did not quit completely on MacOS has been resolved  ","version":"Next","tagName":"h3"},{"title":"Project Aria Tools v1.5​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#project-aria-tools-v15","content":" Project Aria Tools provides open tooling for anyone wanting to work with Aria data. For this release it enables users to request Hand Tracking data using the MPS CLI and provides a new ASR Python code sample.  Update Project Aria Tools:  Python users: Update projectaria_tools Python package Install via Pip to be able to request Hand Tracking data using the MPS CLI C++ users: Update projectaria_tools git repository  Go to https://github.com/facebookresearch/projectaria_tools/releases/tag/1.5.3 for the full release log.  Speech to Text Code Sample (Python)​  This new ASR Python code sample shows how to use Faster Whisper to run Whisper Speech Recognition on an Aria audio stream. The ASR outputs are time aligned with Aria Device Time.  Go to the Automated Speech Recognition Readme for how to get started.    ","version":"Next","tagName":"h3"},{"title":"Aria Glasses OS​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#aria-glasses-os","content":" The latest Project Aria OS build 4968297.250.70 was released on March 11, 2024.  New recording profiles​  We’ve released the following experimental recording profiles. If you use a new profile, we’d love to hear about how it works for you.  Recording Profile 26: Minimal sensors, low RGB frame rate for extended recording times. The aim is 5+ recording hours while having RGB camera data at a low frame rate (unlike Recording Profile 20, which has IMU, but no RGB data).  Recording Profile 27: Designed for streaming over USB.  **Recording Profile 28: **Very high Eye Tracking camera frame rate.  Recording Profile 29: Low RGB frame rate, JPEG with decimation. This recording profile is the same as Profile 26, however 9 out of every 10 JPEG frames are skipped (decimation/sub-sampling). This is our first recording profile that uses decimation. The data from the RGB data stream is saved at a rate that archives a frame rate of 0.1FPS. The aim of this profile is to be able to record for 15+ hours.  Update to the latest version of the OS to access these profiles. Your glasses’ OS should automatically update when connected to Wi-Fi and power. Go to Updating OS to manually trigger an update.  You can see more details about recording profiles in the Mobile Companion app:  Open the Mobile Companion App and select New Recording SessionSelect the existing recording profile (or Choose Profile if you have not set a default recording profile)Tap More Info in the top right corner The More info link may be difficult to see, this will be fixed in a future version of the Mobile Companion app  Or go to Recording Profiles documentation in Project Aria Tools.  Known Issues​  The new recording profiles are experimental and may be tuned further in the future.  Bug Fixes​  Make sure your glasses’ OS is less than 2 months old, to make sure you have a fix for an issue that may cause sluggish behavior in your Aria glasses, difficulties updating the OS, streaming data and reduced upload times. Updating OS includes troubleshooting steps if you have difficulties updating the OS.  ","version":"Next","tagName":"h3"},{"title":"Mobile Companion App v155​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#mobile-companion-app-v155","content":" Please keep in mind, you can’t initiate recordings through the Mobile app if your glasses’ OS is more than 2 months old. Please make sure you update your glasses’ OS.  Updated look and feel of the Mobile app​  The Mobile Companion app has a fresh new look! As part of this:  The Settings tab has been renamed ProfileDevice Settings can be opened by tapping the glasses icon under Paired Glasses    Known Issues​  The More info link on the top right of the Recording Profiles page may be difficult to see.  Bug Fixes​  Minor bug fixes  ","version":"Next","tagName":"h3"},{"title":"QUESTIONS, CONCERNS, FEEDBACK?​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#questions-concerns-feedback-7","content":" Go to our Support page for ways to get in touch.    ","version":"Next","tagName":"h3"},{"title":"Announcing Project Aria MPS CLI Availability and New MPS Feature​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#announcing-project-aria-mps-cli-availability-and-new-mps-feature","content":" February 28 2024  Dear Academic Partners,  We are pleased to announce Project Aria MPS CLI and new MPS capability multi-trajectory map alignment! As part of our ongoing commitment to empowering researchers, the Aria MPS CLI allows you to submit jobs to Machine Perception Services programmatically. This will hopefully bring a new level of efficiency and flexibility to your workflow.  ","version":"Next","tagName":"h2"},{"title":"⭐️Key features for MPS CLI​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#️key-features-for-mps-cli","content":" Request the following MPS services Single and multi-sequence SLAM SLAM Open/Closed loop trajectoriesSemi-Dense point clouds and observationsOnline sensor calibration Eye gaze GeneralPersonalized Recording Upload VrsHealthCheck runs automatically before uploading. Upload is skipped if there is a failure.Unfinished uploads can be resumed for up to 24 hrs Processing Improved efficiency from concurrent processing and upload of multiple recordingsAutomatic download of outputs for immediate accessEnables re-processing of data within a set time-frame without the need for re-upload Workflow integration Support Run MPS CLI as part of a script of other workflow to run MPS on recordings via automation  ","version":"Next","tagName":"h3"},{"title":"How to get started with CLI​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#how-to-get-started-with-cli","content":" To experience the power of Aria MPS CLI, go to the Project Aria MPS CLI page for how to use it.  Videos of this feature are available in the Project Aria Academic Partner Announcements, Feedback &amp; Support workplace group  ","version":"Next","tagName":"h3"},{"title":"⭐️MPS: MULTI-SLAM/MULTI-SEQUENCE MAP ALIGNMENT​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#️mps-multi-slammulti-sequence-map-alignment","content":" This addition brings the ability to generate SLAM output in a common frame of reference and support for multi-sequence SLAM outputs in a common frame of reference. Please go to Multi-SLAM page for more information about the feature and go to the MPS CLI page for how to access it.  ","version":"Next","tagName":"h3"},{"title":"⭐️Feedback and Support​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#️feedback-and-support","content":" If you encounter any problems using Aria MPS CLI, there are several ways to get user support:  Post to Project Aria Academic Partner Announcements, Feedback &amp; Support workplace groupPost to Project Aria DiscordEmail AriaOps@meta.com and you can always request a meeting through this email if you need higher bandwidth discussion.Additional resources are available at Project Aria Github.  ","version":"Next","tagName":"h3"},{"title":"Aria Mobile App v150 is now available​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#aria-mobile-app-v150-is-now-available","content":" February 6 2024  Dear Academic Partners,  The Aria Mobile App v150 is now available. Go to the Downloads page to get the latest version for Android. For iOS, select Update in the TestFlight app. Please update the app.  ","version":"Next","tagName":"h2"},{"title":"BUG FIXES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#bug-fixes-12","content":" Various bug fixes to improve the overall usage.  ","version":"Next","tagName":"h3"},{"title":"PROJECT ARIA LATEST OS​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#project-aria-latest-os-8","content":" The latest Project Aria OS build **4967277.730.70 **was released on 01/25/2024.  ","version":"Next","tagName":"h3"},{"title":"QUESTIONS, CONCERNS, FEEDBACK?​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#questions-concerns-feedback-8","content":" Go to our new Support page for ways to get in touch.  ","version":"Next","tagName":"h3"},{"title":"Project Aria Updates: Aria Mobile App v140 and changes to MPS​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#project-aria-updates-aria-mobile-app-v140-and-changes-to-mps","content":" December 4 2023  Dear Academic Partners,  The Aria Mobile App v140 is now available. Go to the Downloads page to get the latest version for Android. For iOS, select Update in the Lighthouse app.    ","version":"Next","tagName":"h2"},{"title":"NEW & UPDATED FEATURES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#new--updated-features-1","content":" MPS file outputs have been renamed​  Machine Perception Services (MPS) outputs have been renamed, so that they more clearly communicate what is in the outputs:  Trajectory  global_points.csv.gz -&gt; semidense_points.csv.gz  Eye Gaze  generalized_eye_gaze.csv -&gt; general_eye_gaze.csvcalibrated_eye_gaze.csv -&gt; personalized_eye_gaze.csv  ","version":"Next","tagName":"h3"},{"title":"KNOWN ISSUES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#known-issues-6","content":" The MPS examples in Getting Started, Google Colab and the Visualization guide use the old naming conventions for MPS.  ","version":"Next","tagName":"h3"},{"title":"BUG FIXES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#bug-fixes-13","content":" Various bug fixes improving the overall usage, including:  Client SDK/CLI issue resolved. Device Settings tab now automatically refreshes after you’ve allowed Client SDK pairing in the mobile app.  ","version":"Next","tagName":"h3"},{"title":"PROJECT ARIA LATEST OS​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#project-aria-latest-os-9","content":" The latest Project Aria OS build 4965072.660.70 was released on 12/01/2023.  ","version":"Next","tagName":"h3"},{"title":"QUESTIONS, CONCERNS, FEEDBACK?​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#questions-concerns-feedback-9","content":" Go to our new Support page for ways to get in touch.  ","version":"Next","tagName":"h3"},{"title":"Announcing,  Project Aria Updates (ARK V1.5 & Project Aria Tools V1.2)​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#announcing-project-aria-updates-ark-v15--project-aria-tools-v12","content":" October 24, 2023  We’re delighted to announce Project Aria Research Kit V1.5, a point release encompassing new features for every aspect of Project Aria Research Kit (ARK), from a Client SDK and CLI, enabling you to directly interact with the glasses; to new Calibrated Eye Gaze, enabling you to achieve more accurate eye gaze estimations. Plus, you can now download the Mobile Companion app on iOS!  In this release:  Project Aria Client SDK V1.0.2Aria Mobile Companion app V135Project Aria OS build 4965072.660.70Project Aria Tools V1.2  ","version":"Next","tagName":"h2"},{"title":"To access the new ARK features​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#to-access-the-new-ark-features","content":" Update the Mobile Companion app and Aria glasses OS to the latest version:  Follow the instructions in ARK SW Downloads and Updates to download and update the Aria Mobile Companion App on Android or iOSConnect your Aria Glasses to Wi-Fi and power and they will automatically update You can manually trigger an update from the Mobile Companion app’s Device Settings  The Aria Research Kit parts of this release, support research partners with access to Project Aria glasses.  ","version":"Next","tagName":"h3"},{"title":"To access new Open Science features​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#to-access-new-open-science-features","content":" Update Project Aria Tools:  Python users: Update projectaria_tools Python packageC++ users: Update projectaria_tools git repository  Open Science releases can be helpful for users with or without access to Aria glasses.  ","version":"Next","tagName":"h3"},{"title":"Aria Research Kit Release Notes​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#aria-research-kit-release-notes","content":" Project Aria Client SDK with CLI​  projectaria_client_sdk is a new Python package/client library that enables you to directly interact with your Project Aria glasses from a client machine. This enables you to stream Aria data and run MP algorithms on that data, in real-time.  The Client SDK currently offers the ability to:  Connect and disconnect from the device via USB and WiFiRetrieve detailed device information and its current statusControl Aria streaming and recording capabilitiesAccess calibration data for the sensors you're recording withSubscribe and listen to Aria sensors dataVisualize streaming data  A CLI, installed as part of the SDK, provides the ability to:  Pair the glasses via USB or Wi-FiConnect to the glasses via USB or Wi-FiRetrieve the device status and informationControl Aria recording and streaming capabilities  To support this new feature, we’ve added a Client SDK section to ARK Documentation in Project Aria Tools, including:  Project Aria Client SDKSetup GuideCode Sample Examples and Walkthroughs  New &amp; Improved Machine Perception Service: Calibrated Eye Gaze​  You can now generate Calibrated Eye Gaze MPS for sequences captured on Project Aria glasses that have in-session Eye Gaze Calibration. This enables you to capture more data about how Aria wearers look at objects, and improve the eye gaze estimations for any given recording.  When you request Eye Gaze Machine Perception Services (MPS) and the file has an in-session Eye Gaze Calibration as part of the VRS file, you will receive two outputs:  generalized_eye_gaze.csv - based on the standard eye gaze configurationcalibrated_eye_gaze.csv - calibrated eye gaze data based on the calibration data collected in the recording.  Additional improvements to Eye Gaze MPS include:  Summary.json files are also now available for Eye Gaze MPS outputWe’ve created a new visualization tool, MPS Replay Viewer (C++), to render static scene and dynamic elements.  See below for more details:  How to collect in-session eye gaze calibration dataEye Gaze Data Format documentation - updated to reflect new outputs  Mobile Companion App improvements (and now available on iOS)​  The Aria Mobile Companion App allows users to interact with their Aria glasses via mobile device. With this release, users will now be able to download the app on iOS as beta testing software via Testflight.  Installing or updating the Mobile Companion app on Android just became easier. You no longer need to sign in to the partner portal to ARK specific software. You can download it as a direct link from the ARK SW Downloads and Updates page or from an update prompt in the app.  Additionally, you’ll now have the ability to change the default Recording Profiles when you make a recording using the Mobile Companion app.  See below for more details:  ARK SW Downloads and Updates Aria Mobile Companion App documentation  ","version":"Next","tagName":"h3"},{"title":"ARK Bug Fixes​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#ark-bug-fixes","content":" Minor bug fixes and general quality of life improvements.  ","version":"Next","tagName":"h3"},{"title":"Known Issues​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#known-issues-7","content":" Client SDK/CLI​  Unlike Project Aria Tools, the Client SDK is not available on Ubuntu 20.04Device Settings tab does not automatically refresh after you’ve paired your glasses with SDK. If you’re in the Device Settings tab when you select “aria auth pair”, navigate away from Device Settings and back again to view your certificate.  Further resources:  ARK Troubleshooting and Known IssuesClient SDK &amp; CLI Troubleshooting &amp; Known Issues  Getting Support​  If you encounter any problems using ARK resources, there are several ways approved Academic Research Partners can access user support:  Report or view issues on GitHub - can be used to report ARK or Open Science issuesPost to Project Aria discord - best for discussion, feedback or user supportPost to Academic Partners Feedback and Support workplace group - discussion, feedback or user supportEmail AriaOps@meta.com - for feedback or user support  PROJECT ARIA LATEST OS​  The latest Project Aria OS build 4965072.660.70 was released on October 17th, 2023.  ","version":"Next","tagName":"h3"},{"title":"Open Science Updates (for everybody)​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#open-science-updates-for-everybody","content":" The Aria Research Kit, builds on top of projectaria_tools, which is available to anyone wishing to work with Aria data. New features contained within projectaria_tools V1.2 include:  Google Colab Notebooks You can now explore Project Aria Tools without needing to install it on your own machine! Dataprovider Quickstart TutorialMachine Perception Services Tutorial Python binding for the Sophus Library Sophus PyBind provides access to SO3, SE3, interpolate and iterativeMean featuresThis Python binding has been submitted to Sophus and will be officially supported by the Sophus Library GitHub repo soon Python binding type hinting Python type hinting/stubs are now automatically generated if you’ve installed the tools using pipChange the type of hinting in generate_stubs.py MPS Replay Viewer (C++) This new visualization tool renders static scene and dynamic elements: 2D/3D observations raysEye gaze data Output image data from data_provider now contains information about which camera it came from cameraId has been added to ImageDataRecord  Update Project Aria Tools to access these features:  Python users: Update projectaria_tools Python packageC++ users: Update projectaria_tools git repository  Documentation updates and improvements for this release include:  How to Convert VRS to MP4Updates to Machine Perception Services documentation  For more details, please go to Project Aria Tools Release notes.  Open Science Bug Fixes​  Bug fixes include:  When camera intrinsics are updated, all sensor calibration is now updated The Sophus API has been updated, if you encounter issues, please update to v1.2 of Project Aria Tools and update your code This fix resolves: TypeError: 'unsupported operand type(s) for @: '_core_pybinds.sophus.SE3d' and '_core_pybinds.sophus.SE3d'AttributeError: '_core_pybinds.sophus.SE3d' object has no attribute 'to_matrix' The API has been corrected so that the calibration data will match the sensor and device access point: get_sensor_calibration(stream_id).camera_calibration() and provider.get_device_calibration().get_camera_calib(name) will now match  Go to Project Aria Tools Issues on GitHub for more information.  If you encounter any problems working with Open Science tooling or data, we encourage you to post the issue on Github.  Open Science Known Issues​  The Sophus API has been updated, if you encounter issues, please update to v1.2 of Project Aria Tools  Further resources  Project Aria Tools Troubleshooting and Known IssuesReport or view issues on GitHub  ","version":"Next","tagName":"h3"},{"title":"August Release - Project Aria Mobile App v125 is now available​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#august-release---project-aria-mobile-app-v125-is-now-available","content":" 8/22/2023  Dear Academic Partners,  Download the Aria Mobile App v125 to ensure you have all the latest features and capabilities. Sign into the Aria Web Portal on your Android device to download v125.  ","version":"Next","tagName":"h2"},{"title":"NEW & UPDATED FEATURES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#new--updated-features-2","content":" Ability to name VRS files via the Mobile Companion appThe default recording profile (if set via Mobile or Desktop app) will now also be the default profile when initiating recordings via the Mobile appAdded a feature that requires the app to be updated if the app build is flagged as severely buggy.  Ability to name VRS files​  Historically, the VRS files were given random alphanumeric names. This made the recordings difficult to distinguish from one another in the Desktop app and after downloading them. With this new release, the name given to a recording when creating a custom recording defines the VRS file name. You’ll get a warning if there are two recordings with the same name on your Aria device.  To access this feature:  In the mobile app, go to Settings to check that you have v125 Sign into portal.projectaria.com on your phone to get the latest recommended build Go to device settings (select the gear icon next to your glasses) to check that you have 4963656.310.70 of the OS Aria glasses will automatically update if they’re connected to Wi-Fi and powerSelect Check for Updates in Device Settings to make sure you have the most recent version of the OS  How to name a recording:  Set the recording’s name when you create a custom recording, or edit the file name of any recording on your glasses. To edit the name or to give an existing recording a name:  Go to the recording tabSelect a recordingSelect Edit on the top right corner  Default recording profile applied to new recordings​  With this change, the default recording profile selected for the glasses also applies to each recording started from the app. Previously, the default recording profile only applied to recordings started by the capture button on the glasses.  Please double-check that the correct profile is selected before you start each recording, as the behavior has changed. Previously, the app kept its own last-used setting; with this change, the default will flip back to the default recording profile of the glasses on connection. This default recording profile can be updated via Device Settings in the mobile app.  Disable severely buggy builds​  This feature is a contingency plan, in case a severely broken app build is rolled out by mistake. In this situation, the app would show a full-screen dialog that requires it to be updated in order to proceed.  ","version":"Next","tagName":"h3"},{"title":"BUG FIXES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#bug-fixes-14","content":" Various bug fixes implemented to improve the overall usage of the App.  ","version":"Next","tagName":"h3"},{"title":"PROJECT ARIA LATEST OS​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#project-aria-latest-os-11","content":" The latest Project Aria OS build 4963656.310.70 was released on August 22, 2023.  ","version":"Next","tagName":"h3"},{"title":"QUESTIONS, CONCERNS, FEEDBACK?​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#questions-concerns-feedback-10","content":" Contact Aria User Support by posting in Project Aria Academic Partner Announcements, Feedback &amp; Support or email AriaOps@meta.com.  ","version":"Next","tagName":"h3"},{"title":"July Release - new documentation site, Aria Desktop App v37 and Mobile App v120​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#july-release---new-documentation-site-aria-desktop-app-v37-and-mobile-app-v120","content":" 7/11/23  Dear Academic Partners,  With this July release we’re pleased to announce a new documentation site on GitHub, Desktop Companion App v37, and Aria Mobile Companion App v120.  V37 of the Desktop Companion App updates the app to the new Project Aria branding, the ability to run the Desktop app from the command line and users will now be able to request Semi-Dense Point Cloud MPS.  Project Aria Tools is our new documentation site on GitHub. Information that used to be at projectaria.com is now in the Aria Research Kit sectionof the site and no login is required to access the documentation.  Download the updated companion apps from the Aria Web Portal. If you sign in on your desktop computer, you’ll get the Desktop Companion app. If you sign in on your Android mobile device, you’ll download the Mobile Companion app.  ","version":"Next","tagName":"h2"},{"title":"NEW AND UPDATED FEATURES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#new-and-updated-features","content":" ","version":"Next","tagName":"h3"},{"title":"Documentation now on GitHub​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#documentation-now-on-github","content":" The documentation previously hosted at projectaria.com will now be hosted in Project Aria Tools.  Project Aria Tools is the new comprehensive website for all your documentation needs. It contains over 60 pages, covering technical specifications, data formats, data utilities you can use with your data, information and tooling for open data we’ve released and technical insights for deeper dives.  The Aria Research Kit section contains information specific to Academic partners who have access to Aria glasses and MPS. We’ve added a few extra pages and updated the documentation for this release, updates include adding:  Get the Right Size Glasses - sizing information if you’re ordering glasses (our small is surprisingly large!)How to Join the Academic Partners Workplace GroupAria Glasses User Manual - a combination of old and new information. Proper Handling and Cleaning information is now at the top.  There is a lot of new documentation to explore, including:  Aria Data Utilities - including new Jupyter notebook tutorialsMore detailed information about recording profiles (scroll the table sideways to see all the columns)More data format information (including coordinate conventions)New Open Datasets with tooling, Aria Digital Twin Dataset and Aria Synthetic Environments Dataset.  ","version":"Next","tagName":"h3"},{"title":"Desktop App v37 - Semi-dense point cloud​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#desktop-app-v37---semi-dense-point-cloud","content":" From V37 of the Desktop app onwards, users will be able to request semi-dense point clouds as an addition to trajectory MPS services. The same sensor profile requirements for generating trajectory data apply to semi-dense point cloud.  Semi-dense point clouds are used by researchers who need static scene 3D reconstructions, reliable 2D images tracks or a representative visualization of the environment.  About Aria Machine Perception Services (MPS)How to Request MPSMPS Output - Semi-Dense Point Cloud  ","version":"Next","tagName":"h3"},{"title":"Improved UI & UX​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#improved-ui--ux","content":" The Desktop app has been updated to match Aria’s new brandThe recordings view has been revamped, users will now see a thumbnail for each recording and the upload ID can be copiedThe Desktop app can now be run directly from the command line without needing a GUI. It currently includes two utilities as subcommands: Further documentation about how to run the Desktop app from the command line will be added to the ARK wiki soonhealth: use this to run validity checks on an Aria recording (VRS file) It can be run from the command line as follows: AriaHub health vrsFilePath.vrsThese checks are also run on the VRS file automatically, before the file gets uploaded for MPS processing. The results of those checks can be found under your home directory under ./aria/logs vrs: a Swiss army knife utility to manipulate VRS files in different ways It can be run from the command line as follows: AriaHub vrs vrsFilePath.vrsGo to VRS official documentation for a full list of commands  ","version":"Next","tagName":"h3"},{"title":"Mobile App v120 - Quality Screens Feature update​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#mobile-app-v120---quality-screens-feature-update","content":" Some additional features have been added to the quality screen feature that was introduced with v115. You’ll now be able to see timecodes and the overall score percentage for the sensors.  ","version":"Next","tagName":"h3"},{"title":"BUG FIXES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#bug-fixes-15","content":" Various bug fixes improving the overall usage of the Apps.  ","version":"Next","tagName":"h3"},{"title":"PROJECT ARIA LATEST DEVICE OS​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#project-aria-latest-device-os","content":" The latest Project Aria device OS build 4962591.230.70 was released on June 30, 2023.  ","version":"Next","tagName":"h3"},{"title":"QUESTIONS, CONCERNS, FEEDBACK?​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#questions-concerns-feedback-11","content":" Contact Aria User Support by posting in Project Aria Academic Partner Announcements, Feedback &amp; Support or email AriaOps@meta.com.      ","version":"Next","tagName":"h3"},{"title":"Aria Mobile App v115 is now available​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#aria-mobile-app-v115-is-now-available","content":" Dear Academic Partners, The Aria Mobile App v115 for Android is now available for download from the Aria Web Portal (accessed via your Android internet browser).  ","version":"Next","tagName":"h2"},{"title":"NEW & UPDATED FEATURES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#new--updated-features-3","content":" Sensor data quality signalsIn-Session personalized eye gaze calibrationDifferentiate paired glasses in the pairing screen When you select Add glasses, the available Aria devices will be split into Paired Glasses and Other Glasses so that it’s easy to tell which glasses are already paired with the app.  Sensor Data Quality Signals​  While making recordings, researchers will be able to see whether there are any sensor data quality issues (for example, due to thermal mitigation). To check for any issues, in the Mobile Companion app, view the &quot;Sensor Status&quot; section in the Recording Status Screen. Tap on the row to see full details. If you’ve initiated recording via the Capture button, access the Recording Status by selecting Recording in progress on the Mobile Companion app’s main dashboard.  In-Session Personalized Eye Gaze Calibration​  Users will be able to record personalized eye gaze calibrations within an ongoing recording. The eye gaze calibration section of the sequence can be used to improve the eye gaze estimations in the rest of the recording. In the future, Machine Perception Services will be able to consume these recordings and output more accurate gaze information.  In the Mobile Companion app, create a new recording using a profile that includes ET and RGB cameras (such as Profile 15 or 25)Once your recording has started, close the recording window Select X on the top left of the screen Go to Device Settings (select the gear next to your glasses)Select Eye Tracking CalibrationConfirm that you’d like to run the during the current recording sessionFollow the prompts to calibrate your glasses  Eye Gaze Calibration tips​  Things to Avoid​  ❌ Do not wear a face covering during eye calibration.  ❌ Choose an area with ample and even lighting; do not face a bright light, window or reflective surface.  ❌ Do not set your phone screen brightness too high compared to your surroundings.  ❌ Do not fully extend your arm(s) during eye calibration. Your elbows should be bent so that the phone is roughly 1 ft (30 cm) away from your face.  Helpful Tips​  ✅ The phone should be held straight in front of your face, so that you shouldn't look up or down to see the screen. Hold the phone plumb (90 degrees) vertically to the ground.  ✅ The &quot;Leveler&quot; stage appears if the position of your phone isn't within specifications for the calibration process. Adjust the phone in front of you and its distance by bending your elbow until the smaller, black circle turns into a green disk with a check mark.  ✅ Once the &quot;Leveler&quot; stage is successfully completed, do your best to keep your phone in exactly the same position throughout the full eye calibration process. If your phone is moved to a position no longer suited to calibrate your device, the app will return to the &quot;Leveler&quot; stage.  ✅ The eye calibration stages 1 to 10 move your nose towards the direction indicated by the arrow. If you're only following the direction with your gaze without moving your head, the calibration stage will time out and fail. However, you make sure to keep your eyes fixed on the number within the dot the whole time.  ","version":"Next","tagName":"h3"},{"title":"BUG FIXES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#bug-fixes-16","content":" Various bug fixes to improve the overall usage of the Companion App.  ","version":"Next","tagName":"h3"},{"title":"PROJECT ARIA LATEST OS​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#project-aria-latest-os-12","content":" ","version":"Next","tagName":"h3"},{"title":"QUESTIONS, CONCERNS, FEEDBACK?​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#questions-concerns-feedback-12","content":" Contact Aria User Support by posting here in Project Aria Academic Partner Announcements, Feedback &amp; Support or emailing AriaOps@meta.com.  ","version":"Next","tagName":"h3"},{"title":"Aria Mobile App v110 is now available​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#aria-mobile-app-v110-is-now-available","content":" Dear Academic Partners,The Aria Mobile App v110 for Android is now available for download from the Aria Web Portal (accessed from your Android internet browser). Here are the updates this new version brings.  ","version":"Next","tagName":"h2"},{"title":"NEW & UPDATED FEATURES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#new--updated-features-4","content":" App icons and splash screens for Android companion apps have been updated to Aria’s new branding. When you update the app it will now look like this!From v110 onwards, users will be more easily able to tell if their Aria device’s OS is out of date and see prompts to update their devices.If the glasses are significantly (currently set as 2 months) out of date, the app will disable recording until they are updated.As before, glasses automatically update when connected to power and Wi-Fi.Users will get an in-app prompt to update the app if the app build is over 8 weeks old.  ","version":"Next","tagName":"h3"},{"title":"BUG FIXES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#bug-fixes-17","content":" Fixed a bug that occasionally prevented partner mode glasses from being set up for over-the-air software updates properly. Please double-check that your glasses are able to update to a recent build (April 2023 or later). If your glasses are not updating, please reach out to the Aria team.After glasses are unpaired, the app prevents re-pairing with the glasses until they finish rebooting.Other minor bug fixes  ","version":"Next","tagName":"h3"},{"title":"PROJECT ARIA LATEST OS​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#project-aria-latest-os-13","content":" The latest Project Aria OS build was released on May 2, 2023.  ","version":"Next","tagName":"h3"},{"title":"QUESTIONS, CONCERNS, FEEDBACK?​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#questions-concerns-feedback-13","content":" Contact Aria User Support by posting here in Project Aria Academic Partner Announcements, Feedback &amp; Support or emailing AriaOps@meta.com.  ","version":"Next","tagName":"h3"},{"title":"May 3, 2023 Aria Mobile App v110 is now available​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#may-3-2023-aria-mobile-app-v110-is-now-available","content":" Dear Academic Partners,  The Aria Mobile App v110 for Android is now available for download from theAria Web Portal (accessed from your Android internet browser). Here are the updates this new version brings.  ","version":"Next","tagName":"h2"},{"title":"NEW & UPDATED FEATURES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#new--updated-features-5","content":" App icons and splash screens for Android companion apps have been updated to Aria's new branding.From v110 onwards, users will be more easily able to tell if their Aria device's OS is out of date and see prompts to update their devices.If the glasses are significantly (currently set as 2 months) out of date, the app will disable recording until they are updated.As before, glasses automatically update when connected to power and Wi-Fi.Users will get an in-app prompt to update the app if the app build is over 8 weeks old.  ","version":"Next","tagName":"h3"},{"title":"BUG FIXES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#bug-fixes-18","content":" Fixed a bug that occasionally prevented partner mode glasses from being set up for over-the-air software updates properly. Please double-check that your glasses are able to update to a recent build (April 2023 or later). If your glasses are not updating, please reach out to the Aria team.After glasses are unpaired, the app prevents re-pairing with the glasses until they finish rebooting.Other minor bug fixes  ","version":"Next","tagName":"h3"},{"title":"PROJECT ARIA LATEST OS​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#project-aria-latest-os-14","content":" The latest Project Aria OS build 4961244.1190.70 was released on May 2, 2023  ","version":"Next","tagName":"h3"},{"title":"QUESTIONS, CONCERNS, FEEDBACK?​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#questions-concerns-feedback-14","content":" Contact Aria User Support by posting in Project Aria Academic Partner Announcements, Feedback &amp; Support or emailing AriaOps@meta.com.  ","version":"Next","tagName":"h3"},{"title":"April 3, 2023 Aria Desktop App v36 is now available​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#april-3-2023--aria-desktop-app-v36-is-now-available","content":" ","version":"Next","tagName":"h2"},{"title":"IMPORTANT NOTICE​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#important-notice","content":" v36 will clear the app's cache when you start it for the first time. Please make sure you download all of your MPS artifacts (Trajectory, Eye Gaze) before installing and starting v36.  Dear Academic Partners,  The Aria Desktop App v36 for Mac and Windows is now available for download from the Aria Web Portal including the brand new Linux version.  Here are the updates this new version brings:  ","version":"Next","tagName":"h3"},{"title":"NEW FEATURES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#new-features","content":" Aria for Linux is now available as a debian package for Ubuntu, more precisely the 22.04 LTS version. It is important to note the app was only tested for that specific version under Gnome 42.5 and X11 (X Server) as well as Wayland. Any other debian distribution (Ubuntu 22.04 fork such as Kubuntu, Mint etc..) or environment may or may not work.  Find updated instructions in the Aria For Linux Installer section of the Desktop App page to find out how to install Aria on Ubuntu.  ","version":"Next","tagName":"h3"},{"title":"IMPROVEMENTS​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#improvements","content":" Reduced app size bundle (both pre and post install)Reduced app startup time  ","version":"Next","tagName":"h3"},{"title":"BUG FIXES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#bug-fixes-19","content":" Various bug fixes improving the overall usage of the Desktop App.  ","version":"Next","tagName":"h3"},{"title":"QUESTIONS, CONCERNS, FEEDBACK?​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#questions-concerns-feedback-15","content":" Contact Aria User Support by posting in Project Aria Academic Partner Announcements, Feedback &amp; Support or emailing AriaOps@meta.com.    ","version":"Next","tagName":"h3"},{"title":"March 24, 2023 Aria Mobile App v105 is now available​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#march-24-2023-aria-mobile-app-v105-is-now-available","content":" Dear Academic Partners,  The Aria Mobile App v105 for Android is now available for download from the Aria Web Portal (accessed from your Android internet browser). Here are the updates this new version brings.  ","version":"Next","tagName":"h2"},{"title":"NEW & UPDATED FEATURES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#new--updated-features-6","content":" The default recording profile, engaged when starting a recording by pressing the Capture button on the Project Aria device directly, can now be set and viewed on the Mobile Aria App on Android (see screenshot below). This feature used to be only available on the Desktop Aria App.Tapping “Unpair Glasses” mentions the number of on-device recordings (not uploaded) the unpairing will delete through factory reset.The Device ID is now listed in the app, below the serial number in the device settings page.  Go to the Device Info page for more information about Device IDs.  ","version":"Next","tagName":"h3"},{"title":"BUG FIXES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#bug-fixes-20","content":" On Android only, a bug causing difficulties with switching between more than 8 paired glasses has been fixed.The recording setup screen is no longer dismissed if the recording fails to start.Minor bug fixes  ","version":"Next","tagName":"h3"},{"title":"PROJECT ARIA LATEST OS​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#project-aria-latest-os-15","content":" The latest Project Aria OS build 4959822.780.70 was released on February 22, 2023.  ","version":"Next","tagName":"h3"},{"title":"QUESTIONS, CONCERNS, FEEDBACK?​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#questions-concerns-feedback-16","content":" Contact Aria User Support by posting in Project Aria Academic Partner Announcements, Feedback &amp; Support or emailing AriaOps@meta.com.  ","version":"Next","tagName":"h3"},{"title":"February 22, 2023​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#february-22-2023","content":" ","version":"Next","tagName":"h2"},{"title":"Aria Mobile App v100 is now available​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#aria-mobile-app-v100-is-now-available","content":" Dear Academic Partners,  The Aria Mobile App v100 for Android is now available for download from the Aria Web Portal (accessed from your Android internet browser). Here are the updates this new version brings.  ","version":"Next","tagName":"h3"},{"title":"NEW & UPDATED FEATURES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#new--updated-features-7","content":" The on-screen status message stating a recording is starting or completing used to render the whole page temporarily unresponsive until the recording was fully started or saved. Now, an equivalent status message shows up on top of the screen (not as an overlay message in the center), allowing the user to dismiss or interact with the page at any time.Several UI adjustments were made to improve visibility and ease of interaction with various app sections and buttons.  ","version":"Next","tagName":"h3"},{"title":"BUG FIXES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#bug-fixes-21","content":" The issue causing the recording status to occasionally stay on-screen instead of being dismissed when completing a recording by pressing the Capture button has been fixed.Minor bug fixes  ","version":"Next","tagName":"h3"},{"title":"PROJECT ARIA LATEST OS​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#project-aria-latest-os-16","content":" The latest Project Aria OS build 4959822.780.70 was released on February 22, 2023.  ","version":"Next","tagName":"h3"},{"title":"QUESTIONS, CONCERNS, FEEDBACK?​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#questions-concerns-feedback-17","content":" Contact Aria User Support by posting in Project Aria Academic Partner Feedback &amp; Support or emailing AriaOps@meta.com.      ","version":"Next","tagName":"h3"},{"title":"February 9, 2023, Aria Desktop App v35 is now available​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#february-9-2023--aria-desktop-app-v35-is-now-available","content":" Dear Academic Partners,  The Aria Desktop App v35 for Mac and Windows is now available for download from the Aria Web Portal. Here are the updates this new version brings.  ","version":"Next","tagName":"h2"},{"title":"NEW FEATURES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#new-features-1","content":" We revamped the Recordings view to simplify the distinction between accessing Project Aria's device storage and the tools tailored for processing VRS files on the local host with a dedicated VRS Tools tab. Every file operation on the local host (Mac/Windows) is now done using the native file explorer (Finder for Mac, File Explorer for Windows).  Find updated instructions in Desktop App instructions, in the Device Storage section, to know how to copy locally VRS files from the Project Aria device storage.Find updated instructions in Desktop App wiki page, in the Playback section, to know how to read your locally copied VRS files.Find updated instructions in MPS wiki page for how to request MPS.  ","version":"Next","tagName":"h3"},{"title":"BUG FIXES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#bug-fixes-22","content":" Various bug fixes improving the overall usage.  ","version":"Next","tagName":"h3"},{"title":"QUESTIONS, CONCERNS, FEEDBACK?​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#questions-concerns-feedback-18","content":" Contact Aria User Support by posting in Project Aria Academic Partner Feedback &amp; Support or emailing AriaOps@meta.com.      ","version":"Next","tagName":"h3"},{"title":"January 20, 2023, Aria Mobile Companion App v95 is now available​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#january-20-2023--aria-mobile-companion-app-v95-is-now-available","content":" Dear Academic Partners,  The Aria Mobile Companion App v95 for Android is now available for download. Find it in the Aria Web Portal by visiting it directly from your Android internet browser. Here are the updates this new version brings.  ","version":"Next","tagName":"h2"},{"title":"NEW FEATURES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#new-features-2","content":" Backend Update for User Accounts​  Over the next few weeks, we will be changing the way we create user accounts to log into the Aria Mobile App on Android. There should be no perceived difference after this change, even with older versions of the app. If you run into an issue, please contact us immediately.  Mobile App Update Prompt​  When you launch the Mobile Companion app, it should trigger a notification prompting you to download the latest version from Aria Web Portal. If dismissed, the prompt will not show up again within the same day unless you log out and back in. Otherwise, the same notification prompt will show up the following day when launching the app.  ","version":"Next","tagName":"h3"},{"title":"BUG FIXES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#bug-fixes-23","content":" Various bug fixes  ","version":"Next","tagName":"h3"},{"title":"QUESTIONS, CONCERNS, FEEDBACK?​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#questions-concerns-feedback-19","content":" Contact Aria User Support by posting in Project Aria Academic Partner Feedback &amp; Support or emailing AriaOps@meta.com.  ","version":"Next","tagName":"h3"},{"title":"December 16, 2022 Major Feature Release 🚀(Dec 2022)​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#december-16-2022-major-feature-release-dec-2022","content":" ","version":"Next","tagName":"h2"},{"title":"New Feature Summary​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#new-feature-summary","content":" New Machine Perception Services (i.e. MPS): These features are available through the updated version of Desktop App. Improved Trajectory - We will now provide additional Trajectory output, including 1 khz open loop trajectory (instead of low frequency 10 hz previously), 1 khz closed loop trajectory, online calibration at camera frame rate and more robust error messaging for scenarios where Trajectory processing fails.Local Eye Gaze - Provides unit vectors and associated uncertainties for each ET frame. The gaze vectors are expressed in central pupil frame (CPF). We also provide CPF to device frame 6DOF transformation. Aria Data Tools: Open Source tools that provide C++ and Python3 tools to interact with Project Aria data. Read and visualize Project Aria sequences and sensor dataRetrieve calibration data and interact with Aria camera modelsRead and visualize machine perception output from Project Aria sequences (6DoF Trajectory, Local Eye Gaze) Usability &amp; Bug Fixes: Various usability improvements across Desktop, Mobile &amp; Aria Data ToolsImproved documentation, including a “Troubleshooting &amp; Known Issues” sectionWe will now be logging high-level anonymous usage data to better understand how we can improve your experience  ","version":"Next","tagName":"h3"},{"title":"ACCESSING NEW FEATURES​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#accessing-new-features","content":" To gain access to new features, you will need to:  Download the most recent version of the Mobile (v90) and Desktop (v34) Apps from the Portal (projectaria.com). Additionally, we will prompt Mobile App users to update their app.Access Aria Data Tools from Github  Thank you, and again please feel free to provide feedback. We want to hear the good, the bad and the ugly!  ","version":"Next","tagName":"h3"},{"title":"Detailed Release Notes​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#detailed-release-notes","content":" ","version":"Next","tagName":"h2"},{"title":"ARIA MOBILE APP V90​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#aria-mobile-app-v90","content":" New features and bug fixes include:  “Task ID” and “Client Tag” fields have been renamed “Name” and “Notes”.Access Denied screen no longer shows when external users start the app with no internet connection.Wi-Fi can be configured while glasses are not connected to power. It does not allow uploading without plugging in the device.Profile selection screen now shows sensor details for each profile.  Note: If your current version is under v85 it will not update automatically. Please delete and install v90 by signing into the Portal (projectaria.com) with your Android device.  ","version":"Next","tagName":"h3"},{"title":"ARIA DESKTOP APP V34​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#aria-desktop-app-v34","content":" New features and bug fixes include:  New improved trajectory with open/close loop poses and online calibration.Eye Gaze vectors with uncertainty.Various usability improvements and bug fixes  ","version":"Next","tagName":"h3"},{"title":"DEVICE SW (BUILD 4958601.360.70)​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#device-sw-build-495860136070","content":" Please make sure your device is charged and connected to wifi via Aria Mobile App to receive this build.  New features and bug fixes include:  Enabled USB streaming support on Mac OS (not available on AriaHub yet).Added telemetry logging for a subset of device events with an anonymized location.Added new recording profile21.Companion App shows details about a streaming session started from AriaHub.Updated security patch level to November 2022.Added the ability to run RGB in RAW mode at high frame rate (not exposed in a profile yet).Removed profile17 from the list of recording profiles.      ","version":"Next","tagName":"h3"},{"title":"November 11, 2022, Android Aria App v85 is now available​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#november-11-2022-android-aria-app-v85-is-now-available","content":" Dear Academic Partners,The Android Aria App v85 has now been released. Here is what you need to know.### NEW FEATURES AND IMPROVEMENTS  Thumbnails showing what was recorded appear in each Recording Details page under the Recordings tab immediately after that recording is completed and saved (this feature should become available after the next Project Aria OS update around November 15th)If the Bluetooth and/or Location services need to be enabled, the app will now show a prompt to do so upon launching the app.During an ongoing recording, the Profile and Sensors used are now mentioned on the active recording page.Through the app, users can now connect a Project Aria device to an EAP-PWD Wi-Fi network (using username and password for authentication - does not support certificates).The Android app will now show a banner on top of the screen indicating that the user has not selected a default Profile for the paired Project Aria device, which can only be done via the Desktop app.  ","version":"Next","tagName":"h2"},{"title":"IF YOUR CURRENT VERSION IS V80​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#if-your-current-version-is-v80","content":" As long as you're currently using v80 (check the app version in the app Settings page), launching the app should trigger a notification prompting you to download the latest version from the Partner Portal.  ","version":"Next","tagName":"h3"},{"title":"IF YOUR CURRENT VERSION IS OLDER THAN V80​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#if-your-current-version-is-older-than-v80","content":" App versions older than v80 will not receive any notifications, as this is a new feature (as announced in this post). In this case, you will need to uninstall the app and install v85 from the Partner Portal.  ","version":"Next","tagName":"h3"},{"title":"WHAT IF YOU DISMISSED THE PROMPT?​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#what-if-you-dismissed-the-prompt","content":" If you dismissed the notification prompting you to update, it should appear again when you launch the Android Aria App the following day. If you don't want to wait that long, you can directly install the latest version from the Partner Portal.  ","version":"Next","tagName":"h3"},{"title":"VERIFY V85 WAS INSTALLED​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#verify-v85-was-installed","content":" Once the Aria app is installed, login and tap the Settings tab at the bottom right corner of the Dashboard page. On that Settings page, you'll find the app version.  ","version":"Next","tagName":"h3"},{"title":"QUESTIONS, CONCERNS, FEEDBACK?​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#questions-concerns-feedback-20","content":" Contact Aria User Support by posting here in Project Aria Academic Partner Feedback &amp; Support or emailing AriaOps@meta.com.      ","version":"Next","tagName":"h3"},{"title":"October 10, 2022 ARIA DESKTOP APP V32​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#october-10-2022-aria-desktop-app-v32","content":" With v32 a single universal Mac application is now available, supporting both Intel and Apple Silicon architecture  ","version":"Next","tagName":"h2"},{"title":"New Features​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#new-features-3","content":" Single universal Mac application supporting both Intel and Apple Silicon architectureLocal Notifications for both Mac &amp; WindowsMerging both Uploads &amp; Local recordings in the same viewAbility to select a default profile when using the HW recording buttonResizable columns for the Local &amp; Uploads tablesOverall app speed and performance improved  ","version":"Next","tagName":"h3"},{"title":"Deprecated Features​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#deprecated-features","content":" OS update &amp; Wireless connection (you may now use the latest Companion App for that)  ","version":"Next","tagName":"h3"},{"title":"Bug Fixes​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#bug-fixes-24","content":" Frequent inability to select a recording profileInability to visualize the Aria Stream windowVarious issues when using the extracting features via the &quot;More&quot; toolbar buttonWindows app freezing randomlyOn Mac, when in fullscreen, the top toolbar covering the top of the window As always, please make sure to update your glasses to the latest version using the Companion App before starting to use Aria For Mac and Aria for Windows V32      ","version":"Next","tagName":"h3"},{"title":"ARIA Companion App APP V80​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#aria-companion-app-app-v80","content":" With v80, updating the companion app is now easier.  ","version":"Next","tagName":"h2"},{"title":"New Features​","type":1,"pageTitle":"Project Aria Research Kit Release Notes","url":"/projectaria_tools/docs/ARK/sw_release_notes#new-features-4","content":" Starting with v80, the Android app will show a notification prompting you to update it when a newer version becomes available. The on-screen prompt will take you to the location (Aria Web Portal) of the new version, allowing you to update the app faster. ","version":"Next","tagName":"h3"},{"title":"Fix USB Driver Issues in Linux","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/troubleshooting/linux_usb_driver","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Fix USB Driver Issues in Linux","url":"/projectaria_tools/docs/ARK/troubleshooting/linux_usb_driver#overview","content":" If the computer can't detect a Project Aria device, it may be that your Aria device's battery is drained, or in Linux it may be because of your USB driver.  Use the following instructions to resolve USB driver issues in Linux.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Fix USB Driver Issues in Linux","url":"/projectaria_tools/docs/ARK/troubleshooting/linux_usb_driver#prerequisites","content":" Android Device Bridge (ADB)  To install ADB use sudo apt-get android-tools  ","version":"Next","tagName":"h2"},{"title":"Instructions​","type":1,"pageTitle":"Fix USB Driver Issues in Linux","url":"/projectaria_tools/docs/ARK/troubleshooting/linux_usb_driver#instructions","content":" ","version":"Next","tagName":"h2"},{"title":"Look for Aria device​","type":1,"pageTitle":"Fix USB Driver Issues in Linux","url":"/projectaria_tools/docs/ARK/troubleshooting/linux_usb_driver#look-for-aria-device","content":" With your Aria device plugged into your computer, use the command adb devices.  If your device can be found, you'll get an output like:  List of devices attached 1820dc10 device   If you see no permissions:  List of devices attached 1820dc10 no permissions   you likely need to change your udev.  ","version":"Next","tagName":"h3"},{"title":"Change udev​","type":1,"pageTitle":"Fix USB Driver Issues in Linux","url":"/projectaria_tools/docs/ARK/troubleshooting/linux_usb_driver#change-udev","content":" The following instructions were taken from Arch Linux's Android Debug Bridge instructions and Janos Gyerik's Adding udev rules:  Step 1: Get VENDOR_ID and PRODUCT_ID​  Use list devices to find the [VENDOR_ID] and [PRODUCT_ID] of your Aria device.  The command  lsusb   should show something like:  Bus 002 Device 002: ID 2833:0086 Facebook, Inc. Aria   In the example above, [VENDOR_ID] = 2833 and [PRODUCT_ID]=0086  Step 2: Modify 51-android.rules​  Using lsusb will create a new file /etc/udev/rules.d/51-android.rules  Modify 51-android.rules using the following commands or script. Make sure you create a group called adbusers and $USER, so that you have the correct permissions.  Commands  $ cat /etc/udev/rules.d/51-android.rules SUBSYSTEM==&quot;usb&quot;, ATTR{idVendor}==&quot;2833&quot;, MODE=&quot;0660&quot;, GROUP=&quot;adbusers&quot;, TAG+=&quot;uaccess&quot; SUBSYSTEM==&quot;usb&quot;, ATTR{idVendor}==&quot;2833&quot;, ATTR{idProduct}==&quot;0086&quot;, MODE=&quot;0660&quot;, GROUP=&quot;adbusers&quot;, SYMLINK+=&quot;android_adb&quot; SUBSYSTEM==&quot;usb&quot;, ATTR{idVendor}==&quot;2833&quot;, ATTR{idProduct}==&quot;0086&quot;, MODE=&quot;0660&quot;, GROUP=&quot;adbusers&quot;, SYMLINK+=&quot;android_fastboot&quot;   Reboot your workstation to ensure the changes are applied.  Script  This script will will apply the previous commands and reboot your workstation.  IDs=$(lsusb | grep Facebook) if [[ &quot;$?&quot; -ne 0 ]]; then echo &quot;Make sure you have your VROS device connected to your workstation&quot; exit fi IDs=$(echo $IDs | cut -d &quot; &quot; -f 6) VID=$(echo $IDs | cut -d &quot;:&quot; -f 1) PID=$(echo $IDs | cut -d &quot;:&quot; -f 2) conf_f=/etc/udev/rules.d/51-android.rules sudo touch ${conf_f} echo &quot;SUBSYSTEM==\\&quot;usb\\&quot;, ATTR{idVendor}==\\&quot;$VID\\&quot;, MODE=\\&quot;0660\\&quot;, GROUP=\\&quot;adbusers\\&quot;, TAG+=\\&quot;uaccess\\&quot;&quot; &gt;&gt; $conf_f echo &quot;SUBSYSTEM==\\&quot;usb\\&quot;, ATTR{idVendor}==\\&quot;$VID\\&quot;, ATTR{idProduct}==\\&quot;$PID\\&quot;, MODE=\\&quot;0660\\&quot;, GROUP=\\&quot;adbusers\\&quot;, SYMLINK+=\\&quot;android_adb\\&quot;&quot; &gt;&gt; $conf_f echo &quot;SUBSYSTEM==&quot;usb&quot;, ATTR{idVendor}==\\&quot;$VID\\&quot;, ATTR{idProduct}==\\&quot;$PID\\&quot;, MODE=\\&quot;0660\\&quot;, GROUP=\\&quot;adbusers\\&quot;, SYMLINK+=\\&quot;android_fastboot\\&quot;&quot; &gt;&gt; $conf_f sudo groupadd adbusers sudo usermod -aG adbusers $USER  ","version":"Next","tagName":"h3"},{"title":"How to Reduce VRS File Size","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/troubleshooting/reduce_vrs_file_size","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"How to Reduce VRS File Size","url":"/projectaria_tools/docs/ARK/troubleshooting/reduce_vrs_file_size#overview","content":" If an MPS request times out, you may need to reduce the size of the VRS file or try using the MPS CLI Finding a faster internet connection may also help.  Because VRS files store each sensor stream separately, you can use VRS tooling to create copies of VRS files that do not include specific sensor streams. The MPS output can then be used in combination with the original VRS file (with all sensor streams).  MPS does not need the following sensor streams that contain a lot of data:  RGB Sensor Streams (214-1) Should more than halve your file sizeJust removing this stream may be sufficient Microphone Sensor Streams (231-1), contains 8 audio channels  ","version":"Next","tagName":"h2"},{"title":"Instructions​","type":1,"pageTitle":"How to Reduce VRS File Size","url":"/projectaria_tools/docs/ARK/troubleshooting/reduce_vrs_file_size#instructions","content":" ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"How to Reduce VRS File Size","url":"/projectaria_tools/docs/ARK/troubleshooting/reduce_vrs_file_size#prerequisites","content":" These instructions use the VRS tools to create a VRS file without a specific sensor stream.  Install and Build VRS  ","version":"Next","tagName":"h3"},{"title":"Command​","type":1,"pageTitle":"How to Reduce VRS File Size","url":"/projectaria_tools/docs/ARK/troubleshooting/reduce_vrs_file_size#command","content":" In this example, a copy of the VRS file is created that does not include RGB Sensor Streams (214-1)  vrs copy &lt;path/to/recording.vrs&gt; --to &lt;path/to/recording_norgb.vrs&gt; - 214-1 ","version":"Next","tagName":"h3"},{"title":"How to Join the Academic Partners Workplace Group","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ARK/workplacegroup","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"How to Join the Academic Partners Workplace Group","url":"/projectaria_tools/docs/ARK/workplacegroup#overview","content":" As part of the Aria Research Kit (ARK), Academic Research Partners get access to theProject Aria Academic Partner Announcements Feedback and Support Workplace Group.  In addition to being the place where we make announcements, we want to support researchers to:  Provide feedback about their Aria experiencesParticipate in general discussions about AriaAsk our community of researchers and engineers questions (support related or exploring an idea)Engage with other academic researchers and perhaps even set challenges for each other  Once you've joined this group you can post support queries in the group or directly message our user support team.  For academic collaboration This group is specifically for Academic Research Partners who are working with Aria devices. Corporate partners are not added to this workplace group.  ","version":"Next","tagName":"h2"},{"title":"How to Join​","type":1,"pageTitle":"How to Join the Academic Partners Workplace Group","url":"/projectaria_tools/docs/ARK/workplacegroup#how-to-join","content":" Please do not add extra people to this group If someone who is approved for the ARK and has received Aria glasses is missing, please let your Meta point of contact know.  When Academic Partners onboard with Project Aria they receive two emails. One will contain account credentials that you'll use the Companion Apps and the MPS CLI.  The second email is an invitation to join the Project Aria Workplace Group. The email will be from notification@fbworkmail.com and have the subject “Join [person] in Project Aria Academic Partner Announcements, Feedback and Support”.  ","version":"Next","tagName":"h2"},{"title":"To join the Workplace group:​","type":1,"pageTitle":"How to Join the Academic Partners Workplace Group","url":"/projectaria_tools/docs/ARK/workplacegroup#to-join-the-workplace-group","content":" Find the email invitation with the subject Join [Person] in Project Aria Academic Partner Announcements, Feedback &amp; Support and select “Join [person]” If you don't have this email invitation, please email AriaOps@meta.com In your browser, follow the prompts to create a new Workplace To set up a workplace group you just need to answer a few questions: your name, type of role, organization and organization size. Once you've set up your workplace, you should see a prompt saying that you've been invited into a closed multi-company group.Select Accept InviteYou should then see “Welcome [Your Name]!”Select Skip or Next If you can't see Skip or Next, scroll down the page. The next button might not be immediately visible on some mobile devices Follow the prompts until you get to the workplace group You may need to scroll further down the page if you do not see the Next button  ","version":"Next","tagName":"h3"},{"title":"Accessing the Workplace Group​","type":1,"pageTitle":"How to Join the Academic Partners Workplace Group","url":"/projectaria_tools/docs/ARK/workplacegroup#accessing-the-workplace-group","content":" You should be able to continue from your workplace group creation. In addition, once you've created your workplace group you should get an email fromnotification@fbworkmail.com with the subject “[Your Workplace Group name] via Workplace”.  Select Log into Workplace and enter your credentialsYou should see your groups displayed in the Home section On Mobile, select the menu icon on the top right of your screen to view Home ","version":"Next","tagName":"h3"},{"title":"Aria Training and Evaluation toolkit (ATEK)","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/ATEK/about_ATEK","content":"","keywords":"","version":"Next"},{"title":"What is ATEK?​","type":1,"pageTitle":"Aria Training and Evaluation toolkit (ATEK)","url":"/projectaria_tools/docs/ATEK/about_ATEK#what-is-atek","content":" Project Aria has released various public datasets for academic researchers over the past year, such as Aria Everyday Activities, Aria Digital Twin, Aria Synthetic Environments, Nymeria and HOT3D. It is exciting to observe the growing usage of Aria public datasets from the community to accelerate egocentric machine perception tasks. ATEK is designed to accelerate development cycles when working on Deep-Learning related tasks that use Aria data.  ","version":"Next","tagName":"h2"},{"title":"4 key features to accelerate your workflow​","type":1,"pageTitle":"Aria Training and Evaluation toolkit (ATEK)","url":"/projectaria_tools/docs/ATEK/about_ATEK#4-key-features-to-accelerate-your-workflow","content":" An easy-to-use data preprocessing library for Aria datasets for converting VRS-format Aria datasets in to datasets in PyTorch-compatible formatDatastore of preprocessed data sets in WebDataset formatStandardized evaluation libraries that supports the following ML perception tasks for Aria: static 3D object detection3D surface reconstruction Notebooks and script examples including model training, inference, and visualization  AI and ML researchers can learn more, by reading the documentation hereIf you encounter any issues, have feedback or questions, go to our Support page for multiple ways to get in touch! ","version":"Next","tagName":"h2"},{"title":"Collaborative Tools","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/collaborative_tools","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Collaborative Tools","url":"/projectaria_tools/docs/collaborative_tools#overview","content":" This page provides an overview of open source tooling can be useful when working with Project Aria data.  ","version":"Next","tagName":"h2"},{"title":"Used in Project Aria Tools​","type":1,"pageTitle":"Collaborative Tools","url":"/projectaria_tools/docs/collaborative_tools#used-in-project-aria-tools","content":" Some of the open source tools we use are:  ","version":"Next","tagName":"h2"},{"title":"VRS​","type":1,"pageTitle":"Collaborative Tools","url":"/projectaria_tools/docs/collaborative_tools#vrs","content":" VRS is the open source file format used by Project Aria. It is optimized to record &amp; playback streams of sensor data, such as images, audio samples, and any other discrete sensors (IMU, temperature, etc), stored in per-device streams of time-stamped records.  How Project Aria uses VRSVRS ReadmeVRS Core functionality  ","version":"Next","tagName":"h3"},{"title":"Rerun​","type":1,"pageTitle":"Collaborative Tools","url":"/projectaria_tools/docs/collaborative_tools#rerun","content":" Rerun is an open source SDK and engine for visualizing and interacting with multi modal data streams. It can be used from Python, Rust and C++ and provides a log API and a visualizer. Project Aria Tools uses it to create Python visualization tools.  Python Visualization using Project Aria ToolsAn introduction to RerunRerun Readme  ","version":"Next","tagName":"h3"},{"title":"Open source tools that use Project Aria​","type":1,"pageTitle":"Collaborative Tools","url":"/projectaria_tools/docs/collaborative_tools#open-source-tools-that-use-project-aria","content":" ","version":"Next","tagName":"h2"},{"title":"Nerfstudio​","type":1,"pageTitle":"Collaborative Tools","url":"/projectaria_tools/docs/collaborative_tools#nerfstudio","content":" Nerfstudio provides an API that allows for a simplified end-to-end process for creating, training, and testing NeRFs. The library supports a more interpretable implementation of NeRFs by modularizing each component.  Nerfstudio with Project Aria dataNerfstudio ReadmeGaussian Splatting with Nerfstudio ","version":"Next","tagName":"h3"},{"title":"Project Aria VRS","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_formats/aria_vrs","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Project Aria VRS","url":"/projectaria_tools/docs/data_formats/aria_vrs#overview","content":" This page provides an overview of how Project Aria uses VRS. The other pages in this section are:  Project Aria VRS FormatVRS Files Timestamps  ","version":"Next","tagName":"h2"},{"title":"How Project Aria uses VRS​","type":1,"pageTitle":"Project Aria VRS","url":"/projectaria_tools/docs/data_formats/aria_vrs#how-project-aria-uses-vrs","content":" Project Aria chose VRS as its data container because it is a file format designed to record and playback streams of XR (extended reality) sensor data and supports huge file sizes. These VRS files contain streams of time-sorted records generated for each sensor, with one set of sensors per stream. Project Aria data uses VRS for features such as:  Records are structured as a succession of typed content blocks.Project Aria Tools recordings are structured following this VRS DataLayout. These definitions provide an overview of what information can be extracted for each stream from a Project Aria sequence.Streams contain Configuration, State and Data records, each with a timestamp in a common time domain for the whole file.Playback is optimized for timestamp order, which is key for network streaming.Random-access playback is supported via VRS.  Project Aria Tools aims to provide effective tools, APIs and wrappers for working with VRS, so working with VRS tools directly should not be necessary when working with Aria data. If there are VRS functions you wish Project Aria Tools has, please contact us using any of our Support Channels.  ","version":"Next","tagName":"h2"},{"title":"Further resources​","type":1,"pageTitle":"Project Aria VRS","url":"/projectaria_tools/docs/data_formats/aria_vrs#further-resources","content":" VRS ReadmeVRS Core Functionality ","version":"Next","tagName":"h3"},{"title":"Project Aria Data Formats","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_formats","content":"Project Aria Data Formats In this section, we describe: How Project Aria uses VRS to store raw data Project Aria VRS FormattingTimestamps in Aria VRS Files How Machine Perception Services (MPS) data is formatted SLAM (6DoF Trajectory, Semi-Dense Point Cloud, Online Sensor Calibration and Multi-SLAM)Eye Gaze with Depth EstimationHand Tracking (21 landmarks, 6DoF wrist to device transform, wrist and palm normals) Project Aria Coordinate Conventions 2D Image Coordinate System Conventions (intrinsic calibration values)3D Coordinate Frame Conventions for Project Aria Glasses (extrinisic calibration 6-DoF poses, 3D Coordinate frame and system conventions, non-visual sensor coordinate systems and Central Pupil Frame (CPF))","keywords":"","version":"Next"},{"title":"Attribution and Contributing","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/attribution_citation","content":"","keywords":"","version":"Next"},{"title":"Citation​","type":1,"pageTitle":"Attribution and Contributing","url":"/projectaria_tools/docs/attribution_citation#citation","content":" ","version":"Next","tagName":"h2"},{"title":"Project Aria Tools​","type":1,"pageTitle":"Attribution and Contributing","url":"/projectaria_tools/docs/attribution_citation#project-aria-tools","content":" If you use Project Aria tools or data in your research, please consider starring ⭐ our github repository, and citing the Project Aria Whitepaper.  @misc{engel2023project, title={Project Aria: A New Tool for Egocentric Multi-Modal AI Research}, author={Jakob Engel and Kiran Somasundaram and Michael Goesele and Albert Sun and Alexander Gamino and Andrew Turner and Arjang Talattof and Arnie Yuan and Bilal Souti and Brighid Meredith and Cheng Peng and Chris Sweeney and Cole Wilson and Dan Barnes and Daniel DeTone and David Caruso and Derek Valleroy and Dinesh Ginjupalli and Duncan Frost and Edward Miller and Elias Mueggler and Evgeniy Oleinik and Fan Zhang and Guruprasad Somasundaram and Gustavo Solaira and Harry Lanaras and Henry Howard-Jenkins and Huixuan Tang and Hyo Jin Kim and Jaime Rivera and Ji Luo and Jing Dong and Julian Straub and Kevin Bailey and Kevin Eckenhoff and Lingni Ma and Luis Pesqueira and Mark Schwesinger and Maurizio Monge and Nan Yang and Nick Charron and Nikhil Raina and Omkar Parkhi and Peter Borschowa and Pierre Moulon and Prince Gupta and Raul Mur-Artal and Robbie Pennington and Sachin Kulkarni and Sagar Miglani and Santosh Gondi and Saransh Solanki and Sean Diener and Shangyi Cheng and Simon Green and Steve Saarinen and Suvam Patra and Tassos Mourikis and Thomas Whelan and Tripti Singh and Vasileios Balntas and Vijay Baiyya and Wilson Dreewes and Xiaqing Pan and Yang Lou and Yipu Zhao and Yusuf Mansour and Yuyang Zou and Zhaoyang Lv and Zijian Wang and Mingfei Yan and Carl Ren and Renzo De Nardi and Richard Newcombe}, year={2023}, eprint={2308.13561}, archivePrefix={arXiv}, primaryClass={cs.HC} }   ","version":"Next","tagName":"h3"},{"title":"Datasets​","type":1,"pageTitle":"Attribution and Contributing","url":"/projectaria_tools/docs/attribution_citation#datasets","content":" Aria Digital Twin​  If you use Aria Digital Twin dataset and its tools, please cite the Aria Digital Twin dataset paper:  @InProceedings{Pan_2023_ICCV, author = {Pan, Xiaqing and Charron, Nicholas and Yang, Yongqian and Peters, Scott and Whelan, Thomas and Kong, Chen and Parkhi, Omkar and Newcombe, Richard and Ren, Yuheng (Carl)}, title = {Aria Digital Twin: A New Benchmark Dataset for Egocentric 3D Machine Perception}, booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}, month = {October}, year = {2023}, pages = {20133-20143} }   HOT3D Dataset​  If you use HOT3D dataset and its tools, please cite the Aria Digital Twin dataset paper:  @misc{banerjee2024introducinghot3degocentricdataset, title={Introducing HOT3D: An Egocentric Dataset for 3D Hand and Object Tracking}, author={Prithviraj Banerjee and Sindi Shkodrani and Pierre Moulon and Shreyas Hampali and Fan Zhang and Jade Fountain and Edward Miller and Selen Basol and Richard Newcombe and Robert Wang and Jakob Julian Engel and Tomas Hodan}, year={2024}, eprint={2406.09598}, archivePrefix={arXiv}, primaryClass={cs.CV}, url={https://arxiv.org/abs/2406.09598}, }   Aria Everyday Activities Dataset​  If you use the Aria Pilot dataset, please cite  @misc{lv2024ariaeverydayactivitiesdataset, title={Aria Everyday Activities Dataset}, author={Zhaoyang Lv and Nicholas Charron and Pierre Moulon and Alexander Gamino and Cheng Peng and Chris Sweeney and Edward Miller and Huixuan Tang and Jeff Meissner and Jing Dong and Kiran Somasundaram and Luis Pesqueira and Mark Schwesinger and Omkar Parkhi and Qiao Gu and Renzo De Nardi and Shangyi Cheng and Steve Saarinen and Vijay Baiyya and Yuyang Zou and Richard Newcombe and Jakob Julian Engel and Xiaqing Pan and Carl Ren}, year={2024}, eprint={2402.13349}, archivePrefix={arXiv}, primaryClass={cs.CV}, url={https://arxiv.org/abs/2402.13349}, }   Aria Synthetic Environments Dataset​  If you use Aria Synthetic Environments dataset and its tools, please cite the Aria Synthetic Environments dataset paper (coming soon).  ","version":"Next","tagName":"h3"},{"title":"Contributing​","type":1,"pageTitle":"Attribution and Contributing","url":"/projectaria_tools/docs/attribution_citation#contributing","content":" We welcome contributions! See CONTRIBUTING for details about how to get started, and our code of conduct. ","version":"Next","tagName":"h2"},{"title":"VRS command line tool","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_formats/aria_vrs/aria_vrs_command_line","content":"","keywords":"","version":"Next"},{"title":"File validation​","type":1,"pageTitle":"VRS command line tool","url":"/projectaria_tools/docs/data_formats/aria_vrs/aria_vrs_command_line#file-validation","content":" _Commands: check, checksum  ","version":"Next","tagName":"h2"},{"title":"File Integrity Validation​","type":1,"pageTitle":"VRS command line tool","url":"/projectaria_tools/docs/data_formats/aria_vrs/aria_vrs_command_line#file-integrity-validation","content":" The check command &quot;simply&quot; decodes every record in the VRS file and prints how many records were decoded successfully. It proves that the VRS file is correct at the VRS level.  vrs check file.vrs   ","version":"Next","tagName":"h3"},{"title":"Checksums​","type":1,"pageTitle":"VRS command line tool","url":"/projectaria_tools/docs/data_formats/aria_vrs/aria_vrs_command_line#checksums","content":" A logical data checksum can be generated for a VRS file. Could be use to ensure data integrity while doing VRS file transfer/Download when providing datasets.  vrs checksum file.vrs   ","version":"Next","tagName":"h3"},{"title":"File inspection​","type":1,"pageTitle":"VRS command line tool","url":"/projectaria_tools/docs/data_formats/aria_vrs/aria_vrs_command_line#file-inspection","content":" To peek into a VRS file, simply run:  vrs sample_file.vrs   It will print a lot of information, that can be overwhelming the first time you see what's in a VRS file!  To have a simpler view and list only the the available streams and number of records you can do:  vrs sample_file.vrs | grep records   ","version":"Next","tagName":"h2"},{"title":"Trimming​","type":1,"pageTitle":"VRS command line tool","url":"/projectaria_tools/docs/data_formats/aria_vrs/aria_vrs_command_line#trimming","content":" You will find here instructions on how to TRIM data from an existing VRS file (instructions can be combined):  Trimming by stream_idTrimming by time  ","version":"Next","tagName":"h2"},{"title":"Stream removal​","type":1,"pageTitle":"VRS command line tool","url":"/projectaria_tools/docs/data_formats/aria_vrs/aria_vrs_command_line#stream-removal","content":" _Options: [+|-]&lt;stream_id&gt;  As an example, we are here removing the EyeTracking stream ([&quot;211-1&quot;]):  vrs copy original.vrs --to new.vrs - 211   ","version":"Next","tagName":"h3"},{"title":"Time cropping​","type":1,"pageTitle":"VRS command line tool","url":"/projectaria_tools/docs/data_formats/aria_vrs/aria_vrs_command_line#time-cropping","content":" Options: --before [+|-]&lt;max-timestamp&gt;, --after [+|-]&lt;min-timestamp&gt;, --range [+|-]&lt;min-timestamp&gt; [+|-]&lt;max-timestamp&gt;, --around [+|-]&lt;timestamp&gt; &lt;time-range&gt;.  To use _relative timing, i.e 10 seconds of a file while skipping the first 2 seconds of data records, use:  vrs copy original.vrs --to new.vrs --range +2 +12   You can also choose to trim only from a given relative time with (i.e. keep everything after the first 2 seconds):  vrs copy original.vrs --to new.vrs --after +2   To use _absolute timestamps, use:  vrs copy original.vrs --to new.vrs --range 149002622 158702622   For more in-depth details feel free to explore this VRS Cli Tool page.  ","version":"Next","tagName":"h3"},{"title":"VRS tools installation​","type":1,"pageTitle":"VRS command line tool","url":"/projectaria_tools/docs/data_formats/aria_vrs/aria_vrs_command_line#vrs-tools-installation","content":" Please follow our VRS Tools installation guide ","version":"Next","tagName":"h2"},{"title":"VRS Tools installation","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_formats/aria_vrs/aria_vrs_tools_installation","content":"VRS Tools installation VRS Tools can easily be installed on your system using our prepackaged versions. CondaPixi # conda https://conda.io/projects/conda/en/latest/user-guide/install/index.html conda install vrs --channel=conda-forge ","keywords":"","version":"Next"},{"title":"Project Aria VRS Format","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_formats/aria_vrs/aria_vrs_format","content":"","keywords":"","version":"Next"},{"title":"Aria data streams​","type":1,"pageTitle":"Project Aria VRS Format","url":"/projectaria_tools/docs/data_formats/aria_vrs/aria_vrs_format#aria-data-streams","content":" In VRS, data is organized by streams. Each stream stores the data measured by a specific sensor, except for Eye Tracking (both cameras share a single data stream) and microphones (up to 7 microphones share a single stream).  The streams are identified by their stream ID. Each stream ID is composed of two parts, a recordable type ID to categorize the type of the sensor and a stream ID for identify the specific sensor instance. E.g. the first SLAM (aka Mono Scene) camera is identified as 1201-1 where 1201 is the numerical ID for SLAM camera data type, and 1 identifies the cameras as the first instance.  Streams can also be identified by a short label. Labels are used to identify sensors in calibration.  The following table lists the Stream ID and Recordable Type ID, as well as their label. GPS, Wi-Fi and Bluetooth have labels, but are not calibrated. If you use projectaria tools loaders, you do not have to memorize this mapping as there is an API that converts between Stream ID and labels.  Table 1: IDs Used for Sensors   Sensor Stream ID Recordable Type ID label ET camera 211-1 EyeCameraRecordableClass camera-et RGB camera 214-1 RgbCameraRecordableClass camera-rgb Microphone 231-1 StereoAudioRecordableClass mic Barometer 247-1 BarometerRecordableClass baro GPS 281-1 GpsRecordableClass gps GPS App 281-2 GpsRecordableClass gps-app Wi-Fi 282-1 WifiBeaconRecordableClass\twps Bluetooth 283-1 BluetoothBeaconRecordableClass\tbluetooth SLAM/Mono Scene camera left 1201-1 SlamCameraData camera-slam-left SLAM/Mono Scene camera right\t1201-2 SlamCameraData camera-slam-right IMU (1kHz) 1202-1 SlamImuData imu-right IMU (800Hz) 1202-2 SlamImuData imu-left Magnetometer 1203-1 SlamMagnetometerData mag   Each stream also contains a configuration blob that stores sensor-specific information such as image resolution and nominal frame rate.  All data in VRS is timestamped. Go to Timestamps in Aria VRS for more details.  tip We recommend using Trajectory MPS outputs instead of raw IMU data wherever possible. Go to MPS Code Snippets for how to load open loop or closed loop trajectory data.  ","version":"Next","tagName":"h2"},{"title":"Aria sensor data and configuration​","type":1,"pageTitle":"Project Aria VRS Format","url":"/projectaria_tools/docs/data_formats/aria_vrs/aria_vrs_format#aria-sensor-data-and-configuration","content":" Sensor data includes:  Sensor readoutTimestampsAcquisition parameters (exposure and gain settings)Conditions (e.g. temperature) during data collection  Most sensor data of a single stream and at a specific timestamp is stored as a single piece, except for image and audio.  ","version":"Next","tagName":"h2"},{"title":"How data is stored for image recordings​","type":1,"pageTitle":"Project Aria VRS Format","url":"/projectaria_tools/docs/data_formats/aria_vrs/aria_vrs_format#how-data-is-stored-for-image-recordings","content":" Each camera stores a single image frame at a time, with the exception of the ET camera. ET cameras pair share a single image frame by concatenating horizontally. The image frame contains two parts, the image itself and the image record. The image record stores timestamps, frame id, and acquisition parameters, such as exposure and gain. This avoids having to read image data to get the information in the record.  ","version":"Next","tagName":"h3"},{"title":"How data is stored for audio recordings​","type":1,"pageTitle":"Project Aria VRS Format","url":"/projectaria_tools/docs/data_formats/aria_vrs/aria_vrs_format#how-data-is-stored-for-audio-recordings","content":" The audio data is grouped into data chunks of 4096 audio samples from all 7 microphones.Each chunk contains two parts, the data part for the audio signal, and the report part for the timestamps of each audio signal.  ","version":"Next","tagName":"h3"},{"title":"Sensor configuration blob​","type":1,"pageTitle":"Project Aria VRS Format","url":"/projectaria_tools/docs/data_formats/aria_vrs/aria_vrs_format#sensor-configuration-blob","content":" The sensor configuration blob stores the static information of a stream. Common sensor configuration stores information, such as sensor model, sensor serial (if available) as well as frame rate.  Stream-specific information, such as image resolution, is also stored in configurations.  Go to the source code for the detailed implementation of sensor data and configurations. Go to the Advanced Code Snippets for example sensor data and how to access sensor data using Python data utilities.  ","version":"Next","tagName":"h3"},{"title":"Useful VRS tools​","type":1,"pageTitle":"Project Aria VRS Format","url":"/projectaria_tools/docs/data_formats/aria_vrs/aria_vrs_format#useful-vrs-tools","content":" The most intuitive way to access Aria data is via our loaders and visualizers. We provide Python and C++ interface to easily access VRS data.  You may also want to use VRS tools to extract or inspect VRS data. Here are a few common use cases:  ","version":"Next","tagName":"h2"},{"title":"Check the VRS file’s validity and integrity​","type":1,"pageTitle":"Project Aria VRS Format","url":"/projectaria_tools/docs/data_formats/aria_vrs/aria_vrs_format#check-the-vrs-files-validity-and-integrity","content":" The check command decodes every record in the VRS file and prints how many records were decoded successfully. It proves that the VRS file is correct at the VRS level. You can also compute a checksum to ensure you have valid VRS files. For more information go to VRS File Validation.  $ vrs check &lt;file.vrs&gt; $ vrs checksum &lt;file.vrs&gt;   If the file is not valid, it is normally because there is missing data that could lead to invalid behavior with the tooling. All files in our open datasets are valid, so if you encounter issues with these, re-downloading the files should resolve the issue.  ","version":"Next","tagName":"h3"},{"title":"Extract image or audio content to folders​","type":1,"pageTitle":"Project Aria VRS Format","url":"/projectaria_tools/docs/data_formats/aria_vrs/aria_vrs_format#extract-image-or-audio-content-to-folders","content":" Use the following commands to extract JPEG or WAV files. Use the --to &lt;folder_path&gt; to specify a destination folder where the data will be extracted, or it will be added to the current working directory.  $ vrs extract-images &lt;file.vrs&gt; --to &lt;image_folder&gt; $ vrs extract-audio &lt;file.vrs&gt; --to &lt;audio_folder&gt;   To extract RAW image files, use:  vrs extract-images &lt;file.vrs&gt; --raw-images --to &lt;image_folder&gt;   ","version":"Next","tagName":"h3"},{"title":"Extract all content to folders and JSONs​","type":1,"pageTitle":"Project Aria VRS Format","url":"/projectaria_tools/docs/data_formats/aria_vrs/aria_vrs_format#extract-all-content-to-folders-and-jsons","content":" This command lets you extract all images, audio, and metadata into files:  vrs extract-all &lt;file.vrs&gt; --to &lt;folder&gt;   The metadata is extracted into a single JSONS file that contains a succession of json messages, one per line. Each line corresponds to a single record, in timestamp order, so it is possible to parse it even if the number of records is huge. Saving all the data in a single file prevents saturating your disk with possibly millions of small files. Once extracted, your file will look like this:   ├── file.vrs ├── all_data * `NNNN-MM` folders: image folders, one folder per stream containing images. ├── 1201-1 # SLAM Left images ├── *.jpg ├── 1201-2 # SLAM Right images ├── *.jpg ├── 211-1 # Eye Tracking images ├── *.jpg ├── 214-1 # RGB (Color) Camera images ├── *.jpg ├── metadata.jsons └── ReadMe.md   For more information, go to VRS Data Extraction.  ","version":"Next","tagName":"h3"},{"title":"Inspect how many data recordings there are by type​","type":1,"pageTitle":"Project Aria VRS Format","url":"/projectaria_tools/docs/data_formats/aria_vrs/aria_vrs_format#inspect-how-many-data-recordings-there-are-by-type","content":" vrs &lt;file.vrs&gt; | grep &quot;] records.&quot;   Will get you a return like this:  623 Eye Camera Class #1 - device/aria [211-1] records. 1244 RGB Camera Class #1 - device/aria [214-1] records. 729 Stereo Audio Class #1 - device/aria [231-1] records. 3101 Barometer Data Class #1 - device/aria [247-1] records. 65 Time Domain Mapping Class #1 - device/aria [285-1] records. 623 Camera Data (SLAM) #1 - device/aria [1201-1] records. 623 Camera Data (SLAM) #2 - device/aria [1201-2] records. 61965 IMU Data (SLAM) #1 - device/aria [1202-1] records. 50002 IMU Data (SLAM) #2 - device/aria [1202-2] records. 619 Magnetometer Data (SLAM) #1 - device/aria [1203-1] records.   Each line reports how many data records are stored in each data stream as well as the stream ID. For example, in this line:  623 Camera Data (SLAM) #2 - device/aria [1201-2] records.   We can see that:  The stream name is Camera Data (SLAM) #2 (Mono Scene camera on the right) and identified by numerical ID [1201-2]SLAM camera #2 has recorded 623 frames ","version":"Next","tagName":"h3"},{"title":"The VRSPlayer App","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_formats/aria_vrs/aria_vrsplayer","content":"","keywords":"","version":"Next"},{"title":"Open a file​","type":1,"pageTitle":"The VRSPlayer App","url":"/projectaria_tools/docs/data_formats/aria_vrs/aria_vrsplayer#open-a-file","content":" You can either play a file from the File menu, or using the command line vrsplayer sample_file.vrs.  If you want to know more about VRSPlayer, please visit the VRSPlayer VRS's documentation page.  ","version":"Next","tagName":"h2"},{"title":"VRS tools installation​","type":1,"pageTitle":"The VRSPlayer App","url":"/projectaria_tools/docs/data_formats/aria_vrs/aria_vrsplayer#vrs-tools-installation","content":" Please follow our VRS Tools installation guide ","version":"Next","tagName":"h2"},{"title":"Project Aria Glasses 2D Image Coordinate System Conventions","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_formats/coordinate_convention/2d_image_coordinate_system_convention","content":"Project Aria Glasses 2D Image Coordinate System Conventions For any provided camera intrinsic calibration value, we use the convention that the color value of a pixel with integer coordinates (u,v)(u,v)(u,v) is the average color of the square spanning from (u−0.5,v−0.5)(u-0.5,v-0.5)(u−0.5,v−0.5) to (u+0.5,v+0.5)(u+0.5,v+0.5)(u+0.5,v+0.5) in continuous coordinates. This is visualized in the Figure 1, and has the following important consequences: Checking in bound: A pixel (u,v)(u,v)(u,v) is considered to be in bound if −0.5≤u&lt;W−0.5-0.5\\leq u&lt;W-0.5−0.5≤u&lt;W−0.5 and −0.5≤v&lt;H−0.5-0.5\\leq v&lt;H-0.5−0.5≤v&lt;H−0.5.Interpolation: In bilinear interpolation, a point (u,v) can be interpolated of all four neighboring integer-valued pixel coordinates are in-bound. That requires 0≤u≤W−10 \\leq u \\leq W-10≤u≤W−1 and 0≤v≤H−10 \\leq v \\leq H-10≤v≤H−1.Image down-sampling: When downsampling images by a factor of sss, every s×ss \\times ss×s pixel are squeezed into a single pixel. For example, the intensity at pixel s×ss \\times ss×s in the scaled image accounts for all the photons collected in the area [−0.5,s−0.5]×[−0.5,s−0.5][-0.5,s-0.5]\\times[-0.5,s-0.5][−0.5,s−0.5]×[−0.5,s−0.5] (i.e. column 000 to s−1s - 1s−1, and row 000 to s−1s-1s−1 in the discrete coordinate) in the original image. In order to keep this assumption valid, the re-scaled point pscaledp_\\text{scaled}pscaled​ not only needs to scale from the corresponding point in the original image poriginalp_\\text{original}poriginal​ but also accounts for the (0.5,0.5)(0.5,0.5)(0.5,0.5) translation accordingly by pscaled=s(poriginal+0.5)−0.5p_\\text{scaled} =s (p_\\text{original}+0.5)-0.5pscaled​=s(poriginal​+0.5)−0.5 Figure 1: 2D Image Coordinate System Conventions Go to the Project Aria FAQ for more calibration information and resources.","keywords":"","version":"Next"},{"title":"Timestamps in Aria VRS Files","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_formats/aria_vrs/timestamps_in_aria_vrs","content":"","keywords":"","version":"Next"},{"title":"VRS Timestamps (Single Device)​","type":1,"pageTitle":"Timestamps in Aria VRS Files","url":"/projectaria_tools/docs/data_formats/aria_vrs/timestamps_in_aria_vrs#vrs-timestamps-single-device","content":" ","version":"Next","tagName":"h2"},{"title":"Device timestamps​","type":1,"pageTitle":"Timestamps in Aria VRS Files","url":"/projectaria_tools/docs/data_formats/aria_vrs/timestamps_in_aria_vrs#device-timestamps","content":" We strongly recommend always working with device timestamp when working with single-device Aria data.  TimeDomain.DEVICE_TIME Each piece of data captured by Project Aria glasses is associated with a device timestamp. Also called capture timestamp in the VRS file format All sensors on the same pair of Aria glasses share the same device time domain issued from a single clock.  ","version":"Next","tagName":"h3"},{"title":"Record and Host(Arrival) timestamps​","type":1,"pageTitle":"Timestamps in Aria VRS Files","url":"/projectaria_tools/docs/data_formats/aria_vrs/timestamps_in_aria_vrs#record-and-hostarrival-timestamps","content":" When working with Aria data you might encounter timestamps for different time events:  TimeDomain.RECORD_TIME Record timestampsTimestamps stored in the index of VRS files.For Project Aria glasses, these are equal to the device timestamp converted to a double-precision floating point representation. TimeDomain.HOST_TIME Host or arrival timestampsTimestamps when the sensor data is saved to the device Note: this timestamp does not represent the timestamp when the sensor data is captured. Please use TimeDomain.DEVICE_TIME to access the capture timestamp Should not be needed for any purpose  ","version":"Next","tagName":"h3"},{"title":"Audio timestamp​","type":1,"pageTitle":"Timestamps in Aria VRS Files","url":"/projectaria_tools/docs/data_formats/aria_vrs/timestamps_in_aria_vrs#audio-timestamp","content":" For audio data, each record contains 2048 samples that ranges 42ms (or 4096 samples that ranges 84ms). Given a record (a chunk) of audio data, the record timestamp is the first timestamp in the record and the device timestamp is the last timestamp in the record. Therefore, device timestamp - record timestamp for the same record is 42ms for 2048 samples (and 84ms for 4096 samples).  ","version":"Next","tagName":"h3"},{"title":"VRS Timestamps (Multiple Devices)​","type":1,"pageTitle":"Timestamps in Aria VRS Files","url":"/projectaria_tools/docs/data_formats/aria_vrs/timestamps_in_aria_vrs#vrs-timestamps-multiple-devices","content":" Accurate time synchronization is essential when co-ordinating data collection or analyzing data between multiple devices (real world or synthetic). Without synchronization, any device’s built-in recording of time will naturally drift and go out of sync (like how your microwave slowly loses time over a year).  ","version":"Next","tagName":"h2"},{"title":"TimeDomain.TIME_CODE​","type":1,"pageTitle":"Timestamps in Aria VRS Files","url":"/projectaria_tools/docs/data_formats/aria_vrs/timestamps_in_aria_vrs#timedomaintime_code","content":" Multiple devices (either multiple Aria glasses or Aria glasses plus other devices) that are temporally aligned using an external shared clock will include the TimeDomain.TIME_CODE datastream in the VRS file.  We use time sync servers to record pairs of timestamps between the server’s local timestamp and the Aria glasses’ device timestamp. This generates a mapping between the Aria’s device time and the server’s local time. The server’s local time serves as a unified time domain shared by the multiple devices.  Timecode time refers to the same “capture” event as device time, but differs by the clock assigning the timestamps. Thus we can convert between timecode time and device time by looking up values in the time mapping table.  ","version":"Next","tagName":"h3"},{"title":"TimeDomain.TICSync​","type":1,"pageTitle":"Timestamps in Aria VRS Files","url":"/projectaria_tools/docs/data_formats/aria_vrs/timestamps_in_aria_vrs#timedomainticsync","content":" TICSync is an extremely efficient algorithm for learning the mapping between distributed clocks, which typically achieves better than millisecond accuracy within just a few seconds. It works by estimating clock offset and skew between itself (the host/client) and a device (Aria leader or server).  In a TICSync recording, all devices mark video frames with a timestamp in a conceptual TICSync time domain. During the recording, the TICSync algorithm constructs, on-the-fly, the mapping between the conceptual TICSync time domain and the concrete DEVICE_TIME time domains of the glasses. Under the current implementation, the unique server device uses its DEVICE_TIME as the conceptual TICSync time, while all clients use their concrete TIC_SYNC time domains. ","version":"Next","tagName":"h3"},{"title":"3D Coordinate Frame Conventions for Project Aria Glasses","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_formats/coordinate_convention/3d_coordinate_frame_convention","content":"","keywords":"","version":"Next"},{"title":"SE(3) Lie groups​","type":1,"pageTitle":"3D Coordinate Frame Conventions for Project Aria Glasses","url":"/projectaria_tools/docs/data_formats/coordinate_convention/3d_coordinate_frame_convention#se3-lie-groups","content":" Extrinsics in calibration refer to the 6-DoF pose among the sensors. These 6-DoF poses are represented by SE(3) Lie group. The quaternion part of SE(3) uses Hamilton convention following the Eigen library, in which the exact formula to convert a quaternion to a rotation matrix of the SE(3) can be found in the Eigen code repository.  We use the SE3d class in the Sophus Library to represent SE(3) Lie groups, and provide a minimal pybind for the class.  ","version":"Next","tagName":"h2"},{"title":"A note on sensor naming and motivation​","type":1,"pageTitle":"3D Coordinate Frame Conventions for Project Aria Glasses","url":"/projectaria_tools/docs/data_formats/coordinate_convention/3d_coordinate_frame_convention#a-note-on-sensor-naming-and-motivation","content":" T_sensor1_sensor2 represents a relative SE(3) transformation from sensor2 frame to sensor1 frame. An easy mnemonic is the chaining principle is: T_sensor1sensor2 * Tsensor2sensor3 * psensor3 = p_sensor1 (where p_sensor is a 3D point measured from sensor)  ","version":"Next","tagName":"h3"},{"title":"Code​","type":1,"pageTitle":"3D Coordinate Frame Conventions for Project Aria Glasses","url":"/projectaria_tools/docs/data_formats/coordinate_convention/3d_coordinate_frame_convention#code","content":" PythonC++ transform_a_b represents a SE(3) rigid transformation from b coordinate frame to a coordinate frame. p_a represents an R^3 point (or vector) in the coordinate system of a. Easy mnemonics of the chaining principle (a, b, c are coordinate frames): transform_a_c = transform_a_b @ transform_b_c; p_a = transform_a_b @ p_b If you want to get quaternion from the SE3d, please notice the order is consistent to numpy quaternion_a_b = transform_a_b.to_quat() # order is w, x, y, z   3D Coordinate frame conventions  Every sensor on Aria glasses has their own local coordinate system. We represent the 6DoF pose of each sensor as the relative pose (rotation and translation) with regard to the “Device frame&quot;. The device frame is by-default the local frame of the left Mono Scene (SLAM) camera.    ","version":"Next","tagName":"h3"},{"title":"Camera coordinate system convention​","type":1,"pageTitle":"3D Coordinate Frame Conventions for Project Aria Glasses","url":"/projectaria_tools/docs/data_formats/coordinate_convention/3d_coordinate_frame_convention#camera-coordinate-system-convention","content":" A camera's local frame has its origin at the camera's optical center. Coarsely, when the camera is placed up-right, the camera coordinate frame's axes points to left, up and forward.  More rigorously, we define a camera's local frame based on the optical axis and the entrance pupil of its lens. Both are uniquely defined for each camera according to the camera's lens prescription. The origin of a camera's local frame is at center of the camera's entrance pupil. The frame's Z axis is aligned with the optical axis. The camera's X axis are aligned with the projection of the image plane's X axis on the entrance pupil plane. The cross-product of the X and Z axis defines the system's Y axis.    ","version":"Next","tagName":"h2"},{"title":"Non-visual sensor coordinate system​","type":1,"pageTitle":"3D Coordinate Frame Conventions for Project Aria Glasses","url":"/projectaria_tools/docs/data_formats/coordinate_convention/3d_coordinate_frame_convention#non-visual-sensor-coordinate-system","content":" We choose the IMU coordinate systems to have their origins at the position of the accelerometer, oriented along the direction of the accelerometer sensitive axis, eventually orthogonalized to compensate for sensor orthogonalities error. We use a similar arrangement for the magnetometer.    ","version":"Next","tagName":"h2"},{"title":"The nominal Central Pupil Frame (CPF)​","type":1,"pageTitle":"3D Coordinate Frame Conventions for Project Aria Glasses","url":"/projectaria_tools/docs/data_formats/coordinate_convention/3d_coordinate_frame_convention#the-nominal-central-pupil-frame-cpf","content":" The CPF frame is placed at the midpoint between the eye boxes of the left and right eye. CPF's X-axis points left, Y-axis points up and the Z-axis points forward, from the person's perspective. Aria's ET gaze is defined as a vector in the CPF space originating at (0,0,0)(0, 0, 0)(0,0,0) of the CPF frame.   ","version":"Next","tagName":"h2"},{"title":"MPS Output - Hand Tracking","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_formats/mps/hand_tracking","content":"","keywords":"","version":"Next"},{"title":"Hand Tracking Outputs​","type":1,"pageTitle":"MPS Output - Hand Tracking","url":"/projectaria_tools/docs/data_formats/mps/hand_tracking#hand-tracking-outputs","content":" We currently provide hand tracking data. The file outputs are:  hand_tracking_results.csv - the coordinates of 21 hand landmark positions, wrist and palm normals in the device frame, and the full 6DoF transform from the hand frame (origin at wrist position) to the device framewrist_and_palm_poses.csv - the coordinates of the wrist and palm positions, and normal vectors indicating orientation, in the device framesummary.json - high-level report on MPS hand tracking  ","version":"Next","tagName":"h2"},{"title":"hand_tracking_results.csv​","type":1,"pageTitle":"MPS Output - Hand Tracking","url":"/projectaria_tools/docs/data_formats/mps/hand_tracking#hand_tracking_resultscsv","content":" hand_tracking_results.csv contains the following fields: The landmark positions, and wrist and palm normal vectors are given in the device frame in meters. The translation from the hand frame to the device frame is given in meters.    The 21 landmarks are indexed from 0 to 20, representing key points on the hand. These landmarks correspond to specific joints and locations as follows:  Landmark ID\tDescription0\tThumb Fingertip 1\tIndex Finger Fingertip 2\tMiddle Finger Fingertip 3\tRing Finger Fingertip 4\tPinky Finger Fingertip 5\tWrist Joint 6\tThumb Intermediate 7\tThumb Distal 8\tIndex Finger Proximal 9\tIndex Finger Intermediate 10\tIndex Finger Distal 11\tMiddle Finger Proximal 12\tMiddle Finger Intermediate 13\tMiddle Finger Distal 14\tRing Finger Proximal 15\tRing Finger Intermediate 16\tRing Finger Distal 17\tPinky Finger Proximal 18\tPinky Finger Intermediate 19\tPinky Finger Distal 20\tPalm Center  Column\tType\tDescriptiontracking_timestamp_us\tint\tTimestamp, in microseconds, of the SLAM camera frame in device time domain. This is the same time domain in which the MPS trajectory outputs are reported, so these timestamps can be directly used to infer the device pose from the MPS trajectory output. left_tracking_confidence\tfloat\tA value between 0.0 and 1.0 indicating the confidence in the detected left hand. A value of -1.0 indicates that the left hand data is missing for the frame, and the coordinates *_left_landmark_i_device, _left_device_wrist, _left_wrist_device and _left_palm_device should not be used. tx_left_landmark_0_device\tfloat\tX-coordinate of the left landmark 0 position given in the device frame in meters. ty_left_landmark_0_device\tfloat\tY-coordinate of the left landmark 0 position given in the device frame in meters. tz_left_landmark_0_device\tfloat\tZ-coordinate of the left landmark 0 position given in the device frame in meters. tx_left_landmark_1_device\tfloat\tX-coordinate of the left landmark 1 position given in the device frame in meters. ty_left_landmark_1_device\tfloat\tY-coordinate of the left landmark 1 position given in the device frame in meters. tz_left_landmark_1_device\tfloat\tZ-coordinate of the left landmark 1 position given in the device frame in meters. tx_left_landmark_2_device\tfloat\tX-coordinate of the left landmark 2 position given in the device frame in meters. ty_left_landmark_2_device\tfloat\tY-coordinate of the left landmark 2 position given in the device frame in meters. tz_left_landmark_2_device\tfloat\tZ-coordinate of the left landmark 2 position given in the device frame in meters. tx_left_landmark_3_device\tfloat\tX-coordinate of the left landmark 3 position given in the device frame in meters. ty_left_landmark_3_device\tfloat\tY-coordinate of the left landmark 3 position given in the device frame in meters. tz_left_landmark_3_device\tfloat\tZ-coordinate of the left landmark 3 position given in the device frame in meters. tx_left_landmark_4_device\tfloat\tX-coordinate of the left landmark 4 position given in the device frame in meters. ty_left_landmark_4_device\tfloat\tY-coordinate of the left landmark 4 position given in the device frame in meters. tz_left_landmark_4_device\tfloat\tZ-coordinate of the left landmark 4 position given in the device frame in meters. tx_left_landmark_5_device\tfloat\tX-coordinate of the left landmark 5 position given in the device frame in meters. ty_left_landmark_5_device\tfloat\tY-coordinate of the left landmark 5 position given in the device frame in meters. tz_left_landmark_5_device\tfloat\tZ-coordinate of the left landmark 5 position given in the device frame in meters. tx_left_landmark_6_device\tfloat\tX-coordinate of the left landmark 6 position given in the device frame in meters. ty_left_landmark_6_device\tfloat\tY-coordinate of the left landmark 6 position given in the device frame in meters. tz_left_landmark_6_device\tfloat\tZ-coordinate of the left landmark 6 position given in the device frame in meters. tx_left_landmark_7_device\tfloat\tX-coordinate of the left landmark 7 position given in the device frame in meters. ty_left_landmark_7_device\tfloat\tY-coordinate of the left landmark 7 position given in the device frame in meters. tz_left_landmark_7_device\tfloat\tZ-coordinate of the left landmark 7 position given in the device frame in meters. tx_left_landmark_8_device\tfloat\tX-coordinate of the left landmark 8 position given in the device frame in meters. ty_left_landmark_8_device\tfloat\tY-coordinate of the left landmark 8 position given in the device frame in meters. tz_left_landmark_8_device\tfloat\tZ-coordinate of the left landmark 8 position given in the device frame in meters. tx_left_landmark_9_device\tfloat\tX-coordinate of the left landmark 9 position given in the device frame in meters. ty_left_landmark_9_device\tfloat\tY-coordinate of the left landmark 9 position given in the device frame in meters. tz_left_landmark_9_device\tfloat\tZ-coordinate of the left landmark 9 position given in the device frame in meters. tx_left_landmark_10_device\tfloat\tX-coordinate of the left landmark 10 position given in the device frame in meters. ty_left_landmark_10_device\tfloat\tY-coordinate of the left landmark 10 position given in the device frame in meters. tz_left_landmark_10_device\tfloat\tZ-coordinate of the left landmark 10 position given in the device frame in meters. tx_left_landmark_11_device\tfloat\tX-coordinate of the left landmark 11 position given in the device frame in meters. ty_left_landmark_11_device\tfloat\tY-coordinate of the left landmark 11 position given in the device frame in meters. tz_left_landmark_11_device\tfloat\tZ-coordinate of the left landmark 11 position given in the device frame in meters. tx_left_landmark_12_device\tfloat\tX-coordinate of the left landmark 12 position given in the device frame in meters. ty_left_landmark_12_device\tfloat\tY-coordinate of the left landmark 12 position given in the device frame in meters. tz_left_landmark_12_device\tfloat\tZ-coordinate of the left landmark 12 position given in the device frame in meters. tx_left_landmark_13_device\tfloat\tX-coordinate of the left landmark 13 position given in the device frame in meters. ty_left_landmark_13_device\tfloat\tY-coordinate of the left landmark 13 position given in the device frame in meters. tz_left_landmark_13_device\tfloat\tZ-coordinate of the left landmark 13 position given in the device frame in meters. tx_left_landmark_14_device\tfloat\tX-coordinate of the left landmark 14 position given in the device frame in meters. ty_left_landmark_14_device\tfloat\tY-coordinate of the left landmark 14 position given in the device frame in meters. tz_left_landmark_14_device\tfloat\tZ-coordinate of the left landmark 14 position given in the device frame in meters. tx_left_landmark_15_device\tfloat\tX-coordinate of the left landmark 15 position given in the device frame in meters. ty_left_landmark_15_device\tfloat\tY-coordinate of the left landmark 15 position given in the device frame in meters. tz_left_landmark_15_device\tfloat\tZ-coordinate of the left landmark 15 position given in the device frame in meters. tx_left_landmark_16_device\tfloat\tX-coordinate of the left landmark 16 position given in the device frame in meters. ty_left_landmark_16_device\tfloat\tY-coordinate of the left landmark 16 position given in the device frame in meters. tz_left_landmark_16_device\tfloat\tZ-coordinate of the left landmark 16 position given in the device frame in meters. tx_left_landmark_17_device\tfloat\tX-coordinate of the left landmark 17 position given in the device frame in meters. ty_left_landmark_17_device\tfloat\tY-coordinate of the left landmark 17 position given in the device frame in meters. tz_left_landmark_17_device\tfloat\tZ-coordinate of the left landmark 17 position given in the device frame in meters. tx_left_landmark_18_device\tfloat\tX-coordinate of the left landmark 18 position given in the device frame in meters. ty_left_landmark_18_device\tfloat\tY-coordinate of the left landmark 18 position given in the device frame in meters. tz_left_landmark_18_device\tfloat\tZ-coordinate of the left landmark 18 position given in the device frame in meters. tx_left_landmark_19_device\tfloat\tX-coordinate of the left landmark 19 position given in the device frame in meters. ty_left_landmark_19_device\tfloat\tY-coordinate of the left landmark 19 position given in the device frame in meters. tz_left_landmark_19_device\tfloat\tZ-coordinate of the left landmark 19 position given in the device frame in meters. tx_left_landmark_20_device\tfloat\tX-coordinate of the left landmark 20 position given in the device frame in meters. ty_left_landmark_20_device\tfloat\tY-coordinate of the left landmark 20 position given in the device frame in meters. tz_left_landmark_20_device\tfloat\tZ-coordinate of the left landmark 20 position given in the device frame in meters. right_tracking_confidence\tfloat\tA value between 0.0 and 1.0 indicating the confidence in the detected right hand. A value of -1.0 indicates that the right hand data is missing for the frame, and the coordinates *_right_landmark_i_device, _right_device_wrist, _right_wrist_device and _right_palm_device should not be used. tx_right_landmark_0_device\tfloat\tX-coordinate of the right landmark 0 position given in the device frame in meters. ty_right_landmark_0_device\tfloat\tY-coordinate of the right landmark 0 position given in the device frame in meters. tz_right_landmark_0_device\tfloat\tZ-coordinate of the right landmark 0 position given in the device frame in meters. tx_right_landmark_1_device\tfloat\tX-coordinate of the right landmark 1 position given in the device frame in meters. ty_right_landmark_1_device\tfloat\tY-coordinate of the right landmark 1 position given in the device frame in meters. tz_right_landmark_1_device\tfloat\tZ-coordinate of the right landmark 1 position given in the device frame in meters. tx_right_landmark_2_device\tfloat\tX-coordinate of the right landmark 2 position given in the device frame in meters. ty_right_landmark_2_device\tfloat\tY-coordinate of the right landmark 2 position given in the device frame in meters. tz_right_landmark_2_device\tfloat\tZ-coordinate of the right landmark 2 position given in the device frame in meters. tx_right_landmark_3_device\tfloat\tX-coordinate of the right landmark 3 position given in the device frame in meters. ty_right_landmark_3_device\tfloat\tY-coordinate of the right landmark 3 position given in the device frame in meters. tz_right_landmark_3_device\tfloat\tZ-coordinate of the right landmark 3 position given in the device frame in meters. tx_right_landmark_4_device\tfloat\tX-coordinate of the right landmark 4 position given in the device frame in meters. ty_right_landmark_4_device\tfloat\tY-coordinate of the right landmark 4 position given in the device frame in meters. tz_right_landmark_4_device\tfloat\tZ-coordinate of the right landmark 4 position given in the device frame in meters. tx_right_landmark_5_device\tfloat\tX-coordinate of the right landmark 5 position given in the device frame in meters. ty_right_landmark_5_device\tfloat\tY-coordinate of the right landmark 5 position given in the device frame in meters. tz_right_landmark_5_device\tfloat\tZ-coordinate of the right landmark 5 position given in the device frame in meters. tx_right_landmark_6_device\tfloat\tX-coordinate of the right landmark 6 position given in the device frame in meters. ty_right_landmark_6_device\tfloat\tY-coordinate of the right landmark 6 position given in the device frame in meters. tz_right_landmark_6_device\tfloat\tZ-coordinate of the right landmark 6 position given in the device frame in meters. tx_right_landmark_7_device\tfloat\tX-coordinate of the right landmark 7 position given in the device frame in meters. ty_right_landmark_7_device\tfloat\tY-coordinate of the right landmark 7 position given in the device frame in meters. tz_right_landmark_7_device\tfloat\tZ-coordinate of the right landmark 7 position given in the device frame in meters. tx_right_landmark_8_device\tfloat\tX-coordinate of the right landmark 8 position given in the device frame in meters. ty_right_landmark_8_device\tfloat\tY-coordinate of the right landmark 8 position given in the device frame in meters. tz_right_landmark_8_device\tfloat\tZ-coordinate of the right landmark 8 position given in the device frame in meters. tx_right_landmark_9_device\tfloat\tX-coordinate of the right landmark 9 position given in the device frame in meters. ty_right_landmark_9_device\tfloat\tY-coordinate of the right landmark 9 position given in the device frame in meters. tz_right_landmark_9_device\tfloat\tZ-coordinate of the right landmark 9 position given in the device frame in meters. tx_right_landmark_10_device\tfloat\tX-coordinate of the right landmark 10 position given in the device frame in meters. ty_right_landmark_10_device\tfloat\tY-coordinate of the right landmark 10 position given in the device frame in meters. tz_right_landmark_10_device\tfloat\tZ-coordinate of the right landmark 10 position given in the device frame in meters. tx_right_landmark_11_device\tfloat\tX-coordinate of the right landmark 11 position given in the device frame in meters. ty_right_landmark_11_device\tfloat\tY-coordinate of the right landmark 11 position given in the device frame in meters. tz_right_landmark_11_device\tfloat\tZ-coordinate of the right landmark 11 position given in the device frame in meters. tx_right_landmark_12_device\tfloat\tX-coordinate of the right landmark 12 position given in the device frame in meters. ty_right_landmark_12_device\tfloat\tY-coordinate of the right landmark 12 position given in the device frame in meters. tz_right_landmark_12_device\tfloat\tZ-coordinate of the right landmark 12 position given in the device frame in meters. tx_right_landmark_13_device\tfloat\tX-coordinate of the right landmark 13 position given in the device frame in meters. ty_right_landmark_13_device\tfloat\tY-coordinate of the right landmark 13 position given in the device frame in meters. tz_right_landmark_13_device\tfloat\tZ-coordinate of the right landmark 13 position given in the device frame in meters. tx_right_landmark_14_device\tfloat\tX-coordinate of the right landmark 14 position given in the device frame in meters. ty_right_landmark_14_device\tfloat\tY-coordinate of the right landmark 14 position given in the device frame in meters. tz_right_landmark_14_device\tfloat\tZ-coordinate of the right landmark 14 position given in the device frame in meters. tx_right_landmark_15_device\tfloat\tX-coordinate of the right landmark 15 position given in the device frame in meters. ty_right_landmark_15_device\tfloat\tY-coordinate of the right landmark 15 position given in the device frame in meters. tz_right_landmark_15_device\tfloat\tZ-coordinate of the right landmark 15 position given in the device frame in meters. tx_right_landmark_16_device\tfloat\tX-coordinate of the right landmark 16 position given in the device frame in meters. ty_right_landmark_16_device\tfloat\tY-coordinate of the right landmark 16 position given in the device frame in meters. tz_right_landmark_16_device\tfloat\tZ-coordinate of the right landmark 16 position given in the device frame in meters. tx_right_landmark_17_device\tfloat\tX-coordinate of the right landmark 17 position given in the device frame in meters. ty_right_landmark_17_device\tfloat\tY-coordinate of the right landmark 17 position given in the device frame in meters. tz_right_landmark_17_device\tfloat\tZ-coordinate of the right landmark 17 position given in the device frame in meters. tx_right_landmark_18_device\tfloat\tX-coordinate of the right landmark 18 position given in the device frame in meters. ty_right_landmark_18_device\tfloat\tY-coordinate of the right landmark 18 position given in the device frame in meters. tz_right_landmark_18_device\tfloat\tZ-coordinate of the right landmark 18 position given in the device frame in meters. tx_right_landmark_19_device\tfloat\tX-coordinate of the right landmark 19 position given in the device frame in meters. ty_right_landmark_19_device\tfloat\tY-coordinate of the right landmark 19 position given in the device frame in meters. tz_right_landmark_19_device\tfloat\tZ-coordinate of the right landmark 19 position given in the device frame in meters. tx_right_landmark_20_device\tfloat\tX-coordinate of the right landmark 20 position given in the device frame in meters. ty_right_landmark_20_device\tfloat\tY-coordinate of the right landmark 20 position given in the device frame in meters. tz_right_landmark_20_device\tfloat\tZ-coordinate of the left landmark 20 position given in the device frame in meters. tx_left_device_wrist\tfloat\tX-coordinate of the translation from the left hand frame (origin at wrist position) to the device frame in meters. ty_left_device_wrist\tfloat\tY-coordinate of the translation from the left hand frame (origin at wrist position) to the device frame in meters. tz_left_device_wrist\tfloat\tZ-coordinate of the translation from the left hand frame (origin at wrist position) to the device frame in meters. qx_left_device_wrist\tfloat\tX-component of the quaternion representing the rotation from the left hand frame (origin at wrist position) to the device frame. qy_left_device_wrist\tfloat\tY-component of the quaternion representing the rotation from the left hand frame (origin at wrist position) to the device frame. qz_left_device_wrist\tfloat\tZ-component of the quaternion representing the rotation from the left hand frame (origin at wrist position) to the device frame. qw_left_device_wrist\tfloat\tW-component of the quaternion representing the rotation from the left hand frame (origin at wrist position) to the device frame. tx_right_device_wrist\tfloat\tX-coordinate of the translation from the right hand frame (origin at wrist position) to the device frame in meters. ty_right_device_wrist\tfloat\tY-coordinate of the translation from the right hand frame (origin at wrist position) to the device frame in meters. tz_right_device_wrist\tfloat\tZ-coordinate of the translation from the right hand frame (origin at wrist position) to the device frame in meters. qx_right_device_wrist\tfloat\tX-component of the quaternion representing the rotation from the right hand frame (origin at wrist position) to the device frame. qy_right_device_wrist\tfloat\tY-component of the quaternion representing the rotation from the right hand frame (origin at wrist position) to the device frame. qz_right_device_wrist\tfloat\tZ-component of the quaternion representing the rotation from the right hand frame (origin at wrist position) to the device frame. qw_right_device_wrist\tfloat\tW-component of the quaternion representing the rotation from the right hand frame (origin at wrist position) to the device frame. nx_left_palm_device\tfloat\tX-coordinate of the left palm normal given in the device frame in meters. ny_left_palm_device\tfloat\tY-coordinate of the left palm normal given in the device frame in meters. nz_left_palm_device\tfloat\tZ-coordinate of the left palm normal given in the device frame in meters. nx_left_wrist_device\tfloat\tX-coordinate of the left wrist normal given in the device frame in meters. ny_left_wrist_device\tfloat\tY-coordinate of the left wrist normal given in the device frame in meters. nz_left_wrist_device\tfloat\tZ-coordinate of the left wrist normal given in the device frame in meters. nx_right_palm_device\tfloat\tX-coordinate of the right palm normal given in the device frame in meters. ny_right_palm_device\tfloat\tY-coordinate of the right palm normal given in the device frame in meters. nz_right_palm_device\tfloat\tZ-coordinate of the right palm normal given in the device frame in meters. nx_right_wrist_device\tfloat\tX-coordinate of the right wrist normal given in the device frame in meters. ny_right_wrist_device\tfloat\tY-coordinate of the right wrist normal given in the device frame in meters. nz_right_wrist_device\tfloat\tZ-coordinate of the right wrist normal given in the device frame in meters.  ","version":"Next","tagName":"h3"},{"title":"wrist_and_palm_poses.csv​","type":1,"pageTitle":"MPS Output - Hand Tracking","url":"/projectaria_tools/docs/data_formats/mps/hand_tracking#wrist_and_palm_posescsv","content":" wrist_and_palm_poses.csv contains the following fields: The wrist and palm poses and normal vectors are given in the device frame in meters.  Column\tType\tDescriptiontracking_timestamp_us\tint\tTimestamp, in microseconds, of the SLAM camera frame in device time domain. This is the same time domain in which the MPS trajectory outputs are reported, so these timestamps can be directly used to infer the device pose from the MPS trajectory output. left_tracking_confidence\tfloat\tA value between 0.0 and 1.0 indicating the confidence in the reported left wrist and palm positions. A value of -1.0 indicates that the left wrist and palm tracking data is missing for the frame, and the coordinates left_wrist_device_* and left_palm_device_* should not be used. tx_left_wrist_device\tfloat\tX-coordinate of the left wrist position given in the device frame in meters. ty_left_wrist_device\tfloat\tY-coordinate of the left wrist position given in the device frame in meters. tz_left_wrist_device\tfloat\tZ-coordinate of the left wrist position given in the device frame in meters. tx_left_palm_device\tfloat\tX-coordinate of the left palm position given in the device frame in meters. ty_left_palm_device\tfloat\tY-coordinate of the left palm position given in the device frame in meters. tz_left_palm_device\tfloat\tZ-coordinate of the left palm position given in the device frame in meters. right_tracking_confidence\tfloat\tA value between 0.0 and 1.0 indicating the confidence in the reported right wrist and palm positions. A value of -1.0 indicates that the right wrist and palm tracking data is missing for the frame, and the coordinates right_wrist_device_* and right_palm_device_* should not be used. tx_right_wrist_device\tfloat\tX-coordinate of the right wrist position given in the device frame in meters. ty_right_wrist_device\tfloat\tY-coordinate of the right wrist position given in the device frame in meters. tz_right_wrist_device\tfloat\tZ-coordinate of the right wrist position given in the device frame in meters. tx_right_palm_device\tfloat\tX-coordinate of the right palm position given in the device frame in meters. ty_right_palm_device\tfloat\tY-coordinate of the right palm position given in the device frame in meters. tz_right_palm_device\tfloat\tZ-coordinate of the right palm position given in the device frame in meters. nx_left_palm_device\tfloat\tX-coordinate of the left palm normal given in the device frame in meters. ny_left_palm_device\tfloat\tY-coordinate of the left palm normal given in the device frame in meters. nz_left_palm_device\tfloat\tZ-coordinate of the left palm normal given in the device frame in meters. nx_left_wrist_device\tfloat\tX-coordinate of the left wrist normal given in the device frame in meters. ny_left_wrist_device\tfloat\tY-coordinate of the left wrist normal given in the device frame in meters. nz_left_wrist_device\tfloat\tZ-coordinate of the left wrist normal given in the device frame in meters. nx_right_palm_device\tfloat\tX-coordinate of the right palm normal given in the device frame in meters. ny_right_palm_device\tfloat\tY-coordinate of the right palm normal given in the device frame in meters. nz_right_palm_device\tfloat\tZ-coordinate of the right palm normal given in the device frame in meters. nx_right_wrist_device\tfloat\tX-coordinate of the right wrist normal given in the device frame in meters. ny_right_wrist_device\tfloat\tY-coordinate of the right wrist normal given in the device frame in meters. nz_right_wrist_device\tfloat\tZ-coordinate of the right wrist normal given in the device frame in meters.  ","version":"Next","tagName":"h2"},{"title":"summary.json​","type":1,"pageTitle":"MPS Output - Hand Tracking","url":"/projectaria_tools/docs/data_formats/mps/hand_tracking#summaryjson","content":" summary.json contains the Operator Summary, listed in MPS Basics, as well as the following fields:  Field\tType\tDescriptionmean_confidence\tfloat\tAverage left_tracking_confidence and right_tracking_confidence value for frames with valid results total_frames\tint\tTotal number of frames valid_frame_fraction\tfloat\tFraction of frames that have reported a valid tracking result ","version":"Next","tagName":"h2"},{"title":"Basics","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_formats/mps/mps_summary","content":"","keywords":"","version":"Next"},{"title":"MPS File Structure​","type":1,"pageTitle":"Basics","url":"/projectaria_tools/docs/data_formats/mps/mps_summary#mps-file-structure","content":" MPS outputs use the following structure, in this example recording1.vrs was used to generate MPS.   └── Example folder ├── mps_recording1_vrs │ ├── eye_gaze │ │ ├── general_eye_gaze.csv │ │ └── summary.json │ ├── slam │ │ ├── closed_loop_trajectory.csv │ │ ├── online_calibration.jsonl │ │ ├── open_loop_trajectory.csv │ │ ├── semidense_observations.csv.gz │ │ ├── semidense_points.csv.gz │ │ └── summary.json │ ├── hand_tracking │ │ ├── wrist_and_palm_poses.csv │ │ └── summary.json │ ├── vrs_health_check.json │ └── vrs_health_check_slam.json └── recording1.vrs   mps_[name of VRS file]_vrs Sibling directory where all the intermediate data and MPS output is saved vrs_health_check.json Output of the health check performed on your computer before data is uploadedContains information about data drops in all the sensor streams vrs_health_check_slam.json Summary of SLAM specific checks on the VRS filesIf the health check fails it will contain details about which health checks failed slam folder Contains outputs after running SLAM (Trajectory and Semi-Dense Point Cloud data) eye_gaze folder Contains eye gaze outputs  ","version":"Next","tagName":"h2"},{"title":"Common terminologies​","type":1,"pageTitle":"Basics","url":"/projectaria_tools/docs/data_formats/mps/mps_summary#common-terminologies","content":" ","version":"Next","tagName":"h2"},{"title":"graph_uid​","type":1,"pageTitle":"Basics","url":"/projectaria_tools/docs/data_formats/mps/mps_summary#graph_uid","content":" graph_uid is a unique identifier for the world coordinate frame. For all the 3D geometric instances like pose and points in the world frames (having _world in the suffix), when they have the same graph_uid, they are in the same coordinate frame.  For simulation (such as Aria Synthetic Environments) and Aria Digital Twin(ADT) datasets we use the same random value for one space, e.g. the same graph_uid for one ADT/simulation space.  ","version":"Next","tagName":"h3"},{"title":"tracking_timestamp_us​","type":1,"pageTitle":"Basics","url":"/projectaria_tools/docs/data_formats/mps/mps_summary#tracking_timestamp_us","content":" tracking_timestamp_us's values are shaped by whether it is real world or synthetic data.  For real world data, tracking_timestamp_us provides the Device timestamps from your Aria glasses. Go to Timestamps in Aria VRS for a definition of the device timestamps.  In simulation datasets, this will be the timestamp in the simulator.  In tracking_timestamp_us  This clock has arbitrary starting points, which are not synchronized between recording sessions or devices.This clock is strictly monotonic, has stable clock speed, and is accurate in duration If you want to compute the time duration between two timestamps (especially when touching dynamics, e.g. integrating acceleration to velocity over time), you should use this timestamp.  ","version":"Next","tagName":"h3"},{"title":"utc_timestamp_ns​","type":1,"pageTitle":"Basics","url":"/projectaria_tools/docs/data_formats/mps/mps_summary#utc_timestamp_ns","content":" utc_timestamp_ns is the timestamp from Aria real-time clock (RTC). This time is synchronized to the cell phone time via the Aria Mobile Companion app to get UTC time at the beginning of the recording which is a rough estimate of the external standard clock.  This clock is not available in the simulation datasets.This clock provides rough synchronization between sessions and devices.This clock is not guaranteed to be monotonic, or have stable clock speed, due to synchronization with NTP. So do not compute duration between two UTC timestamps.  ","version":"Next","tagName":"h3"},{"title":"Operator summary​","type":1,"pageTitle":"Basics","url":"/projectaria_tools/docs/data_formats/mps/mps_summary#operator-summary","content":" The operator summary includes individual operator’s status and whether the operation was successful. There are three possible status flags:  SUCCESS: the operator successfully finished, without known issues.WARNING: The operator finished, but internally it detected problem(s) that may affect results quality. The operator still outputs the results, but we don’t have confidence in the quality of the results, so consume the results with caution.ERROR: the operator did not finish, finished with major error, or the quality of the results are too bad to be consumed. Results may or may not be generated, and any results should not be consumed.  The summary also provides information about processes as well as any warning or error messages available  Summary JSON output example:   &quot;SLAM&quot;: { &quot;status&quot;: &quot;SUCCESS&quot;, &quot;info&quot;: [ &quot;Recording total time: 1104.00s; Trajectory total length: 155.42m&quot;, &quot;Total Vision Translational Correction (mm): p50: 0.048; p99: 0.451&quot;, &quot;Rotational Correction (deg): p50: 0.001; p99: 0.007&quot; ], &quot;warnings&quot;: [], &quot;errors&quot;: [] }, ...  ","version":"Next","tagName":"h2"},{"title":"MPS Output - SLAM","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_formats/mps/slam","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"MPS Output - SLAM","url":"/projectaria_tools/docs/data_formats/mps/slam#overview","content":" This section covers SLAM Machine Perception Services (MPS). SLAM and Multi-SLAM outputs may be part of Open Dataset Releases or Project Aria Partners can request MPS services on their own data.  SLAM outputs are available for all recordings made with SLAM cameras and IMU enabled. Partner data is not made available to Meta researchers or Meta’s affiliates. Go to MPS Data Lifecycle for more details about how partner data is processed and stored.  The following outputs are generated if you request SLAM data using the MPS CLI:  6DoF TrajectorySemi-Dense Point CloudOnline Sensor Calibration  Multi-SLAM data can only be generated via the MPS CLI  Multi-SLAM Requires multiple VRS recordings to generate ","version":"Next","tagName":"h2"},{"title":"MPS Output - Calibration","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_formats/mps/slam/mps_calibration","content":"","keywords":"","version":"Next"},{"title":"Online calibration​","type":1,"pageTitle":"MPS Output - Calibration","url":"/projectaria_tools/docs/data_formats/mps/slam/mps_calibration#online-calibration","content":" online_calibration.jsonl contains one json online calibration record per line. Each record is a json dict object that contains timestamp metadata and the result of online calibration for the cameras and IMUs. Note that after the v1.1.0 MPS SLAM release, we improved the RGB camera online calibration for time offsets estimation, intrinsics/extrinsics estimation, as well as exposing the image readout time for compensating the rolling shutter effect.  The calibration parameters contain intrinsics and extrinsics parameters for each sensor as well as a time offsets which best temporally align their data.  For how to load and read online calibrations in Python and C++, please check the code example ","version":"Next","tagName":"h2"},{"title":"MPS Output - Eye Gaze","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_formats/mps/mps_eye_gaze","content":"","keywords":"","version":"Next"},{"title":"Eye Gaze Data Format​","type":1,"pageTitle":"MPS Output - Eye Gaze","url":"/projectaria_tools/docs/data_formats/mps/mps_eye_gaze#eye-gaze-data-format","content":" Project Aria's Machine Perception Services (MPS) uses Aria's Eye Tracking (ET) camera images to estimate the direction the user is looking. This eye gaze estimation is in Central Pupil Frame. Eye Gaze outputs may be part of Open Dataset Releases or Project Aria Partners can request MPS services on their own data.  Eye Gaze outputs are available for all recordings made with Eye Tracking cameras enabled. Partner data is not made available to Meta researchers or Meta’s affiliates. Go to MPS Data Lifecycle for more details about how partner data is processed and stored.  In March 2024, we updated our eye gaze model to support depth estimation. We do this by providing left and right eye gaze directions (yaw values) along with the depth at which these gaze directions intersect (translation values).    Figure 1: Diagram of the March 2024 Model, showing vergence Left, Right and Combined Eye Gaze Directions.  In the new model, the convergence points and distances are derived from the predicted gaze directions. The combined direction’s yaw is used to populate the yaw field of the EyeGaze object for backwards compatibility. The pitch is common to left, right and combined gaze directions.  Eye Gaze MPS file outputs are:  summary.json - high level report on MPS eye gaze generationgeneral_eye_gaze.csv - based on the standard eye gaze configurationpersonalized_eye_gaze.csv - only if you’ve made the recording with in-session Eye Gaze Calibration  ","version":"Next","tagName":"h2"},{"title":"What's the difference between eye tracking and eye gaze?​","type":1,"pageTitle":"MPS Output - Eye Gaze","url":"/projectaria_tools/docs/data_formats/mps/mps_eye_gaze#whats-the-difference-between-eye-tracking-and-eye-gaze","content":" Simply put, eye tracking refers to the video recording of a wearer's eye motions using Aria's inward-facing cameras. If it helps, you can think of eye tracking as eye recording. Eye gaze, on the other hand, is extrapolated information about where the user's visual attention is focused based on the eye recordings. Eye gaze information is computed by Machine Perception Services as opposed to being native to the process of recording with Aria like eye tracking is.  ","version":"Next","tagName":"h3"},{"title":"Further resources​","type":1,"pageTitle":"MPS Output - Eye Gaze","url":"/projectaria_tools/docs/data_formats/mps/mps_eye_gaze#further-resources","content":" Visualize Data Using PythonVisualize Data Using C++Eye Gaze Code Snippets, includes: Data loadingVector conversion (yaw pitch to 3 vector)Coordinates System (CPF_To_Device)  ","version":"Next","tagName":"h3"},{"title":"general_eye_gaze.csv​","type":1,"pageTitle":"MPS Output - Eye Gaze","url":"/projectaria_tools/docs/data_formats/mps/mps_eye_gaze#general_eye_gazecsv","content":" general_eye_gaze.csv outputs are available for all recordings made with Eye Tracking cameras.  At this time, all Open Dataset Eye Gaze data was computed using the older model. Aria Digital Twin (ADT) used their ground truth system to compute eye gaze depth for their release.  Where a cell shows 0, the data is not provided for that model, but to ensure backwards compatibility it will be represented as 0 in Project Aria Tools readers.  Column\tType\tPre March 2024 Model\tNew Modeltracking_timestamp_us\tint\tThe timestamp, in microseconds, of the eye tracking camera frame in device time domain. The MPS location output also contains pose estimations in the same time domain and these timestamps can be directly used to infer the device pose from the MPS location output.\tSame. yaw_rads_cpf\tfloat\tEye gaze yaw angle in radians in CPF frame. The yaw angle is the angle between the projection of the eye gaze vector (originating at CPF) on XZ plane and the Z axis in the CPF frame.\tNot provided, however this value can be computed using helper functions and is automatically read when parsed with our data utilities. pitch_rads_cpf\tfloat\tEye gaze pitch angle in radians in CPF frame. The pitch angle is the angle between the projection of the eye gaze vector (originating at CPF) on YZ plane and the Z axis in the CPF frame.\tThis now corresponds to the common pitch of the left and right gaze direction. depth_m\tfloat\tNot available.\tAbsolute depth in meters of the 3D gaze point in CPF frame. Depth values are capped at 4m. If there are timestamps where the predicted values are degenerate, this cell is empty. yaw_low_rads_cpf\tfloat\tLower bound of the confidence interval for the yaw estimation.\t0 pitch_low_rads_cpf\tfloat\tLower bound of the confidence interval for the pitch estimation.\t0 yaw_high_rads_cpf\tfloat\tUpper bound of the confidence interval for the yaw estimation.\t0 pitch_high_rads_cpf\tfloat\tUpper bound of the confidence interval for the pitch estimation.\t0 left_yaw_rads_cpf\tfloat\t0\tLeft eye gaze yaw angle in radians in the CPF frame. The left yaw angle is the angle between the projection of the left eye gaze vector (originating at CPF) on the XZ plane and the Z axis in the CPF frame. right_yaw_rads_cpf\tfloat\t0\tRight eye gaze yaw angle in radians in the CPF frame. The right yaw angle is the angle between the projection of the left eye gaze vector (originating at CPF) on the XZ plane and the Z axis in the CPF frame. left_yaw_low_rads_cpf\tfloat\t0\tLower bound of left eye gaze yaw angle in radians in CPF frame. right_yaw_low_rads_cpf\tfloat\t0\tLower bound of the right eye gaze yaw angle in radians in the CPF frame. left_yaw_high_rads_cpf\tfloat\t0\tUpper bound of the left eye gaze yaw angle in radians in the CPF frame. right_yaw_high_rads_cpf\tfloat\t0\tUpper bound of the right eye gaze yaw angle in radians in the CPF frame. tx_left_eye_cpf\tfloat\t0\tTranslation along the X direction from CPF origin to left eye position. ty_left_eye_cpf\tfloat\t0\tTranslation along the Y direction from CPF origin to left eye position. tz_left_eye_cpf\tfloat\t0\tTranslation along the Z direction from CPF origin to left eye position. tx_right_eye_cpf\tfloat\t0\tTranslation along the X direction from CPF origin to right eye position. ty_right_eye_cpf\tfloat\t0\tTranslation along the Y direction from CPF origin to right eye position. tz_right_eye_cpf\tfloat\t0\tTranslation along the Z direction from CPF origin to right eye position. session_uid\tstring\tUnique identifier for a session within the VRS file\tSame.  ","version":"Next","tagName":"h2"},{"title":"personalized_eye_gaze.csv​","type":1,"pageTitle":"MPS Output - Eye Gaze","url":"/projectaria_tools/docs/data_formats/mps/mps_eye_gaze#personalized_eye_gazecsv","content":" personalized_eye_gaze.csv outputs are only generated if the recording has in-session Eye Gaze Calibration data. The schema is exactly the same as general_eye_gaze.csv. The session_uids between general and personalized output will be the same.  The in-session calibration is used to compute user specific calibration (gaze correction parameters). The yaw and pitch values will be adjusted based on this calibration. If the instructions for in-session calibration are followed correctly, the calibrated eye gaze is expected to be more accurate compared to general eye gaze.  ","version":"Next","tagName":"h2"},{"title":"Personalized calibration parameters, pre March 2024 model​","type":1,"pageTitle":"MPS Output - Eye Gaze","url":"/projectaria_tools/docs/data_formats/mps/mps_eye_gaze#personalized-calibration-parameters-pre-march-2024-model","content":" Four coefficients are used to generate the output [s_y, s_p, o_y, o_p]:  corrected_yaw = s_y * yaw + o_ycorrected_pitch = s_p * pitch + o_p  ","version":"Next","tagName":"h3"},{"title":"Personalized calibration parameters, new model​","type":1,"pageTitle":"MPS Output - Eye Gaze","url":"/projectaria_tools/docs/data_formats/mps/mps_eye_gaze#personalized-calibration-parameters-new-model","content":" Six coefficients are used in the new model output [s_ly, s_ry, s_p, o_ly, o_ry, o_p]:  corrected_left_yaw = s_ly * left_yaw + o_lycorrected_right_yaw = s_ry * right_yaw + o_rycorrected_pitch = s_p * pitch + o_p  We also use a diopter delta parameter that corrects the depth:  corrected_depth = 1 / ((1 / depth) - diopter_delta)  The diopter delta is calculated during calibration (see Stage 2 below) is the median of difference between 1 / predicted_depth and 1 / groundtruth_depth values.  ","version":"Next","tagName":"h3"},{"title":"General Principles​","type":1,"pageTitle":"MPS Output - Eye Gaze","url":"/projectaria_tools/docs/data_formats/mps/mps_eye_gaze#general-principles","content":" The following principles apply to general_eye_gaze.csv and personalized_eye_gaze.csv  ","version":"Next","tagName":"h2"},{"title":"Confidence Intervals​","type":1,"pageTitle":"MPS Output - Eye Gaze","url":"/projectaria_tools/docs/data_formats/mps/mps_eye_gaze#confidence-intervals","content":" The confidence intervals represent the models uncertainty estimation. A smaller interval represents higher confidence and a wider interval represents lower confidence. The confidence interval angles are in radians and in CPF frame. Some common factors that impact uncertainty include:  BlinkingHair occluding the eye tracking camerasRe-adjusting glasses or taking them off to clean them  For utility function to load the eye gaze in Python and C++, please check the code examples  ","version":"Next","tagName":"h3"},{"title":"Session_uid​","type":1,"pageTitle":"MPS Output - Eye Gaze","url":"/projectaria_tools/docs/data_formats/mps/mps_eye_gaze#session_uid","content":" When there are multiple users in the same vrs file (users handing off glasses to a different user without stopping the recording), session_uid identifies intervals corresponding to different calibration sessions if in-app calibration is performed during the hand-offs.  All the rows with the same session_uid belong to the same session within the VRS fileIf there are multiple calibration sessions, the session_uid would be unique for each session  general_eye_gaze.csv  There will be a single value when there is no in-session eye calibration or only one in-session calibrationThe session_uid column values will always match those in personalized_eye_gaze.csv  Examples​  No calibrated eye gaze - general_eye_gaze will have one session_uid across all rowsOne in-session calibration - general_eye_gaze will have one session_uid across all rows and this value will be identical in personalized_eye_gazek &gt; 1 in-session calibrations - both general and calibrated eye gaze will have k unique session_uid that start when in-session calibration begins and this value will be identical in personalized_eye_gaze  ","version":"Next","tagName":"h3"},{"title":"summary.json​","type":1,"pageTitle":"MPS Output - Eye Gaze","url":"/projectaria_tools/docs/data_formats/mps/mps_eye_gaze#summaryjson","content":" The summary.json file provides a high level overview of the output for each of the major stages. This is similar to the operator summary output from the MPS location pipeline.  For each stage of the ET pipeline, there will be one section in this file. If the section is missing, that means that the stage is not applicable or was not run.  Stage 1: GazeInference (all recordings)​  Uncalibrated Eye Gaze derived data has been generated. If you’re able to download the data to view the .json file it will say SUCCESS.  Name\tType\tDescriptionstatus\tstring\tSUCCESS (if you are able to download the data and view this file) message\tstring\tAny further details, if available  Stage 2: InSessionCalibration (if in-session calibration available)​  If the recording contains one or more valid in-session calibration intervals, the ET pipeline will compute the calibration parameters.  Each calibration session found in the VRS file will generate the following information:  Name\tType\tDescriptionstatus\tstring\tSUCCESS / FAIL message\tstring\tAny further details, if available session_uid\tstring\tUnique ID representing the session start_time_us\tint\tWhen the first wearer starts using the Aria glasses, or when subsequent wearer begins in-session calibration (2nd eye calibration onwards) end_time_us\tint\tWhen a wearer session or recording ends params\tArray[float]\tThe calibration parameters (4 floats)  note The status should be SUCCESS, unless there was an issue where the wearer began the in-session calibration, but did not generate the necessary data. In this case it would FAIL.  Stage 3: CalibrationCorrection​  If Stage 2 has been successful, CalibrationCorrection will contain details about calibrated eye gaze. For each calibration session, we will output the following information:  Name\tType\tDescriptionstatus\tstring\tSUCCESS / FAIL message\tstring\tAny further details, if available session_uid\tstring\tUnique id representing the session generalized_gaze_error_rads\tdict\tGeneral gaze error in radians calibrated_gaze_error_rads\tdict\tCalibrated gaze error in radians  If the previous stages completed successfully, the status for this stage should always be SUCCESS.  Example summary.json files​  Scenario 1: No calibration available​  This report is quite short, as no in-session calibration data is available. Eye Gaze MPS was successfully created:  { &quot;GazeInference&quot;: { &quot;status&quot;: &quot;SUCCESS&quot; } }   Scenario 2: In-session calibration available​  In this example, there were multiple calibration sessions:  In session one calibration was completed successfullyIn session two, the user began the in-session calibration, but did not generate the necessary data.  { &quot;GazeInference&quot;: { &quot;status&quot;: &quot;SUCCESS&quot; }, &quot;InSessionCalibration&quot;: [ { &quot;Status&quot;: &quot;SUCCESS&quot;, &quot;session_uid&quot;: &quot;01ac9bf2-334a-49c6-9dc6-fdc07ab08a2a&quot;, &quot;message&quot;: &quot;&quot;, &quot;start_time_us&quot;: 147588973, &quot;end_time_us&quot;: 208304973, &quot;num_calibu_frames&quot;: 1000, &quot;parameters&quot;:[1.02361481, 1.05426864, 0.01158671, 0.01403982] }, { &quot;Status&quot;: &quot;FAIL&quot;, &quot;message&quot;: &quot;Couldn't compute GT gaze vectors for the interval [487241235, 508304973]&quot;, &quot;session_uid&quot;: &quot;6063bf11-84ef-4ed5-a785-ac44b4328fdc&quot;, &quot;start_time_us&quot;: 487241235, &quot;end_time_us&quot;: 508304973, &quot;num_calibu_frames&quot;: 10, } ], &quot;CalibrationCorrection&quot;: [ { &quot;status&quot;: &quot;SUCCESS&quot;, &quot;message&quot;: &quot;&quot;, &quot;session_uid&quot;: &quot;01ac9bf2-334a-49c6-9dc6-fdc07ab08a2a&quot;, &quot;generalized_gaze_error_rads&quot;: { &quot;mean&quot;: 0.047444001119500284, &quot;std&quot;: 0.015775822542178554, &quot;min&quot;: 0.009264740570696107, &quot;max&quot;: 0.16895371875829926, &quot;p25&quot;: 0.036160872560797655, &quot;p50&quot;: 0.04529629090291307, &quot;p75&quot;: 0.05761677117669144, &quot;p95&quot;: 0.0675233675673802 }, &quot;calibrated_gaze_error_rads&quot;: { &quot;mean&quot;: 0.037444001119500284, &quot;std&quot;: 0.005775822542178554, &quot;min&quot;: 0.006364740570696107, &quot;max&quot;: 0.06835371875829926, &quot;p25&quot;: 0.026060872560797655, &quot;p50&quot;: 0.02519329090291307, &quot;p75&quot;: 0.03760677117669144, &quot;p95&quot;: 0.0474232675673802 } }, { &quot;status&quot;: &quot;FAILURE&quot;, &quot;message&quot;: &quot;No calibration available for this session&quot;, &quot;session_uid&quot;: &quot;6063bf11-84ef-4ed5-a785-ac44b4328fdc&quot; } ] }  ","version":"Next","tagName":"h2"},{"title":"MPS Output - Semi-Dense Point Cloud","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_formats/mps/slam/mps_pointcloud","content":"","keywords":"","version":"Next"},{"title":"What are semi-dense points?​","type":1,"pageTitle":"MPS Output - Semi-Dense Point Cloud","url":"/projectaria_tools/docs/data_formats/mps/slam/mps_pointcloud#what-are-semi-dense-points","content":" Semi-dense points are the 3D points associated with tracks from our semi-dense tracking pipeline. Semi-dense tracks are continually created in pixel locations of input frames that lie in regions of high image gradient, and are then successively tracked in the following frames. Each track is associated with a 3D point, parameterized as an inverse distance along a ray originating from the track's first initial observation, as well as its uncertainty in inverse distance and distance. These points are transformed from their original camera coordinate spaces to the same coordinate frame associated with the closed loop trajectory of the sequence.  ","version":"Next","tagName":"h2"},{"title":"User needs to define how to enforce quality​","type":1,"pageTitle":"MPS Output - Semi-Dense Point Cloud","url":"/projectaria_tools/docs/data_formats/mps/slam/mps_pointcloud#user-needs-to-define-how-to-enforce-quality","content":" To support user flexibility the tool outputs the associated points of all tracks regardless of quality. This means the data will contain a number of points whose positions have high uncertainty and are geometrically less accurate.  Users will either need to threshold the point cloud by setting a maximum allowed inverse distance / distance certainty or correctly weight points by their certainty when using them in downstream tasks.  Nominal threshold values are a maximum inv_dist_std of 0.005 and a maximum dist_std of 0.01.  ","version":"Next","tagName":"h3"},{"title":"Points in the world coordinate frame​","type":1,"pageTitle":"MPS Output - Semi-Dense Point Cloud","url":"/projectaria_tools/docs/data_formats/mps/slam/mps_pointcloud#points-in-the-world-coordinate-frame","content":" This file is the gzip compressed semi-dense points in the world coordinate system. The world coordinate frame is the same frame of the closed loop trajectory. For utility function to load the points in Python and C++, please check the code examples  Column\tType\tDescriptionuid\tint\tA unique identifier of this point within this map graph_uid\tstring\tUnique identifier of the world coordinate frame. Associated with an equivalent graph_uid found in close_loop_trajectory.csv, depending on the frame this point was first observed in p{x,y,z}_world\tfloat\tPoint location in the world coordinate frame p_world inv_dist_std\tfloat\tStandard deviation of the inverse distance estimate, in meter^-1. Could be used for determining the quality of the 3D point position estimate dist_std\tfloat\tStandard deviation of the distance estimate, in meters. Could be used for determining the quality of the 3D point position estimate  ","version":"Next","tagName":"h2"},{"title":"Point observations​","type":1,"pageTitle":"MPS Output - Semi-Dense Point Cloud","url":"/projectaria_tools/docs/data_formats/mps/slam/mps_pointcloud#point-observations","content":" The observation file is the gzip compressed semi-dense 2D observations, described in image pixel 2D coordinate frame. For utility function to load the observations in Python and C++, please check the code examples  Column\tType\tDescriptionuid\tint\tA unique identifier integer of this point within this map frame_tracking_timestamp_us\tint\tAria device timestamp of the host frame’s center of exposure, in microsecond camera_serial\tstring\tThe serial number of the camera which observes this point u\tfloat\tThe sub-pixel-accuracy observed measurement of the point in pixels, in the observing frame’s camera v\tfloat\tThe sub-pixel-accuracy observed measurement of the point in pixels, in the observing frame’s camera ","version":"Next","tagName":"h2"},{"title":"MPS output - Trajectory","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_formats/mps/slam/mps_trajectory","content":"","keywords":"","version":"Next"},{"title":"Open loop trajectory​","type":1,"pageTitle":"MPS output - Trajectory","url":"/projectaria_tools/docs/data_formats/mps/slam/mps_trajectory#open-loop-trajectory","content":" Open loop trajectory is the high frequency (IMU rate, which is 1kHz) odometry estimation output by the visual-inertial odometry (VIO), in an arbitrary odometry coordinate frame. The estimation includes pose and dynamics (translational and angular velocities).  The open loop trajectory has good “relative” and “local” accuracy: the relative transformation between two poses is accurate when the time span between two frames is short (within a few minutes). However, the open loop trajectory has increased drift error accumulated over time spent and travel distance. Consider using closed loop trajectory if you are looking for trajectory without drift error.  For the utility function to load the open loop trajectory in Python and C++, please check the code examples  Column\tType\tDescriptiontracking_timestamp_us\tint\tAria device timestamp in microseconds utc_timestamp_ns\tint\tWall clock UTC time in nanoseconds. If not available, the value will be -1 session_uid\tstring\tUnique identifier of the odometry coordinate frame. When the session_uid is the same, poses and velocities are defined in the same coordinate frame {tx,ty,tz,qx,qy,qz,qw}_odometry_device\tfloat\tPose of the device coordinate frame in odometry frame T_odometry_device, include translation (tx, ty, tz) in meters and rotation quaternion (qx, qy, qz, qw) device_linear_velocity_{x,y,z}_odometry\tfloat\tVelocity of device coordinate frame in odometry frame, (x, y, z) in meter/s angular_velocity_{x,y,z}_device\tfloat\tAngular velocity of device coordinate frame in device frame, (x, y, z) in rad/s gravity_{x,y,z}_odometry\tfloat\tEarth gravity vector in odometry frame, (x, y, z) in meter/s^2. This vector is pointing toward the ground, and includes gravitation and centrifugal forces from earth rotation quality_score\tfloat\tA quality score between 0.0 to 1.0. The larger the score is, the higher confidence the estimation has higher quality  ","version":"Next","tagName":"h2"},{"title":"Closed loop trajectory​","type":1,"pageTitle":"MPS output - Trajectory","url":"/projectaria_tools/docs/data_formats/mps/slam/mps_trajectory#closed-loop-trajectory","content":" Closed loop trajectory is the high frequency (IMU rate, which is 1kHz) pose estimation output by our mapping process, in an arbitrary gravity aligned world coordinate frame. The estimation includes pose and dynamics (translational and angular velocities).  Closed loop trajectories are fully bundle adjusted with detected loop closures, reducing the VIO drift which is present in the open loop trajectories. However, due to the loop closure correction, the “relative” and “local” trajectory accuracy within a short time span (i.e. seconds) might be worse compared to open loop trajectories.  In some open datasets we also share and use this format for trajectory pose ground truth from simulation or Optitrack, and the files will be called in a different file name aria_gt_trajectory.csv.  For the utility function to load the closed loop trajectory in Python and C++, please check the code examples  Column\tType\tDescriptiongraph_uid\tstring\tUnique identifier of the world coordinate frame tracking_timestamp_us\tint\tAria device timestamp in microsecond utc_timestamp_ns\tint\tWall clock UTC time in nanosecond. If not available, the value will be -1 {tx,ty,tz,qx,qy,qz,qw}_world_device\tfloat\tPose of the device coordinate frame in world frame T_world_device, translation (tx, ty, tz) in meters and rotation quaternion (qx, qy, qz, qw) device_linear_velocity_{x,y,z}_device\tfloat\tVelocity of device coordinate frame in device frame, (x, y, z) in meter/s angular_velocity_{x,y,z}_device\tfloat\tAngular velocity of device coordinate frame in device frame, (x, y, z) in rad/s gravity_{x,y,z}_world\tfloat\tGravity vector (x, y, z) in the world frame, in meter/s^2. MPS output will all have fixed value` [0, 0, -9.81]’, while other source (e.g. simulation or Optitrack ground truth) may give different values quality_score\tfloat\tA quality score between 0.0 to 1.0. The larger the score is, the higher confidence the estimation has higher quality` ","version":"Next","tagName":"h2"},{"title":"Multi-SLAM","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_formats/mps/slam/mps_multi_slam","content":"Multi-SLAM Multi-SLAM is a Project Aria Machine Perception Service (MPS) that can be requested on two or more recordings. It creates SLAM MPS outputs in a shared co-ordinate frame. Multi-SLAM data can be visualized in the Python version of MPS Viewer. Open datasets that contain Multi-SLAM outputs where there are recordings with 2 or more Project Aria glasses: Aria Everyday Activities (AEA) datasetAria Digital Twin (ADT) dataset The Multi-SLAM outputs are mostly the same as the standard SLAM MPS outputs. The differences are: Multi-SLAM can only be requested via MPS CLIAll the recordings that were aligned together will have the same graph_uid in the output.The output may contain multiple aligned islands and multiple consecutive graph_uids. Outputs are saved to a user defined directory. Each numbered folder contains the outputs for a specific VRS file: └── multi_slam_output # user defined directory for outputs ├── 0 │ ├── slam │ │ ├── closed_loop_trajectory.csv │ │ ├── online_calibration.jsonl │ │ ├── open_loop_trajectory.csv │ │ ├── semidense_observations.csv.gz │ │ ├── semidense_points.csv.gz │ │ └── summary.json │ ├── vrs_health_check.json │ └── vrs_health_check_slam.json ├── 1 │ ├── slam │ │ ├── closed_loop_trajectory.csv │ │ ├── online_calibration.jsonl │ │ ├── open_loop_trajectory.csv │ │ ├── semidense_observations.csv.gz │ │ ├── semidense_points.csv.gz │ │ └── summary.json │ ├── vrs_health_check.json │ └── vrs_health_check_slam.json └── vrs_to_multi_slam.json vrs_to_multi_slam.json associates the VRS file name with a numbered folder, for example: { &quot;/example/recording1.vrs&quot;: &quot;0&quot;, &quot;/example/recording2.vrs&quot;: &quot;1&quot;, &quot;/example2/recording1.vrs&quot;: &quot;2&quot;, } ","keywords":"","version":"Next"},{"title":"Project Aria Tools","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Project Aria Tools","url":"/projectaria_tools/docs/data_utilities#overview","content":" Project Aria Tools provides open source Python and C++ code and APIs for working with Aria data. Due to the multimodal nature of Project Aria data, the data is stored in VRS files. Project Aria Tools provides API wrappers for using VRS tools, specifically tailored to working with Aria data. Using Project Aria Tools should make it easier to plug your data into other applications with more accessible formatting and labeling.  If there are VRS functions you wish Project Aria Tools had, please contact us using any of our Support Channels.  Getting started A quickstart guide showing how to install Project Aria Tools using a Python package, followed by tutorials using Jupyter notebooks. It contains: Dataprovider quickstart tutorial: a walk-through of accessing sensor data from VRS file, obtaining sensor calibrations and accessing project/unproject functionalities, undistorting an image, etc.Sophus Pybind Tutorial: access Sophus Library SO3, SE3, interpolate and iterativeMean featuresMachine Perception Services (MPS) tutorial: How to visualize MPS derived data Installation guide Various installation processes for Project Aria Tools API in Python and C++:Download the CodebaseDownload MPS Sample DataPython Package InstallationC++ InstallationCMake for your projectsPython Type AnnotationInstall and Build Troubleshooting Visualizers Tutorials showing how to visualize raw Aria data and MPS data Python VisualizationC++ Visualization Core Code Snippets Python and C++ code snippets for Project Aria Tools core functionality Data Provider: open and load Aria raw data (VRS files)Image: Access and manage Aria imagesCalibration: Access device, 6DoF and sensor calibrationMPS - General: How to load MPS outputs into data structures that can be used by downstream applicationsMPS - Eye Gaze: code snippets for working with eye gaze data Advanced Code Snippets Plotting Sensor Data (Python) Save images as PNGPlot the raw sensor data of a VRS file and store the plots in PDF files Image Utilities (Python and C++)Export VRS to MP4 (Python) ","version":"Next","tagName":"h2"},{"title":"Export a VRS file to MP4","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/vrs_to_mp4","content":"","keywords":"","version":"Next"},{"title":"Install dependencies​","type":1,"pageTitle":"Export a VRS file to MP4","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/vrs_to_mp4#install-dependencies","content":" python3 -m pip install projectaria_tools moviepy   ","version":"Next","tagName":"h2"},{"title":"Usage​","type":1,"pageTitle":"Export a VRS file to MP4","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/vrs_to_mp4#usage","content":" vrs_to_mp4 tool is available via command line or through Python as a library.  PythonBash from projectaria_tools.utils.vrs_to_mp4_utils import convert_vrs_to_mp4 convert_vrs_to_mp4(input_vrs, output_video, log_folder, down_sample_factor)   ","version":"Next","tagName":"h2"},{"title":"MISC​","type":1,"pageTitle":"Export a VRS file to MP4","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/vrs_to_mp4#misc","content":" ","version":"Next","tagName":"h2"},{"title":"Extract VRS timestamps from MP4​","type":1,"pageTitle":"Export a VRS file to MP4","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/vrs_to_mp4#extract-vrs-timestamps-from-mp4","content":" Historically, when VRS files were converted to MP4, accurate timestamp information was lost. To maintain compatibility with the rest of our tooling, this tool now logs a VRS timestamp for each MP4 frame. VRS timestamps are saved to the MP4's metadata and are measured in nanoseconds. Now for every MP4 extracted using vrs_to_mp4 tools, this timestamp is conveniently saved as a metadata.  To extract the vrs timestamps, use the get_timestamp_from_mp4 function. The resulting timestamp is in TimeDomain.DEVICE_TIME  from projectaria_tools.utils.vrs_to_mp4_utils import get_timestamp_from_mp4 vrs_device_timestamps_nanoseconds = get_timestamp_from_mp4(mp4_file)   ","version":"Next","tagName":"h3"},{"title":"Calibration for upright RGB​","type":1,"pageTitle":"Export a VRS file to MP4","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/vrs_to_mp4#calibration-for-upright-rgb","content":" The output RGB frames in the MP4 file are rotated upright (clockwise 90 degrees). The camera calibration will be changed after such rotation and can be obtained using the following interface.  from projectaria_tools.utils.calibration_utils import ( rotate_upright_image_and_calibration, ) upright_image, upright_calibration = rotate_upright_image_and_calibratio(original_rgb_image, rgb_camera_calibration)  ","version":"Next","tagName":"h3"},{"title":"Calibration Code Snippets","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/calibration","content":"","keywords":"","version":"Next"},{"title":"Accessing device calibration​","type":1,"pageTitle":"Calibration Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/calibration#accessing-device-calibration","content":" Device calibration stores:  The device's CAD model, which contains the 6DoF poses of sensors of the device as designed.The calibration of all sensors on a single Aria device. See the Accessing sensor calibration section for details.The device's sub-type (DVT-S or DVT-L to indicate small or large)  PythonC++ from projectaria_tools.core import data_provider, calibration from projectaria_tools.core.stream_id import StreamId vrsfile = &quot;example.vrs&quot; provider = data_provider.create_vrs_data_provider(vrsfile) # returns None if vrs does not have a calibration device_calib = provider.get_device_calibration() print(device_calib.get_device_subtype())   ","version":"Next","tagName":"h3"},{"title":"Accessing 6DoF poses of sensors with Sophus Python binding​","type":1,"pageTitle":"Calibration Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/calibration#accessing-6dof-poses-of-sensors-with-sophus-python-binding","content":" All 6DoF poses (a.k.a. extrinsic parameters) are represented as relative to the device frame. The device frame is a specific sensor frame, identified by the sensor's label. Aria device frame is by default camera-slam-left. We also provide the pose of the central-pupil-frame in the device frame or as relative to a sensor frame.  PythonC++ label = &quot;camera-slam-right&quot; transform_device_sensor = device_calib.get_transform_device_sensor(label) transform_device_cpf = device_calib.get_transform_device_cpf() transform_cpf_sensor = device_calib.get_transform_cpf_sensor(label)   ","version":"Next","tagName":"h3"},{"title":"Accessing sensor calibration​","type":1,"pageTitle":"Calibration Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/calibration#accessing-sensor-calibration","content":" Each sensor on the device may have a corresponding stream in the vrs and may have a corresponding calibration. However, some types of sensors may not have calibration defined for them (e.g. GPS, WPS, bluetooth), and some sensors may not record stream in a specific vrs. For sensor streams where calibration is available, they can be accessed by labels:  PythonC++ # returns None if vrs does not have a calibration device_calib = provider.get_device_calibration() sensor_calib = device_calib.get_sensor_calib(label) More conveniently, you can just do stream_id = StreamId(&quot;1201-1&quot;) calib = provider.get_sensor_calibration(stream_id) If you know the calibration type, you can also do # returns None if the calibration label does not exist cam_calib = device_calib.get_camera_calib(&quot;camera-rgb&quot;); imu_calib = device_calib.get_imu_calib(&quot;imu-left&quot;);   ","version":"Next","tagName":"h3"},{"title":"Accessing ET and Microphone calibration​","type":1,"pageTitle":"Calibration Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/calibration#accessing-et-and-microphone-calibration","content":" Note Aria's ET camera stream and audio are special types:  Aria's ET stream switches the stream for left and right ET together, thus its calibration is a pair of CameraCalibration.Aria's Audio stream has 7 channels, thus its calibration is an array of seven microphone calib.  PythonC++ # returns None if the calibration label does not exist et_calib = device_calib.get_aria_et_camera_calib() print(et_calib[0].get_label()) mic_calib = device_calib.get_aria_microphone_calib() print(mic_calib[0].get_label())     ","version":"Next","tagName":"h3"},{"title":"Python binding for Sophus library​","type":1,"pageTitle":"Calibration Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/calibration#python-binding-for-sophus-library","content":" Sophus Python PyBind implements Python binding for Sophus that provides access to SO3, SE3, interpolate and iterativeMean features.  This Python binding has been submitted to Sophus and will be officially supported by the Sophus Library GitHub repo soon. Once it is available through the Sophus Library, this section will point to Sophus documentation and code, to avoid duplication.  The user interface is inspired by scipy.spatial.transform.Rotation.  ","version":"Next","tagName":"h2"},{"title":"Feature list​","type":1,"pageTitle":"Calibration Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/calibration#feature-list","content":" SO3 Initialize with from_quat(), from_matrix(), exp()Convert to functions: to_quat(), to_matrix(), log()Multiplication with SO3 or 3D pointsOperator [] for setting/getting items with index or slicesInverse, copy, print, and lenFunction vectorization SE3 Initialize with from_quat_and_translation(), from_matrix(), from_matrix3x4(), exp()Convert to functions to_quat_and_translation(), to_matrix(), to_matrix3x4(), log()Multiplication with SE3 or 3D pointsGet rotation and translation component with rotation() and translation()Operator [] for setting/getting items with index or slicesFunction vectorizationInverse, copy, print, and lenInterpolate between two SE3Iterative mean of a group of SE3  ","version":"Next","tagName":"h3"},{"title":"Import Sophus Python binding​","type":1,"pageTitle":"Calibration Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/calibration#import-sophus-python-binding","content":" from projectaria_tools.core.sophus import SO3, SE3, interpolate, iterativeMean   ","version":"Next","tagName":"h3"},{"title":"Example code​","type":1,"pageTitle":"Calibration Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/calibration#example-code","content":" Example code is provided in projectaria_tools/core/examples/sophus_quickstart_tutorial.ipynb  python3 -m jupyter ensures that the Jupyter comes from the virtual environment that contains the projectaria_tools module.  ","version":"Next","tagName":"h3"},{"title":"Vectorization detail​","type":1,"pageTitle":"Calibration Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/calibration#vectorization-detail","content":" In Python, we chose to export our Sophus::SO3 as a vector of SO3 objects by binding the cpp object SO3Group defined below. This is because numerical code in Python tends to work with array of values so that the program can be efficient. This approach is inspired by scipy.spatial.transform.Rotation.  ","version":"Next","tagName":"h3"},{"title":"Passing a single SO3/SE3 object to C++ code in Python binding​","type":1,"pageTitle":"Calibration Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/calibration#passing-a-single-so3se3-object-to-c-code-in-python-binding","content":" To allow other Python binding C++ code to take in a single SO3/SE3 object, we built a caster so that, even if we wrap SO3Group/SE3Group in Python, those can be implicitly converted to the C++ Sophus::SO3/SE3 object at the boundaries between languages.  This enables us to pass the Python SO3/SE3 object to a C++ function as if they were a regular 1-element Sophus::SO3/SE3 object. This simplifies binding the rest of the C++ code. The implicit cast fails if the Python object is not a 1-element object. ","version":"Next","tagName":"h3"},{"title":"Advanced Image Utilities","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/image_utilities","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Advanced Image Utilities","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/image_utilities#overview","content":" This page provides advanced image utilities code snippets for Project Aria Tools, see also Image Code Snippets.  ","version":"Next","tagName":"h2"},{"title":"Image debayer​","type":1,"pageTitle":"Advanced Image Utilities","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/image_utilities#image-debayer","content":" Some recording profiles outputs raw RGB images (Profile 7 in Recording Profile). We provide functionalities to debayer them and perform white-balancing to get RGB images.  PythonC++ from projectaria_tools.core import data_provider, image stream_id = provider.get_stream_id_from_label(&quot;camera-rgb&quot;) image_data = provider.get_image_data_by_index(stream_id, 0) image_data_array = image_data[0].to_numpy_array() debayered_array = image.debayer(image_data_array)     See projectaria_tools/core/image/utility/Debayer.cpp for implementation  ","version":"Next","tagName":"h2"},{"title":"Image undistortion​","type":1,"pageTitle":"Advanced Image Utilities","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/image_utilities#image-undistortion","content":" In this example, we remove distortions in raw sensor data so that straight 3D lines appear straight in the undistorted images. There is existing C++ implementation and Python wrapper of this helper function in the data utilities.  PythonC++ from projectaria_tools.core import data_provider, calibration camera_label = &quot;camera-slam-left&quot; stream_id = provider.get_stream_id_from_label(camera_label) calib = provider.get_device_calibration().get_camera_calib(camera_label) pinhole = calibration.get_linear_camera_calibration(512, 512, 150) raw_image = provider.get_image_data_by_index(stream_id, 0)[0].to_numpy_array() undistorted_image = calibration.distort_by_calibration(raw_image, pinhole, calib)     Go to projectaria_tools/core/calibration/utility/Distort.cpp for implementation.  Go to the Project Aria FAQ for more calibration information and resources.  ","version":"Next","tagName":"h2"},{"title":"Rotated image clockwise 90 degrees​","type":1,"pageTitle":"Advanced Image Utilities","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/image_utilities#rotated-image-clockwise-90-degrees","content":" In this example, we rotated the RGB image 90 degrees and provide the new calibration object.  Calibration rotation only applies to pinhole camera modelPinhole camera calibration object needs to be initialized as pinhole = calibration.get_linear_camera_calibration(512, 512, 150, camera_label, calib.get_transform_device_camera())with camera_label and the pose calib.get_transform_device_camera() so that pinhole_cw90 can have the correct transformation matrix when unprojecting a pixel to get ray_in_device_frame.  PythonC++ camera_label = &quot;camera-rgb&quot; stream_id = provider.get_stream_id_from_label(camera_label) calib = provider.get_device_calibration().get_camera_calib(camera_label) pinhole = calibration.get_linear_camera_calibration(512, 512, 150, camera_label, calib.get_transform_device_camera()) raw_image = provider.get_image_data_by_index(stream_id, 0)[0].to_numpy_array() undistorted_image = calibration.distort_by_calibration(raw_image, pinhole, calib) # Rotated image by CW90 degrees rotated_image = np.rot90(undistorted_image, k=3) # Get rotated image calibration pinhole_cw90 = calibration.rotate_camera_calib_cw90deg(pinhole) # Unproject a pixel and get a ray from device coordinate frame test_pixel_in_rotated_image = [10,0] ray_in_device_frame = pinhole_cw90.get_transform_device_camera() @ pinhole_cw90.unproject_no_checks(test_pixel_in_rotated_image)   ","version":"Next","tagName":"h2"},{"title":"Color Correction​","type":1,"pageTitle":"Advanced Image Utilities","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/image_utilities#color-correction","content":" Videos and images taken with earlier versions of the Aria OS may show color distortion due to inconsistent gamma curves and unconventional color temperatures, leading to inconsistent and overly blue hues. To address this, we've introduced a Color Correction feature in the ARK v1.13 update. With this update, images captured by Aria devices now have more consistent colors and are calibrated to a standard color temperature of 5000K. We also recommend applying devignetting to the images to achieve more accurate image quality.  ","version":"Next","tagName":"h2"},{"title":"Color Correction user recommendations​","type":1,"pageTitle":"Advanced Image Utilities","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/image_utilities#color-correction-user-recommendations","content":" The new Color Correction feature is available as an option in data campaigns and the Companion App. It's enabled by default in the new Aria OS, and we recommend users utilize the feature for future Aria captures to achieve optimal image quality. We also suggest keeping existing images captured with the old Aria OS and combining them with new Aria captures for research purposes. For advanced users, we provide a Color Correction API in Project Aria Tools that can correct color distortions in old captures and images. We advise using this API with caution for the following reasons:  The post-processing API can effectively convert and correct color distortion in older images, particularly in environments that are not overexposed. If your images do not contain overexposed areas, we recommend converting them for optimal image quality.If an image contains overexposed areas, the colors in the overexposed regions may be oversaturated due to distortion in color settings: this may make it impossible to accurately recover the proper colors, potentially resulting in artifacts. This is more common in outdoor environments, where oversaturated areas can appear around the sky or sun. In such cases, we recommend retaining the old images without converting them.  ","version":"Next","tagName":"h3"},{"title":"Potential failure cases​","type":1,"pageTitle":"Advanced Image Utilities","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/image_utilities#potential-failure-cases","content":" The issue with overexposed captures is demonstrated in the picture below. In the left image, captured by the old Aria OS, the central area is oversaturated due to strong light around the sun, resulting in the loss of digital values above 255 for higher-range colors. These lost colors cannot be recovered. The middle image, where the old image has been color-corrected and devignetted, displays a yellowish tint after the color correction conversion due to the original image's oversaturation. This problem does not occur in images captured with the updated Aria OS, shown in the right image.    Below, we demonstrate how to apply color correction:  Set set_color_correction as True (the default value is False).The output from provider.get_image_data_by_index would be color-corrected.  PythonC++ from projectaria_tools.core import data_provider camera_label = &quot;camera-rgb&quot; stream_id = provider.get_stream_id_from_label(camera_label) # turn on color correction provider.set_color_correction(True) # the output image from get_image_data_by_index will be color corrected color_corrected_image_array = provider.get_image_data_by_index(stream_id, 0)[0].to_numpy_array()   Go to projectaria_tools/core/image/utility/ColorCorrect.cpp for implementation.  ","version":"Next","tagName":"h3"},{"title":"Image devignetting​","type":1,"pageTitle":"Advanced Image Utilities","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/image_utilities#image-devignetting","content":" Devignetting corrects uneven lighting, enhancing image uniformity and clarity. We provide devignetting for camera-rgb full size image [2880, 2880], camera-rgb half size image[1408, 1408] and slam image [640, 480].  Download Aria devignetting masks containing following files:  devignetting_masks_bin |- new_isp |- slam_devignetting_mask.bin |- rgb_half_devignetting_mask.bin |- rgb_full_devignetting_mask.bin |- old_isp |- slam_devignetting_mask.bin |- rgb_half_devignetting_mask.bin |- rgb_full_devignetting_mask.bin   Turn on devignetting. Set devignetting mask folder path with the local aria camera devignetting masks folder path. set_devignetting(True) mask_folder_path = &quot;devignetting_masks_bin&quot; set_devignetting_mask_folder_path(mask_folder_path) The image data from get_image_data_by_index will be devignetted.(Optional) If you don't want to devignetting feature, turn off by calling set_devignetting(False)  PythonC++ from projectaria_tools.core import data_provider camera_label = &quot;camera-rgb&quot; stream_id = provider.get_stream_id_from_label(camera_label) # set devignetting mask path devignetting_mask_folder_path = &lt;FOLDER_PATH_CONTAINING_DEVIGNETTING_MASK&gt; provider.set_devignetting_mask_folder_path(devignetting_mask_folder_path) # turn on devignetting provider.set_devignetting(True) # the output image from get_image_data_by_index will be devignetted devignetted_image = provider.get_image_data_by_index(stream_id, 0)     Go to projectaria_tools/core/image/utility/Devignetting.cpp for implementation. ","version":"Next","tagName":"h2"},{"title":"Aria Data Provider Code Snippets","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/data_provider","content":"","keywords":"","version":"Next"},{"title":"Open a VRS file​","type":1,"pageTitle":"Aria Data Provider Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/data_provider#open-a-vrs-file","content":" PythonC++ from projectaria_tools.core import data_provider from projectaria_tools.core.sensor_data import TimeDomain, TimeQueryOptions from projectaria_tools.core.stream_id import RecordableTypeId, StreamId vrsfile = &quot;example.vrs&quot; provider = data_provider.create_vrs_data_provider(vrsfile) assert provider is not None, &quot;Cannot open file&quot;   ","version":"Next","tagName":"h3"},{"title":"Mapping between labels and stream ids​","type":1,"pageTitle":"Aria Data Provider Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/data_provider#mapping-between-labels-and-stream-ids","content":" PythonC++ Stream IDs can be mapped from labels by using get_stream_id_from_label: stream_id = provider.get_stream_id_from_label(&quot;camera-slam-left&quot;) Inversely, you can retrieve a label from a stream ID by using get_stream_id_from_label: label = provider.get_label_from_stream_id(StreamId(&quot;1201-1&quot;))   ","version":"Next","tagName":"h3"},{"title":"Random access data by index​","type":1,"pageTitle":"Aria Data Provider Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/data_provider#random-access-data-by-index","content":" PythonC++ for stream_id in provider.get_all_streams(): for i in range(0, provider.get_num_data(stream_id)): sensor_data = provider.get_sensor_data_by_index(stream_id, i)   ","version":"Next","tagName":"h3"},{"title":"Random access data by timestamp​","type":1,"pageTitle":"Aria Data Provider Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/data_provider#random-access-data-by-timestamp","content":" Project Aria data has four kinds of TimeDomain entries. We strongly recommend always working with DEVICE_TIME when using single-device Aria data. The TIME_CODE TimeDomain is used when synchronizing time across multiple devices. Go to Timestamps in Aria VRS Files for more information.  PythonC++ TimeDomain.RECORD_TIMETimeDomain.DEVICE_TIME - recommendedTimeDomain.HOST_TIMETimeDomain.TIME_CODE - for multiple devices You can also search using three different time query options: TimeQueryOptions.BEFORE (default): last data with t &lt;= t_queryTimeQueryOptions.AFTER : first data with t &gt;= t_queryTimeQueryOptions.CLOSEST : the data where |t - t_query| is smallest for stream_id in provider.get_all_streams(): t_first = provider.get_first_time_ns(stream_id, TimeDomain.DEVICE_TIME) t_last = provider.get_last_time_ns(stream_id, TimeDomain.DEVICE_TIME) query_timestamp = (t_first + t_last) // 2 # example query timestamp sensor_data = provider.get_sensor_data_by_time_ns(stream_id, query_timestamp, TimeDomain.DEVICE_TIME, TimeQueryOptions.CLOSEST)   ","version":"Next","tagName":"h3"},{"title":"Deliver all sensor data in VRS​","type":1,"pageTitle":"Aria Data Provider Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/data_provider#deliver-all-sensor-data-in-vrs","content":" PythonC++ Async iterator to deliver sensor data for all streams in device time order: for data in provider.deliver_queued_sensor_data(): print(data.get_time_ns(TimeDomain.DEVICE_TIME)) Alternatively, you can use iterator-type syntax: seq = provider.deliver_queued_sensor_data() obj = next(seq) while True: print(obj.get_time_ns(TimeDomain.DEVICE_TIME)) try: obj = next(seq) except StopIteration: break Deliver with sub-stream selection, time truncation, and frame rate sub-sampling: # Starts by default options which activates all sensors deliver_option = provider.get_default_deliver_queued_options() # Only play data from two cameras, also reduce framerate to half of vrs deliver_option.deactivate_stream_all() for label in [&quot;camera-slam-left&quot;, &quot;camera-slam-right&quot;]: streamId = provider.get_stream_id_from_label(label) deliver_option.activate_stream(streamId) deliver_option.set_subsample_rate(streamId, 2) # skip first 100ns deliver_option.set_truncate_first_device_time_ns(100) for data in provider.deliver_queued_sensor_data() : print(data.get_time_ns(TimeDomain.DEVICE_TIME))  ","version":"Next","tagName":"h3"},{"title":"Image Code Snippets","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/image","content":"","keywords":"","version":"Next"},{"title":"Raw sensor data​","type":1,"pageTitle":"Image Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/image#raw-sensor-data","content":" Raw image data is stored in ImageData. ImageData is a type alias of an std::pair. The two components of that pair are:  The image frame stored in vrs::PixelFrame class (potentially compressed) We recommend that users do not directly use PixelFrame Image data records Image acquisition information such as timestamps, exposure and gain  PythonC++ from projectaria_tools.core import data_provider, image from projectaria_tools.core.stream_id import StreamId vrsfile = &quot;example.vrs&quot; provider = data_provider.create_vrs_data_provider(vrsfile) stream_id = provider.get_stream_id_from_label(&quot;camera-slam-left&quot;) image_data = provider.get_image_data_by_index(stream_id, 0) pixel_frame = image_data[0].pixel_frame   ","version":"Next","tagName":"h2"},{"title":"Manipulating images​","type":1,"pageTitle":"Image Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/image#manipulating-images","content":" PythonC++ In Python, we provide an interface for converting from ImageData into numpy arrays. image_array = image_data[0].to_numpy_array()  ","version":"Next","tagName":"h2"},{"title":"MPS Code Snippets","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/mps","content":"","keywords":"","version":"Next"},{"title":"Load MPS output​","type":1,"pageTitle":"MPS Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/mps#load-mps-output","content":" The loaders for MPS outputs (projectaria_tools/main/core/mps) make it easer to use the data downstream. As part of this, the loaders put the outputs into data structures that are easier for other tools to consume.  MPS Data Formats provides details about output schemas and the specifics of each MPS output. This page focuses loading APIs in Python and C++, where there isn't a standalone code samples page:  Eye Gaze Code Samples  ","version":"Next","tagName":"h2"},{"title":"Open loop/Closed loop trajectory​","type":1,"pageTitle":"MPS Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/mps#open-loopclosed-loop-trajectory","content":" PythonC++ import projectaria_tools.core.mps as mps open_loop_path = &quot;/path/to/mps/output/trajectory/open_loop_trajectory.csv&quot; open_loop_traj = mps.read_open_loop_trajectory(open_loop_path) closed_loop_path = &quot;/path/to/mps/output/trajectory/closed_loop_trajectory.csv&quot; closed_loop_traj = mps.read_closed_loop_trajectory(closed_loop_path) # example: get transformation from this device to a world coordinate frame for closed_loop_pose in closed_loop_traj: transform_world_device = closed_loop_pose.transform_world_device # example: query to find the closest Timestamp device pose and move it to the Aria RGB camera pose from projectaria_tools.core import data_provider from projectaria_tools.core.mps.utils import get_nearest_pose from projectaria_tools.core.stream_id import StreamId query_timestamp_ns = int(closed_loop_traj[1].tracking_timestamp.total_seconds() * 1e9) # to be updated with your VRS timestamps pose_info = get_nearest_pose(closed_loop_traj, query_timestamp_ns) if pose_info: T_world_device = pose_info.transform_world_device # Move this pose to the Project Aria RGB camera vrs_file = &quot;example.vrs&quot; vrs_data_provider = data_provider.create_vrs_data_provider(vrs_file) rgb_stream_id = StreamId(&quot;214-1&quot;) rgb_stream_label = vrs_data_provider.get_label_from_stream_id(rgb_stream_id) device_calibration = vrs_data_provider.get_device_calibration() rgb_camera_calibration = device_calibration.get_camera_calib(rgb_stream_label) T_device_rgb_camera = rgb_camera_calibration.get_transform_device_camera() T_world_rgb_camera = T_world_device @ T_device_rgb_camera print(T_world_rgb_camera)   ","version":"Next","tagName":"h3"},{"title":"Point cloud​","type":1,"pageTitle":"MPS Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/mps#point-cloud","content":" Always filter global point clouds in 3D Post-filtering the point cloud using inverse distance and distance certainty is required to get point cloud accurate in 3D space. There are points cannot be accurately estimated in 3D space due to low parallax, but those points are well tracked in 2D images, and produce valid 2D observations. We choose to output all the points, include those have poor 3D estimations, in case researchers need them. Go to the Semi-Dense Point Cloud page for more information.  Loading observations could be slow When the Aria recording is long, loading point observations could be memory and time consuming (&gt; 1 minute). A typical 20 minutes long Aria recording will have roughly total 10+ millions of 3D points with total 100+ millions of 2D observations.  PythonC++ import projectaria_tools.core.mps as mps from projectaria_tools.core.mps.utils import filter_points_from_confidence global_points_path = &quot;/path/to/mps/output/trajectory/semidense_points.csv.gz&quot; points = mps.read_global_point_cloud(global_points_path) # filter the point cloud using thresholds on the inverse depth and distance standard deviation inverse_distance_std_threshold = 0.001 distance_std_threshold = 0.15 filtered_points = filter_points_from_confidence(points, inverse_distance_std_threshold, distance_std_threshold) # example: get position of this point in the world coordinate frame for point in filtered_points: position_world = point.position_world observations_path = &quot;/path/to/mps/output/trajectory/semidense_observations.csv.gz&quot; observations = mps.read_point_observations(observations_path)   ","version":"Next","tagName":"h3"},{"title":"Online calibration​","type":1,"pageTitle":"MPS Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/mps#online-calibration","content":" PythonC++ import projectaria_tools.core.mps as mps online_calib_path = &quot;/path/to/mps/output/trajectory/online_calibration.jsonl&quot; online_calibs = mps.read_online_calibration(online_calib_path) for calib in online_calibs: # example: get left IMU's online calibration for imuCalib in calib.imu_calibs: if imuCalib.get_label() == &quot;imu-left&quot;: leftImuCalib = imuCalib # example: get left SLAM camera's online calibration for camCalib in calib.camera_calibs: if camCalib.get_label() == &quot;camera-slam-left&quot;: leftCamCalib = camCalib   ","version":"Next","tagName":"h3"},{"title":"Hand Tracking​","type":1,"pageTitle":"MPS Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/mps#hand-tracking","content":" PythonC++ import projectaria_tools.core.mps as mps from projectaria_tools.core.mps.utils import get_nearest_hand_tracking_result ## Load hand tracking results hand_tracking_results_path = &quot;/path/to/mps/output/hand_tracking/hand_tracking_results.csv&quot; hand_tracking_results = mps.hand_tracking.read_hand_tracking_results( hand_tracking_results_path ) # Example query: find the nearest hand tracking data outputs in relation to a specific timestamp hand_tracking_result = get_nearest_hand_tracking_result( hand_tracking_results, query_timestamp_ns ) # Get left and right-hand confidences, landmarks wrist and palm positions and normals in the device frame. Note that if hands are not found, per-hand result is None. # Left-hand side if hand_tracking_result.left_hand: left_pose_confidence = hand_tracking_result.left_hand.confidence left_landmark_positions_device = hand_tracking_result.left_hand.landmark_positions_device left_transform_device_wrist = hand_tracking_result.left_hand.transform_device_wrist # Use helper functions to get wrist and palm positions in the device frame left_wrist_position_device = hand_tracking_result.left_hand.get_wrist_position_device() left_palm_position_device = hand_tracking_result.left_hand.get_palm_position_device() if hand_tracking_result.left_hand.wrist_and_palm_normal_device: left_wrist_normal_device = hand_tracking_result.left_hand.wrist_and_palm_normal_device.wrist_normal_device left_palm_normal_device = hand_tracking_result.left_hand.wrist_and_palm_normal_device.palm_normal_device # Right-hand side if hand_tracking_result.right_hand: right_pose_confidence = hand_tracking_result.right_hand.confidence right_landmark_positions_device = hand_tracking_result.right_hand.landmark_positions_device right_transform_device_wrist = hand_tracking_result.right_hand.transform_device_wrist # Use helper functions to get wrist and palm positions in the device frame right_wrist_position_device = hand_tracking_result.right_hand.get_wrist_position_device() right_palm_position_device = hand_tracking_result.right_hand.get_palm_position_device() if hand_tracking_result.right_hand.wrist_and_palm_normal_device: right_wrist_normal_device = hand_tracking_result.right_hand.wrist_and_palm_normal_device.wrist_normal_device right_palm_normal_device = hand_tracking_result.right_hand.wrist_and_palm_normal_device.palm_normal_device  ","version":"Next","tagName":"h3"},{"title":"Eye Gaze Code Snippets","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/eye_gaze_code","content":"","keywords":"","version":"Next"},{"title":"Further resources​","type":1,"pageTitle":"Eye Gaze Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/eye_gaze_code#further-resources","content":" Eye Gaze Data Format  ","version":"Next","tagName":"h3"},{"title":"General​","type":1,"pageTitle":"Eye Gaze Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/eye_gaze_code#general","content":" ","version":"Next","tagName":"h2"},{"title":"Load Eye Gaze in Python and C++​","type":1,"pageTitle":"Eye Gaze Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/eye_gaze_code#load-eye-gaze-in-python-and-c","content":" Data loaders for MPS outputs are provided as part of Project Aria Tools (projectaria_tools/main/core/mps). As part of this, the loaders put the outputs into data structures that are easier for other tools to consume.  In this example, we set the default eye gaze depth to 1 meter. We use 1m because it is close to what people could reach with their arms and look at if they were grabbing objects. You can also use .depth to define depth_m when using data generated using the new eye gaze model.  PythonC++ import projectaria_tools.core.mps as mps gaze_path = &quot;/path/to/mps/output/eye_gaze/general_eye_gaze.csv&quot; gaze_cpf = mps.read_eyegaze(eye_gaze_path) # Set default eye gaze depth for 3D points to 1 meter depth_m = 1.0 gaze_point_cpf = mps.get_eyegaze_point_at_depth(gaze_cpf[1].yaw, gaze_cpf[1].pitch, depth_m) # Example query: find the nearest eye gaze data outputs in relation to a specific timestamp from projectaria_tools.core import data_provider from projectaria_tools.core.stream_id import StreamId from projectaria_tools.core.mps.utils import ( get_gaze_vector_reprojection, get_nearest_eye_gaze ) # Query Eye Gaze data at a desired timestamp # For this example we use an eyegaze data timestamp # You can also use a VRS timestamp (i.e timestamp from a loop reading all the images) query_timestamp_ns = int(gaze_cpf[1].tracking_timestamp.total_seconds() * 1e9) eye_gaze_info = get_nearest_eye_gaze(gaze_cpf, query_timestamp_ns) if eye_gaze_info: # Re-project the eye gaze point onto the RGB camera data vrs_file = &quot;example.vrs&quot; vrs_data_provider = data_provider.create_vrs_data_provider(vrs_file) rgb_stream_id = StreamId(&quot;214-1&quot;) rgb_stream_label = vrs_data_provider.get_label_from_stream_id(rgb_stream_id) device_calibration = vrs_data_provider.get_device_calibration() rgb_camera_calibration = device_calibration.get_camera_calib(rgb_stream_label) gaze_projection = get_gaze_vector_reprojection( eye_gaze_info, rgb_stream_label, device_calibration, rgb_camera_calibration, depth_m, ) print(gaze_projection)   ","version":"Next","tagName":"h3"},{"title":"Yaw/Pitch to 3D vector conversion​​","type":1,"pageTitle":"Eye Gaze Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/eye_gaze_code#yawpitch-to-3d-vector-conversion","content":" Convert the gaze angles into 3D vectors. To convert a gaze measurement (yaw/pitch) into a 3D gaze vector originating at the origin of CPF (or 3D gaze point) use the Eigen::Vector3d getEyeGazePointAtDepth operation in EyeGazeReader.h. This function can be used with the new model output and old model output (go to Eye Gaze Data Format for more information about Eye Gaze changes).  ","version":"Next","tagName":"h3"},{"title":"Get Central Pupil Frame(CPF) to Device Transforms (Python)​","type":1,"pageTitle":"Eye Gaze Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/eye_gaze_code#get-central-pupil-framecpf-to-device-transforms-python","content":"   vrs_file = &quot;example.vrs&quot; vrs_data_provider = data_provider.create_vrs_data_provider(vrs_file) rgb_stream_id = StreamId(&quot;214-1&quot;) rgb_stream_label = vrs_data_provider.get_label_from_stream_id(rgb_stream_id) device_calibration = vrs_data_provider.get_device_calibration() rgb_camera_calibration = device_calibration.get_camera_calib(rgb_stream_label) # transform_cpf_sensor is the transform of sensor to cpf frame transform_cpf_sensor = device_calibration.get_transform_cpf_sensor(stream_id_label)   ","version":"Next","tagName":"h3"},{"title":"Helper functions to support the new Eye Gaze model​","type":1,"pageTitle":"Eye Gaze Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/eye_gaze_code#helper-functions-to-support-the-new-eye-gaze-model","content":" The following commands can be used to generate yaw_rads_cpf values.  ","version":"Next","tagName":"h2"},{"title":"Get combined gaze direction and depth from left and right gaze direction angles​","type":1,"pageTitle":"Eye Gaze Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/eye_gaze_code#get-combined-gaze-direction-and-depth-from-left-and-right-gaze-direction-angles","content":" PythonC++ gaze_path = &quot;/path/to/mps/output/eye_gaze/general_eye_gaze.csv&quot; gaze_cpf = mps.read_eyegaze(eye_gaze_path) depth, combined_yaw, combined_pitch = ( mps.compute_depth_and_combined_gaze_direction( gaze_cpf[1].vergence.left_yaw, gaze_cpf[1].vergence.right_yaw, gaze_cpf[1].pitch ) )   ","version":"Next","tagName":"h3"},{"title":"Get gaze intersection in 3D coordinates in CPF frame from left and right gaze direction angles​","type":1,"pageTitle":"Eye Gaze Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/eye_gaze_code#get-gaze-intersection-in-3d-coordinates-in-cpf-frame-from-left-and-right-gaze-direction-angles","content":" PythonC++ gaze_x, gaze_y, gaze_z = mps.get_gaze_intersection_point( gaze_cpf[1].vergence.left_yaw, gaze_cpf[1].vergence.right_yaw, gaze_cpf[1].pitch )   ","version":"Next","tagName":"h3"},{"title":"Get left and right gaze directions in CPF frame​","type":1,"pageTitle":"Eye Gaze Code Snippets","url":"/projectaria_tools/docs/data_utilities/core_code_snippets/eye_gaze_code#get-left-and-right-gaze-directions-in-cpf-frame","content":" Get left_gaze and right_gaze direction vectors (numpy arrays) given left and right yaws and common pitch. The vectors are from left and right eye positions to the point of intersection. Currently left eye and right eye origins are [0.0315, 0, 0] and [-0.0315, 0, 0] in CPF coordinates respectively.  PythonC++ left_gaze, right_gaze = mps.get_gaze_vectors( gaze_cpf[1].vergence.left_yaw, gaze_cpf[1].vergence.right_yaw, gaze_cpf[1].pitch )  ","version":"Next","tagName":"h3"},{"title":"Getting Started with Project Aria Data Utilities","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities/getting_started","content":"","keywords":"","version":"Next"},{"title":"Running Jupyter Notebooks on Google Colab​","type":1,"pageTitle":"Getting Started with Project Aria Data Utilities","url":"/projectaria_tools/docs/data_utilities/getting_started#running--jupyter-notebooks-on-google-colab","content":" Use the following links to run the Python notebooks in an installation free playground:  Dataprovider Quickstart Tutorial - core tutorialMachine Perception Services Tutorial - core tutorialAria Digital Twin (ADT) - open data tutorialProject Aria TICSync Tutorial - Time synchronization between multiple devices tutorial, demo data provided  ","version":"Next","tagName":"h2"},{"title":"Running the Jupyter Notebooks locally​","type":1,"pageTitle":"Getting Started with Project Aria Data Utilities","url":"/projectaria_tools/docs/data_utilities/getting_started#running-the-jupyter-notebooks-locally","content":" ","version":"Next","tagName":"h2"},{"title":"Step 0 : Check system requirements and download codebase​","type":1,"pageTitle":"Getting Started with Project Aria Data Utilities","url":"/projectaria_tools/docs/data_utilities/getting_started#step-0--check-system-requirements-and-download-codebase","content":" Go to the Download Codebase page to:  Check your system is supportedDownload projectaria_tools codebase from the github  ","version":"Next","tagName":"h3"},{"title":"Step 1 : Install/Update Python 3​","type":1,"pageTitle":"Getting Started with Project Aria Data Utilities","url":"/projectaria_tools/docs/data_utilities/getting_started#step-1--installupdate-python-3","content":" To use the Jupyter notebooks in this tutorial you'll need Python 3.9 or above (3.10 if you are on Apple Silicon). To ensure all utilities work effectively, we recommend keeping Python 3 up to date.  Python 3 download pageTo check what what version of Python 3 you have use python3 --version  ","version":"Next","tagName":"h3"},{"title":"Step 2 : Create a virtual environment​","type":1,"pageTitle":"Getting Started with Project Aria Data Utilities","url":"/projectaria_tools/docs/data_utilities/getting_started#step-2--create-a-virtual-environment","content":" rm -rf $HOME/projectaria_tools_python_env python3 -m venv $HOME/projectaria_tools_python_env source $HOME/projectaria_tools_python_env/bin/activate   ","version":"Next","tagName":"h3"},{"title":"Step 3 : Install projectaria_tools from PyPI​","type":1,"pageTitle":"Getting Started with Project Aria Data Utilities","url":"/projectaria_tools/docs/data_utilities/getting_started#step-3--install-projectaria_tools-from-pypi","content":" python3 -m pip install --upgrade pip python3 -m pip install projectaria-tools'[all]'   ","version":"Next","tagName":"h3"},{"title":"Step 4: Run Dataprovider quickstart tutorial​","type":1,"pageTitle":"Getting Started with Project Aria Data Utilities","url":"/projectaria_tools/docs/data_utilities/getting_started#step-4-run-dataprovider-quickstart-tutorial","content":" cd $HOME/Documents/projectaria_sandbox jupyter notebook projectaria_tools/core/examples/dataprovider_quickstart_tutorial.ipynb   Jupyter notebook error If you get a Jupyter notebook error please upgrade Python 3 to the latest version and recreate your virtual environment.  ","version":"Next","tagName":"h3"},{"title":"Step 5: Run Sophus Pybind Tutorial​","type":1,"pageTitle":"Getting Started with Project Aria Data Utilities","url":"/projectaria_tools/docs/data_utilities/getting_started#step-5-run-sophus-pybind-tutorial","content":" cd $HOME/Documents/projectaria_sandbox jupyter notebook projectaria_tools/core/examples/sophus_quickstart_tutorial.ipynb   ","version":"Next","tagName":"h3"},{"title":"Step 6: Run Machine Perception Services (MPS) quickstart tutorial​","type":1,"pageTitle":"Getting Started with Project Aria Data Utilities","url":"/projectaria_tools/docs/data_utilities/getting_started#step-6-run-machine-perception-services-mps-quickstart-tutorial","content":" In the MPS tutorial, the notebook walks through how to visualize gaze, trajectory, and point cloud from MPS data.  cd $HOME/Documents/projectaria_sandbox jupyter notebook projectaria_tools/core/examples/mps_quickstart_tutorial.ipynb   ","version":"Next","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"Getting Started with Project Aria Data Utilities","url":"/projectaria_tools/docs/data_utilities/getting_started#troubleshooting","content":" Check the Troubleshooting Guide if you encounter issues using this tutorial.  ","version":"Next","tagName":"h2"},{"title":"Other Useful Links​","type":1,"pageTitle":"Getting Started with Project Aria Data Utilities","url":"/projectaria_tools/docs/data_utilities/getting_started#other-useful-links","content":" TroubleshootingInstallation guideVisualizers PythonC++ ","version":"Next","tagName":"h2"},{"title":"How to Use CMake with Project Aria Tools","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities/installation/build_with_cmake","content":"","keywords":"","version":"Next"},{"title":"Example code​","type":1,"pageTitle":"How to Use CMake with Project Aria Tools","url":"/projectaria_tools/docs/data_utilities/installation/build_with_cmake#example-code","content":" Please refer to the sample project for a full example.  Install targets are coming soon! So that you can install projectaria_tools and access it using find_package() in your CMakeLists.txt ","version":"Next","tagName":"h3"},{"title":"How to Download Project Aria Tools","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities/installation/download_codebase","content":"","keywords":"","version":"Next"},{"title":"Supported Platforms​","type":1,"pageTitle":"How to Download Project Aria Tools","url":"/projectaria_tools/docs/data_utilities/installation/download_codebase#supported-platforms","content":" The codebase is supported on:  x64 Linux distributions of: Fedora 36, 37, 38Ubuntu focal (20.04 LTS) and jammy (22.04 LTS)Windows (MSVC 2019/2022) Mac Intel or Mac ARM-based (M1) with MacOS 11 (Big Sur) or newer  ","version":"Next","tagName":"h2"},{"title":"Stable versus Develop​","type":1,"pageTitle":"How to Download Project Aria Tools","url":"/projectaria_tools/docs/data_utilities/installation/download_codebase#stable-versus-develop","content":" Access the latest stable version of Project Aria Tools with git TAGS. The develop version is pushed continuously to the main branch.  ","version":"Next","tagName":"h2"},{"title":"Download codebase​","type":1,"pageTitle":"How to Download Project Aria Tools","url":"/projectaria_tools/docs/data_utilities/installation/download_codebase#download-codebase","content":" StableDevelop mkdir -p $HOME/Documents/projectaria_sandbox cd $HOME/Documents/projectaria_sandbox git clone https://github.com/facebookresearch/projectaria_tools.git -b 1.7.1  ","version":"Next","tagName":"h2"},{"title":"How to Download MPS sample dataset","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities/installation/download_mps_sample_data","content":"How to Download MPS sample dataset This sample (hosted at projectaria.com) contains a raw VRS file and all the corresponding MPS outputs. info If you are in zsh (default on Mac OS), you may need to run setopt shwordsplit for the following instructions to work export MPS_SAMPLE_PATH=/tmp/mps_sample export BASE_URL=&quot;https://www.projectaria.com/async/sample/download/?bucket=mps&amp;filename=&quot; mkdir -p $MPS_SAMPLE_PATH export OPTIONS=&quot;-C - -O -L&quot; curl -o $MPS_SAMPLE_PATH/sample.vrs $OPTIONS &quot;${BASE_URL}sample.vrs&quot; curl -o $MPS_SAMPLE_PATH/slam.zip $OPTIONS &quot;${BASE_URL}slam_v1_1_0.zip&quot; curl -o $MPS_SAMPLE_PATH/eye_gaze.zip $OPTIONS &quot;${BASE_URL}eye_gaze_v3_1_0.zip&quot; curl -o $MPS_SAMPLE_PATH/hand_tracking.zip $OPTIONS &quot;${BASE_URL}hand_tracking_v2_0_0.zip&quot; unzip -o $MPS_SAMPLE_PATH/eye_gaze.zip -d $MPS_SAMPLE_PATH unzip -o $MPS_SAMPLE_PATH/slam.zip -d $MPS_SAMPLE_PATH unzip -o $MPS_SAMPLE_PATH/hand_tracking.zip -d $MPS_SAMPLE_PATH The above commands will download the most recent sample dataset from Project Aria. You can also download a more limited sample dataset from our GitHub repository.","keywords":"","version":"Next"},{"title":"Tutorial: How to Plot Sensor Data Using Python","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/plotting_sensor_data","content":"","keywords":"","version":"Next"},{"title":"Getting started​","type":1,"pageTitle":"Tutorial: How to Plot Sensor Data Using Python","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/plotting_sensor_data#getting-started","content":" Use the data_provider to open the VRS file you want to work with.  from projectaria_tools.core import data_provider, image from projectaria_tools.core.stream_id import StreamId vrsfile = &quot;example.vrs&quot; provider = data_provider.create_vrs_data_provider(vrsfile)     ","version":"Next","tagName":"h2"},{"title":"Save Images as PNGs​","type":1,"pageTitle":"Tutorial: How to Plot Sensor Data Using Python","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/plotting_sensor_data#save-images-as-pngs","content":" Project Aria glasses store images in JPEG or RAW format. Project Aria Tools supports converting image data to numpy arrays. These images can then be converted to PIL images and saved as PNG files.  from PIL import Image stream_mappings = { &quot;camera-slam-left&quot;: StreamId(&quot;1201-1&quot;), &quot;camera-slam-right&quot;: StreamId(&quot;1201-2&quot;), &quot;camera-rgb&quot;: StreamId(&quot;214-1&quot;), &quot;camera-eyetracking&quot;: StreamId(&quot;211-1&quot;), } index = 1 # sample index (as an example) for [stream_name, stream_id] in stream_mappings.items(): image = provider.get_image_data_by_index(stream_id, index) Image.fromarray(image[0].to_numpy_array()).save(f'{stream_name}.png')   These snippets will save the following images to the local folder:  SLAM images Eye Tracking images RGB images\t  ","version":"Next","tagName":"h2"},{"title":"Plot raw sensor data​","type":1,"pageTitle":"Tutorial: How to Plot Sensor Data Using Python","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/plotting_sensor_data#plot-raw-sensor-data","content":" ","version":"Next","tagName":"h2"},{"title":"Audio​","type":1,"pageTitle":"Tutorial: How to Plot Sensor Data Using Python","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/plotting_sensor_data#audio","content":" Audio data can vary, depending on whether 7 or 2 spatial microphones were recording. Most recording profiles use 7 microphones.  7 microphones 7x4096 data chunks2 microphones 2x2048 data chunks  Load the audio data  stream_id = provider.get_stream_id_from_label(&quot;mic&quot;) timestamps = [] audio = [[] for c in range(0, 7)] for index in range(0, 2): audio_data_i = provider.get_audio_data_by_index(stream_id, index) audio_signal_block = audio_data_i[0].data timestamps_block = [t * 1e-9 for t in audio_data_i[1].capture_timestamps_ns]; timestamps += timestamps_block for c in range(0, 7): audio[c] += audio_signal_block[c::7]   Plot the data with matplotlib  plt.figure() fig, axes = plt.subplots(1, 1, figsize=(12, 5)) fig.suptitle(f&quot;Microphone signal&quot;) for c in range(0, 7): plt.plot(timestamps, audio[c], '-', label = f&quot;channel {c}&quot;) axes.legend(loc='upper left') axes.grid('on') axes.set_xlabel('timestamps (s)') axes.set_ylabel('audio readout') plt.savefig(&quot;audio.pdf&quot;, format=&quot;pdf&quot;, bbox_inches=&quot;tight&quot;)     ","version":"Next","tagName":"h3"},{"title":"IMU​","type":1,"pageTitle":"Tutorial: How to Plot Sensor Data Using Python","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/plotting_sensor_data#imu","content":" We recommend using Trajectory MPS outputs instead of raw IMU data wherever possible. There may be instances, however, when you need to work directly with the IMU data.  Project Aria glasses have two IMUs. They operate at different rates, so that any errors do not align.  IMU (1kHz) - imu-rightIMU (800Hz) - imu-left  Organize the data into 6 lists. Each list stores one axis of a specific IMU.  stream_id = provider.get_stream_id_from_label(&quot;imu-left&quot;) accel_x = [] accel_y = [] accel_z = [] gyro_x = [] gyro_y = [] gyro_z = [] timestamps = [] for index in range(0, provider.get_num_data(stream_id)): imu_data = provider.get_imu_data_by_index(stream_id, index) accel_x.append(imu_data.accel_msec2[0]) accel_y.append(imu_data.accel_msec2[1]) accel_z.append(imu_data.accel_msec2[2]) gyro_x.append(imu_data.gyro_radsec[0]) gyro_y.append(imu_data.gyro_radsec[1]) gyro_z.append(imu_data.gyro_radsec[2]) timestamps.append(imu_data.capture_timestamp_ns * 1e-9)   Plot the data with matplotlib  plt.figure() fig, axes = plt.subplots(1, 2, figsize=(12, 5)) fig.suptitle(f&quot;{stream_id.get_name()}&quot;) axes[0].plot(timestamps, accel_x, 'r-', label=&quot;x&quot;) axes[0].plot(timestamps, accel_y, 'g-', label=&quot;y&quot;) axes[0].plot(timestamps, accel_z, 'b-', label=&quot;z&quot;) axes[0].legend(loc='upper left') axes[0].grid('on') axes[0].set_xlabel('timestamps (s)') axes[0].set_ylabel('accelerometer readout (m/sec2)') axes[1].plot(timestamps, gyro_x, 'r-', label=&quot;x&quot;) axes[1].plot(timestamps, gyro_y, 'g-', label=&quot;y&quot;) axes[1].plot(timestamps, gyro_z, 'b-', label=&quot;z&quot;) axes[1].legend(loc='upper left') axes[1].grid('on') axes[1].set_xlabel('timestamps (s)') axes[1].set_ylabel('gyroscope readout (rad/sec)')   The plotted image looks like this:  Save the plot to PDF  plt.savefig(&quot;imu.pdf&quot;, format=&quot;pdf&quot;, bbox_inches=&quot;tight&quot;)   ","version":"Next","tagName":"h3"},{"title":"Magnetometer​","type":1,"pageTitle":"Tutorial: How to Plot Sensor Data Using Python","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/plotting_sensor_data#magnetometer","content":" Plotting magnetometer is similar to plotting IMU.  Organize the data into 3 lists. Each list stores one axis of magnetometer data.  stream_id = provider.get_stream_id_from_label(&quot;mag0&quot;) mag_x = [] mag_y = [] mag_z = [] timestamps = [] for index in range(0, provider.get_num_data(stream_id)): mag_data = provider.get_magnetometer_data_by_index(stream_id, index) mag_x.append(mag_data.mag_tesla[0] * 1e6) mag_y.append(mag_data.mag_tesla[1] * 1e6) mag_z.append(mag_data.mag_tesla[2] * 1e6) timestamps.append(mag_data.capture_timestamp_ns * 1e-9)   Plot the data with matplotlib  plt.figure() fig, axes = plt.subplots(1, 1, figsize=(12, 5)) fig.suptitle(f&quot;Magnetometer signal&quot;) axes.plot(timestamps, mag_x, 'r-', label=&quot;x&quot;) axes.plot(timestamps, mag_y, 'g-', label=&quot;y&quot;) axes.plot(timestamps, mag_z, 'b-', label=&quot;z&quot;) axes.legend(loc='upper left') axes.grid('on') axes.set_xlabel('timestamps (s)') axes.set_ylabel('magnetometer readout (uT)') plt.savefig(&quot;mag.pdf&quot;, format=&quot;pdf&quot;, bbox_inches=&quot;tight&quot;)     ","version":"Next","tagName":"h3"},{"title":"Barometer​","type":1,"pageTitle":"Tutorial: How to Plot Sensor Data Using Python","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/plotting_sensor_data#barometer","content":" Load and plot the data using the following commands  plt.figure() fig, axes = plt.subplots(1, 2, figsize=(12, 5)) fig.suptitle(f&quot;Barometer signal&quot;) stream_id = provider.get_stream_id_from_label(&quot;baro0&quot;) pressure = [] temperature = [] timestamps = [] for index in range(0, provider.get_num_data(stream_id)): baro_data = provider.get_barometer_data_by_index(stream_id, index) pressure.append(baro_data.pressure * 1e-3) temperature.append(baro_data.temperature) timestamps.append(baro_data.capture_timestamp_ns * 1e-9) axes[0].plot(timestamps, pressure, 'r-') axes[0].grid('on') axes[0].set_xlabel('timestamps (s)') axes[0].set_ylabel('pressure readout (kPascal)') axes[1].plot(timestamps, temperature, 'r-') axes[1].grid('on') axes[1].set_xlabel('timestamps (s)') axes[1].set_ylabel('temperature readout (C)') plt.savefig(&quot;baro.pdf&quot;, format=&quot;pdf&quot;, bbox_inches=&quot;tight&quot;)     ","version":"Next","tagName":"h3"},{"title":"GPS​","type":1,"pageTitle":"Tutorial: How to Plot Sensor Data Using Python","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/plotting_sensor_data#gps","content":" GPS data can be visualized with 2D or 3D plots.  2D plots​  plt.figure() fig, axes = plt.subplots(1, 3, figsize=(12, 3)) fig.suptitle(f&quot;GPS signal&quot;) stream_id = provider.get_stream_id_from_label(&quot;gnss&quot;) latitude = [] longitude = [] altitude = [] timestamps = [] for index in range(100, 300): gps_data = provider.get_gps_data_by_index(stream_id, index) latitude.append(gps_data.latitude) longitude.append(gps_data.longitude) altitude.append(gps_data.altitude) timestamps.append(gps_data.capture_timestamp_ns * 1e-9) ax = axes[0] ax.plot(timestamps, latitude, 'r-') ax.grid('on') ax.set_xlabel('timestamps (s)') ax.set_ylabel('latitude') ax.yaxis.set_major_formatter(ticker.ScalarFormatter(useMathText=True, useOffset=False)) ax = axes[1] ax.plot(timestamps, longitude, 'r-') ax.grid('on') ax.set_xlabel('timestamps (s)') ax.set_ylabel('longitude') ax.yaxis.set_major_formatter(ticker.ScalarFormatter(useMathText=True, useOffset=False)) ax = axes[2] ax.plot(timestamps, altitude, 'r-') ax.grid('on') ax.set_xlabel('timestamps (s)') ax.set_ylabel('altitude') ax.yaxis.set_major_formatter(ticker.ScalarFormatter(useMathText=True, useOffset=False)) fig.tight_layout() plt.savefig(&quot;gps.pdf&quot;, format=&quot;pdf&quot;, bbox_inches=&quot;tight&quot;)     3D plots​  plt.figure() fig = plt.figure() axes = fig.add_subplot(projection='3d') axes.plot(latitude, longitude, altitude) axes.view_init(elev=20., azim=-35, roll=0) plt.savefig(&quot;gps3d.pdf&quot;, format=&quot;pdf&quot;, bbox_inches=&quot;tight&quot;)     ","version":"Next","tagName":"h3"},{"title":"Wi-Fi beacon​","type":1,"pageTitle":"Tutorial: How to Plot Sensor Data Using Python","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/plotting_sensor_data#wi-fi-beacon","content":" Group the Wi-Fi beacon data by mac bssid  stream_id = provider.get_stream_id_from_label(&quot;wps&quot;) rssi = {} timestamps = {} print(provider.get_num_data(stream_id)) for index in range(0, provider.get_num_data(stream_id)): wps_data = provider.get_wps_data_by_index(stream_id, index) if wps_data.bssid_mac not in rssi: rssi[wps_data.bssid_mac] = [] timestamps[wps_data.bssid_mac] = [] rssi[wps_data.bssid_mac].append(wps_data.rssi) timestamps[wps_data.bssid_mac].append(wps_data.board_timestamp_ns * 1e-9)   Plot the mac address This example has &gt; 15 samples  plt.figure() fig, ax = plt.subplots(1, 1, figsize=(6, 5)) fig.suptitle(f&quot;Wi-Fi beacon signal&quot;) for ssid in list(timestamps.keys()): if len(timestamps[ssid]) &lt; 15: continue ax.scatter(timestamps[ssid], rssi[ssid], label=ssid) ax.grid('on') ax.set_xlabel('timestamps (s)') ax.set_ylabel('Wi-Fi RSSI(dBm)') plt.legend(loc='upper left') plt.savefig(&quot;wifi.pdf&quot;, format=&quot;pdf&quot;, bbox_inches=&quot;tight&quot;)     ","version":"Next","tagName":"h3"},{"title":"Bluetooth beacon​","type":1,"pageTitle":"Tutorial: How to Plot Sensor Data Using Python","url":"/projectaria_tools/docs/data_utilities/advanced_code_snippets/plotting_sensor_data#bluetooth-beacon","content":" Group data by unique_id (similar to Wi-Fi grouping)  stream_id = provider.get_stream_id_from_label(&quot;bluetooth&quot;) rssi = {} timestamps = {} for index in range(0, provider.get_num_data(stream_id)): bluetooth_data = provider.get_bluetooth_data_by_index(stream_id, index) if bluetooth_data.unique_id not in rssi: rssi[bluetooth_data.unique_id] = [] timestamps[bluetooth_data.unique_id] = [] rssi[bluetooth_data.unique_id].append(bluetooth_data.rssi) timestamps[bluetooth_data.unique_id].append(bluetooth_data.board_timestamp_ns * 1e-9)   Plot the data per unique_id  plt.figure() fig, ax = plt.subplots(1, 1, figsize=(6, 5)) fig.suptitle(f&quot;Bluetooth beacon signal&quot;) for ssid in list(timestamps.keys()): ax.plot(timestamps[ssid], rssi[ssid], '.') ax.grid('on') ax.set_xlabel('timestamps (s)') ax.set_ylabel('bluetooth RSSI(dBm') fig.tight_layout() plt.savefig(&quot;ble.pdf&quot;, format=&quot;pdf&quot;, bbox_inches=&quot;tight&quot;)    ","version":"Next","tagName":"h3"},{"title":"How to Install Project Aria Tools for C++","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities/installation/installation_cpp","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"How to Install Project Aria Tools for C++","url":"/projectaria_tools/docs/data_utilities/installation/installation_cpp#overview","content":" This page provides instructions on how to:  Build projectaria_tools for C++, without visualizationBuild projectaria_tools for C++, with visualization To install dependencies, follow the instructions for build without visualization first Build projectaria_tools in isolation with a package manager (Linux/MacOs/Windows) This is the most convenient way to check that code and unit test pass when performing Contribution/GitHub PR to the project  Go to the Troubleshooting Guide if you encounter any issues.  ","version":"Next","tagName":"h2"},{"title":"Build from source without visualization​","type":1,"pageTitle":"How to Install Project Aria Tools for C++","url":"/projectaria_tools/docs/data_utilities/installation/installation_cpp#build-from-source-without-visualization","content":" ","version":"Next","tagName":"h2"},{"title":"Step 1: Install dependencies​","type":1,"pageTitle":"How to Install Project Aria Tools for C++","url":"/projectaria_tools/docs/data_utilities/installation/installation_cpp#step-1-install-dependencies","content":" UbuntuFedoraMacOS # Install build essentials sudo apt install build-essential git cmake # Install VRS/Pangolin dependencies sudo apt install libgtest-dev libgmock-dev libgoogle-glog-dev libfmt-dev \\ liblz4-dev libzstd-dev libxxhash-dev libboost-all-dev libpng-dev \\ libjpeg-turbo8-dev libturbojpeg0-dev libglew-dev libgl1-mesa-dev libeigen3-dev   ","version":"Next","tagName":"h3"},{"title":"Step 2: Compile C++ source code​","type":1,"pageTitle":"How to Install Project Aria Tools for C++","url":"/projectaria_tools/docs/data_utilities/installation/installation_cpp#step-2-compile-c-source-code","content":" cd $HOME/Documents/projectaria_sandbox mkdir -p build &amp;&amp; cd build # compile the C++ API cmake ../projectaria_tools/ make -j2     ","version":"Next","tagName":"h3"},{"title":"Build from source with visualization​","type":1,"pageTitle":"How to Install Project Aria Tools for C++","url":"/projectaria_tools/docs/data_utilities/installation/installation_cpp#build-from-source-with-visualization","content":" ","version":"Next","tagName":"h2"},{"title":"Step 1: Install dependencies​","type":1,"pageTitle":"How to Install Project Aria Tools for C++","url":"/projectaria_tools/docs/data_utilities/installation/installation_cpp#step-1-install-dependencies-1","content":" Follow the above steps to install dependencies build from source  ","version":"Next","tagName":"h3"},{"title":"Step 2: Compile Pangolin​","type":1,"pageTitle":"How to Install Project Aria Tools for C++","url":"/projectaria_tools/docs/data_utilities/installation/installation_cpp#step-2--compile-pangolin","content":" The viewers are built using Pangolin.  # compile &amp; install Pangolin cd /tmp git clone --recursive https://github.com/stevenlovegrove/Pangolin.git mkdir -p Pangolin_Build &amp;&amp; cd Pangolin_Build cmake -DCMAKE_BUILD_TYPE=Release -DBUILD_TOOLS=OFF -DBUILD_PANGOLIN_PYTHON=OFF \\ -DBUILD_EXAMPLES=OFF ../Pangolin/ make -j2 sudo make install   ","version":"Next","tagName":"h3"},{"title":"Step 3: Build projectaria_tools with visualization​","type":1,"pageTitle":"How to Install Project Aria Tools for C++","url":"/projectaria_tools/docs/data_utilities/installation/installation_cpp#step-3-build-projectaria_tools-with-visualization","content":" cd $HOME/Documents/projectaria_sandbox mkdir -p build &amp;&amp; cd build # Build C++ Aria Viewer cmake ../projectaria_tools -DPROJECTARIA_TOOLS_BUILD_TOOLS=ON make -j2   ","version":"Next","tagName":"h3"},{"title":"Step 4: Verify installation by running the viewer​","type":1,"pageTitle":"How to Install Project Aria Tools for C++","url":"/projectaria_tools/docs/data_utilities/installation/installation_cpp#step-4-verify-installation-by-running-the-viewer","content":" cd $HOME/Documents/projectaria_sandbox/build # Running the Aria Viewer with default example data ./tools/visualization/aria_viewer \\ --vrs ../projectaria_tools/data/mps_sample/sample.vrs   ","version":"Next","tagName":"h3"},{"title":"Build projectaria_tools in isolation with a package manager​","type":1,"pageTitle":"How to Install Project Aria Tools for C++","url":"/projectaria_tools/docs/data_utilities/installation/installation_cpp#build-projectaria_tools-in-isolation-with-a-package-manager","content":" You can build projectaria_tools in isolation using pixi on and for your local platform (Linux, macOS Intel/Silicon, Windows) using the exact same command lines. Pixi is a convenient package and environment manager that simplifies the logic to compile and run projects by defining dependencies and tasks in a single pixi.toml file for all platforms.  note For Windows platform we assume that Microsoft Visual C++ Compiler, MSVC 2017 or 2022 has been installed.  ","version":"Next","tagName":"h2"},{"title":"Step 1: Install Pixi​","type":1,"pageTitle":"How to Install Project Aria Tools for C++","url":"/projectaria_tools/docs/data_utilities/installation/installation_cpp#step-1-install-pixi","content":" Linux &amp; macOSWindows # Install Pixi curl -fsSL https://pixi.sh/install.sh | bash   ","version":"Next","tagName":"h3"},{"title":"Step 2: Compile and test the code​","type":1,"pageTitle":"How to Install Project Aria Tools for C++","url":"/projectaria_tools/docs/data_utilities/installation/installation_cpp#step-2-compile-and-test-the-code","content":" # When this command line first run, it will collect required dependencies, and then trigger the build and unit test pixi run run_c   ","version":"Next","tagName":"h3"},{"title":"Step 3: Access the compiled artifacts​","type":1,"pageTitle":"How to Install Project Aria Tools for C++","url":"/projectaria_tools/docs/data_utilities/installation/installation_cpp#step-3-access-the-compiled-artifacts","content":" # Activate the environment pixi shell # You can now run and use things from the projects   ","version":"Next","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"How to Install Project Aria Tools for C++","url":"/projectaria_tools/docs/data_utilities/installation/installation_cpp#troubleshooting","content":" Check the Troubleshooting Guide if you encounter any issues. ","version":"Next","tagName":"h2"},{"title":"How to Install the Python Package for Project Aria Tools","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities/installation/installation_python","content":"","keywords":"","version":"Next"},{"title":"Install via virtual environment​","type":1,"pageTitle":"How to Install the Python Package for Project Aria Tools","url":"/projectaria_tools/docs/data_utilities/installation/installation_python#install-via-virtual-environment","content":" ","version":"Next","tagName":"h2"},{"title":"Step 1 : Install Python​","type":1,"pageTitle":"How to Install the Python Package for Project Aria Tools","url":"/projectaria_tools/docs/data_utilities/installation/installation_python#step-1--install-python","content":" To use projectaria_tools and its Jupyter notebooks, you'll need Python 3.9 or above (3.10 if you are on Apple Silicon or Windows). To ensure all utilities work effectively, we recommend keeping Python 3 up to date.  Python 3 download pageTo check what what version of Python 3 you have use python3 --version  ","version":"Next","tagName":"h3"},{"title":"Step 2 : Create a virtual environment​","type":1,"pageTitle":"How to Install the Python Package for Project Aria Tools","url":"/projectaria_tools/docs/data_utilities/installation/installation_python#step-2--create-a-virtual-environment","content":" Linux &amp; macOSWindows rm -rf $HOME/projectaria_tools_python_env python3 -m venv $HOME/projectaria_tools_python_env source $HOME/projectaria_tools_python_env/bin/activate   ","version":"Next","tagName":"h3"},{"title":"Step 3 : Install the required Python packages​","type":1,"pageTitle":"How to Install the Python Package for Project Aria Tools","url":"/projectaria_tools/docs/data_utilities/installation/installation_python#step-3--install-the-required-python-packages","content":" python3 -m pip install --upgrade pip python3 -m pip install projectaria-tools'[all]'   ","version":"Next","tagName":"h3"},{"title":"Step 4 : Use Project Aria Tools​","type":1,"pageTitle":"How to Install the Python Package for Project Aria Tools","url":"/projectaria_tools/docs/data_utilities/installation/installation_python#step-4--use-project-aria-tools","content":" A few resources are:  Getting Started with Project Aria Data Utilities - Python TutorialRequest Machine Perception Services - for Research Partners  Python Module Error? Check the Python module import error section of Data Utilities Troubleshooting Guide if you encounter this issue  ","version":"Next","tagName":"h3"},{"title":"Building Python bindings from source (advanced user)​","type":1,"pageTitle":"How to Install the Python Package for Project Aria Tools","url":"/projectaria_tools/docs/data_utilities/installation/installation_python#building-python-bindings-from-source-advanced-user","content":" You'll need to install C++ dependencies to build Python bindings from source.  Go to the C++ Installation page and follow the instructions to install dependenciesGo to the projectaria_tools code folderEnter the following commands  cd $HOME/Documents/projectaria_sandbox/projectaria_tools python3 -m pip install --upgrade pip python3 -m pip install .   info For multithread build use the following: CMAKE_BUILD_PARALLEL_LEVEL=4 pip3 install . If you encounter error during the build process like ERROR: No matching distribution found for library X: Double check that you are using the right pip, or Python version and then use python -m pip install . or python3 -m pip install .  ","version":"Next","tagName":"h2"},{"title":"Generate and install type hinting from source (advanced user)​","type":1,"pageTitle":"How to Install the Python Package for Project Aria Tools","url":"/projectaria_tools/docs/data_utilities/installation/installation_python#generate-and-install-type-hinting-from-source-advanced-user","content":" Install pybind11-stubgen to generate the stub filesGenerate Python type hinting with generate_stubs.py script once projectaria_tools package is installedInstall type hinting package for projectaria_tools  python3 -m pip install pybind11-stubgen==1.1 cd $HOME/Documents/projectaria_sandbox/projectaria_tools python3 generate_stubs.py cp -r projectaria_tools-stubs/projectaria_tools . python3 -m pip install .   ","version":"Next","tagName":"h2"},{"title":"Building Python bindings from source with a package manager​","type":1,"pageTitle":"How to Install the Python Package for Project Aria Tools","url":"/projectaria_tools/docs/data_utilities/installation/installation_python#building-python-bindings-from-source-with-a-package-manager","content":" You can build projectaria_tools python bindings in isolation using pixi on and for your local platform (Linux, macOS Intel/Silicon, Windows) using the exact same command lines. Pixi is a convenient package and environment manager that simplifies the logic to compile and run projects by defining dependencies and tasks in a single pixi.toml file for all platforms.  note For Windows platform we assume that Microsoft Visual C++ Compiler, MSVC 2017 or 2022 has been installed.  ","version":"Next","tagName":"h2"},{"title":"Step1 Install Pixi​","type":1,"pageTitle":"How to Install the Python Package for Project Aria Tools","url":"/projectaria_tools/docs/data_utilities/installation/installation_python#step1-install-pixi","content":" Linux &amp; macOSWindows # Install Pixi curl -fsSL https://pixi.sh/install.sh | bash   ","version":"Next","tagName":"h3"},{"title":"Step2 Compile and test the code​","type":1,"pageTitle":"How to Install the Python Package for Project Aria Tools","url":"/projectaria_tools/docs/data_utilities/installation/installation_python#step2-compile-and-test-the-code","content":" # When this command line first run, it will collect required dependencies, and then trigger the build and unit test pixi run run_python   ","version":"Next","tagName":"h3"},{"title":"Step3 Access the compiled artifacts​","type":1,"pageTitle":"How to Install the Python Package for Project Aria Tools","url":"/projectaria_tools/docs/data_utilities/installation/installation_python#step3-access-the-compiled-artifacts","content":" # Activate the environment pixi shell # You can now run and use things from the projects python import projectaria_tools dir(projectaria_tools) # Will print &gt;&gt;&gt;['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'core']  ","version":"Next","tagName":"h3"},{"title":"Project Aria Tools Troubleshooting","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities/installation/troubleshooting","content":"","keywords":"","version":"Next"},{"title":"Jupyter Notebook issues​","type":1,"pageTitle":"Project Aria Tools Troubleshooting","url":"/projectaria_tools/docs/data_utilities/installation/troubleshooting#jupyter-notebook-issues","content":" ","version":"Next","tagName":"h2"},{"title":"Jupyter Notebook error​","type":1,"pageTitle":"Project Aria Tools Troubleshooting","url":"/projectaria_tools/docs/data_utilities/installation/troubleshooting#jupyter-notebook-error","content":" Jupyter Notebook works with Python 3.9 or above.  If you have problems using Jupyter examples, please upgrade Python 3 to the latest version. If you are using a virtual environment you'll need to recreate it to bring in the update.  To check what what version of Python 3 you have use python3 --version  ","version":"Next","tagName":"h3"},{"title":"Python module import error​","type":1,"pageTitle":"Project Aria Tools Troubleshooting","url":"/projectaria_tools/docs/data_utilities/installation/troubleshooting#python-module-import-error","content":" There are several things that could cause this error message.  Python version mismatch​  When running a Jupyter Notebook, it might use a Python 3 version that's not in the virtual environment. There are two ways you can resolve this issue.  Remove the Jupyter Notebook from outside of the virtual environmentDirectly start the Jupyter Notebook from the virtual environment bin folder.  If the virtual environment was created using python3 -m venv $HOME/projectaria_tools_python_env, you can directly call Jupyter from the virtual env as:  $HOME/projectaria_tools_python_env/bin/jupyter notebook notebook_example.ipynb   Old version of projectaria_tools​  You may also encounter a Python module import error if you are running an old version of projectaria_tools. Make sure you've installed the latest version of projectaria_tools.  ","version":"Next","tagName":"h3"},{"title":"Visualizer issues​","type":1,"pageTitle":"Project Aria Tools Troubleshooting","url":"/projectaria_tools/docs/data_utilities/installation/troubleshooting#visualizer-issues","content":" ","version":"Next","tagName":"h2"},{"title":"Visualizer does not build​","type":1,"pageTitle":"Project Aria Tools Troubleshooting","url":"/projectaria_tools/docs/data_utilities/installation/troubleshooting#visualizer-does-not-build","content":" If the visualizer does not build it may be because of missing Pangolin functions. Aria Digital Twin (ADT) dataset depends on very recent changes to Pangolin's master branch. If ADT depends on Pangolin functions that are not available on your installed version of Pangolin, please reinstall using the most recent master.  ","version":"Next","tagName":"h3"},{"title":"Runtime errors/missing libraries​","type":1,"pageTitle":"Project Aria Tools Troubleshooting","url":"/projectaria_tools/docs/data_utilities/installation/troubleshooting#runtime-errorsmissing-libraries","content":" Runtime errors can be caused by missing libraries. The following commands may resolve the issue.  # Missing libpango_geometry.so, libpango_windowing.so, etc sudo ldconfig LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/lib/ export LD_LIBRARY_PATH   ","version":"Next","tagName":"h3"},{"title":"Visualizer Window freezes - X11 known issue​","type":1,"pageTitle":"Project Aria Tools Troubleshooting","url":"/projectaria_tools/docs/data_utilities/installation/troubleshooting#visualizer-window-freezes---x11-known-issue","content":" If you are running a platform that uses X11 the Visualizer window may freeze. This is most likely because of a graphics driver bug in X11.Pangolin has a discussion on the issue.  If the issue is triggered by Pangolin Plotter, the fix is swap from X11 to EGL.  Step 1: Check the cause​  Test to see if it's a display driver issue triggered by Pangolin Plotter.  Build the latest version of Pangolin  cd /tmp/Pangolin_Build cmake -DCMAKE_BUILD_TYPE=Release -DBUILD_TOOLS=OFF -DBUILD_PANGOLIN_PYTHON=OFF \\ -DBUILD_EXAMPLES=ON ../Pangolin/ make -j2   Run the following example, it should work without issues  ./examples/BasicOpenGl/tutorial_3_gl_intro_classic_triangle_vbo_shader   Run the following example, if it shows a black window and the machine freezes, this may be the graphics driver issue. Move on to Step 2.  ./examples/SimplePlot/SimplePlot   Step 2: Checkout the fix and rebase onto master​  Use the following commands to checkout the fix on github and rebase onto master  cd /tmp/Pangolin git fetch origin pull/389/head:x11_to_egl git checkout x11_to_egl git rebase master # rebuild pangolin cd /tmp/Pangolin_Build cmake -DCMAKE_BUILD_TYPE=Release -DBUILD_TOOLS=OFF -DBUILD_PANGOLIN_PYTHON=OFF \\ -DBUILD_EXAMPLES=ON ../Pangolin/ make -j2 sudo make install   Confirm this is the correct fix by rebuilding and retesting SimplePlot from Step 1.  Step 3: Patch CMake for Visualizers​  Update the AriaViewer CMakeLists in$HOME/Documents/projectaria_sandbox/projectaria_tools/tools/CMakeLists.txtUpdate AriaDigitalTwinViewer CMakeLists in$HOME/Documents/projectaria_sandbox/projectaria_tools/projects/AriaDigitalTwinDatasetTools/visualization/CMakeLists.txt  By adding the following line:  find_package(OpenGL QUIET COMPONENTS EGL)   Step 4. Rebuild Aria Viewer and validate that it works​  cd $HOME/Documents/projectaria_sandbox/build cmake ../projectaria_tools -DPROJECTARIA_TOOLS_BUILD_TOOLS=ON make -j2 ./tools/visualization/aria_viewer \\ --vrs ../projectaria_tools/data/mps_sample/sample.vrs  ","version":"Next","tagName":"h3"},{"title":"How to use projectaria_tools type annotation","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities/installation/type_hinting","content":"","keywords":"","version":"Next"},{"title":"How to use type hinting​","type":1,"pageTitle":"How to use projectaria_tools type annotation","url":"/projectaria_tools/docs/data_utilities/installation/type_hinting#how-to-use-type-hinting","content":" Go to the Python Package Installation page to install the projectaria_tools package.  ","version":"Next","tagName":"h2"},{"title":"Type hinting setup for VS Code​","type":1,"pageTitle":"How to use projectaria_tools type annotation","url":"/projectaria_tools/docs/data_utilities/installation/type_hinting#type-hinting-setup-for-vs-code","content":" Install Python extensionSelect your own virtual environment on the bottom right cornerEnter the virtual environment path (i.e $HOME/projectaria_tools_python_env/bin/python)Type hinting appears when hover the mouse over the imported functions in your Python code  ","version":"Next","tagName":"h3"},{"title":"Type hinting setup for PyCharm​","type":1,"pageTitle":"How to use projectaria_tools type annotation","url":"/projectaria_tools/docs/data_utilities/installation/type_hinting#type-hinting-setup-for-pycharm","content":" Add new interpreterEnter the virtual environment path (i.e $HOME/projectaria_tools_python_env/bin/python)Type hinting appears when hover the mouse over the imported functions in your Python code ","version":"Next","tagName":"h3"},{"title":"Visualization Guide","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities/visualization","content":"Visualization Guide Project Aria Tools offers two kinds of tools for visualizing data: Python tools and C++ tools. Project Aria Tools Python Visualization - Our core Python visualization tools, developed with Rerun.C++ Visualization - Our C++ visualization tools, including sample datasets for testing.","keywords":"","version":"Next"},{"title":"Project Aria Tools C++ Visualization","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities/visualization/visualization_cpp","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Project Aria Tools C++ Visualization","url":"/projectaria_tools/docs/data_utilities/visualization/visualization_cpp#overview","content":" This page introduces our C++ visualization tools available in Project Aria Tools. We've provided example datasets to test these tools.  Aria Viewer: visualize raw Aria dataMPS 3D Scene Viewer: renders a static scene using Aria data with trajectories, global point cloud, and static camera posesMPS 3D Replay Viewer: renders static scene and dynamic elements: 2D/3D observations rays + eye gaze dataMPS Eye Gaze Viewer: visualize Aria data with eye gaze data    ","version":"Next","tagName":"h2"},{"title":"Requirements​","type":1,"pageTitle":"Project Aria Tools C++ Visualization","url":"/projectaria_tools/docs/data_utilities/visualization/visualization_cpp#requirements","content":" ","version":"Next","tagName":"h2"},{"title":"Step 0 : Check system requirements and download codebase​","type":1,"pageTitle":"Project Aria Tools C++ Visualization","url":"/projectaria_tools/docs/data_utilities/visualization/visualization_cpp#step-0--check-system-requirements-and-download-codebase","content":" Go to the Download Codebase page to:  Check your system is supportedDownload projectaria_tools codebase from the GitHub  ","version":"Next","tagName":"h3"},{"title":"Step 1 : Download the sample dataset​","type":1,"pageTitle":"Project Aria Tools C++ Visualization","url":"/projectaria_tools/docs/data_utilities/visualization/visualization_cpp#step-1--download-the-sample-dataset","content":" Go to the Download MPS Sample dataset to retrieve a raw VRS file and all the corresponding MPS outputs.  ","version":"Next","tagName":"h3"},{"title":"Step 2 : Build and install visualizers​","type":1,"pageTitle":"Project Aria Tools C++ Visualization","url":"/projectaria_tools/docs/data_utilities/visualization/visualization_cpp#step-2--build-and-install-visualizers","content":" The visualizers need the C++ version of Project Aria Tools to run.  In the C++ Installation Guide, follow the instructions to build from source with visualization    ","version":"Next","tagName":"h3"},{"title":"Run Aria Viewer​","type":1,"pageTitle":"Project Aria Tools C++ Visualization","url":"/projectaria_tools/docs/data_utilities/visualization/visualization_cpp#run-aria-viewer","content":" Aria Viewer enable you to to visualize Aria device recorded VRS files. It shows all sensor data including:  Camera imagesIMUAudio (visualization of waveform, sound is not available)  cd $HOME/Documents/projectaria_sandbox/build ./tools/visualization/aria_viewer --vrs $MPS_SAMPLE_PATH/sample.vrs       ","version":"Next","tagName":"h2"},{"title":"Run MPS 3D Scene Viewer​","type":1,"pageTitle":"Project Aria Tools C++ Visualization","url":"/projectaria_tools/docs/data_utilities/visualization/visualization_cpp#run-mps-3d-scene-viewer","content":" The MPS 3D Scene Viewer renders a static scene using location MPS output.  Through this tool you can create visualizations using:  Closed loop trajectoriesGlobal point cloudStatic camera posesOpen loop trajectories Because open loop is in odometry frame of reference, it shouldn’t be visualized with closed loop trajectories, global points or static camera poses  This tutorial generates a visualization containing:  Closed loop trajectoriesGlobal point cloud  cd $HOME/Documents/projectaria_sandbox/build ./tools/mps_visualization/mps_3d_scene_viewer \\ --closed-loop-traj $MPS_SAMPLE_PATH/trajectory/closed_loop_trajectory.csv \\ --global-point-cloud $MPS_SAMPLE_PATH/trajectory/global_points.csv.gz     info Because the sample dataset doesn't have static cameras you won't be able to interact with the static camera settings    ","version":"Next","tagName":"h2"},{"title":"Run MPS 3D Replay Viewer​","type":1,"pageTitle":"Project Aria Tools C++ Visualization","url":"/projectaria_tools/docs/data_utilities/visualization/visualization_cpp#run-mps-3d-replay-viewer","content":" The MPS 3D Replay Viewer renders static scene and dynamic elements at each frame: Aria's pose + 2D/3D observations rays + eye gaze data.  Through this tool you can create visualizations using:  Closed loop trajectoriesSemi-Dense Point Cloud Global point cloudPoint observations Static camera poses  This tutorial generates a visualization containing:  Static elements Closed loop trajectoriesGlobal point cloud Dynamic elements One closed loop trajectory for replay2D/3D point observations raysGeneralized and Personalized Eye Gaze vectors  cd $HOME/Documents/projectaria_sandbox/build ./tools/mps_visualization/mps_3d_replay_viewer \\ --vrs $MPS_SAMPLE_PATH/sample.vrs \\ --replay-trajectory $MPS_SAMPLE_PATH/trajectory/closed_loop_trajectory.csv \\ --closed-loop-traj $MPS_SAMPLE_PATH/trajectory/closed_loop_trajectory.csv \\ --global-point-cloud $MPS_SAMPLE_PATH/trajectory/global_points.csv.gz \\ --point-obs $MPS_SAMPLE_PATH/trajectory/semidense_observations.csv.gz \\ --generalized-eye-gaze $MPS_SAMPLE_PATH/eye_gaze/generalized_eye_gaze.csv \\ --calibrated-eye-gaze $MPS_SAMPLE_PATH/eye_gaze/calibrated_eye_gaze.csv \\ --hands $MPS_SAMPLE_PATH/hand_tracking/wrist_and_palm_poses.csv       ","version":"Next","tagName":"h2"},{"title":"MPS Eye Gaze visualizer​","type":1,"pageTitle":"Project Aria Tools C++ Visualization","url":"/projectaria_tools/docs/data_utilities/visualization/visualization_cpp#mps-eye-gaze-visualizer","content":" The MPS Eye Gaze visualizer renders the computed eye gaze and vrs data side by side. The visualizer contains:  Eye Tracking camera streamRGB, Mono Scene (SLAM) left and right camera streams A red dot shows the projection of the eye gaze onto the imageThe projection is computed using a fixed depth of 1m 2D graph plot of the gaze yaw and pitch angles in radians2D radar plot of the eye gaze yaw and pitch angles  ","version":"Next","tagName":"h2"},{"title":"Run visualizer and visualize both generalized and optional calibrated eye gaze​","type":1,"pageTitle":"Project Aria Tools C++ Visualization","url":"/projectaria_tools/docs/data_utilities/visualization/visualization_cpp#run-visualizer-and-visualize-both-generalized-and-optional-calibrated-eye-gaze","content":" cd $HOME/Documents/projectaria_sandbox/build ./tools/mps_visualization/mps_eyegaze_viewer --vrs $MPS_SAMPLE_PATH/sample.vrs \\ --generalized-eye-gaze $MPS_SAMPLE_PATH/eye_gaze/general_eye_gaze.csv \\ --calibrated-eye-gaze $MPS_SAMPLE_PATH/eye_gaze/personalized_eye_gaze.csv     ","version":"Next","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"Project Aria Tools C++ Visualization","url":"/projectaria_tools/docs/data_utilities/visualization/visualization_cpp#troubleshooting","content":" Check the Troubleshooting Guide if you encounter issues using this tutorial. ","version":"Next","tagName":"h2"},{"title":"Introduction to Project Aria Docs","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/intro","content":"","keywords":"","version":"Next"},{"title":"New to Project Aria?​","type":1,"pageTitle":"Introduction to Project Aria Docs","url":"/projectaria_tools/docs/intro#new-to-project-aria","content":" Go to projectaria.com to get an overview of the programGo to Aria Dataset Explorer to search and preview some of Aria’s open dataGo to the Project Aria FAQ for an overview of our current service offerings, capabilities and software ecosystem  ","version":"Next","tagName":"h2"},{"title":"Want to get involved with Open Science Initiatives (OSI)?​","type":1,"pageTitle":"Introduction to Project Aria Docs","url":"/projectaria_tools/docs/intro#want-to-get-involved-with-open-science-initiatives-osi","content":" We believe that open source accelerates the pace of innovation in the world. We’re excited to share our code, models and data and collaborate together on Open Science that can help shape the future.  Open datasets powered by Project Aria data include:  Aria Everyday Activities (AEA) - a re-release of Aria’s first Pilot Dataset, updated with new tooling and location data, to accelerate the state of machine perception and AI.Aria Digital Twin (ADT) - a real-world dataset, with hyper-accurate digital counterpart &amp; comprehensive ground-truth annotationAria Synthetic Environments (ASE) - a procedurally-generated synthetic Aria dataset for large-scale ML researchHOT3D - a new benchmark dataset for vision-based understanding of 3D hand-object interactionsNymeria - a large-scale multimodal egocentric dataset for full-body motion understanding  Research challenges, using our open datasets, are posted to projectaria.com.  Models created using Aria data include:  EgoBlur - an open source AI model from Meta to preserve privacy by detecting and blurring PII from images. Designed to work with egocentric data (such as Aria data) and non-egocentric data.Project Aria Eye Tracking - an open source inference code for the Pre March 2024 Eye Gaze Model used by Machine Perception Services (MPS)  Aria data is recorded using VRS, an open source file format. Our open source code, Project Aria Tools, provides a C++ and Python interface that helps people incorporate VRS data into a wide range of downstream applications.  ","version":"Next","tagName":"h2"},{"title":"Interested in getting access to the Aria Research Kit (ARK)?​","type":1,"pageTitle":"Introduction to Project Aria Docs","url":"/projectaria_tools/docs/intro#interested-in-getting-access-to-the-aria-research-kit-ark","content":" Through our OSI data, tooling and research challenges we aim to support the broadest audience of the research community. For researchers who also need access to a physical Project Aria device, we offer the Aria Research Kit (ARK).  Project Aria devices can be used to:  Collect data In addition to capturing data from a single device, TICSync can be used to capture time synchronized data from multiple Aria devices in a shared world locationCloud based Machine Perception Services (MPS) are available to generate SLAM, Multi-SLAM, Eye Gaze and Hand Tracking derived data outputs. Partner data is only used to serve MPS requests. Partner data is not available to Meta researchers or Meta’s affiliates. Stream data Use the Project Aria Client SDK to stream and subscribe to data on your local machine  Before applying for the ARK, please explore our OSI offerings, FAQ. Once you are confident that the ARK is a good match for your research, please apply for the Aria Research Kit.  Project Aria: Academic Partnership Interest FormProject Aria: Corporate Partnership Interest Form  Our team will review your application and reach out to you with next steps if you are approved for the ARK.  ","version":"Next","tagName":"h2"},{"title":"Just received Project Aria glasses?​","type":1,"pageTitle":"Introduction to Project Aria Docs","url":"/projectaria_tools/docs/intro#just-received-project-aria-glasses","content":" About ARK provides an overview of different ways you can use the deviceThe Quickstart Guide covers how to set up your glassesThe Glasses User Manual provides a range of information, including how to factory reset your glasses  ","version":"Next","tagName":"h2"},{"title":"Unlock the power of Aria Training and Evaluation toolkit (ATEK)​","type":1,"pageTitle":"Introduction to Project Aria Docs","url":"/projectaria_tools/docs/intro#unlock-the-power-of-aria-training-and-evaluation-toolkit-atek","content":" ATEK is an e2e framework for training and evaluating deep learning models on Aria data, for both 3D egocentric-specific and general machine perception tasks.  About ATEKRead about ATEK on projectaria.comLearn more about the ATEK on ATEK GitHub Repo  If you encounter any issues, go to our Support page for multiple ways to get in touch! ","version":"Next","tagName":"h2"},{"title":"Project Aria Open Datasets","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets","content":"Project Aria Open Datasets This section provides information about how to use Project Aria's open data. Aria Dataset Explorer - a centralized hub for Project Aria public datasets that helps users efficiently find, preview and download Aria dataAria Everyday Activities Dataset - multiple activity sequences where 1-2 users wearing Project Aria glasses participate in everyday activities to capture time synchronized data in a shared world locationAria Digital Twin Dataset - raw and synthesized sensor data from Project Aria glasses, combined with ground truth data generated using a motion capture system including depth images, device trajectories, object trajectories and bounding boxes, and human trackingDTC Object Explorer - an interactive webtool that allows researchers to find, visualize, and download high-quality Digital Twin object models in the common GLB format.Aria Synthetic Environments Dataset - a large scale dataset of 100K unique procedurally-generated scenes of interior layouts of apartments filled with 3D objects, and simulated with the sensor characteristics of Aria glassesEgo-Exo4D Dataset - more than 800 participants from 13 cities worldwide performed these activities in 131 different natural scene contexts, yielding long-form captures from 1 to 42 minutes each and 1,422 hours of video combinedHOT3D Dataset - a new benchmark dataset for vision-based understanding of 3D hand-object interactionsReading in the Wild Dataset - a new benchmark dataset designed to inform models that determine whether or not subjects are reading.","keywords":"","version":"Next"},{"title":"Project Aria Tools Python Visualization","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/data_utilities/visualization/visualization_python","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Project Aria Tools Python Visualization","url":"/projectaria_tools/docs/data_utilities/visualization/visualization_python#overview","content":" This page introduces our core Python visualization tools, developed with Rerun, that are part of Project Aria Tools.  Aria Sensor Viewer: 3D visualization of Aria sensorsMPS Viewer: renders MPS metadata (point cloud, device trajectory and wearer eye gaze)  ","version":"Next","tagName":"h2"},{"title":"Requirements​","type":1,"pageTitle":"Project Aria Tools Python Visualization","url":"/projectaria_tools/docs/data_utilities/visualization/visualization_python#requirements","content":" Python Project Aria Tools is installedMPS Sample data Official Sample Data - most up to date sample dataGitHub Repo Sample dataset - useful for unit testing etc. It will work, but is not maintained and updated the way the official sample data is    ","version":"Next","tagName":"h2"},{"title":"Run Aria Sensor Viewer​","type":1,"pageTitle":"Project Aria Tools Python Visualization","url":"/projectaria_tools/docs/data_utilities/visualization/visualization_python#run-aria-sensor-viewer","content":" viewer_aria_sensors displays the relative position and orientation of all most of Project Aria glasses sensors (cameras, IMUs, microphones, magnetometer &amp; barometer) in a common reference.  viewer_aria_sensors --vrs $MPS_SAMPLE_PATH/sample.vrs     tip Selecting the different sensors in the Blueprint left column will help you quickly identify where a given sensor is located    ","version":"Next","tagName":"h2"},{"title":"Run MPS Viewer​","type":1,"pageTitle":"Project Aria Tools Python Visualization","url":"/projectaria_tools/docs/data_utilities/visualization/visualization_python#run-mps-viewer","content":" viewer_mps displays an interactive visualization of the Aria VRS RGB frames along with MPS data (Closed loop trajectory, Global point cloud, Wearer eye gaze). As you are playing or moving along the timeline, you can see the position of the camera and the wearer eye gaze direction at the timestamp of your choice.   viewer_mps --vrs $MPS_SAMPLE_PATH/sample.vrs   or to specify each MPS file  viewer_mps --vrs $MPS_SAMPLE_PATH/sample.vrs \\ --trajectory $MPS_SAMPLE_PATH/trajectory/closed_loop_trajectory.csv \\ --points $MPS_SAMPLE_PATH/trajectory/global_points.csv.gz \\ --eyegaze $MPS_SAMPLE_PATH/eye_gaze/personalized_eye_gaze.csv \\ --hands $MPS_SAMPLE_PATH/hand_tracking/wrist_and_palm_poses.csv       tip Switching between device_time and timestamp timeline allows you to retrieve a specific timestamp for the VRS sequence or MPS annotation  ","version":"Next","tagName":"h2"},{"title":"Visualization of Multi-SLAM data​","type":1,"pageTitle":"Project Aria Tools Python Visualization","url":"/projectaria_tools/docs/data_utilities/visualization/visualization_python#visualization-of-multi-slam-data","content":" The MPS Viewer can also be used to visualize 3D data derived from multiple Project Aria device recordings, if the MPS data has been generated using Multi-SLAM or is part of datasets that contain this type of data, such as Aria Everyday Activities (AEA).  AEA Example​  The example below uses Trajectory and Semi-Dense Point Cloud data from AEA. Go to AEA Dataset Download for how to download this data.  viewer_mps --trajectory `find -P ~/Documents/projectaria_tools_aea_data/loc1*/*/*/closed_loop_trajectory.csv -print` --points `find -P Documents/projectaria_tools_aea_data/loc1*/*/*/semidense_points.csv.gz -print`     MPS CLI Example​  The following visualization uses Multi-Slam data generated using MPS CLI sample data. The MPS CLI is part of the Aria Research Kit.  viewer_mps --trajectory `find -P ~/documents/multi_slam_output/*/*/closed_loop_trajectory.csv -print` --points `find -P ~/documents/multi_slam_output/*/*/semidense_points.csv.gz -print`       ","version":"Next","tagName":"h3"},{"title":"An introduction to Rerun​","type":1,"pageTitle":"Project Aria Tools Python Visualization","url":"/projectaria_tools/docs/data_utilities/visualization/visualization_python#an-introduction-to-rerun","content":" Rerun is an open source SDK and engine for visualizing and interacting with multi modal data streams. It's usable from Python, Rust and C++. Rerun consists in a log API and a visualizer.  The main GUI sections of the Rerun visualizer are:  BluePrint A: User defined Scene Graph (Entities and hierarchy you define)B: Visual view of the Scene Graph (User customizable) Timeline C: Interactive navigation and inspection of log events on multiple timeline (log, frame, or device time) Visibility and property control D: Fine grained control and inspection of Entities/Components   ","version":"Next","tagName":"h2"},{"title":"Project Aria Docs FAQ","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/faq","content":"","keywords":"","version":"Next"},{"title":"What is Project Aria?​","type":1,"pageTitle":"Project Aria Docs FAQ","url":"/projectaria_tools/docs/faq#what-is-project-aria","content":" Project Aria is a research program, with supporting spatial AI machine perception technologies and open science initiatives, created to serve as foundational building blocks for higher-level contextualized AI and the novel compute and interaction paradigms needed in order to make AR successful.  Project Aria glasses are at the program's core. A multimodal data collection device that collects data from the most human perspective. Project Aria glasses have five cameras (two Mono Scene, one RGB, and two Eye Tracking cameras) as well as non-visual sensors (two IMUs, magnetometer, barometer, GPS, Wi-Fi beacon, Bluetooth beacon and Microphones). On-device compute power is used to store information into VRS files which preserve the integrity of individual sensor streams that are time aligned using shared time stamps.  The whitepaper, “Project Aria: A New Tool for Egocentric Multi-Modal AI Research”, provides more details about Project Aria glasses, the data generated and motivation for the program.  ","version":"Next","tagName":"h2"},{"title":"What Project Aria data is available?​","type":1,"pageTitle":"Project Aria Docs FAQ","url":"/projectaria_tools/docs/faq#what-project-aria-data-is-available","content":" Researchers have used Project Aria to develop a wide range of powerful datasets. Open datasets powered by Project Aria data include:  Aria Everyday Activities (AEA) (Project Aria)Aria Digital Twin (ADT) (Project Aria)Aria Synthetic Environments (ASE) (Project Aria)Ego-Exo4D (Reality Labs FAIR collaboration with universities around the globe)Multi-Modal Conversations in Smart Glasses (MMCSG)HOT3D (Project Aria)Nymeria (Project Aria)  Preview some Aria open data prior to downloading via the Aria Dataset Explorer.  ","version":"Next","tagName":"h2"},{"title":"What can Project Aria be used for?​","type":1,"pageTitle":"Project Aria Docs FAQ","url":"/projectaria_tools/docs/faq#what-can-project-aria-be-used-for","content":" Project Aria devices can be used to:  Collect data In addition to capturing data from a single device, TICSync can be used to capture time synchronized data from multiple Aria devices in a shared world locationCloud based Machine Perception Services (MPS) are available to generate SLAM, Multi-Slam, Eye Gaze and Hand Tracking derived data outputs. Partner data is only used to serve MPS requests. Partner data is not available to Meta researchers or Meta’s affiliates. Stream data Use the Project Aria Client SDK to stream and subscribe to data on your local machine  A range of models have been created using Aria data, including:  EgoBlur: an open source AI model from Meta to preserve privacy by detecting and blurring PII from images. Designed to work with egocentric data (such as Aria data) and non-egocentric data.Project Aria Eye Tracking: an open source inference code for the Pre March 2024 Eye Gaze Model used by Machine Perception Services (MPS)SceneScript: an AI model and method to understand and describe 3D spaces  ","version":"Next","tagName":"h2"},{"title":"What is the software ecosystem?​","type":1,"pageTitle":"Project Aria Docs FAQ","url":"/projectaria_tools/docs/faq#what-is-the-software-ecosystem","content":" While some tools run on other platforms, when developing desktop tools we primarily support:  x64 Linux distributions of: Fedora 36, 37, 38Ubuntu Jammy (22.04 LTS) and Focal (20.04 LTS) Mac Intel or Mac ARM-based (M1) with MacOS 11 (Big Sur) or newer  To use the Python version of our tools, you'll generally need Python 3.9+ (3.10+ if you are on Apple Silicon). Venv virtual environments are used and tested for most applications, in most instances Conda should also work, with the exception of the Project Aria Client SDK.  Python 3 download pageTo check which version of Python 3 you have, use python3 --version  ","version":"Next","tagName":"h2"},{"title":"Project Aria Tools​","type":1,"pageTitle":"Project Aria Docs FAQ","url":"/projectaria_tools/docs/faq#project-aria-tools","content":" Project Aria Tools provides open source Python and C++ code and APIs for working with Aria data. Due to the multimodal nature of Project Aria data, the data is stored in VRS files. Project Aria Tools provides API wrappers for using VRS tools, specifically tailored to working with Aria data. Using Project Aria Tools should make it easier to plug your data into other applications with more accessible formatting and labeling.  If there are VRS functions you wish Project Aria Tools had, please contact us using any of our Support Channels.  APIs​  DataProvider Read Project Aria sequences and raw sensor data (VRS files) Calibration Retrieve calibration data and interact with Aria camera models MPS Read Project Aria MPS (derived) data Images Python Visualization GuideC++ Visualization GuideTooling enables users to: Visualize time-synced recordings for one or more Project Aria sequences.Visualize recordings and see 3D visualizations of pre-computed trajectory using time-synced dataDisplay an interactive visualization of the Aria VRS RGB frames along with MPS data (Closed loop trajectory, Global point cloud, Wearer eye gaze).Display the relative position and orientation of Project Aria glasses sensors (cameras, IMUs, microphones, magnetometer &amp; barometer) in a common reference. (Python only)  Check the Project Aria Tools release log for the latest updates.  Data review​  The vrs_to_mp4 script exports a VRS file to MP4 and was created to help researchers send files for review more easily. Please note, MP4 files will not contain the full sensor suite.  Code samples​  VRS Mutation (C++) Create a copy of a VRS file that contains on the fly custom image modification (either from images in memory, or previously exported to disk): Export VRS FramesModify the image frame Such as blur/annotate objects manually or with a script Create copy of the VRS file that includes the modifications Speech to Text (Python) Use Faster Whisper to run Whisper Speech Recognition on an Aria audio stream. The ASR outputs are time aligned with Aria Device Time. Run EfficientSAM with Eye Gaze image reprojection as prompt (Python) Use EfficientSAM to create image masks, based on eye gaze inference  Aria Research Kit (ARK)​  Project Aria Tools code that is only relevant if you have access to Project Aria Glasses and are collecting your own data:  Project Aria MPS Command Line Interface (MPS CLI) is a convenient way to request Machine Perception Services (MPS). Through the MPS CLI you can request: SLAM, Eye Gaze and Hand Tracking dataMulti-sequence SLAM data - SLAM data generated using multiple VRS recordings in a shared coordinate frame  ","version":"Next","tagName":"h3"},{"title":"Aria Research Kit​","type":1,"pageTitle":"Project Aria Docs FAQ","url":"/projectaria_tools/docs/faq#aria-research-kit","content":" The Aria Research Kit (ARK) provides Aria glasses, services and tools to enable researchers to gather and process their own Aria data. Go to Aria Research Kit intro page at projectaria.com to find out more, or to apply to become a research partner.  The following SW and capabilities that are only relevant if you have access to Project Aria glasses and can collect your own data.  Mobile Companion app (Required)​  The Mobile Companion App, provides the ability to interact &amp; record with your Aria glasses via Bluetooth. When you receive your device, you must set them up using the Mobile Companion app, so that your glasses can receive OTA software updates.  If you're using Android, your mobile device will need:  Android OS version 10 or above installedARM64 processor preferred(Optional) ARCore Depth API (https://developers.google.com/ar/devices) support needed for eye-tracking calibration64-bit is preferred, although 32-bit is supported  If you're using iOS:  OS version must be iOS 14 or above(Optional) TrueDepth camera (iPhone X or later) needed for eye-tracking calibration  Project Aria Client SDK with CLI​  Through the Client SDK with CLI, you can directly interact with Aria glasses and stream data from the glasses to a computer. Not supported on Ubuntu Focal (20.04 LTS).  After connecting the Project Aria glasses and PC via USB or WiFi, the SDK enables users to:  Retrieve device information, status and sensor calibration dataConfigure, start and stop recording for a single deviceConfigure, start and stop recording for multiple time synchronized devices (using TICSync)Configure, start and stop streaming from the glassesSubscribe to streaming data from the glassesUse TICSync to create time synchronized recordings with multiple Aria glasses  You will need to install the Client SDK with CLI using a venv virtual environment. The authorization certificates required to pair your glasses with the computer will not be sent correctly in Conda.  MPS CLI​  Accessed via the PyPI installation of Project Aria Tools, the Project Aria MPS Command Line Interface (MPS CLI) provides a streamlined way to request Machine Perception Services (MPS):  Request SLAM, Eye Gaze and Hand Tracking dataRequest Multi-Sequence SLAM data  Aria Studio​  Aria Studio is an application will launch in your default web-browser and provides the ability to:  Examine preview thumbnails and metadata of Aria recordings (on device or stored locally)Transfer recordings from the device to the local computer or delete recordings stored on the device or local computerVisualize recordings We show visualizations of all the sensor streams except for audio. Audio outputs are not supported at this time. Group recordings for multi-sequence processing jobs Grouped recordings are a special feature unlocked by Aria Studio. Users can group VRS recordings from anywhere in their directory for organizational purposes, or to request Multi-SLAM outputs.Groups are stored on your local machine and associated with your Aria user account Request and manage Machine Perception Services (MPS) requests  Like the MPS CLI, you can have granular control over settings such as uploads and download speeds by customizing $HOME/.projectaria/mps.ini.  ","version":"Next","tagName":"h3"},{"title":"EgoBlur​","type":1,"pageTitle":"Project Aria Docs FAQ","url":"/projectaria_tools/docs/faq#egoblur","content":" EgoBlur is an open source AI model from Meta to preserve privacy by detecting and blurring PII from images. We provide two FasterRCNN-based detector models, for face blurring and license plates and perform consistently across the full range of ‘responsible AI labels’, as defined by the CCV2 dataset.  EgoBlur Research Paper - EgoBlur: Responsible Innovation in AriaAbout EgoBlur Download the Models EgoBlur VRS Utilities - C++ tool for working with Aria VRS files Go to the EgoBlur VRS Utilities Readme for detailed instructions EgoBlur Demo - Python3 tool for working with PNG, JPEG or MP4 files Go to the EgoBlur Readme for detailed instructions  ","version":"Next","tagName":"h3"},{"title":"Project Aria Eye Tracking​","type":1,"pageTitle":"Project Aria Docs FAQ","url":"/projectaria_tools/docs/faq#project-aria-eye-tracking","content":" Project Aria Eye Tracking provides an open source inference code for the Pre March 2024 Eye Gaze Model used by MPS.  Provides the ability to generate eye gaze data using the old model, if that is more suited to your research use case or if you’re not able to request MPSThis code can be used on downloaded data or when streaming data using the Aria Client SDK  ","version":"Next","tagName":"h3"},{"title":"How do I use Aria data?​","type":1,"pageTitle":"Project Aria Docs FAQ","url":"/projectaria_tools/docs/faq#how-do-i-use-aria-data","content":" Because of Aria’s unique data structure, we recommend using Project Aria Tools as an interface to access the data via an abstraction (an X_provider string) that allows users to iterate through all the sensor recordings.  How Project Aria Uses VRSGetting Started Guide (view sample data via Jupyter Notebook or Google Colab)Download Project Aria Tools Codebase  ","version":"Next","tagName":"h2"},{"title":"How do I use the IMU data?​","type":1,"pageTitle":"Project Aria Docs FAQ","url":"/projectaria_tools/docs/faq#how-do-i-use-the-imu-data","content":"   We recommend using Trajectory MPS outputs instead of raw IMU data wherever possible. Go to MPS Code Snippets for how to load open loop or closed loop trajectory data.  ","version":"Next","tagName":"h3"},{"title":"What calibration information is available?​","type":1,"pageTitle":"Project Aria Docs FAQ","url":"/projectaria_tools/docs/faq#what-calibration-information-is-available","content":" General calibration concepts​  2D Image Coordinate System Conventions (intrinsics)3D Coordinate Frame Conventions (extrinsics, non-visual sensor coordinate systems and CPF)Calibration Code SnippetsTech Insights - technical deeper dives on domain-specific topics, such as how we model sensors mathematically in calibration: Camera intrinsic modelsSensor measurement model  SLAM MPS Calibration data​  Online_calibration.jsonl is a SLAM MPS output that contains one json online calibration record per line. Each record is a json dict object that contains timestamp metadata and the result of online calibration for the cameras and IMUs.  The calibration parameters contain intrinsics and extrinsics parameters for each sensor as well as a time offsets that best temporally align their data.  In-Session Eye Gaze Calibration​  This is not related to standard calibration. In-Session Eye Gaze Calibration enables researchers to improve the eye gaze estimations in Eye Gaze MPS outputs, enabling researchers to more accurately determine where wearers are looking during the recordings.  When users collect data using Aria glasses, they can initiate In-Session Eye Gaze Calibration. Once they’ve performed this task personalized, as well as general Eye Gaze MPS, can be generated. Every person is unique in terms of how they move their eyes and look at objects, so the personalized_eye_gaze estimation is expected to be more accurate for the individual.  ","version":"Next","tagName":"h3"},{"title":"Should I use VRS or Project Aria Tools?​","type":1,"pageTitle":"Project Aria Docs FAQ","url":"/projectaria_tools/docs/faq#should-i-use-vrs-or-project-aria-tools","content":" Project Aria Tools provides VrsDataProvider which offers some, but not all, of the VRS API capabilities. Go to How Project Aria Uses VRS for more information.  We’ve tailored VrsDataProvider to meet the requirements of people using Aria data. If there are capabilities in the VRS API that you would like to access when using Project Aria Tools, please use one of our Support channels to let us know what you’re looking for and what features you would like.  ","version":"Next","tagName":"h3"},{"title":"How do I get involved?​","type":1,"pageTitle":"Project Aria Docs FAQ","url":"/projectaria_tools/docs/faq#how-do-i-get-involved","content":" The Project Aria Contact Us page contains a variety of ways to get in touch. We’d love to hear from you.  Download and explore our various open datasets and let us know how it goes!  For questions or feature requests relating to Project Aria Tools, we encourage you to post to the Issues page on GitHub.  Go to our Attribution and Contributing page if you’d like to find out how to cite Project Aria or contribute to our open tooling.  Research Challenges are posted to Projectaria.com. Open research challenges allow us to collectively accelerate the state-of-the-art with the community. We believe these challenges will help to establish a baseline for external researchers to build and foster reproducible research on egocentric Computer Vision and AI/ML algorithms for scene perception, reconstruction and understanding, and contextual AI.  If you’d like to collect your own data, or use Project Aria glasses in streaming applications go to Aria Research Kit intro page at projectaria.com to find out how to become a research partner. ","version":"Next","tagName":"h2"},{"title":"Aria Digital Twin Dataset","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Aria Digital Twin Dataset","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset#overview","content":" Project Aria Tools provides Python and C++ APIs to access the Aria Digital Twin (ADT) dataset(paper).  ","version":"Next","tagName":"h2"},{"title":"About the data​","type":1,"pageTitle":"Aria Digital Twin Dataset","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset#about-the-data","content":" ADT provides raw and synthesized sensor data from Project Aria glasses, combined with groundtruth data generated using a motion capture system including depth images, device trajectories, object trajectories and bounding boxes, and human tracking. We also provide processed sensor data from our Machine Perception Services. Go to ADT Data Format to see a full list of the data we provide.  The ADT dataset contains 236 sequences recording single and dual-person activities. The data was recorded in two spaces: an apartment and a single room office. There are 74 single-instance dynamic objects shared between the two spaces.  Go to the Getting Started Tutorial to explore the sample dataset (available on Google colab, no download necessary) or the Dataset Download page to get started.  The sample dataset is a single-user dataset with body pose in the Apartment. It is a pretty representative example that should give you an idea of the dataset.  In addition to the sensor data and annotated ground truth data, we also provide high quality 3D object models (in .glb) for each of the objects in ADT. For more info on downloading and viewing these models, see Object Model page.  NEW UPDATE! 3D object model release is new as of September 2024  ","version":"Next","tagName":"h2"},{"title":"Apartment scene​","type":1,"pageTitle":"Aria Digital Twin Dataset","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset#apartment-scene","content":" 284 sequences were recorded in the apartment scene. The apartment comprised of a living room, kitchen, dining room and bedroom and contained 281 unique stationary objects.  Given some objects have multiple instances that may differ slightly, the apartment has 324 stationary object instances in total.  ","version":"Next","tagName":"h3"},{"title":"Office scene​","type":1,"pageTitle":"Aria Digital Twin Dataset","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset#office-scene","content":" 52 sequences were recorded in the office scene, a single room with minimal office furniture. The office room contained 15 unique stationary objects and 20 stationary object instances.  ","version":"Next","tagName":"h3"},{"title":"Activities​","type":1,"pageTitle":"Aria Digital Twin Dataset","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset#activities","content":" In the office scene, users examined objects. For the apartment scene we designed five single-person activities and three dual-person activities.  The single-person activities were:  Room decorationMeal preparationWorkObject examinationRoom cleaning  The dual-person activities included:  PartyingRoom cleaningDining table cleaning  Every activity has 10 to 50 sequences and the activity names are embedded into the sequence_names.  info We provide a mix of datasets where users may or may not be wearing an Aria and/or a bodysuit. Please refer to the skeleton_aria_association.json to see the case for each specific sub-sequence.  ","version":"Next","tagName":"h3"},{"title":"Documentation​","type":1,"pageTitle":"Aria Digital Twin Dataset","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset#documentation","content":" The ADT section of the wiki covers:  Getting Started A quickstart tutorial available as a Google colab project or install Project Aria Tools Python package to run locally, run the ADT notebook to access and visualize ground-truth data. Dataset Download A walkthrough of using aria_dataset_downloader to download the ADT dataset. Object Models Everything you need to know about the released ADT object models including: a walkthrough of using dtc_object_downloader to download the ADT object models, and brief description of data content and how to visualize the models Data Format How ADT data is organized and stored Data Loader APIs to load ADT data with handy code snippets Visualizers Compile and run our visualizer using an example that accesses ADT data in C++. Advanced tutorials MultiPerson Synchronization: A guide to learn how device synchronization works in ADT.Depth Maps to Pointcloud: An example notebook showing how to convert depth maps &amp; RGB images from ADT to a combined colored pointcloud in the Scene frame. ADT Challenges Learn more about the ADT Grand Challenge ","version":"Next","tagName":"h3"},{"title":"ADT Object Detection 2023-4 Challenges [CLOSED]","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/adt_challenges","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"ADT Object Detection 2023-4 Challenges [CLOSED]","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/adt_challenges#overview","content":" The 2023 Aria Digital Twin (ADT) challenges aim to accelerate research into 3D object detection and spatialization, serving as a catalyst for the broader research community.  ","version":"Next","tagName":"h2"},{"title":"Tasks​","type":1,"pageTitle":"ADT Object Detection 2023-4 Challenges [CLOSED]","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/adt_challenges#tasks","content":" ","version":"Next","tagName":"h2"},{"title":"Challenge One - 3D Object detection & tracking with scene generalization​","type":1,"pageTitle":"ADT Object Detection 2023-4 Challenges [CLOSED]","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/adt_challenges#challenge-one---3d-object-detection--tracking-with-scene-generalization","content":" The ADT benchmark dataset consists of a number of moving objects and recorded sequences ground-truthed in two distinct scenes. For this task, generate accurate 6DoF poses for the same set of objects within recordings from a completely new and unseen scene we provide in the challenge data.  ","version":"Next","tagName":"h3"},{"title":"Challenge Two - Few-shot 3D Object detection & tracking​","type":1,"pageTitle":"ADT Object Detection 2023-4 Challenges [CLOSED]","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/adt_challenges#challenge-two---few-shot-3d-object-detection--tracking","content":" We've provided a short period of ground-truth data for a specific target object within each sequence, your task is to leverage this data to accurately detect the same object and predict its 6DoF poses for the remaining duration of the sequence.  ","version":"Next","tagName":"h3"},{"title":"Prize per challenge​","type":1,"pageTitle":"ADT Object Detection 2023-4 Challenges [CLOSED]","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/adt_challenges#prize-per-challenge","content":" First place: $10,000 and a certificate. Second and Third places will receive certificates and recognition on the Challenge leaderboard. Additionally, teams with the best performing or noteworthy submissions may be invited to present their work at the CVPR conference in June 2024.  ","version":"Next","tagName":"h2"},{"title":"Important dates​","type":1,"pageTitle":"ADT Object Detection 2023-4 Challenges [CLOSED]","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/adt_challenges#important-dates","content":" Submission deadline for results: March 22, 2024 (11:59PM PST)Presentation of awards: CVPR in June 2024 (pending confirmation of our workshop)  ","version":"Next","tagName":"h2"},{"title":"Key participation terms​","type":1,"pageTitle":"ADT Object Detection 2023-4 Challenges [CLOSED]","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/adt_challenges#key-participation-terms","content":" Entrants can register as an Individual or TeamPredictions are submitted online at Eval.aiWinners agree to share a public report (e.g. whitepaper or publication) describing the method used for generating the PredictionsParticipants are encouraged (but not required) to make source code used to generate the Predictions available under an Approved OSS License  Full terms can be found on ProjectAria.com/challenges/terms.  ","version":"Next","tagName":"h2"},{"title":"How to participate​","type":1,"pageTitle":"ADT Object Detection 2023-4 Challenges [CLOSED]","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/adt_challenges#how-to-participate","content":" Follow the detailed instructions at Eval.ai. ","version":"Next","tagName":"h2"},{"title":"Advanced Tutorials","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/advanced_tutorials","content":"","keywords":"","version":"Next"},{"title":"Tutorials​","type":1,"pageTitle":"Advanced Tutorials","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/advanced_tutorials#tutorials","content":" Tutorial 1 : Multi-person Synchronization  Tutorial 2 : Depth Maps to Pointcloud  ","version":"Next","tagName":"h2"},{"title":"Troubleshooting​","type":1,"pageTitle":"Advanced Tutorials","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/advanced_tutorials#troubleshooting","content":" Go to Data Utilities Troubleshooting if you experience issues using these guides. ","version":"Next","tagName":"h2"},{"title":"Creating pointclouds from depth maps & RGB images","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/advanced_tutorials/depth_maps_to_pointcloud","content":"","keywords":"","version":"Next"},{"title":"Methodology​","type":1,"pageTitle":"Creating pointclouds from depth maps & RGB images","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/advanced_tutorials/depth_maps_to_pointcloud#methodology","content":" ADT provides a depth map corresponding to each Aria image for each of the 3 cameras (1 RGB and 2 SLAM).  To create a combined 3D pointcloud with color, the tutorial will:  Get RGB image timestampsiterate through the timestamps and sample the RGB image and depth image with that timestampget the camera pose at the query timestampiterate through all pixels and get the RGB &amp; XYZ data: back project pixel coordinate using the camera model to get the ray directionget the ray depth from depth map using the current pixel coordinatesget the 3D point in the camera frame by multiplying the ray by its depthtransform the point from the camera frame to scene frame using the camera poseassign RGB values using the RGB image with the current pixel coordinates  ","version":"Next","tagName":"h2"},{"title":"Run tutorial notebook​","type":1,"pageTitle":"Creating pointclouds from depth maps & RGB images","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/advanced_tutorials/depth_maps_to_pointcloud#run-tutorial-notebook","content":" You can run this tutorial on Google Colab via the ADT Depth Maps to Pointcloud Tutorial.  To run the notebook locally using Jupyter, follow these steps:  ","version":"Next","tagName":"h2"},{"title":"Step 0 : Install project_aria_tools package and create venv if not already done​","type":1,"pageTitle":"Creating pointclouds from depth maps & RGB images","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/advanced_tutorials/depth_maps_to_pointcloud#step-0--install-project_aria_tools-package-and-create-venv-if-not-already-done","content":" Follow Step 0 to Step 3 in Getting Started.  ","version":"Next","tagName":"h3"},{"title":"Step 1 : Download codebase​","type":1,"pageTitle":"Creating pointclouds from depth maps & RGB images","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/advanced_tutorials/depth_maps_to_pointcloud#step-1--download-codebase","content":" Follow the instructions in Download codebase to clone the code  ","version":"Next","tagName":"h3"},{"title":"Step 2 : Run the tutorial notebook​","type":1,"pageTitle":"Creating pointclouds from depth maps & RGB images","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/advanced_tutorials/depth_maps_to_pointcloud#step-2--run-the-tutorial-notebook","content":" From your Python virtual environment, run:  cd $HOME/Documents/projectaria_sandbox jupyter notebook projectaria_tools/projects/AriaDigitalTwinDatasetTools/examples/adt_depth_maps_to_pointcloud_tutorial.ipynb   This notebook automatically downloads the 10s sample sequence. Feel free to use the ADT downloader to try on other sequences! ","version":"Next","tagName":"h3"},{"title":"Multi-person Synchronization","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/advanced_tutorials/multiperson_synchronization","content":"","keywords":"","version":"Next"},{"title":"Further resources​","type":1,"pageTitle":"Multi-person Synchronization","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/advanced_tutorials/multiperson_synchronization#further-resources","content":" Timestamps in Aria VRS Files  ","version":"Next","tagName":"h2"},{"title":"How does time synchronization work in ADT?​","type":1,"pageTitle":"Multi-person Synchronization","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/advanced_tutorials/multiperson_synchronization#how-does-time-synchronization-work-in-adt","content":" In a single-person ADT sequence, all ground truth data is stored in the device capture time of the Project Aria device used by the person.  For a multi-person ADT sequence, we store ground truth files in two separate folders, each representing a person's Aria recording. Each subsequence is self-contained such that all ground truth data is also stored in the device capture time domain of the associated Aria device. To synchronize the two Aria devices, we store a mapping between timecode timestamps and device capture timestamps in each Aria data.  ","version":"Next","tagName":"h2"},{"title":"Run tutorial notebook​","type":1,"pageTitle":"Multi-person Synchronization","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/advanced_tutorials/multiperson_synchronization#run-tutorial-notebook","content":" ","version":"Next","tagName":"h2"},{"title":"Step 0 : Install project_aria_tools package and create venv if not already done​","type":1,"pageTitle":"Multi-person Synchronization","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/advanced_tutorials/multiperson_synchronization#step-0--install-project_aria_tools-package-and-create-venv-if-not-already-done","content":" Follow Step 0 to Step 3 in Getting Started.  ","version":"Next","tagName":"h3"},{"title":"Step 1 : Prepare to download​","type":1,"pageTitle":"Multi-person Synchronization","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/advanced_tutorials/multiperson_synchronization#step-1--prepare-to-download","content":" Follow Step 1 to Step 3 in Download the sample ADT sequence  ","version":"Next","tagName":"h3"},{"title":"Step 2 : Download sample multi-person sequence​","type":1,"pageTitle":"Multi-person Synchronization","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/advanced_tutorials/multiperson_synchronization#step-2--download-sample-multi-person-sequence","content":" From your Python virtual environment, run:  adt_benchmark_dataset_downloader -c $HOME/Documents/projectaria_tools_adt_data/aria_digital_twin_dataset_download_urls.json \\ -o $HOME/Documents/projectaria_tools_adt_data/ -l Apartment_release_multiskeleton_party_seq114 \\ -d 0   ","version":"Next","tagName":"h3"},{"title":"Step 3 : Download codebase​","type":1,"pageTitle":"Multi-person Synchronization","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/advanced_tutorials/multiperson_synchronization#step-3--download-codebase","content":" Follow the instructions in Download codebase to clone the code  ","version":"Next","tagName":"h3"},{"title":"Step 4 : Run the tutorial notebook​","type":1,"pageTitle":"Multi-person Synchronization","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/advanced_tutorials/multiperson_synchronization#step-4--run-the-tutorial-notebook","content":" From your Python virtual environment, run:  cd $HOME/Documents/projectaria_sandbox jupyter notebook projectaria_tools/projects/AriaDigitalTwinDatasetTools/examples/adt_multiperson_tutorial.ipynb  ","version":"Next","tagName":"h3"},{"title":"Data Loader","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_loader","content":"","keywords":"","version":"Next"},{"title":"AriaDigitalTwinDataPathsProvider​","type":1,"pageTitle":"Data Loader","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_loader#ariadigitaltwindatapathsprovider","content":" The main goal of this loader is to give the user an easy way to load all filepaths associated with an ADT sequence and the sequence metadata given an input to the sequence folder root path. The AriaDigitalTwinDataPathsProvider also allows the user to select specific annotations to load (e.g., with or without skeleton). Once a class is instantiated, this can be used manually to query for specific files without needing to know filenames, or the data paths class can be extracted and fed directly to the AriaDigitalTwinDataProvider described below.  PythonC++ from projectaria_tools.projects.adt import AriaDigitalTwinDataPathsProvider # define the sequence path you want to load sequence_path = &quot;PATH/TO/An_ADT_sequence&quot; # create path provider paths_provider = AriaDigitalTwinDataPathsProvider(sequence_path) # load the set of ground truth data files without skeleton occlusion data_paths_without_skeleton_occlusion = paths_provider.get_datapaths(False) # load the set of ground truth data files with skeleton occlusion data_paths_with_skeleton_occlusion = paths_provider.get_datapaths(True)   ","version":"Next","tagName":"h2"},{"title":"AriaDigitalTwinDataProvider​","type":1,"pageTitle":"Data Loader","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_loader#ariadigitaltwindataprovider","content":" This is the core data loader that takes an instance of the AriaDigitalTwinDataPaths class (generated by the AriaDigitalTwinDataPathsProvider) and provides you will query functions to access all ADT data. The following shows an example snippet to load ground truth data with the AriaDigitalTwinDataProvider:  PythonC++ from projectaria_tools.projects.adt import AriaDigitalTwinDataPathsProvider, AriaDigitalTwinDataProvider # define the sequence path you want to load sequence_path = &quot;PATH/TO/An_ADT_sequence&quot; # create path provider paths_provider = AriaDigitalTwinDataPathsProvider(sequence_path) # load the set of ground truth data files with skeleton occlusion data_paths_with_skeleton_occlusion = paths_provider.get_datapaths(True) # create data provider gt_provider = AriaDigitalTwinDataProvider(data_paths_with_skeleton_occlusion)   ","version":"Next","tagName":"h2"},{"title":"Skip Data loading​","type":1,"pageTitle":"Data Loader","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_loader#skip-data-loading","content":" All data loaders are designed to allow the user to skip the loading of specific data types. You can do this by setting the path to an empty string in your AriaDigitalTwinDataPathsinstance prior to constructing the AriaDigitalTwinDataProvider.  ","version":"Next","tagName":"h3"},{"title":"Check Data Existence​","type":1,"pageTitle":"Data Loader","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_loader#check-data-existence","content":" Since we allow users to skip specific data type loading as explained above, we also provide functions in in AriaDigitalTwinDataProviderto check if data exists by calling their appropriate functions before calling the corresponding getter functions. E.g. hasObject3dBoundingboxes()  ","version":"Next","tagName":"h2"},{"title":"Ground Truth Data Getter Functions​","type":1,"pageTitle":"Data Loader","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_loader#ground-truth-data-getter-functions","content":" For a full example of the Python getters, please refer to the notebook in the Getting Started. For a full example of the C++ getters, please refer to the visualizer example.  Getting Instance Information​  In ADT, we define an instance to be either a human or an object. The attributes of an instance is defined in class InstanceInfo in AriaDigitalTwinDataTypes. We use instanceType to differentiate a human and an object.  Time Query Options​  You may have also noticed the timeQueryOptions parameter in the above getter functions. Same as dataprovider, all getter functions for timestamped data allow you to specify how to query the timestamps. The options are defined in TimeTypes  Accessing Timestamped Data​  All timestamped data query APIs return a templated DataWithDt class. For example, BoundingBox2dDataWithDt defined in AriaDigitalTwinDataTypes as:  using BoundingBox2dDataWithDt = DataWithDt&lt;TypeBoundingBox2dMap&gt;;   The goal of wrapping all data in a DataWithDt class is to ensure all returned timestamped data has two fields: isValid, and dtNs. Where isValid defines whether or not the returned data is valid, since all timestamp queries may be invalid times, and dtNs to ensure the user always knows the time difference between the returned data and the query time.  Interpolation Function​  We provide interpolation functions for 6DoF Aria poses and Object 3d bounding boxes called &quot;getInterpolatedAria3dPoseAtTimestampNs&quot; and &quot;getInterpolatedObject3dBoundingBoxesAtTimestampNs&quot; in AriaDigitalTwinDataProvider  ","version":"Next","tagName":"h3"},{"title":"Time Synchronization Between Subsequences​","type":1,"pageTitle":"Data Loader","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_loader#time-synchronization-between-subsequences","content":" The Multiperson Synchronization advanced tutorial shows how to synchronize subsequences in an ADT sequence.  Further resources:  Timestamps in Aria VRS Files - how Project Aria timestamp data is formatted in VRS for single and multiple devicesProject Aria Device Timestamping - how the hardware is configuredTemporal Alignment of Aria Sensor Data - how the data is temporally aligned and how to finely align IMU, barometer and magnetometer data  ","version":"Next","tagName":"h3"},{"title":"Skeleton Data​","type":1,"pageTitle":"Data Loader","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_loader#skeleton-data","content":" Separate from the 2D skeleton data, we also have skeleton frames as measured by Optitrack. This data can be accessed directly from the AriaDigitalTwinDataProvider, or using the AriaDigitalTwinSkeletonProvider which can be extracted from AriaDigitalTwinDataProvider. Motive, the software that runs the Optitrack system, generates two types of skeleton data:  Skeleton Markers: a set of 3D marker positions of all visible markers that are attached to the bodysuit. If markers are occluded, they are set to [0,0,0]. We provide a helper function to get the labels: getMarkerLabels() in AriaDigitalTwinSkeletonProvider. For more information see motive’s Biomech57 template Skeleton Joints: a set of estimated 3D joint positions. We provide a list of these joint positions for each timestamp, as well as the joint labels getJointConnections(), and connections getJointLabels() in in AriaDigitalTwinSkeletonProvider  Note that both the markers and the joints are provided in the ADT Scene frame to be consistent with all other ground truth data. ","version":"Next","tagName":"h3"},{"title":"Getting Started With ADT","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/getting_started","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Getting Started With ADT","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/getting_started#overview","content":" This section provides a step-by-step guide to run the Aria Digital Twin (ADT) quickstart tutorial in a Jupyter notebook.  This notebook provides a walkthrough of:  Loading an ADT sequenceAccessing and visualizing all ADT ground-truth data: 6DoF object poses2D object bounding boxSegmentation imagesDepth imagesSkeleton and synthetic renderingEye gaze An example of undistorting ADT ground-truth data  We also have an two Advanced Tutorials that will walk you through getting synchronized ground truth data in a multi-person sequence, and using depth maps + RGB images to generated 3D colored pointclouds.  ","version":"Next","tagName":"h2"},{"title":"Run Jupyter Notebook on Google Colab​","type":1,"pageTitle":"Getting Started With ADT","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/getting_started#run-jupyter-notebook-on-google-colab","content":" Use the following link to run the Python notebook in an installation free playground:  Aria Digital Twin (ADT)  ","version":"Next","tagName":"h2"},{"title":"Running the Jupyter Notebook locally​","type":1,"pageTitle":"Getting Started With ADT","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/getting_started#running-the-jupyter-notebook-locally","content":" ","version":"Next","tagName":"h2"},{"title":"Step 0 : Check system requirements and download codebase​","type":1,"pageTitle":"Getting Started With ADT","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/getting_started#step-0--check-system-requirements-and-download-codebase","content":" Ensure your system is supported and then download projectaria_tools codebase from the github  ","version":"Next","tagName":"h3"},{"title":"Step 1 : Install Python​","type":1,"pageTitle":"Getting Started With ADT","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/getting_started#step-1--install-python","content":" If you have already installed projectaria-tools using Python Package Installation, you can skip to Step 4. The ADT Python code is part of the main projectaria-tools package.  Jupyter notebook error If you have problems using Jupyter examples, please upgrade python3 to the latest version.  ","version":"Next","tagName":"h3"},{"title":"Step 2 : Create a virtual environment​","type":1,"pageTitle":"Getting Started With ADT","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/getting_started#step-2--create-a-virtual-environment","content":" rm -rf $HOME/projectaria_tools_python_env python3 -m venv $HOME/projectaria_tools_python_env source $HOME/projectaria_tools_python_env/bin/activate   ","version":"Next","tagName":"h3"},{"title":"Step 3 : Install projectaria_tools from PyPI​","type":1,"pageTitle":"Getting Started With ADT","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/getting_started#step-3--install-projectaria_tools-from-pypi","content":" python3 -m pip install --upgrade pip python3 -m pip install projectaria-tools'[all]'   ","version":"Next","tagName":"h3"},{"title":"Step 4 : Download Sample Sequence:​","type":1,"pageTitle":"Getting Started With ADT","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/getting_started#step-4--download-sample-sequence","content":" Download the sample ADT sequence by following steps 0 to 4 inHow to Download the ADT Dataset.  ","version":"Next","tagName":"h3"},{"title":"Step 5 : Run Tutorial​","type":1,"pageTitle":"Getting Started With ADT","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/getting_started#step-5--run-tutorial","content":" From your projectaria_tools Python virtual environment, run:  cd $HOME/Documents/projectaria_sandbox jupyter notebook projectaria_tools/projects/AriaDigitalTwinDatasetTools/examples/adt_quickstart_tutorial.ipynb   ","version":"Next","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"Getting Started With ADT","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/getting_started#troubleshooting","content":" Go to Data Utilities Troubleshooting if you have issues implementing this guide.  ","version":"Next","tagName":"h2"},{"title":"Other useful links​","type":1,"pageTitle":"Getting Started With ADT","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/getting_started#other-useful-links","content":" Use projectaria_tools with CMakeTimestamps in Aria VRS Files ","version":"Next","tagName":"h2"},{"title":"ADT Data Format","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_format","content":"","keywords":"","version":"Next"},{"title":"Sequence structure​","type":1,"pageTitle":"ADT Data Format","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_format#sequence-structure","content":" |Sequence1Name| ├──video.vrs # Aria recording data ├──instances.json # metadata of all instances in a sequence. An instance can be an object or a skeleton ├──aria_trajectory.csv # 6DoF Aria trajectory ├──2d_bounding_box.csv # 2D bounding box data for instances in three Aria sensors: RGB camera, left SLAM camera, right SLAM camera ├──3d_bounding_box.csv # 3D AABB of each object ├──scene_objects.csv # 6 DoF poses of objects ├──eyegaze.csv # Eye gaze ├──synthetic_video.vrs # Synthetic rendering of video.vrs ├──depth_images.vrs # Depth images of video.vrs ├──segmentations.vrs # Instance segmentations of video.vrs ├──skeleton_aria_association.json [optional] # File showing association between Aria devices and skeletons, if they exist. Omitted if a sequence does not have skeleton ground truth. ├──Skeleton_*.json [optional] # Body skeleton data. * is the skeleton name. Omitted if a sequence does not have skeleton ground truth ├──2d_bounding_box_with_skeleton.csv [optional] # 2D bounding box data with body mesh occlusions. Omitted if a sequence does not have skeleton ground truth ├──depth_images_with_skeleton.vrs [optional] # Depth images with body mesh occlusions. Omitted if a sequence does not have skeleton ground truth ├──segmentations_with_skeleton.vrs [optional] # Segmentations with body mesh occlusions. Omitted if a sequence does not have skeleton ground truth ├──metadata.json # stores important information about the sequence ├──MPS # Go to Data Formats/MPS Output for more information about the data in this directory ├── eye_gaze ├── general_eye_gaze.csv ├── summary.json ├── slam ├── alignment_results.json # Alignment results between the MPS closed loop trajectory and the ADT GT trajectory ├── closed_loop_trajectory.csv ├── open_loop_trajectory.csv ├── online_calibration.csv ├── semidense_observations.csv.gz ├── semidense_points.csv.gz ├── summary.json   SkeletonMetaData.json name change Prior to v1.1 of the dataset, skeleton_aria_association.json was called SkeletonMetaData.json.  ","version":"Next","tagName":"h2"},{"title":"Timestamps Mapping Data​","type":1,"pageTitle":"ADT Data Format","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_format#timestamps-mapping-data","content":" Project Aria glasses recording concurrently in the same location leverage SMPTE timecode to receive a synchronized time clock with sub-millisecond accuracy.  The mapping between device time and timecode clock for each sequence is stored in the VRS file as a Time Domain Mapping Class. Go to Timestamps in Aria VRS Files for more information about how Aria sensor data is timestamped.  Go to Multiperson Synchronization for how to get synchronized ground truth data in a multi-person sequence.  ","version":"Next","tagName":"h3"},{"title":"Ground Truth Data​","type":1,"pageTitle":"ADT Data Format","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_format#ground-truth-data","content":" You can use the AriaDigitalTwinDataPathProvider to load a sequence and select a subsequence.AriaDigitalTwinDataPathProvider will manage all the ground truth files in a subsequence folder (not the MPS files).  ","version":"Next","tagName":"h2"},{"title":"Aligning Ground Truth and MPS Data​","type":1,"pageTitle":"ADT Data Format","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_format#aligning-ground-truth-and-mps-data","content":" The alignment_results.json file in mps/slam directory contains the alignment results between the MPS closed loop trajectory and the ADT GT trajectory. The alignment results have already been applied to the closed loop trajectory and the semidense pointcloud to convert from the SLAM frame to the ADT frame, ensuring all ADT data is expressed in the same coordinate frame for all sequences.  ","version":"Next","tagName":"h3"},{"title":"Skeleton Data and Availability​","type":1,"pageTitle":"ADT Data Format","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_format#skeleton-data-and-availability","content":" Not all ADT sequences have skeleton tracking. For those sequences with skeleton tracking enabled, we use the marker measurements from the bodysuit to generate a 3D mesh estimate of the wearer which is then used in our ground truth generation pipeline to calculate 2D bounding boxes, segmentation images and depth images.  In these cases, ADT provides two sets of ground truth data: one with skeleton occlusion, one without.  segmentations.vrs vs. segmentations_with_skeleton.vrsdepth_images.vrs vs. depth_images_with_skeleton.vrs'2d_bounding_box.csv' vs. '2d_bounding_box_with_skeleton.csv'  You can use AriaDigitalTwinDataPathsProvider to switch between these two sets.  ","version":"Next","tagName":"h2"},{"title":"Ground Truth Data Format​","type":1,"pageTitle":"ADT Data Format","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_format#ground-truth-data-format","content":" Our data loader loads all this data into a single class with useful tools for accessing data. For more information on the data classes returned by the loader, go to the Data Loader page.  ","version":"Next","tagName":"h2"},{"title":"2d_bounding_box.csv or 2d_bounding_box_with_skeleton.csv​","type":1,"pageTitle":"ADT Data Format","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_format#2d_bounding_boxcsv-or-2d_bounding_box_with_skeletoncsv","content":" Column\tType\tDescriptionstream_id\tstring\tcamera stream id associated with the bounding box image object_uid\tuint64_t\tid of the instance (object or skeleton) timestamp[ns]\tint64_t\ttimestamp of the image in nanoseconds x_min[pixel]\tint\tminimum dimension in the x axis x_max[pixel]\tint\tmaximum dimension in the x axis y_min[pixel]\tint\tminimum dimension in the y axis y_max[pixel]\tint\tmaximum dimension in the y axis visibility_ratio[%]\tdouble\tpercentage of the object that is visible (0: not visible, 1: fully visible)  ","version":"Next","tagName":"h3"},{"title":"3d_bounding_box.csv​","type":1,"pageTitle":"ADT Data Format","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_format#3d_bounding_boxcsv","content":" Column\tType\tDescriptionobject_uid\tuint64_t\tid of the instance (object or skeleton) timestamp[ns]\tint64_t\ttimestamp of the image in nanoseconds. -1 means the instance is static p_local_obj_xmin[m]\tdouble\tminimum dimension in the x axis (in meters) of the bounding box p_local_obj_xmax[m]\tdouble\tmaximum dimension in the x axis (in meters) of the bounding box p_local_obj_ymin[m]\tdouble\tminimum dimension in the y axis (in meters) of the bounding box p_local_obj_ymax[m]\tdouble\tmaximum dimension in the y axis (in meters) of the bounding box p_local_obj_zmin[m]\tdouble\tminimum dimension in the z axis (in meters) of the bounding box p_local_obj_zmax[m]\tdouble\tmaximum dimension in the z axis (in meters) of the bounding box  ","version":"Next","tagName":"h3"},{"title":"aria_trajectory.csv​","type":1,"pageTitle":"ADT Data Format","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_format#aria_trajectorycsv","content":" ADT uses the same trajectory format as closed loop trajectory in MPS.  While the data structure is the same, the file is generated by the ADT ground truth system, not by MPS.  ","version":"Next","tagName":"h3"},{"title":"eyegaze.csv​","type":1,"pageTitle":"ADT Data Format","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_format#eyegazecsv","content":" ADT uses the same eye gaze format as MPS.  Unlike MPS outputs, the ground truth eyegaze.csv contains depth mapping estimated by the ADT ground truth system.  ","version":"Next","tagName":"h3"},{"title":"scene_objects.csv​","type":1,"pageTitle":"ADT Data Format","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_format#scene_objectscsv","content":" Column\tType\tDescriptionobject_uid\tuint64_t\tid of the instance (object or skeleton) timestamp[ns]\tint64_t\ttimestamp of the image in nanoseconds. -1 means the instance is static t_wo_x[m]\tdouble\tx translation from object frame to world (scene) frame (in meters) t_wo_y[m]\tdouble\ty translation from object frame to world (scene) frame (in meters) t_wo_z[m]\tdouble\tz translation from object frame to world (scene) frame (in meters) q_wo_w\tdouble\tw component of quaternion from object frame to world (scene) frame q_wo_x\tdouble\tx component of quaternion from object frame to world (scene) frame q_wo_y\tdouble\ty component of quaternion from object frame to world (scene) frame q_wo_z\tdouble\tz component of quaternion from object frame to world (scene) frame  ","version":"Next","tagName":"h3"},{"title":"instances.json​","type":1,"pageTitle":"ADT Data Format","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_format#instancesjson","content":" { &quot;IID1&quot;: { &quot;instance_id&quot;: IID1, &quot;instance_name&quot;: &quot;XXXX&quot;, &quot;prototype_name&quot;: &quot;XXXX&quot;, &quot;category&quot;: &quot;XXXX&quot;, &quot;category_uid&quot;: XXXX, &quot;motion_type&quot;: &quot;static/dynamic&quot;, &quot;instance_type&quot;: &quot;object/human&quot;, &quot;rigidity&quot;: &quot;rigid/deformable&quot;, &quot;rotational_symmetry&quot;: { &quot;is_annotated&quot;: true/false }, &quot;canonical_pose&quot;: { &quot;up_vector&quot;: [ x, y, z ], &quot;front_vector&quot;: [ x, y, z ] } }, ... }   ","version":"Next","tagName":"h3"},{"title":"Skeleton_T.json or Skeleton_C.json​","type":1,"pageTitle":"ADT Data Format","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_format#skeleton_tjson-or-skeleton_cjson","content":" { &quot;dt_optitrack_minus_device_ns&quot;: { &quot;1WM103600M1292&quot;: XXXXX }, &quot;frames&quot;: [ { &quot;markers&quot;: [ [ mx1 my1 mz1 ], ... ], &quot;joints&quot;: [ [ jx1 jy1 jz1 ], ... ], &quot;timestamp_ns&quot;: tsns1 }, ... ] }   ","version":"Next","tagName":"h3"},{"title":"skeleton_aria_association.json​","type":1,"pageTitle":"ADT Data Format","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_format#skeleton_aria_associationjson","content":" This file shows the skeleton info including name, Id, and associated Aria device for each human in the sequence.  Because it's possible to have a person wearing a bodysuit that is not wearing an Aria device, it's possible to have a skeleton with no associated AriaDeviceSerial.  It's also possible to have an Aria wearer with no bodysuit, which means there may be an empty skeleton Id and a name associated with an Aria device.  { &quot;SkeletonMetadata&quot;: [ { &quot;AssociatedDeviceSerial&quot;: &quot;AriaSerial1/NONE&quot;, &quot;SkeletonId&quot;: ID1, &quot;SkeletonName&quot;: &quot;SkeletonName1/NONE&quot; }, ... ] }   ","version":"Next","tagName":"h3"},{"title":"video.vrs​","type":1,"pageTitle":"ADT Data Format","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_format#videovrs","content":" video.vrs contains the raw sensor recording from the Aria device.  Aria Hardware Specifications shows the sensors used to make recordings Images were all recorded at 30 fps  ","version":"Next","tagName":"h3"},{"title":"depth_images.vrs​","type":1,"pageTitle":"ADT Data Format","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_format#depth_imagesvrs","content":" depth_images.vrs1 contains 3 streams of images corresponding to the exact streams in video.vrs.  Each depth image is the same size as their corresponding raw image, where the pixel contents are integers expressing the depth in the camera’s Z-axis, in units of mm. This should not to be confused with ASE depth images, which describe the depth along each pixel ray Depth data is calculated using ADT’s ground truth system  ","version":"Next","tagName":"h3"},{"title":"segmentations.vrs​","type":1,"pageTitle":"ADT Data Format","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/data_format#segmentationsvrs","content":" segmentations.vrs contains 3 streams of images corresponding to the exact streams in video.vrs.  Each segmentation image is the same size as their corresponding raw image, where the pixel contents are integers expressing the Instance Id that was observed by that pixelSegmentation data is calculated using ADT’s ground truth system ","version":"Next","tagName":"h3"},{"title":"ADT Object Models","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/object_models","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"ADT Object Models","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/object_models#overview","content":" This page covers how to download the 3D object models associated with the Aria Digital Twin (ADT) dataset, and briefly describe the data content and how to visualize the models.  By downloading the object models as described in this page, you agree that you have read and accepted the terms of the Aria Digital Twin Object Models License Agreement.  ","version":"Next","tagName":"h2"},{"title":"Download the ADT Object Models​","type":1,"pageTitle":"ADT Object Models","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/object_models#download-the-adt-object-models","content":" ","version":"Next","tagName":"h2"},{"title":"Step 0: install project_aria_tools package and create venv if not done before​","type":1,"pageTitle":"ADT Object Models","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/object_models#step-0-install-project_aria_tools-package-and-create-venv-if-not-done-before","content":" Follow Step 0 to Step 3 in Getting Started.  ","version":"Next","tagName":"h3"},{"title":"Step 1 : Signup and get the download links file​","type":1,"pageTitle":"ADT Object Models","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/object_models#step-1--signup-and-get-the-download-links-file","content":" To use the downloader CLI, you need to download a file which contains all data URLs plus some metadata for the downloader script. We currently offer two ways of getting this file:  Visit ADT website and sign up.  Scroll down to the bottom of the page. Enter your email and select Access the Datasets.    Once you've selected Access the Datasets you'll be taken back to the top of the ADT page.  Scroll down the page to select ADT Object 3D Models link and download the file to the folder $HOME/Downloads.    The download-links file will expire in 14 days You can redownload the download links whenever they expire  ","version":"Next","tagName":"h3"},{"title":"Step 2 : Set up a folder for object models​","type":1,"pageTitle":"ADT Object Models","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/object_models#step-2--set-up-a-folder-for-object-models","content":" mkdir -p $HOME/Documents/adt_object_library   ","version":"Next","tagName":"h3"},{"title":"Step 3 : Download the object models via CLI:​","type":1,"pageTitle":"ADT Object Models","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/object_models#step-3--download-the-object-models-via-cli","content":" To download all ADT models (~5.5 GB), run the following command from your Python virtual environment:  dtc_object_downloader -r ADT -c {PATH_TO_YOUR_CDN_FILE} -o $HOME/Documents/adt_object_library   To download only a select subset of ADT models, run the following command from your Python virtual environment:  dtc_object_downloader -r ADT -c {PATH_TO_YOUR_CDN_FILE} -o $HOME/Documents/adt_object_library -l {SELECTED_OBJECT_NAMES}   You can retrieve object names from the ADT data providers using your downloaded sequence data, or have a look at the instances.json file in your sequence data.  warning For legal reasons, we have removed the object DinoToy from the available models, however, this object is still present in the ADT sequence data.  ","version":"Next","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"ADT Object Models","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/object_models#troubleshooting","content":" Check Project Aria Tools troubleshooting if you are having issues using this guide.  ","version":"Next","tagName":"h3"},{"title":"Object Model Data Format​","type":1,"pageTitle":"ADT Object Models","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/object_models#object-model-data-format","content":" When downloading the object models using dtc_object_downloader, each object will download into its own folder, named using the instance name of the object. Under each object folder, there will be three files as shown below.  |InstanceName1| ├──3d-asset.glb # model file ├──CC_BY-SA.txt # license file ├──metadata.json |InstanceName2| {...}   All poses associated with the objects are stored in each sequence ground truth data, not in the downloaded object data. For example code to learn how to load object models and their associated poses (including static and dynamic objects), refer to the ADT Rerun Viewer  ","version":"Next","tagName":"h2"},{"title":"Visualizing Object Models​","type":1,"pageTitle":"ADT Object Models","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/object_models#visualizing-object-models","content":" The GLB files for each object model can be viewed using any off-the-shelf GLB viewer. For example, the open web browser based Babylon viewer can be used by dragging and dropping the GLB file.  To visualize multiple object models, along with their ground truth poses, the ADT rerun viewer can be used. To run the viewer, enter the following command from your virtual environment with projectaria_tools installed:  viewer_projects_adt --sequence_path {PATH_TO_YOUR_SEQUENCE} --object_library_path $HOME/Documents/adt_object_library --display_objects {INSTANCE_NAMES}     Example command to produce the above visualization:  viewer_projects_adt --sequence_path $HOME/Documents/adt_sequence_data/Apartment_release_golden_skeleton_seq100_M1292 \\ --object_library_path $HOME/Documents/adt_object_library \\ --display_objects BlackCoffeeTable BirdHouseToy YellowArmChair GreySofa RedArmChair  ","version":"Next","tagName":"h2"},{"title":"How to Download the ADT Dataset","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/dataset_download","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"How to Download the ADT Dataset","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/dataset_download#overview","content":" This page covers how to download the sample Aria Digital Twin (ADT) sequence as well as how to download specific sequences and types of data. Follow the instructions to download the sample datasets and from there you'll be able to use the CLI to download more data. For information on how to download ADT object models, see Object Models page  The sample dataset is a single-user dataset with body pose in the Apartment. This is a pretty representative example that should give you an idea of the dataset.  By downloading the datasets you agree that you have read and accepted the terms of the Aria Digital Twin Dataset License Agreement.  ","version":"Next","tagName":"h2"},{"title":"Download the sample Aria Digital Twin (ADT) sequence​","type":1,"pageTitle":"How to Download the ADT Dataset","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/dataset_download#download-the-sample-aria-digital-twin-adt-sequence","content":" ","version":"Next","tagName":"h2"},{"title":"Step 0: install project_aria_tools package and create venv if not done before​","type":1,"pageTitle":"How to Download the ADT Dataset","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/dataset_download#step-0-install-project_aria_tools-package-and-create-venv-if-not-done-before","content":" Follow Step 0 to Step 3 in Getting Started.  ","version":"Next","tagName":"h3"},{"title":"Step 1 : Signup and get the download links file​","type":1,"pageTitle":"How to Download the ADT Dataset","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/dataset_download#step-1--signup-and-get-the-download-links-file","content":" To use the downloader CLI, you need to download a file which contains all data URLs plus some metadata for the downloader script. We currently offer two ways of getting this file:  Option 1 - Aria Dataset Explorer:  Go to the Aria Dataset Explorer website. Here you can subselect sequences according to some filter options, or get the links to all sequences.  Option 2 - ADT Website:  Visit ADT website and sign up.  Scroll down to the bottom of the page. Enter your email and select Access the Datasets.    Once you've selected Access the Datasets you'll be taken back to the top of the ADT page.  Scroll down the page to select ADT Sequences link and download the file to the folder $HOME/Downloads.    The download-links file will expire in 14 days You can redownload the download links whenever they expire  ","version":"Next","tagName":"h3"},{"title":"Step 2 : Set up a folder for ADT data​","type":1,"pageTitle":"How to Download the ADT Dataset","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/dataset_download#step-2--set-up-a-folder-for-adt-data","content":" mkdir -p $HOME/Documents/projectaria_tools_adt_data   ","version":"Next","tagName":"h3"},{"title":"Step 3 : Download the sample sequence (~500MB) via CLI:​","type":1,"pageTitle":"How to Download the ADT Dataset","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/dataset_download#step-3--download-the-sample-sequence-500mb-via-cli","content":" From your Python virtual environment, run:  aria_dataset_downloader -c ${PATH_TO_YOUR_CDN_FILE} \\ -o $HOME/Documents/projectaria_tools_adt_data/ \\ -l Apartment_release_golden_skeleton_seq100_10s_sample_M1292 \\ -d 0 1 2 3 4 5 6 7 8 9   The sample dataset is a single-user dataset with body pose in the Apartment. This is a pretty representative example to give an idea of the dataset. For more information on the content in the other sequences, see the Data Content section below  ","version":"Next","tagName":"h3"},{"title":"Download the Aria Digital Twin (ADT) benchmark dataset​","type":1,"pageTitle":"How to Download the ADT Dataset","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/dataset_download#download-the-aria-digital-twin-adt-benchmark-dataset","content":" ","version":"Next","tagName":"h2"},{"title":"Data size​","type":1,"pageTitle":"How to Download the ADT Dataset","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/dataset_download#data-size","content":" The Aria Digital Twin dataset consists of 236 sequences. It is split into 4 data types that can be downloaded individually, plus MPS data. Go to Project Aria Machine Perception Services for more information about MPS data. The MPS data is also broken into chunks that can be included or excluded at download time. ADT data without MPS is approximately 3.5TB.  ","version":"Next","tagName":"h3"},{"title":"Download via CLI​","type":1,"pageTitle":"How to Download the ADT Dataset","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/dataset_download#download-via-cli","content":" ADT uses the general Aria dataset downloader, which is available in the projectaria_tools PyPI (pip install) package.  To use the downloader, use the following commands in the virtual environment where you've installed projectaria_tools:  aria_dataset_downloader   To get a description of the arguments that the script needs, run:  aria_dataset_downloader --help   The following are some example use cases:  Download VRS for all sequences​  aria_dataset_downloader --cdn_file ${PATH_TO_YOUR_CDN_FILE} --output_folder ${OUTPUT_FOLDER_PATH} --data_types 0   Download VRS + main ground truth data for all sequences​  aria_dataset_downloader --cdn_file ${PATH_TO_YOUR_CDN_FILE} --output_folder ${OUTPUT_FOLDER_PATH} --data_types 0 6   Download all data for all sequences​  aria_dataset_downloader --cdn_file ${PATH_TO_YOUR_CDN_FILE} --output_folder ${OUTPUT_FOLDER_PATH} --data_types 0 1 2 3 4 5 6 7 8 9   Download VRS for 2 specific sequences​  aria_dataset_downloader --cdn_file ${PATH_TO_YOUR_CDN_FILE} --output_folder ${OUTPUT_FOLDER_PATH} --data_types 0 --sequence_names Apartment_release_clean_seq131_M1292 Apartment_release_clean_seq133_M1292   Download VRS for all sequences and overwrite​  aria_dataset_downloader --cdn_file ${PATH_TO_YOUR_CDN_FILE} --output_folder ${OUTPUT_FOLDER_PATH} --data_types 0 --overwrite   ","version":"Next","tagName":"h3"},{"title":"Deprecation Notes​","type":1,"pageTitle":"How to Download the ADT Dataset","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/dataset_download#deprecation-notes","content":" ","version":"Next","tagName":"h2"},{"title":"Dataset Metadata​","type":1,"pageTitle":"How to Download the ADT Dataset","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/dataset_download#dataset-metadata","content":" We previously provided a dataset metadata JSON “aria_digital_twin_benchmark_metadata.json”, which was downloadable with the old adt_benchmark_dataset_downloader. The goal of this file was to allow users to filter sequences by data fields provided in each sequence metadata. The Aria Dataset Explorer has replaced this old method for filtering sequences.  ","version":"Next","tagName":"h3"},{"title":"Challenge Data​","type":1,"pageTitle":"How to Download the ADT Dataset","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/dataset_download#challenge-data","content":" We will be removing access to the ADT challenge data now that the challenge has concluded. The challenge downloader will be available until the end of 2024. You can find instructions for using this downloader on the ADT Challenge Website. To use this downloader, however, you will need the deprecated CDN links file. Please email us at ariaoperations@meta.com before December 31 2024 to get access to this metadata file.  ","version":"Next","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"How to Download the ADT Dataset","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/dataset_download#troubleshooting","content":" Check Project Aria Tools troubleshooting if you are having issues using this guide. ","version":"Next","tagName":"h2"},{"title":"ADT Visualizers","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/visualizers","content":"","keywords":"","version":"Next"},{"title":"Python Visualizer​","type":1,"pageTitle":"ADT Visualizers","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/visualizers#python-visualizer","content":" viewer_projects_adt displays an interactive view of an ADT sequence with Rerun. It allows to see all data in 3D context.  viewer_projects_adt --sequence_path Apartment_release_multiskeleton_party_seq104     tip The timeline enables you to see when a given object is being moved The 3D world view is clickable, you can quickly select an object and see its instance name You can quickly see where an object has been moved all along the sequence using the “Visible Time Range” feature in the RIGHT panel.    ","version":"Next","tagName":"h2"},{"title":"C++ Visualizer​","type":1,"pageTitle":"ADT Visualizers","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/visualizers#c-visualizer","content":" AriaDigitalTwinViewer is a C++ binary written to visualize ADT data with toggles for each ground truth data type and a slider bar for frame selection. The image below shows an example screenshot of the viewer.    ","version":"Next","tagName":"h2"},{"title":"Step 1 : Download Sample Sequence:​","type":1,"pageTitle":"ADT Visualizers","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/visualizers#step-1--download-sample-sequence","content":" Download the sample Aria Digital Twin (ADT) sequencefollow this guide.  ","version":"Next","tagName":"h3"},{"title":"Step 2 : Build projectaria_tools C++ libraries​","type":1,"pageTitle":"ADT Visualizers","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/visualizers#step-2--build-projectaria_tools-c-libraries","content":" Follow the entire C++ installation to build projectaria_tools C++ libraries with visualization.  ","version":"Next","tagName":"h3"},{"title":"Step 3 : Build AriaDigitalTwinViewer​","type":1,"pageTitle":"ADT Visualizers","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/visualizers#step-3--build-ariadigitaltwinviewer","content":" cd $HOME/Documents/projectaria_sandbox/build cmake ../projectaria_tools -DPROJECTARIA_TOOLS_BUILD_PROJECTS=ON -DPROJECTARIA_TOOLS_BUILD_PROJECTS_ADT=ON make -j2   ","version":"Next","tagName":"h3"},{"title":"Step 4 : Run AriaDigitalTwinViewer​","type":1,"pageTitle":"ADT Visualizers","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/visualizers#step-4--run-ariadigitaltwinviewer","content":" cd $HOME/Documents/projectaria_sandbox/build ./projects/AriaDigitalTwinDatasetTools/visualization/AriaDigitalTwinViewer \\ --sequence-path $HOME/Documents/projectaria_tools_adt_data/Apartment_release_golden_skeleton_seq100_10s_sample/ \\ --device-num 0 --skeleton-flag 0   ","version":"Next","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"ADT Visualizers","url":"/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/visualizers#troubleshooting","content":" Go to Data Utilities Troubleshooting if you experience any issues. ","version":"Next","tagName":"h2"},{"title":"Aria Everyday Activities Dataset","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Aria Everyday Activities Dataset","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset#overview","content":" The Aria Everyday Activities (AEA) dataset provides sequences collected using Project Aria glasses in a variety of egocentric scenarios, including: cooking, exercising, playing games and spending time with friends. The goal of AEA is to provide researchers with data to engage in solving problems related to the challenges of always-on egocentric vision. AEA contains multiple activity sequences where 1-2 users wearing Project Aria glasses participate in scenarios to capture time synchronized data in a shared world location.  For more information on the activities, see:  Activities: details about the scenarios and where specific activities are in the recording sequenceRecording Scripts: more details about each scenario  Aria Everyday Activities Dataset was first released as part of the Aria Pilot Dataset (Project Aria’s first open dataset). It has now been improved with a new data format, new tooling that enables it to be used with Project Aria Tools, and additional machine perception data. The data was recorded using Recording Profile 9.  ","version":"Next","tagName":"h2"},{"title":"About the data​","type":1,"pageTitle":"Aria Everyday Activities Dataset","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset#about-the-data","content":" In addition to providing raw sensor data from Project Aria glasses, this dataset also contains annotated speech to text data, and results from our Machine Perception Services that provide additional context to the spatial-temporal reference frames. We provide:  Per-frame eye tracking Generalized Eye Gaze data Accurate 3D trajectories of users across multiple everyday activities in the same location Trajectory and Semi-Dense Point Cloud data Timestamped with a shared clock to allow synchronization of data from multiple concurrent recordingsLocation information expressed in a shared global coordinate frame for all recordings collected in the same physical locationOnline calibration information for the cameras and IMUsSpeech-to-text annotation  The dataset contains:  143 recordings for Everyday ActivitiesRecording in 5 locations, with 53 sequences of 2 users simultaneous recordingOver 1 million imagesOver 7.5 accumulated hours    Figure 1: Shared 3D Global Trajectories for Multi-User Activities in the Same Location  Documentation  The AEA section of this wiki covers:  Getting Started With the AEA DatasetHow to Download the AEA DatasetAEA Data FormatAEA VisualizerAEA Everyday ActivitiesAria Wearer Data Scripts ","version":"Next","tagName":"h2"},{"title":"AEA Activities","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_activities","content":"","keywords":"","version":"Next"},{"title":"Recording Sessions​","type":1,"pageTitle":"AEA Activities","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_activities#recording-sessions","content":" We collected these scripted activities in five indoor location across the USA.  Table 1: Every Day Activity Statistics and Scripts Used Per Recording Location  Location\tScripts recorded in this location\tNumber of Wearers\tNumber of Recordings\tNumber of Frames\tNumber of RGB Frames\tTotal duration (hours)Location_1_indoor\tScripts 1-5\t1-2 Wearers\t29\t230,487\t115,235\t1.6 Location_2_indoor\tScripts 1-5\t1-2 Wearers\t43\t339,650\t169,824\t2.3 Location_3_indoor\tScripts 1-5\t1-2 Wearers\t38\t259,027\t129,514\t1.7 Location_4_indoor\tScripts 1-5\t1-2 Wearers\t19\t94,029\t47,015\t0.6 Location_5_indoor\tScripts 4-5\t1 Wearer\t14\t168,428\t84,214\t1.1  ","version":"Next","tagName":"h2"},{"title":"Activity Sequences​","type":1,"pageTitle":"AEA Activities","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_activities#activity-sequences","content":" Each script contains multiple daily activities recorded in a sequence. Use the list below to identify where scenarios are used in scripts. Numbers indicate how many wearers were in a sequence.  Making coffee​  Script 2: Caffeination (2)Script 4: Perk up (1)  Prepare snacks​  Script 1: Set out food and drink (1)Script 2: Grab the cream, sugar, and some snacks (2)  Cooking​  Script 3: Cooking (2)Script 4: Breaking the Fast (1)Script 5: Check the provisions (1)  Clean the place​  Script 1: Clean the place (1)Script 2: Clean up spilled coffee (2)Script 2: Time to go (2)Script 3: Clean up (2)Script 4: Cleaning up (1)  Dining​  Script 3: Set the table (2)Script 4: Eating (1)  Organization and laundry​  Script 1: Put up decorations (1)Script 5: Get home (1)Script 5: Wash clothes (1)Script 5: Straighten up (1)Script 5: Dry clothes (1)Script 5: Get and fold laundry (1)  Reading, games and exercise​  Script 1: Get the blood pumping (1)Script 1: Watch a TV Show (1)Script 1: Video game break (1)Script 1: Read and wait (1)Script 4: Waking up (1)Script 4: Exercise (1)Script 4: Play console video game (1)Script 5: Catch up on email and social media (1)  Touring the room​  Script 2: Tour of the house (2)  Multi-person indoor activities​  Script 2: Guest arrives (2)Script 2: Play a board game (2)Script 2: Share videos (2)Script 3: Arriving home (2)  Activities with indoor outdoor transitions​  Script 2: Guest arrives (2)Script 3: Arriving home (2)Script 5: Get home (1) ","version":"Next","tagName":"h2"},{"title":"How to Download the AEA Dataset","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_download_dataset","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"How to Download the AEA Dataset","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_download_dataset#overview","content":" This page covers how to download sample Aria Everyday Activities (AEA) sequences, as well as how to download specific sequences and types of data. Follow the instructions to download the sample sequence and from there you'll be able to use the CLI to download more data.  By downloading the datasets you agree that you have read and accepted the terms of the Aria Everyday Activities Dataset License Agreement.  ","version":"Next","tagName":"h2"},{"title":"AEA Data Format","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_data_format","content":"","keywords":"","version":"Next"},{"title":"SLAM output​","type":1,"pageTitle":"AEA Data Format","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_data_format#slam-output","content":" The SLAM outputs were created in a shared coordinate frame. The Multi-SLAM data format page contains more information about this output. Please note the file structure is slightly different (vrs_to_multi_slam.json is not necessary) compared to MPS CLI Multi-SLAM requests, as the shared coordinate frame is organized by location.  ","version":"Next","tagName":"h2"},{"title":"Speech to Text annotation​","type":1,"pageTitle":"AEA Data Format","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_data_format#speech-to-text-annotation","content":" Speech to Text annotation provides text strings generated by Automatic Speech Recognition (ASR) with timestamps and confidence rating. The ASR annotation used an in-house proprietary system. Similar results can be acquired via open-source ASR solutions.  Table 2: speech.csv Structure  startTime_ns\tendTime_ns\twritten\tconfidence 54040\t55040\tI’m\t0.25608 72920\t73920\tlooking\t0.84339  ","version":"Next","tagName":"h2"},{"title":"Timestamps Mapping Data​","type":1,"pageTitle":"AEA Data Format","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_data_format#timestamps-mapping-data","content":" Project Aria glasses and multi-view devices operating in proximity to each other (&lt;100m) can leverage SMPTE timecode to receive a synchronized time clock with sub-millisecond accuracy.  The mapping between device time and timecode clock for each sequence is stored in the VRS file as a Time Domain Mapping Class. Go to Timestamps in Aria VRS Files for more information about how Aria sensor data is timestamped.  To translate the local timestamp of an arbitrary piece of data to the timecode time domain, you can interpolate between device timestamps in the time domain mapping data. An implementation of this mechanism is already provided in VrsDataProvider. To synchronize data from a secondary device, you can query that second VRS with this timecode time. This functionality is also already implemented in the VrsDataProvider class. ","version":"Next","tagName":"h2"},{"title":"Download the sample AEA sequence​","type":1,"pageTitle":"How to Download the AEA Dataset","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_download_dataset#download-the-sample-aea-sequence","content":" ","version":"Next","tagName":"h2"},{"title":"Step 0: Install project_aria_tools package and create a virtual environment if not already done​","type":1,"pageTitle":"How to Download the AEA Dataset","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_download_dataset#step-0-install-project_aria_tools-package-and-create-a-virtual-environment-if-not-already-done","content":" Follow Step 0 to Step 3 in Getting Started.  ","version":"Next","tagName":"h3"},{"title":"Step 1 : Visit the AEA website and sign up.​","type":1,"pageTitle":"How to Download the AEA Dataset","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_download_dataset#step-1--visit-the-aea-website-and-sign-up","content":" Scroll down to the bottom of the page. Enter your email and select Access the Datasets.    ","version":"Next","tagName":"h3"},{"title":"Step 2 : Download the download-links file​","type":1,"pageTitle":"How to Download the AEA Dataset","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_download_dataset#step-2--download-the-download-links-file","content":" Once you've selected Access the Datasets you'll be taken back to the top of the AEA page.  Scroll down the page to select AEA Download Links and download the file to the folder $HOME/Downloads.  The download-links file will expire in 14 days You can re-download the download-links whenever they expire  ","version":"Next","tagName":"h3"},{"title":"Step 3 : Set up a folder for AEA data​","type":1,"pageTitle":"How to Download the AEA Dataset","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_download_dataset#step-3--set-up-a-folder-for-aea-data","content":" mkdir -p $HOME/Documents/projectaria_tools_aea_data mv $HOME/Downloads/aria_everyday_activities_dataset_download_urls.json $HOME/Documents/projectaria_tools_aea_data/   ","version":"Next","tagName":"h3"},{"title":"Download the AEA dataset​","type":1,"pageTitle":"How to Download the AEA Dataset","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_download_dataset#download-the-aea-dataset","content":" ","version":"Next","tagName":"h2"},{"title":"Data size​","type":1,"pageTitle":"How to Download the AEA Dataset","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_download_dataset#data-size","content":" The AEA dataset contains 143 sequences and the total size of the dataset is about 353GB. The dataset is split into main VRS data, annotations, and MPS outputs Go to Project Aria Machine Perception Services for more information about MPS data. The MPS data is also broken into chunks that can be included or excluded at download time.  ","version":"Next","tagName":"h3"},{"title":"Download via CLI​","type":1,"pageTitle":"How to Download the AEA Dataset","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_download_dataset#download-via-cli","content":" AEA uses the general Aria dataset downloader, which is available in the projectaria_tools PyPI (pip install) package.  To use the downloader, use the following commands in the virtual environment where you've installed projectaria_tools:  aria_dataset_downloader   To get a description of the arguments that the script needs, run:  aria_dataset_downloader --help   ","version":"Next","tagName":"h3"},{"title":"Download Examples​","type":1,"pageTitle":"How to Download the AEA Dataset","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_download_dataset#download-examples","content":" tip All these commands must be run from your Python virtual environment that has the projectaria-tools package and dependencies installed.  Download all data for a single sequence​  aria_dataset_downloader -c ${PATH_TO_YOUR_CDN_FILE} -o ${OUTPUT_FOLDER_PATH} -l loc5_script4_seq6_rec1   Download VRS only for all sequences​  aria_dataset_downloader -c ${PATH_TO_YOUR_CDN_FILE} -o ${OUTPUT_FOLDER_PATH} -d 0   Download all data for all sequences​  aria_dataset_downloader -c ${PATH_TO_YOUR_CDN_FILE} -o ${OUTPUT_FOLDER_PATH}   ","version":"Next","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"How to Download the AEA Dataset","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_download_dataset#troubleshooting","content":" Go to troubleshooting if you experience issues using this guide. ","version":"Next","tagName":"h2"},{"title":"Getting Started With AEA","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_getting_started","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Getting Started With AEA","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_getting_started#overview","content":" This section provides a step-by-step guide to run the Aria Everyday Activities (AEA) quickstart tutorial in a Jupyter notebook.  This notebook covers how to:  Access raw sensor data (VRS files)Visualize Eye Gaze dataVisualize Speech dataLoad concurrent sequences from multiple Project Aria glasses in a shared space locationUse Timecode to get synchronized data between two devices  ","version":"Next","tagName":"h2"},{"title":"Run Jupyter Notebook on Google Colab​","type":1,"pageTitle":"Getting Started With AEA","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_getting_started#run-jupyter-notebook-on-google-colab","content":" Use the following link to run the Python notebook in an installation free playground:  Aria Everyday Activities (AEA)  ","version":"Next","tagName":"h2"},{"title":"Running the Jupyter Notebook locally​","type":1,"pageTitle":"Getting Started With AEA","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_getting_started#running-the-jupyter-notebook-locally","content":" ","version":"Next","tagName":"h2"},{"title":"Step 0 : Check system requirements and download codebase​","type":1,"pageTitle":"Getting Started With AEA","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_getting_started#step-0--check-system-requirements-and-download-codebase","content":" Ensure your system is supported and then download projectaria_tools codebase from the github  ","version":"Next","tagName":"h3"},{"title":"Step 1 : Install Python​","type":1,"pageTitle":"Getting Started With AEA","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_getting_started#step-1--install-python","content":" If you have already installed projectaria-tools using Python Package Installation, you can skip to Step 4. The AEA Python code is part of the main projectaria-tools package.  Jupyter notebook error If you have problems using Jupyter examples, please upgrade python3 to the latest version.  ","version":"Next","tagName":"h3"},{"title":"Step 2 : Create a virtual environment​","type":1,"pageTitle":"Getting Started With AEA","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_getting_started#step-2--create-a-virtual-environment","content":" Linux &amp; macOSWindows rm -rf $HOME/projectaria_tools_python_env python3 -m venv $HOME/projectaria_tools_python_env source $HOME/projectaria_tools_python_env/bin/activate   ","version":"Next","tagName":"h3"},{"title":"Step 3 : Install projectaria-tools from PyPI​","type":1,"pageTitle":"Getting Started With AEA","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_getting_started#step-3--install-projectaria-tools-from-pypi","content":" python3 -m pip install --upgrade pip python3 -m pip install projectaria-tools'[all]'   ","version":"Next","tagName":"h3"},{"title":"Step 4 : Run Tutorial​","type":1,"pageTitle":"Getting Started With AEA","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_getting_started#step-4--run-tutorial","content":" From your projectaria_tools Python virtual environment, run:  cd $HOME/Documents/projectaria_sandbox jupyter notebook projectaria_tools/projects/AriaEverydayActivities/examples/aea_quickstart_tutorial.ipynb   ","version":"Next","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"Getting Started With AEA","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_getting_started#troubleshooting","content":" Go to Data Utilities Troubleshooting if you experience issues implementing this guide.  ","version":"Next","tagName":"h2"},{"title":"Other useful links​","type":1,"pageTitle":"Getting Started With AEA","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_getting_started#other-useful-links","content":" Use projectaria_tools with CMakeTimestamps in Aria VRS Files ","version":"Next","tagName":"h2"},{"title":"Aria Wearer Data Scripts","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_scripts","content":"","keywords":"","version":"Next"},{"title":"Script 1: Lazy Morning Before the Party​","type":1,"pageTitle":"Aria Wearer Data Scripts","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_scripts#script-1-lazy-morning-before-the-party","content":" Activities: Communication, Media Consumption, Chores and Home Relaxation  Objects: Clothes, Vacuum Cleaner, Game Console, Party Decorations, Cell Phone and Food  Number of Participants: 1  ","version":"Next","tagName":"h2"},{"title":"Scenario description​","type":1,"pageTitle":"Aria Wearer Data Scripts","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_scripts#scenario-description","content":" Guests are coming over for a party later today. Finish up a relaxing morning before getting the house ready for guests.  ","version":"Next","tagName":"h3"},{"title":"Sequence​","type":1,"pageTitle":"Aria Wearer Data Scripts","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_scripts#sequence","content":" Get the blood pumping. Healthy activity/exercise.Watch a TV show.Clean the place, straighten stuff up, vacuum and clean. May or may not be cleaning to music.Put up decorations and make the place look festive.Video game break, play a console video game.Set out food and drink.Read and wait for guests to arrive.  ","version":"Next","tagName":"h3"},{"title":"Script 2: Catch Up and Have Some Fun​","type":1,"pageTitle":"Aria Wearer Data Scripts","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_scripts#script-2-catch-up-and-have-some-fun","content":" Activities: Social Small Group, Home Relaxation, Fun and Games  Objects: Coffee, Coffee Maker, Board Game, Cream and Sugar  Number of Participants: 2 (Host &amp; Guest)  ","version":"Next","tagName":"h2"},{"title":"Scenario description​","type":1,"pageTitle":"Aria Wearer Data Scripts","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_scripts#scenario-description-1","content":" One friend is visiting another at their new apartment. They will catch up, share some food and coffee and play a board game.  ","version":"Next","tagName":"h3"},{"title":"Sequence​","type":1,"pageTitle":"Aria Wearer Data Scripts","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_scripts#sequence-1","content":" Guest arrives at Host’s home - greetings and gift from Guest at the door.Tour of the house.Caffeination. Brew coffee Guest provided as a gift.Grab cream, sugar, and some snacks.Play a board game.Clean up spilled coffee.Share videos. Host and Guest show each other videos on their phones.Time to go. Guest helps Host tidy a little. Host walks guest to the door.  Figure 1: Script 2, Catch Up and Have Some Fun  ","version":"Next","tagName":"h3"},{"title":"Script 3: What Do You Want For Dinner?​","type":1,"pageTitle":"Aria Wearer Data Scripts","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_scripts#script-3--what-do-you-want-for-dinner","content":" Activities Social Small Group, Chores and Home Relaxation  Objects Groceries, Bags, Pots and Food  Number of Participants: 2 (Actor 1 &amp; Actor 2)  ","version":"Next","tagName":"h2"},{"title":"Scenario description​","type":1,"pageTitle":"Aria Wearer Data Scripts","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_scripts#scenario-description-2","content":" Actor 1 is already at home. Actor 2 returns home with groceries needed for an easy meal. They make and eat dinner together and clean up afterwards.  ","version":"Next","tagName":"h3"},{"title":"Sequence​","type":1,"pageTitle":"Aria Wearer Data Scripts","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_scripts#sequence-2","content":" Arriving home. Actor 1 entertains themselves (texting, reading, etc). Actor 2 comes home with groceries.Relax and talk about the day.Cooking. Discussing a plan for what to cook, gathering ingredients, prepare and cook meal.Set the table, serve dinner and eat.Clean up, put leftovers in containers, stack dishwasher.  ","version":"Next","tagName":"h3"},{"title":"Script 4 : Easy Like Sunday Morning​","type":1,"pageTitle":"Aria Wearer Data Scripts","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_scripts#script-4--easy-like-sunday-morning","content":" Activities: Chores and Home Relaxation  Objects: Bed, Coffee Maker, Yoga Mat and Pans  Number of Participants: 1  ","version":"Next","tagName":"h2"},{"title":"Scenario description​","type":1,"pageTitle":"Aria Wearer Data Scripts","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_scripts#scenario-description-3","content":" Getting up and having a lazy Sunday morning (with a little bit of exercise).  ","version":"Next","tagName":"h3"},{"title":"Sequence​","type":1,"pageTitle":"Aria Wearer Data Scripts","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_scripts#sequence-3","content":" Waking up. Get out of bed, brush teeth, stretch in the living room, go to the kitchen.Perk up a bit. Make and pour some coffee.Exercise. Put an exercise video on the TV and do a short workout.Breaking the fast. Take items out of the fridge and freezer and then cook breakfast.Eating. Watch a video on YouTube while eating breakfast.Cleaning up. Put dishes in the sink, brush teeth, do hair.Play console video game.  ","version":"Next","tagName":"h3"},{"title":"Script 5: Get Home, Then Get Going​","type":1,"pageTitle":"Aria Wearer Data Scripts","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_scripts#script-5-get-home-then-get-going","content":" Activities: Chores and Home Relaxation  Objects: Suitcase, Toiletries Bag, Laundry, Cell Phone, Pen and Paper  Number of Participants: 1  ","version":"Next","tagName":"h2"},{"title":"Scenario Description​","type":1,"pageTitle":"Aria Wearer Data Scripts","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_scripts#scenario-description-4","content":" Returning home from a trip. Washing clothes, laying out clothes for a party and planning a shopping list.  ","version":"Next","tagName":"h3"},{"title":"Sequence​","type":1,"pageTitle":"Aria Wearer Data Scripts","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_scripts#sequence-4","content":" Get home. Walk in the front door, lock it, dump bag’s contents on bed.Wash clothes. Go through clothes pile and put them in the wash.Straighten up, put away everything else from the trip.Dry clothes. Put clothes in the dryer.Catch up on email and social media.Get and fold laundry. Take clothes out of the dryer, fold them and put them away or hang them up in cupboard. Put aside clothes for the party.Check the provisions. Go through the kitchen and see what supplies you have. Create a shopping list: 10 minutes ","version":"Next","tagName":"h3"},{"title":"Aria Everyday Activities Visualization","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_visualizers","content":"","keywords":"","version":"Next"},{"title":"Python Visualizer​","type":1,"pageTitle":"Aria Everyday Activities Visualization","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_visualizers#python-visualizer","content":" viewer_projects_aea displays an interactive view of one or two synchronized sequences using Rerun. It also allows you to see all data in a 3D context.  ","version":"Next","tagName":"h2"},{"title":"Visualization of a single sequence:​","type":1,"pageTitle":"Aria Everyday Activities Visualization","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_visualizers#visualization-of-a-single-sequence","content":" viewer_projects_aea --rotate-image --path loc1_script2_seq1_rec1   ","version":"Next","tagName":"h3"},{"title":"Visualization of two time synchronized recordings “locX_scriptY_seqZ_rec(1/2)”​","type":1,"pageTitle":"Aria Everyday Activities Visualization","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_visualizers#visualization-of-two-time-synchronized-recordings-locx_scripty_seqz_rec12","content":" The following command enables you to visualize recordings that were made in the same space and time. Select timecode_ns from the timeline to view the data via synchronized timestamps, and select device_time_ns to see device specific timestamps.  Go to Timestamp Definitions to find out more about Project Aria data timestamps.  viewer_projects_aea --rotate-image --path loc1_script2_seq6_rec1 loc1_script2_seq6_rec2     ","version":"Next","tagName":"h3"},{"title":"Visualization of aligned 3D data in a single location​","type":1,"pageTitle":"Aria Everyday Activities Visualization","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_visualizers#visualization-of-aligned-3d-data-in-a-single-location","content":" The MPS Viewer can be used to visualize all the recordings, across multiple sessions and users that were recorded in the same location.  Go to Visualization of Multi-SLAM data for more details.    ","version":"Next","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"Aria Everyday Activities Visualization","url":"/projectaria_tools/docs/open_datasets/aria_everyday_activities_dataset/aea_visualizers#troubleshooting","content":" Go to Data Utilities Troubleshooting if you experience any issues. ","version":"Next","tagName":"h2"},{"title":"Aria Everyday Objects Dataset","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_everyday_objects","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Aria Everyday Objects Dataset","url":"/projectaria_tools/docs/open_datasets/aria_everyday_objects#overview","content":" Aria Everyday Objects (AEO) is a small, challenging 3D object detection dataset for egocentric data. AEO consists of approximately 45 minutes of egocentric data across 25 sequences captured by non-computer vision experts collected in a diverse set of locations throughout the US. Oriented 3D bounding boxes have been annotated for each sequence. Annotation is done in 3D, using the camera calibration, SLAM trajectory and SLAM semi-dense point cloud to assist with annotation. 1037 3D object bounding box instances across 17 classes: Bed, Chair, Couch, Door, Floor, Lamp, Mirror, Plant, Refrigerator, Screen, Sink, Storage, Table, Wall, WallArt, WasherDryer, and Window. The dataset is designed to accelerate research in egocentric 3D perception.  For more details, please visit the project page, view the dataset online, explore the github repository and read the ECCV 2024 paper.  ","version":"Next","tagName":"h2"},{"title":"What is in the dataset?​","type":1,"pageTitle":"Aria Everyday Objects Dataset","url":"/projectaria_tools/docs/open_datasets/aria_everyday_objects#what-is-in-the-dataset","content":" Aria RGB video at 10 HzAria SLAM x 2 video at 10 HzAria IMU x 2 dataHigh-quality static 3D bounding boxes of 17 object classes  ","version":"Next","tagName":"h2"},{"title":"How to download the dataset?​","type":1,"pageTitle":"Aria Everyday Objects Dataset","url":"/projectaria_tools/docs/open_datasets/aria_everyday_objects#how-to-download-the-dataset","content":" Visit the AEO webpage here. Scroll down, enter your email, then click the button “Access the Dataset”. Follow the instructions there.  ","version":"Next","tagName":"h2"},{"title":"BibTeX Citation​","type":1,"pageTitle":"Aria Everyday Objects Dataset","url":"/projectaria_tools/docs/open_datasets/aria_everyday_objects#bibtex-citation","content":" @article{straub24efm, title={EFM3D: A Benchmark for Measuring Progress Towards 3D Egocentric Foundation Models}, author={Julian Straub and Daniel DeTone and Tianwei Shen and Nan Yang and Chris Sweeney and Richard Newcombe}, booktitle={arXiv preprint arXiv:2406.10224}, year={2024}, url={https://arxiv.org/abs/2406.10224}, }   ","version":"Next","tagName":"h2"},{"title":"License​","type":1,"pageTitle":"Aria Everyday Objects Dataset","url":"/projectaria_tools/docs/open_datasets/aria_everyday_objects#license","content":" AEO dataset is released under a non-commercial license described here.  ","version":"Next","tagName":"h2"},{"title":"Contributors​","type":1,"pageTitle":"Aria Everyday Objects Dataset","url":"/projectaria_tools/docs/open_datasets/aria_everyday_objects#contributors","content":" Julian Straub, Daniel DeTone, Tianwei Shen, Nan Yang, Chris Sweeney, Richard Newcombe ","version":"Next","tagName":"h2"},{"title":"Aria Synthetic Environments Dataset","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Aria Synthetic Environments Dataset","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset#overview","content":" Project Aria Tools provides Python and C++ APIs to access the Aria Synthetic Environments (ASE) dataset.  ","version":"Next","tagName":"h2"},{"title":"About the data​","type":1,"pageTitle":"Aria Synthetic Environments Dataset","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset#about-the-data","content":" Aria Synthetic Environments (ASE) is a large scale dataset of 100K unique procedurally-generated scenes of interior layouts of apartments filled with 3D objects, and simulated with the sensor characteristics of Aria glasses. For each scene we have the rendering of a person walking around the synthetically generated rooms of the layout. These rooms vary from living rooms, bedrooms &amp; kitchens to bathrooms. In addition to the renders, each of these scenes come with semi-dense maps for the Aria walkthrough, which are aligned to the Ground Truth (GT) scene layout.  This dataset was created to provide the wider research community with a dataset large enough to surface new challenges and research opportunities for first person object detection and tracking that had not been feasible.  In the ASE: Scene Reconstruction Challenge, we invite researchers to train full scene structured language description models, drawing from the 100K annotated scenes, and then test their models on 1K test scenes provided in the challenge.  ","version":"Next","tagName":"h2"},{"title":"Dataset Contents​","type":1,"pageTitle":"Aria Synthetic Environments Dataset","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset#dataset-contents","content":" 100,000 unique multi-room interior scenesSimulated with realistic device trajectoriesAcross ~2-minute trajectoriesPopulated with ~8000 3D objectsWith semi-dense map representationsAnnotated using ASE Scene Language User oriented natural language mapping with architectural features, such as doors, windows and pillars, described with a CAD-like language that includes the feature type, location and dimensionsUnlocks new exciting ways to tackle research challenges related to reconstruction and detection tasks  Simulated sensor data per sequence  1 x outward-facing RGB camera streamSimulated Aria camera &amp; lens characteristics  Ground Truth Annotations  6DoF camera trajectory3D floor plan in Euston Structure Scene Language (SSL) format2D instance segmentation With per-scene mappings from the object instance image IDs to object classes 2D depth map  ","version":"Next","tagName":"h2"},{"title":"Dataset Statistics​","type":1,"pageTitle":"Aria Synthetic Environments Dataset","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset#dataset-statistics","content":" Number of scenes: 100KNumber of images: 58M+Trajectories Total time: 67 daysTotal distance: London -&gt; San Francisco(7800 km) Rooms: Up to 5 complex Manhattan rooms All surfaces in the world are aligned with three dominant directions, typically corresponding to the X, Y, and Z axes Dataset size: ~23TB  ","version":"Next","tagName":"h2"},{"title":"Documentation​","type":1,"pageTitle":"Aria Synthetic Environments Dataset","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset#documentation","content":" The ASE section of this wiki covers:  Getting started A quickstart tutorial for installing the necessary tooling, download the dataset and visualize the data Dataset download A walkthrough using aria_synthetic_environments_downloader to download the ASE dataset. Data Format Aria Synthetic Environments (ASE) data formats and organization Data Tools and Visualization Data helper toolsVisualization notebook ASE Scene Reconstruction 2023-4 Challenge ","version":"Next","tagName":"h3"},{"title":"ASE Scene Reconstruction 2023-4 Challenge","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_challenges","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"ASE Scene Reconstruction 2023-4 Challenge","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_challenges#overview","content":" The 2023 Aria Synthetic Environments (ASE) challenge aims to accelerate research into scene reconstruction, serving as a catalyst for the broader research community.  ","version":"Next","tagName":"h2"},{"title":"Task​","type":1,"pageTitle":"ASE Scene Reconstruction 2023-4 Challenge","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_challenges#task","content":" Given the ASE training data sequences with ground truth structured language, your task is to generate a full scene structured language description of the main elements of the scene, consisting of walls, doors and windows using the unseen 1K test scenes provided.  ","version":"Next","tagName":"h2"},{"title":"Prize​","type":1,"pageTitle":"ASE Scene Reconstruction 2023-4 Challenge","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_challenges#prize","content":" First place: $10,000 and a certificate (per each challenge). Second and Third places will receive certificates and recognition on the Challenge leaderboard. Additionally, teams with the best performing or noteworthy submissions may be invited to present their work at the CVPR conference in June 2024.  ","version":"Next","tagName":"h2"},{"title":"Important dates​","type":1,"pageTitle":"ASE Scene Reconstruction 2023-4 Challenge","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_challenges#important-dates","content":" Submission deadline for results: March 22, 2024 (11:59PM PST)Presentation of awards: CVPR in June 2024 (pending confirmation of our workshop)  ","version":"Next","tagName":"h2"},{"title":"Key participation terms​","type":1,"pageTitle":"ASE Scene Reconstruction 2023-4 Challenge","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_challenges#key-participation-terms","content":" Entrants can register as an Individual or TeamPredictions are submitted online at Eval.aiWinners agree to share a public report (e.g. whitepaper or publication) describing the method used for generating the PredictionsParticipants are encouraged (but not required) to make source code used to generate the Predictions available under an Approved OSS License.  Full terms can be found on ProjectAria.com/challenges/terms.  ","version":"Next","tagName":"h2"},{"title":"How to participate​","type":1,"pageTitle":"ASE Scene Reconstruction 2023-4 Challenge","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_challenges#how-to-participate","content":" Follow the detailed instructions at Eval.ai. ","version":"Next","tagName":"h2"},{"title":"ASE Data Format","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_data_format","content":"","keywords":"","version":"Next"},{"title":"Overall Data Organization​","type":1,"pageTitle":"ASE Data Format","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_data_format#overall-data-organization","content":" Each scene has its own subdirectory with a unique ID (0-100K)Each scene directory contains separate files and directories for each type of data  &lt;sceneID&gt; ├── rgb │ └── vignette0000000.jpg │ └── vignette0000001.jpg │ ... │ └── vignette0xxn.jpg ├── depth │ └── depth0000000.jpg │ └── depth0000001.jpg │ ... │ └── depth0xxn.jpg ├── instances │ └── instance0000000.jpg │ └── instance0000001.jpg │ ... │ └── instance0xxn.jpg ├── ase_scene_language.txt ├── trajectory.txt ├── semidense_points.csv.gz ├── semidense_observations.csv.gz └── object_instances_to_classes.json   rgb - 2D RGB fisheye images Synthetically generated Aria RGB images at 10 FPSEach image is saved in JPEG format depth - 2D depth maps (16 bit) Each depth image is the same size as the corresponding synthetic RGB image, where the pixel contents are integers expressing the depth along the pixel’s ray direction, in units of mm. This should not be confused with ADT depth images, which describe the depth in the camera’s Z-axis Each image is saved in PNG format instances - 2D segmentation maps (16 bit) Each segmentation image is the same size as the corresponding synthetic RGB image, where the pixel contents are integers expressing the object Id that was observed by the pixelEach image is saved as PNG format ase_scene_language.txt - 3D floor plan definition Describes the scene in the form of a language.Each row is a command which includes its own set of parameters. A set of such commands describe the geomtery of the scene specified.Go to ASE scene language format below for more details trajectory.txt - Ground-truth trajectory Go to MPS Output - Trajectory for how the data is structured While the file structure is the same, please note, this is the ground truth trajectory, not an output generated by MPS semidense_points.csv.gz - Semi-dense map points Go to MPS Output - Semi-Dense Point Cloud for how the data is structuredProduced by MPS run on synthetic SLAM (mono scene) camera data semidense_observations.csv.gz - Semi-dense map observations Go to MPS Output - Semi-Dense Point Cloud for how the data is structuredProduced by MPS run on synthetic SLAM (mono scene) camera data object_instances_to_classes.json Per-scene mappings from the object instance image IDs to object classesGiven an instance image pixel value/object ID, one will then be able to look up the class from this mapping How to convert them to point clouds based on depth images and RGB images  ","version":"Next","tagName":"h2"},{"title":"Aria RGB Sensor - Image, Depth and Instance Segmentation​","type":1,"pageTitle":"ASE Data Format","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_data_format#aria-rgb-sensor---image-depth-and-instance-segmentation","content":" For each frame from the RGB sensor we provide:  A vignetted sensor imageSimulated 16 bit metric depth (mm) in PNG image formatA segmentation image (16 bit PNG)  The images in each folder are in sync. This means there will be same number of images in each folder. We also provide example data visualizers to load these images and/or associate them.    ","version":"Next","tagName":"h2"},{"title":"ASE Scene Language Format​","type":1,"pageTitle":"ASE Data Format","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_data_format#ase-scene-language-format","content":" The ASE Scene Language format is set of hand-designed procedural commands in pure text form. To handle commonly encountered static indoor layout elements, we use three commands:  make_wall - the full set of parameters specifies a gravity-aligned oriented boxmake_door - specify box-based cutouts from wallsmake_window - specify box-based cutouts from wall  Each command includes its own set of parameters, as described below. Given the command’s full set of parameters, a geometry is completely specified.  A single scene is described via a sequence of multiple commands stored in ase_scene_language.txt. The sequence length is arbitrary and follows no specific ordering. The interpretation of the command and its arguments is carried out by a customized interpreter responsible for parsing the sequence and generating a 3D mesh of the scene.    ","version":"Next","tagName":"h2"},{"title":"Trajectory and Semi-Dense Map Points​","type":1,"pageTitle":"ASE Data Format","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_data_format#trajectory-and-semi-dense-map-points","content":" Ground-truth trajectory data provides poses for each frame generated from a simulation at 10 FPS. We are follow the same trajectory format as the closed loop trajectory used by Machine Perception Services (MPS).  For semi-dense map point clouds and their observations, we follow the same point cloud points and observations format as MPS. The semi-dense map point cloud is generated using same algorithm as MPS, with the addition of ground-truth trajectory and simulated SLAM camera images. ","version":"Next","tagName":"h2"},{"title":"Synthetic Environments Data Tools and Visualization","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_data_tools","content":"","keywords":"","version":"Next"},{"title":"Data Helper Tools​","type":1,"pageTitle":"Synthetic Environments Data Tools and Visualization","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_data_tools#data-helper-tools","content":" These helper functions are broadly categorized into the following types:  Data interpreter: interpreter.py Provides an interpreter for the ASE Scene Language to convert them into a 3D model in the form of bounding boxes Data readers: readers.py Provide readers for the: ASE Scene LanguageGround-truth trajectorySemi-dense Map points Data Plotters: plotters.py Provide simple plotting functions for the: 3D scene from ASE Scene LanguageGround-truth trajectorySemi Dense Map points  ","version":"Next","tagName":"h2"},{"title":"Visualization​","type":1,"pageTitle":"Synthetic Environments Data Tools and Visualization","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_data_tools#visualization","content":" ","version":"Next","tagName":"h2"},{"title":"Python sample​","type":1,"pageTitle":"Synthetic Environments Data Tools and Visualization","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_data_tools#python-sample","content":" Viewer_projects_ase displays an interactive view of an ASE sequence with Rerun. It allows you to see all data in 3D context.  You can call as the following (frame_id being optional):  viewer_projects_ase --dataset_path ase_data/7 --frame_id 745     tip The timeline enables you to switch before device_time and frame_id to know to which image frame_id a camera pose corresponds toThe 3D world view is clickable, you can quickly select an object and see its instance name  ","version":"Next","tagName":"h2"},{"title":"Python Notebook​","type":1,"pageTitle":"Synthetic Environments Data Tools and Visualization","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_data_tools#python-notebook","content":" We also provide Jupyter notebooks to visualize the data for each sequence. To get started download ASE data following steps from Dataset Download  cd /path_to/projectaria_tools jupyter notebook projects/AriaSyntheticEnvironment/tutorial/ase_tutorial_notebook.ipynb   ","version":"Next","tagName":"h2"},{"title":"Part 1: 3D visualization of the scene​","type":1,"pageTitle":"Synthetic Environments Data Tools and Visualization","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_data_tools#part-1-3d-visualization-of-the-scene","content":" This section will introduce the dataset’s 3D components as well as code snippets to help users get familiar with them.  You will be taken through examples of how to load the 3D dataset annotations namely: the ground-truth trajectory, the ASE Scene Language, and the Semi-dense Map point cloud. In addition, we provide examples of how they can each be plotted.  At the end of the section you should see 3D plots containing:  The Semi-dense Map point cloud,The layout annotations, visualized as 3D box wireframes,The trajectory plotted as a dotted line in 3D.  Example scene visualization:  ","version":"Next","tagName":"h3"},{"title":"Part 2: Loading and Plotting Images and Image Annotations​","type":1,"pageTitle":"Synthetic Environments Data Tools and Visualization","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_data_tools#part-2-loading-and-plotting-images-and-image-annotations","content":" Since the file structure and format are straightforward, the code consists of very simple PIL and matplotlib code to show the 3 images (RGB, depth and instance maps) side-by-side:  ","version":"Next","tagName":"h3"},{"title":"Part 3: Projecting Points into Images​","type":1,"pageTitle":"Synthetic Environments Data Tools and Visualization","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_data_tools#part-3-projecting-points-into-images","content":" Running the final part of the notebook will load the camera calibration, as well as the pointcloud, trajectory and select a random frame. Then given the device pose from the trajectory, we project the points into the frame.  Points that project outside of the valid radius, should not be plotted   ","version":"Next","tagName":"h3"},{"title":"Download the ASE Dataset","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_download_dataset","content":"","keywords":"","version":"Next"},{"title":"Download via CLI​","type":1,"pageTitle":"Download the ASE Dataset","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_download_dataset#download-via-cli","content":" Follow the ASE quickstart guide to get the system ready to download data/example data. This section will introduce how to download the dataset using the aria_synthetic_environments_downloader Python script.  ","version":"Next","tagName":"h2"},{"title":"Detailed arguments​","type":1,"pageTitle":"Download the ASE Dataset","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_download_dataset#detailed-arguments","content":" Arguments\tType\tDescription--cdn_file\tstr\tThe download-urls file you downloaded from the ASE website page after signing up --output-dir\tstr\tA local path where the downloaded files will be stored --set\tstr\tDownload either train / test data. All the 100K data is for training data and comes with GT ASE language. At a later point we will add test data without GT ASE language, which will be used for evaluation --scene-ids\tstr\tRange of scene ids to download --unzip\tbool\tAllows the user to unzip in the output directory or keep it as a zip  ","version":"Next","tagName":"h3"},{"title":"Examples​","type":1,"pageTitle":"Download the ASE Dataset","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_download_dataset#examples","content":" ","version":"Next","tagName":"h2"},{"title":"Download ASE datasets​","type":1,"pageTitle":"Download the ASE Dataset","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_download_dataset#download-ase-datasets","content":" cd $HOME/Documents/projectaria_sandbox/projectaria_tools   Download first 10 scenes​  python3 projects/AriaSyntheticEnvironment/aria_synthetic_environments_downloader.py --set train --scene-ids 0-9 --cdn-file aria_synthetic_environments_dataset_download_urls.json --output-dir $HOME/projectaria_tools_ase_data --unzip True   Download a large number of scenes: This downloads all 100 scenes (10 chunks)​  python3 projects/AriaSyntheticEnvironment/aria_synthetic_environments_downloader.py --set train --scene-ids 0-99 --cdn-file aria_synthetic_environments_dataset_download_urls.json --output-dir $HOME/projectaria_tools_ase_data --unzip True   Download specific scenes: 560-569​  python3 projects/AriaSyntheticEnvironment/aria_synthetic_environments_downloader.py --set train --scene-ids 560-569 --cdn-file aria_synthetic_environments_dataset_download_urls.json --output-dir $HOME/projectaria_tools_ase_data --unzip True  ","version":"Next","tagName":"h3"},{"title":"Getting Started With the Synthetic Environments Dataset","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_getting_started","content":"","keywords":"","version":"Next"},{"title":"Quickstart Tutorial - Python​","type":1,"pageTitle":"Getting Started With the Synthetic Environments Dataset","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_getting_started#quickstart-tutorial---python","content":" ","version":"Next","tagName":"h2"},{"title":"Step 0 : Check system requirements and download codebase​","type":1,"pageTitle":"Getting Started With the Synthetic Environments Dataset","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_getting_started#step-0--check-system-requirements-and-download-codebase","content":" Ensure your system is supported and then download projectaria_tools codebase from the github  ","version":"Next","tagName":"h3"},{"title":"Step 1 : Install Python​","type":1,"pageTitle":"Getting Started With the Synthetic Environments Dataset","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_getting_started#step-1--install-python","content":" Ensure python3 is installed on the system (check with python3 --version)  ","version":"Next","tagName":"h3"},{"title":"Step 2 : Create a virtual environment​","type":1,"pageTitle":"Getting Started With the Synthetic Environments Dataset","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_getting_started#step-2--create-a-virtual-environment","content":" rm -rf $HOME/projectaria_tools_python_env python3 -m venv $HOME/projectaria_tools_python_env source $HOME/projectaria_tools_python_env/bin/activate   ","version":"Next","tagName":"h3"},{"title":"Step 3 : Install projectaria_tools from PyPI​","type":1,"pageTitle":"Getting Started With the Synthetic Environments Dataset","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_getting_started#step-3--install-projectaria_tools-from-pypi","content":" python3 -m pip install --upgrade pip python3 -m pip install projectaria-tools'[all]'   The ASE Python tooling for projection of 3D points to RGB images is included in the projectaria_tools package, so no further steps are needed. The following packages used in this tutorial are standard Python packages that are also included in project_aria_tools build:  plotlynumpyscipypandasmatplotlibrequeststqdmjupyter  ","version":"Next","tagName":"h3"},{"title":"Step 4 : Download sample data​","type":1,"pageTitle":"Getting Started With the Synthetic Environments Dataset","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_getting_started#step-4--download-sample-data","content":" Navigate to the ASE page on the Project Aria Website and follow the instructions to download the download-urls file. This same download-urls file can be used for any dataset download until the link expiresSetup ASE local folder and move download-urls file:  mkdir -p $HOME/Documents/projectaria_sandbox/projectaria_tools_ase_data mv $HOME/Downloads/aria_synthetic_environments_dataset_download_urls.json $HOME/Documents/projectaria_sandbox/projectaria_tools_ase_data/   Download sample dataset using the download tool:  cd $HOME/Documents/projectaria_sandbox/projectaria_tools python3 projects/AriaSyntheticEnvironment/aria_synthetic_environments_downloader.py --set train --scene-ids 0-10 --cdn-file $HOME/Documents/projectaria_sandbox/projectaria_tools_ase_data/aria_synthetic_environments_dataset_download_urls.json --output-dir $HOME/Documents/projectaria_sandbox/projectaria_tools_ase_data --unzip True   ","version":"Next","tagName":"h3"},{"title":"Step 5 : Run the visualization notebooks​","type":1,"pageTitle":"Getting Started With the Synthetic Environments Dataset","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_getting_started#step-5--run-the-visualization-notebooks","content":" jupyter notebook projects/AriaSyntheticEnvironment/tutorial/ase_tutorial_notebook.ipynb   ","version":"Next","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"Getting Started With the Synthetic Environments Dataset","url":"/projectaria_tools/docs/open_datasets/aria_synthetic_environments_dataset/ase_getting_started#troubleshooting","content":" Go to Data Utilities Troubleshooting if you experience issues implementing this guide. ","version":"Next","tagName":"h2"},{"title":"Digital Twin Catalog Dataset","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog","content":"","keywords":"","version":"Next"},{"title":"About the data​","type":1,"pageTitle":"Digital Twin Catalog Dataset","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog#about-the-data","content":" The Digital Twin Catalog (DTC) dataset provides static objects capturing sequences collected using Project Aria glasses and digital single-lens reflex (DSLR) cameras, combined with high quality ground truth data including device trajectories, reconstructed objects’ digital twin copies with geometry and material assets, aligned object poses. We also provide processed sensor data from our Machine Perception Services. Go to DTC Data Format to see a full list of the data we provide.  The DTC dataset contains 2000 accurately scanned and reconstructed 3D models with detailed geometry and material information (in .glb), 200 Aria sequences and 100 DSLR sequences recording static objects whose digital twin copies are within the 2000 released models. In the Aria sequences, there are 100 single-instance static objects each recorded under two different human walking trajectories, active and passive (casual). In the DSLR sequences, there are 50 single-instance static objects each recorded under two different lighting conditions.  Go to the Dataset Download page to get started with the sequence data. For more info on downloading and viewing captured models, see Object Models page.  ","version":"Next","tagName":"h2"},{"title":"Active Trajectory​","type":1,"pageTitle":"Digital Twin Catalog Dataset","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog#active-trajectory","content":" 100 Aria sequences were recorded by taking a 360 walk around the captured objects. Each sequence will record one single object. The Aria glasses will stay within a relatively close distance to the object.  ","version":"Next","tagName":"h3"},{"title":"Passive (Casual) Trajectory​","type":1,"pageTitle":"Digital Twin Catalog Dataset","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog#passive-casual-trajectory","content":" 100 Aria sequences were recorded by taking a casual walk around the captured objects. Each sequence will record one single object. The sequence will include trajectories such as walking towards the object, circling around the object and walking away from the object.  ","version":"Next","tagName":"h3"},{"title":"DSLR Sequence​","type":1,"pageTitle":"Digital Twin Catalog Dataset","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog#dslr-sequence","content":" 105 DSLR sequences were recorded under 2 different lighting conditions. The sequences were recorded using three DSLR cameras mounted on robot arms with three different viewing directions towards the object. We also provide 2 corresponding lighting environment maps for the DSLR sequences.  ","version":"Next","tagName":"h3"},{"title":"Documentation​","type":1,"pageTitle":"Digital Twin Catalog Dataset","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog#documentation","content":" The DTC section of the wiki covers:  Getting Started A quick start tutorial available to install Project Aria Tools Python package to run locally. Dataset Download A walkthrough of using aria_dataset_downloader to download the DTC sequence dataset. Object Models Everything you need to know about the released DTC object models including: a walkthrough of using dtc_object_downloader to download the DTC object models, and brief description of data content and how to visualize the models Data Format How DTC data is organized and stored Tooling Run our tools using an example that access DTC data in Python. ","version":"Next","tagName":"h3"},{"title":"Aria Dataset Explorer","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/dataset_explorer","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Aria Dataset Explorer","url":"/projectaria_tools/docs/open_datasets/dataset_explorer#overview","content":" The Aria Dataset Explorer is a centralized hub for Project Aria public datasets that helps users efficiently find, preview and download Aria data. For this initial launch the Explorer supports browsing and downloading the Aria Digital Twin (ADT) dataset and provides a preview of Nymeria.  Through the Explorer, researchers can:  Better understand the contents of research datasets captured using Project Aria glassesPreview the data using Project Aria Tools visualizationsPreview the raw and MPS dataFilter and download a subset of the data  ","version":"Next","tagName":"h2"},{"title":"Using Aria Dataset Explorer​","type":1,"pageTitle":"Aria Dataset Explorer","url":"/projectaria_tools/docs/open_datasets/dataset_explorer#using-aria-dataset-explorer","content":" ","version":"Next","tagName":"h2"},{"title":"Primary Explorer View​","type":1,"pageTitle":"Aria Dataset Explorer","url":"/projectaria_tools/docs/open_datasets/dataset_explorer#primary-explorer-view","content":" Select which dataset you’d like to search from the dropdown next to the filter field.  To change what metadata is displayed, you can add metadata properties to the filter.    If you select Download found sequences you will download a JSON file that contains the download links for all the recordings. The first time you try to download data for a specific dataset, you’ll need to provide your email address. That will give you a JSON file with download links that last for thirty days.  JSON file with download information​  The JSON file you download contains download urls for each sequence provided in the filter view. Each type of data in the sequence has its own download url. The downloader script uses the pip install version of Project Aria Tools.  How to download ADT dataInstructions for downloading the Nymeria dataset coming soon  The sha1sum value listed for each output is the SHA-1 hash string of the contents of the file. It can be used as a checksum to ensure the integrity of the downloaded data.  ","version":"Next","tagName":"h3"},{"title":"Sequence View​","type":1,"pageTitle":"Aria Dataset Explorer","url":"/projectaria_tools/docs/open_datasets/dataset_explorer#sequence-view","content":" Select any recording from the primary Explorer view to open a preview of that sequence.  On the sequence view, you can preview visualizations available for that dataset using the buttons to the right of the visualization.  The preview page also contains a list of all metadata stored with the sequence, and provides the ability to download specific components from that sequence (e.g. main_vrs, depth, segmentation etc). ","version":"Next","tagName":"h3"},{"title":"Aria Dataset Explorer Filters","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/dataset_explorer/dataset_explorer_filters","content":"","keywords":"","version":"Next"},{"title":"Universal filters​","type":1,"pageTitle":"Aria Dataset Explorer Filters","url":"/projectaria_tools/docs/open_datasets/dataset_explorer/dataset_explorer_filters#universal-filters","content":" Name\tType\tDescriptionvideo_uid\tString\tUID of sequence device_serial\tString\tSerial number of the Project Aria glasses used to make the recording, eg 1WM103600M1292 duration_s\tfloat\tNumber of seconds a recording took, eg 122.47 sensor.slam_camera_fps\tint\tNumber of frames per second recorded by the monoscene/SLAM cameras, eg 30 sensor.rgb_camera_fps\tint\tNumber of frames per second recorded by the RGB camera, eg 30 sensor.et_camera_fps\tint\tNumber of frames per second recorded by the Eye Tracking cameras sensor.rgb_image_height\tint\tHow many pixels high the image is, eg 1408 sensor.rgb_image_width\tint\tHow many pixels wide the image is, eg 1408 light_intensity_lux_median\tfloat\tMedian level of illumination in a recording, measured in lux, eg 107.26 computed.trajectory_length_m\tfloat\tNumber of meters traversed in the recording, derived from MPS Closed Loop Trajectory data, eg 63.41. This is calculated by summing the total distance in between each image frame. computed.covered_area_m2\tfloat\tArea traversed in the recording in meters squared, derived from MPS Closed Loop Trajectory data, eg 28.20. This is calculated by computing a bounding box in the xy plane defined by the trajectory, and taking its area. computed.covered_volume_m3\tfloat\tVolume traversed in the recording in meters cubed, derived from MPS Closed Loop Trajectory data, eg 17.12. This is calculated by computing a 3D bounding box in the xyz plane defined by the trajectory, and taking its volume. computed.speed_mps_mean\tfloat\tAverage speed in the recording in meters per second, calculated by dividing the trajectory length by the duration of the recording, eg 0.5177  ","version":"Next","tagName":"h2"},{"title":"Dataset Specific Filters​","type":1,"pageTitle":"Aria Dataset Explorer Filters","url":"/projectaria_tools/docs/open_datasets/dataset_explorer/dataset_explorer_filters#dataset-specific-filters","content":" Some datasets have additional metadata that can be used as filters, such as activities or objects interacted with in the Aria Digital Twin (ADT) dataset.  ","version":"Next","tagName":"h2"},{"title":"ADT Specific Filters​","type":1,"pageTitle":"Aria Dataset Explorer Filters","url":"/projectaria_tools/docs/open_datasets/dataset_explorer/dataset_explorer_filters#adt-specific-filters","content":" Name\tType\tDescriptionScene\tString\tScene name for the sequence (either Apartment or Office) is_multi_person\tBool\tTrue if the sequence was recording concurrently with another sequence num_skeletons\tInt\tNumber of skeletons tracked in the sequence gt_creation_time\tString\tDate and timestamp of when the data was collected activity\tString\tActivity name associated with this sequence. Go to ADT Documentation for a list of activities. visible_object_names\tString\tObjects that were visible in the sequence. An object is considered visible if the visibility ratio is larger than 0 for at least one frame visible_object_categories\tString\tCategories of objects that were visible in the sequence. This filters by categories object included in the visible_object_names list object_names_interacted_with\tString\tObjects that have been moved during the sequence. An object is considered to have been moved if the total motion is greater than 0.5m object_categories_interacted_with\tString\tCategories of objects that have been moved during the sequence. This filters by categories of object that were included in the object_names_interacted_with list  Go to ADT Documentation for more details.  ","version":"Next","tagName":"h3"},{"title":"Nymeria Specific Filters​","type":1,"pageTitle":"Aria Dataset Explorer Filters","url":"/projectaria_tools/docs/open_datasets/dataset_explorer/dataset_explorer_filters#nymeria-specific-filters","content":" Name\tType\tDescriptiondate\tString\tDate the data was captured participant_id\tString\tID of the participant act_id\tString\tThe index of recording sequences per participant location\tString\tLocation where the data was collected script\tString\tRecording script used action_duration_sec\tfloat\tActivity duration in second has_two_participants\tbool\tTrue, if the sequence has two participants pt2\tString\tFor two-participant sequences, this field records the UID of the other associated sequence. For single-participant sequences, this field is empty. body_motion\tbool\tTrue, if the sequence includes ground-truth full-body motion [head/left_wrist/right_wrist/observer]_data\tbool\tEach individual flag is true, if the sequence includes the raw data from the participant’s Project Aria glasses, participant’s left miniAria wristband, participant’s right miniAria wristband, and observer’s Project Aria glasses, respectively. [head/left_wrist/right_wrist/observer]_slam\tbool\tEach individual flag is true, if the sequence includes the MPS location output for the participant’s Project Aria glasses, participant’s left miniAria wristband, participant’s right miniAria wristband, and observer’s Project Aria glasses, respectively. [head/left_wrist/right_wrist/observer]_trajectory_m\tfloat\tEach float value is the corresponding device moving trajectory length in meters for the participant’s Project Aria glasses, participant’s left miniAria wristband, participant’s right miniAria wristband, and observer’s Project Aria glasses, respectively. [head/left_wrist/right_wrist/observer]_duration_sec\tfloat\tEach float value is the device recording time in seconds for the participant’s Project Aria glasses, participant’s left miniAria wristband, participant’s right miniAria wristband, and observer’s Project Aria glasses, respectively. [head/observer]_general_gaze\tbool\tTrue, if the sequence includes the generalized eye gaze estimation for the participant and the observer, respectively. [head/observer]_personalized_gaze\tbool\tTrue, if the sequence includes the personalized eye gaze estimation for the participant and the observer, respectively. timesync\tbool\tTrue, if all devices are synchronized. motion_narration\tbool\tTrue, if the sequence is annotated with body motion narration. atomic_action\tbool\tTrue, if the sequence is annotated with atomic action. activity_summarization\tbool\tTrue, if the sequence is annotated with the activity summarization. participant_gender\tString\tAssigned gender at birth reported by participants. participant_height_cm\tfloat\tHeight of the participant in centimeter as measured and used in mocap calibration participant_weight_kg\tfloat\tWeight of participants in Kg as self-reported values participant_bmi\tfloat\tBMI of participants according to CDC formula participant_age_group\tString\tAge group of participants. Possible values are 18-24, 25-30, 36-40, 41-45, 46-50 participant_ethnicity\tString\tEthnicity of participants. Possible values are caucasian, hispanic, african american, east asian, south asian, southeast asian, and other/mixed participant_xsens_suit_size\tString\tXSens suit size worn by participants. Possible values are M, L, XL, 2XL, 4XL ","version":"Next","tagName":"h3"},{"title":"Dataset Download","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/dataset_download","content":"","keywords":"","version":"Next"},{"title":"Downloading Open Datasets​","type":1,"pageTitle":"Dataset Download","url":"/projectaria_tools/docs/open_datasets/dataset_download#downloading-open-datasets","content":" This page provides guidance on how to use our generic downloader to download any Project Aria dataset.  ","version":"Next","tagName":"h2"},{"title":"Step 0: Install projectaria_tools package and create venv if not done before​​","type":1,"pageTitle":"Dataset Download","url":"/projectaria_tools/docs/open_datasets/dataset_download#step-0-install-projectaria_tools-package-and-create-venv-if-not-done-before","content":" Follow Step 0 to Step 3 in Getting Started.  ","version":"Next","tagName":"h3"},{"title":"Step 1 : Signup and get the download links file​​","type":1,"pageTitle":"Dataset Download","url":"/projectaria_tools/docs/open_datasets/dataset_download#step-1--signup-and-get-the-download-links-file","content":" To use the downloader CLI, you need to download a file which contains all data URLs plus some metadata for the downloader script. We currently offer two ways of getting this file:  Option 1 - Aria Dataset Explorer:​  Go to the Aria Dataset Explorer website and choose a dataset by clicking on &quot;View&quot; button. Here you can select sequences according to some filter options, or get the links to all sequences by clicking &quot;Download found sequences&quot;.    Option 2 - Dataset Webpage on projectaria.com:​  For example, to download Aria Digital Twin, visit the ADT Webpage on projectaria.com. Scroll down to the bottom of the page. Enter your email and select Access the Datasets. Visit and sign up.    Once you've selected Access the Datasets you'll be taken back to the top of the page.  Scroll down the page to select &lt;your_dataset&gt; Download Links and download the file to the folder $HOME/Downloads.    The download-links file will expire in 14 days You can redownload the download links whenever they expire  This will download a file called [your_dataset_name]_download_urls.json.  ","version":"Next","tagName":"h3"},{"title":"Step 2 : Set up a folder for your dataset​","type":1,"pageTitle":"Dataset Download","url":"/projectaria_tools/docs/open_datasets/dataset_download#step-2--set-up-a-folder-for-your-dataset","content":" mkdir -p $HOME/Documents/projectaria_tools_[your_dataset_name]_data mv $HOME/Downloads/[your_dataset_name]_download_urls.json $HOME/Documents/projectaria_tools_[your_dataset_name]_data/   ","version":"Next","tagName":"h3"},{"title":"Step 3 : Download the sequences using the downloader CLI​","type":1,"pageTitle":"Dataset Download","url":"/projectaria_tools/docs/open_datasets/dataset_download#step-3--download-the-sequences-using-the-downloader-cli","content":" To use the downloader, use the following commands in the virtual environment where you've installed projectaria_tools.  To get a description of the arguments that the script needs, run:  aria_dataset_downloader --help   The following are some example use cases:  Download VRS for all sequences​  To download VRS files for all sequences, run the following command:  aria_dataset_downloader --cdn_file ${PATH_TO_YOUR_CDN_FILE} --output_folder ${OUTPUT_FOLDER_PATH} --data_types 0   This will download VRS files for all sequences in the CDN file to the specified output folder.  Get a list of available data groups​  To get a list of the data groups available for download, run the following command:  aria_dataset_downloader --cdn_file ${PATH_TO_YOUR_CDN_FILE} --output_folder ${OUTPUT_FOLDER_PATH}   This will display a list of the data groups that can be downloaded.  Download specific data groups​  To download specific data groups, specify their numbers separated by spaces after the --data_types argument. For example, to download data groups 0, 1, 2, 3, 4, 5, 6, 7, and 8 for all sequences, run the following command:  aria_dataset_downloader --cdn_file ${PATH_TO_YOUR_CDN_FILE} --output_folder ${OUTPUT_FOLDER_PATH} --data_types 0 1 2 3 4 5 6 7 8   This will download the specified data groups for all sequences in the CDN file to the specified output folder.  Download VRS for specific sequences​  To download VRS files for specific sequences, specify their names after the --sequence_names argument. For example, to download VRS files for sequences &quot;Apartment_release_clean_seq131_M1292&quot; and &quot;Apartment_release_clean_seq133_M1292&quot;, run the following command:  aria_dataset_downloader --cdn_file ${PATH_TO_YOUR_CDN_FILE} --output_folder ${OUTPUT_FOLDER_PATH} --data_types 0 --sequence_names Apartment_release_clean_seq131_M1292 Apartment_release_clean_seq133_M1292   This will download VRS files for the specified sequences to the specified output folder.  Overwrite existing files​  To overwrite existing files when downloading data, use the --overwrite flag. For example, to download VRS files for all sequences and overwrite any existing files, run the following command:  aria_dataset_downloader --cdn_file ${PATH_TO_YOUR_CDN_FILE} --output_folder ${OUTPUT_FOLDER_PATH} --data_types 0 --overwrite  ","version":"Next","tagName":"h3"},{"title":"Getting Started With DTC","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_getting_started","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Getting Started With DTC","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_getting_started#overview","content":" This section provides a step-by-step guide to set up the Python environment that the Digital Twin Catalog (DTC) tool requires.  ","version":"Next","tagName":"h2"},{"title":"Environment Set Up​","type":1,"pageTitle":"Getting Started With DTC","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_getting_started#environment-set-up","content":" ","version":"Next","tagName":"h2"},{"title":"Step 0 : Check system requirements and download codebase​","type":1,"pageTitle":"Getting Started With DTC","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_getting_started#step-0--check-system-requirements-and-download-codebase","content":" Ensure your system is supported and then download projectaria_tools codebase from the github  ","version":"Next","tagName":"h3"},{"title":"Step 1 : Install Python​","type":1,"pageTitle":"Getting Started With DTC","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_getting_started#step-1--install-python","content":" See Python Package Installation for detailed instructions.  ","version":"Next","tagName":"h3"},{"title":"Step 2 : Create a virtual environment​","type":1,"pageTitle":"Getting Started With DTC","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_getting_started#step-2--create-a-virtual-environment","content":" rm -rf $HOME/projectaria_tools_python_env python3 -m venv $HOME/projectaria_tools_python_env source $HOME/projectaria_tools_python_env/bin/activate   ","version":"Next","tagName":"h3"},{"title":"Step 3 : Install projectaria_tools from PyPI​","type":1,"pageTitle":"Getting Started With DTC","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_getting_started#step-3--install-projectaria_tools-from-pypi","content":" python3 -m pip install --upgrade pip python3 -m pip install projectaria-tools'[all]'   ","version":"Next","tagName":"h3"},{"title":"Step 4 : Install Other Dependent Python Package:​","type":1,"pageTitle":"Getting Started With DTC","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_getting_started#step-4--install-other-dependent-python-package","content":" python3 -m pip install opencv-python trimesh   ","version":"Next","tagName":"h3"},{"title":"Troubleshooting​​","type":1,"pageTitle":"Getting Started With DTC","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_getting_started#troubleshooting","content":" Go to Data Utilities Troubleshooting if you have issues implementing this guide. ","version":"Next","tagName":"h2"},{"title":"DTC Data Format","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_data_format","content":"","keywords":"","version":"Next"},{"title":"Aria Sequence Structure​","type":1,"pageTitle":"DTC Data Format","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_data_format#aria-sequence-structure","content":" |Sequence1Name| ├──video.vrs # Aria recording data ├──CC_BY-SA.txt # license file ├──object_pose.json # aligned object pose file ├──mps # derived data generated by Project Aria’s MPS ├──metadata.json   The object_pose.json file in each sequence directory contains the object pose aligned with the MPS data including the camera trajectories as well as the semi-dense point clouds.  ","version":"Next","tagName":"h2"},{"title":"DSLR Sequence Structure​","type":1,"pageTitle":"DTC Data Format","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_data_format#dslr-sequence-structure","content":" |Sequence1Name| ├──exrs # DSLR captured high dynamic range frames ├──pngs # converted low dynamic range frames ├──CC_BY-SA.txt # license file ├──camera_poses.json # DSLR camera poses file ├──object_pose.json # aligned object pose file ├──metadata.json ├──env.exr # scene environment file   The camera_poses.json file in each sequence directory contains DSLR camera poses. The object_pose.json file in each sequence directory contains the object pose aligned with the camera poses.  ","version":"Next","tagName":"h2"},{"title":"Data Format​","type":1,"pageTitle":"DTC Data Format","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_data_format#data-format","content":" ","version":"Next","tagName":"h2"},{"title":"video.vrs​","type":1,"pageTitle":"DTC Data Format","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_data_format#videovrs","content":" video.vrs contains the raw sensor recording from the Aria device.  Aria Hardware Specifications shows the sensors used to make recordings RGB images were all recorded at 10 fps with fixed exposure (2ms) and gain (3)  ","version":"Next","tagName":"h3"},{"title":"mps​","type":1,"pageTitle":"DTC Data Format","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_data_format#mps","content":" mps folder contains the derived data from Machine Perception Service (MPS) including  open_loop_trajectory.csv records the camera trajectory estimated using the online estimationclosed_loop_trajectory.csv records the camera trajectory estimated using the offline optimizationsemidense_observations.csv.gz records the per frame observability for each point in the semi-dense point cloudsemidense_points.csv.gz records the full semi-dense point cloud estimated from the recorded sequenceonline_calibration.jsonl records the online calibration results for SLAM cameras, RGB camera and IMU  ","version":"Next","tagName":"h3"},{"title":"object_pose.json​","type":1,"pageTitle":"DTC Data Format","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_data_format#object_posejson","content":" object_pose.json contains the pose (i.e. transformation from object space to world space) of the captured object’s digital copy (defined in object space) aligned with the MPS derived data or the DSLR camera poses.  { &quot;mesh&quot;: { &quot;T_world_object&quot;: [ … ] } }   ","version":"Next","tagName":"h3"},{"title":"exrs​","type":1,"pageTitle":"DTC Data Format","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_data_format#exrs","content":" exrs folder contains the high dynamic range images captured using three DSLR cameras.  ","version":"Next","tagName":"h3"},{"title":"pngs​","type":1,"pageTitle":"DTC Data Format","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_data_format#pngs","content":" pngs folder contains the converted png files of the images in the exrs folder.  ","version":"Next","tagName":"h3"},{"title":"camera_poses.json​","type":1,"pageTitle":"DTC Data Format","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_data_format#camera_posesjson","content":" camera_poses.json contains the estimated camera poses (i.e. transformation from world to camera), camera intrinsics and corresponding image files of DSLR cameras.  { &quot;cameras&quot;: [ { &quot;T_camera_world&quot;: [ … ], &quot;focal&quot;: …, &quot;height&quot;: …, &quot;image&quot;: &quot;...&quot;, &quot;intrinsic&quot;: { … }, &quot;width&quot;: … }, … ] }   ","version":"Next","tagName":"h3"},{"title":"env.exr​","type":1,"pageTitle":"DTC Data Format","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_data_format#envexr","content":" env.exr contains the high dynamic range environment map of DSLR captures aligned with the camera poses. ","version":"Next","tagName":"h3"},{"title":"DTC Object Models","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_object_models","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"DTC Object Models","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_object_models#overview","content":" This page covers how to download the 3D object models associated with the Digital Twin Catalog (DTC) dataset, and briefly describe the data content and how to visualize the models.  ","version":"Next","tagName":"h2"},{"title":"Download the DTC Object Models​","type":1,"pageTitle":"DTC Object Models","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_object_models#download-the-dtc-object-models","content":" ","version":"Next","tagName":"h2"},{"title":"Step 0: install project_aria_tools package and create venv if not done before​​","type":1,"pageTitle":"DTC Object Models","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_object_models#step-0-install-project_aria_tools-package-and-create-venv-if-not-done-before","content":" Follow Step 0 to Step 4 in Getting Started  ","version":"Next","tagName":"h3"},{"title":"Step 1 : Signup and get the download links file​","type":1,"pageTitle":"DTC Object Models","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_object_models#step-1--signup-and-get-the-download-links-file","content":" To use the downloader CLI, you need to download a file which contains all data URLs plus some metadata for the downloader script.  Visit Digital Twin Catalog on Project Aria Website and sign up.  Scroll down to the bottom of the page. Enter your email and select Access the Datasets.    Once you've selected Access the Datasets you'll be taken back to the top of the DTC page.  Scroll down the page to select the DTC Objects All Download Links link and download the file to the folder $HOME/Downloads.    ","version":"Next","tagName":"h3"},{"title":"Step 2 : Set up a folder for object models​","type":1,"pageTitle":"DTC Object Models","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_object_models#step-2--set-up-a-folder-for-object-models","content":" mkdir -p $HOME/Documents/dtc_object_library   ","version":"Next","tagName":"h3"},{"title":"Step 3 : Download the object models via CLI:​","type":1,"pageTitle":"DTC Object Models","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_object_models#step-3--download-the-object-models-via-cli","content":" To download all DTC models (~100GB), run the following command from your Python virtual environment:  dtc_object_downloader -c {PATH_TO_YOUR_DTC_OBJECT_CDN_FILE} -o $HOME/Documents/dtc_object_library   To download only a select subset of DTC models, run the following command from your Python virtual environment:  dtc_object_downloader -c {PATH_TO_YOUR_DTC_OBJECT_CDN_FILE} -o $HOME/Documents/dtc_object_library -l {SELECTED_OBJECT_NAMES}   You can retrieve object names from the DTC data providers using your downloaded sequence data, or have a look at the metadata.json file in your sequence data.  ","version":"Next","tagName":"h3"},{"title":"Troubleshooting​​","type":1,"pageTitle":"DTC Object Models","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_object_models#troubleshooting","content":" Check Project Aria Tools troubleshooting if you are having issues using this guide.  ","version":"Next","tagName":"h3"},{"title":"Object Model Data Format​","type":1,"pageTitle":"DTC Object Models","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_object_models#object-model-data-format","content":" When downloading the object models using dtc_object_downloader, each object will download into its own folder, named using the instance name of the object. Under each object folder, there will be three files as shown below.  |InstanceName1| ├──3d-asset.glb # model file ├──CC_BY-SA.txt # license file ├──metadata.json |InstanceName2| {...}   All poses associated with the objects are stored in each sequence object pose data, not in the downloaded object data. For example code to learn how to load object models and their associated poses, refer to the visualizers in Tooling  ","version":"Next","tagName":"h2"},{"title":"Visualizing Object Models​","type":1,"pageTitle":"DTC Object Models","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_object_models#visualizing-object-models","content":" The GLB files for each object model can be viewed using any off-the-shelf GLB viewer. For example, the open web browser based Babylon viewer can be used by dragging and dropping the GLB file. ","version":"Next","tagName":"h2"},{"title":"How to Download the DTC Dataset","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_download_dataset","content":"","keywords":"","version":"Next"},{"title":"Overview​​","type":1,"pageTitle":"How to Download the DTC Dataset","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_download_dataset#overview","content":" This page covers how to download Digital Twin Catalog (DTC) sequences. Follow the instructions to download the sample datasets and from there you'll be able to use the CLI to download more data. For information on how to download DTC object models, see Object Models page  The sample dataset is an Aria sequence recording a single object with active trajectory. This is a pretty representative example that should give you an idea of the dataset.  By downloading the datasets you agree that you have read and accepted the terms of the Digital Twin Catalog Dataset License Agreement.  ","version":"Next","tagName":"h2"},{"title":"Download the sample Digital Twin Catalog (DTC) sequence​","type":1,"pageTitle":"How to Download the DTC Dataset","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_download_dataset#download-the-sample-digital-twin-catalog-dtc-sequence","content":" ","version":"Next","tagName":"h2"},{"title":"Step 0: install project_aria_tools package and create venv if not done before​​","type":1,"pageTitle":"How to Download the DTC Dataset","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_download_dataset#step-0-install-project_aria_tools-package-and-create-venv-if-not-done-before","content":" Follow Step 0 to Step 4 in Getting Started.  ","version":"Next","tagName":"h3"},{"title":"Step 1 : Signup and get the download links file​","type":1,"pageTitle":"How to Download the DTC Dataset","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_download_dataset#step-1--signup-and-get-the-download-links-file","content":" To use the downloader CLI, you need to download a file which contains all data URLs plus some metadata for the downloader script. We currently offer two ways of getting this file:  Option 1 - Aria Dataset Explorer:​  Go to the Aria Dataset Explorer website. Here you can subselect sequences according to some filter options, or get the links to all sequences.  Option 2 - Digital Twin Catalog on Project Aria Website:​  Visit Digital Twin Catalog on Project Aria Website and sign up. Scroll down to the bottom of the page. Enter your email and select Access the Datasets.  Once you've selected Access the Datasets you'll be taken back to the top of the DTC page. Scroll down the page to select the Digital Twin Catalog Download Links and download the file to the folder $HOME/Downloads.  ","version":"Next","tagName":"h3"},{"title":"Step 2 : Set up a folder for DTC data​","type":1,"pageTitle":"How to Download the DTC Dataset","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_download_dataset#step-2--set-up-a-folder-for-dtc-data","content":" mkdir -p $HOME/Documents/projectaria_tools_dtc_data   ","version":"Next","tagName":"h3"},{"title":"Step 3 : Download the sample sequence(~1.3GB) via CLI:​","type":1,"pageTitle":"How to Download the DTC Dataset","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_download_dataset#step-3--download-the-sample-sequence13gb-via-cli","content":" From your Python virtual environment, run:  aria_dataset_downloader -c ${PATH_TO_YOUR_Aria_Sequence_CDN_FILE} \\ -o $HOME/Documents/projectaria_tools_dtc_data/ \\ -l BirdHouseRedRoofYellowWindows_active \\ -d 0 1 2 3 6   For more information on the content in the other sequences, see the Data Content section below  ","version":"Next","tagName":"h3"},{"title":"Step 4 : Set up a folder for DTC DSLR data​","type":1,"pageTitle":"How to Download the DTC Dataset","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_download_dataset#step-4--set-up-a-folder-for-dtc-dslr-data","content":" mkdir -p $HOME/Documents/projectaria_tools_dtc_dslr   ","version":"Next","tagName":"h3"},{"title":"Step 5 : Download the DSLR sample sequence (~2.0GB) via CLI:​","type":1,"pageTitle":"How to Download the DTC Dataset","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_download_dataset#step-5--download-the-dslr-sample-sequence-20gb-via-cli","content":" From your Python virtual environment, run:  aria_dataset_downloader -c ${PATH_TO_YOUR_DSLR_Sequence_CDN_FILE} \\ -o $HOME/Documents/projectaria_tools_dtc_dslr/ \\ -l Airplane_B097C7SHJH_WhiteBlue_Lighting001 \\ -d 0 1   For more information on the content in the other sequences, see the Data Content section below  ","version":"Next","tagName":"h3"},{"title":"Download the Digital Twin Catalog (DTC) dataset​","type":1,"pageTitle":"How to Download the DTC Dataset","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_download_dataset#download-the-digital-twin-catalog-dtc-dataset","content":" ","version":"Next","tagName":"h2"},{"title":"Data size​​","type":1,"pageTitle":"How to Download the DTC Dataset","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_download_dataset#data-size","content":" The Digital Twin dataset consists of 200 Aria sequences and 105 DSLR sequences. The Aria sequences will include MPS data, which can be downloaded individually. Go to Project Aria Machine Perception Services for more information about MPS data. The size of each data type is shown below.  Sequence Type\tData Type What’s included Per sequence size Total size for all sequences Aria Aria VRS VRS sequence captured using Aria ~ 1.3 GB ~ 260 GB MPS Derived data using MPS service, including camera trajectories, semi-dense point cloud, online calibration, etc ~ 35 MB ~ 6.8 GB Object Pose Object pose aligned with MPS data &lt; 1 KB &lt; 1 MB DSLR\tEXR High dynamic range DSLR captures ~ 1.5 GB ~ 158 GB PNG Converted low dynamic range captures ~ 540 MB ~ 55.4 GB Camera Poses DSLR camera poses &lt; 1 MB &lt; 20 MB Object Pose Object pose aligned with camera poses &lt; 1 KB &lt; 1 MB Environment Map\tEnvironment map aligned with camera poses ~ 3.0 MB\t~ 315 MB   ","version":"Next","tagName":"h3"},{"title":"Download via CLI​","type":1,"pageTitle":"How to Download the DTC Dataset","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_download_dataset#download-via-cli","content":" DTC supports both using the general Aria dataset downloader, which is available in the projectaria_tools PyPI (pip install) package, to download sequences and using our open sourced Aria dataset downloader Python wrapper, which is available in the DTC code repo, to download sequences with captured models.  To use the Aria downloader, use the following commands in the virtual environment where you've installed projectaria_tools:  aria_dataset_downloader   To get a description of the arguments that the script needs, run:  aria_dataset_downloader --help   The following are some example use cases:  ","version":"Next","tagName":"h3"},{"title":"Download VRS for all sequences​","type":1,"pageTitle":"How to Download the DTC Dataset","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_download_dataset#download-vrs-for-all-sequences","content":" aria_dataset_downloader --cdn_file ${PATH_TO_YOUR_DTC_SEQUENCE_CDN_FILE} --output_folder ${OUTPUT_FOLDER_PATH} --data_types 0   ","version":"Next","tagName":"h3"},{"title":"Download VRS + main ground truth data for all sequences​","type":1,"pageTitle":"How to Download the DTC Dataset","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_download_dataset#download-vrs--main-ground-truth-data-for-all-sequences","content":" aria_dataset_downloader --cdn_file ${PATH_TO_YOUR_DTC_SEQUENCE_CDN_FILE} --output_folder ${OUTPUT_FOLDER_PATH} --data_types 0 6   ","version":"Next","tagName":"h3"},{"title":"Download all data for all sequences​","type":1,"pageTitle":"How to Download the DTC Dataset","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_download_dataset#download-all-data-for-all-sequences","content":" aria_dataset_downloader --cdn_file ${PATH_TO_YOUR_DTC_SEQUENCE_CDN_FILE} --output_folder ${OUTPUT_FOLDER_PATH} --data_types 0 1 2 3 4 5 6 7 8 9   ","version":"Next","tagName":"h3"},{"title":"Download VRS for 2 specific sequences​","type":1,"pageTitle":"How to Download the DTC Dataset","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_download_dataset#download-vrs-for-2-specific-sequences","content":" aria_dataset_downloader --cdn_file ${PATH_TO_YOUR_DTC_SEQUENCE_CDN_FILE} --output_folder ${OUTPUT_FOLDER_PATH} --data_types 0 --sequence_names BirdHouseRedRoofYellowWindows_active BirdHouseRedRoofYellowWindows_passive   ","version":"Next","tagName":"h3"},{"title":"Download VRS for all sequences and overwrite​","type":1,"pageTitle":"How to Download the DTC Dataset","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_download_dataset#download-vrs-for-all-sequences-and-overwrite","content":" aria_dataset_downloader --cdn_file ${PATH_TO_YOUR_DTC_SEQUENCE_CDN_FILE} --output_folder ${OUTPUT_FOLDER_PATH} --data_types 0 --overwrite   ","version":"Next","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"How to Download the DTC Dataset","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_download_dataset#troubleshooting","content":" Check Project Aria Tools troubleshooting if you are having issues using this guide. ","version":"Next","tagName":"h2"},{"title":"DTC Tooling","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_tooling","content":"","keywords":"","version":"Next"},{"title":"Step 0: Prepare data and venv if not done before​​","type":1,"pageTitle":"DTC Tooling","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_tooling#step-0-prepare-data-and-venv-if-not-done-before","content":" Follow Step 0 to Step 5 in Dataset Download.Follow Step 0 to Step 3 in Object Models.  ","version":"Next","tagName":"h3"},{"title":"Step 1: Clone the code repo to local​","type":1,"pageTitle":"DTC Tooling","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_tooling#step-1-clone-the-code-repo-to-local","content":" mkdir ${PATH_TO_LOCAL_CODE_REPO} cd ${PATH_TO_LOCAL_CODE_REPO} git clone https://github.com/facebookresearch/DigitalTwinCatalogue.git   ","version":"Next","tagName":"h3"},{"title":"Rerun Visualizer​","type":1,"pageTitle":"DTC Tooling","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_tooling#rerun-visualizer","content":" ","version":"Next","tagName":"h2"},{"title":"Step 2: Run the visualizer​","type":1,"pageTitle":"DTC Tooling","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_tooling#step-2-run-the-visualizer","content":" python {PATH TO THE RERUN VIEWER PYTHON SCRIPT} --sequence_folder {PATH_TO_YOUR_SELECTED_ARIA_SEQUENCE_FOLDER} \\ --model_folder ${PATH_TO_YOUR_SELECTED_MODEL_FOLDER}   The Rerun visualizer will visualize the Aria RGB camera, object model and semi-dense point cloud in the same coordinate frame.  ","version":"Next","tagName":"h3"},{"title":"Mask Generation​","type":1,"pageTitle":"DTC Tooling","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_tooling#mask-generation","content":" ","version":"Next","tagName":"h2"},{"title":"Step 2: Run the mask generation​","type":1,"pageTitle":"DTC Tooling","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_tooling#step-2-run-the-mask-generation","content":" mkdir -p $HOME/Documents/dtc_aria_mask_output   Run the Aria mask generator, from the python virtual environment. Choose the rectification setting of RGB frame (width, height, focal) and subsample rate of RGB stream (optional)  python {PATH TO THE OBJECT MASK GENERATOR PYTHON SCRIPT} --sequence_folder {PATH_TO_YOUR_SELECTED_SEQUENCE_FOLDER} \\ --model_folder {PATH_TO_YOUR_SELECTED_MODEL_FOLDER} \\ --output_folder $HOME/Documents/dtc_aria_mask_output \\ --width 800 --height 800 --focal 400 --subsample_rate 5   Three subfolders will be created in the output folder, “image” (containing rectified RGB frames), “mask” (corresponding object masks) and “overlay” (overlay mask frame on top of corresponding RGB image frame). Corresponding frames will look as below  ","version":"Next","tagName":"h3"},{"title":"Download Model with Sequence​","type":1,"pageTitle":"DTC Tooling","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_tooling#download-model-with-sequence","content":" ","version":"Next","tagName":"h2"},{"title":"Step 2: Run the download script​","type":1,"pageTitle":"DTC Tooling","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_tooling#step-2-run-the-download-script","content":" mkdir -p $HOME/Documents/dtc_sequence_with_object   Run the download script. It will download specified Aria sequence and its captured 3D model  python {PATH TO THE DOWNLOAD PYTHON SCRIPT} -s ${PATH_TO_YOUR_Aria_Sequence_CDN_FILE} \\ -m ${PATH_TO_YOUR_DTC_MODEL_CDN_FILE} \\ -o $HOME/Documents/dtc_sequence_with_object \\ -l {YOUR_SELECTED_SEQUENCE_NAME}   Two subfolders will be created in the output folder, “sequences” (sequences you specified to download) and “models” (models captured by the sequences).  ","version":"Next","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"DTC Tooling","url":"/projectaria_tools/docs/open_datasets/digital_twin_catalog/digital_twin_catalog_tooling#troubleshooting","content":" Go to Project Aria Tools Troubleshooting if you experience any issues. ","version":"Next","tagName":"h2"},{"title":"Ego-Exo4D Dataset","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/ego-exo4d","content":"","keywords":"","version":"Next"},{"title":"Resources:​","type":1,"pageTitle":"Ego-Exo4D Dataset","url":"/projectaria_tools/docs/open_datasets/ego-exo4d#resources","content":" Ego-Exo4D websiteEgo-Exo4D documentationEgoExo4D TutorialEgo4D and Ego-Exo4D feedback and support  ","version":"Next","tagName":"h3"},{"title":"Ego-Exo4D resources on Project Aria Tools​","type":1,"pageTitle":"Ego-Exo4D Dataset","url":"/projectaria_tools/docs/open_datasets/ego-exo4d#ego-exo4d-resources-on-project-aria-tools","content":" There are a range of resources available for working with Aria data using Project Aria Tools, and Load Static Calibration Data was specifically created to support working with Ego-Exo4D data.  EgoExo4D Data Format and LoaderGetting Started With Project Aria Data UtilitiesAria Data Formats ","version":"Next","tagName":"h2"},{"title":"Ego-Exo4D Data Format and Loader","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/ego-exo4d/ego-exo4d_data_format","content":"","keywords":"","version":"Next"},{"title":"3D Static camera calibration​","type":1,"pageTitle":"Ego-Exo4D Data Format and Loader","url":"/projectaria_tools/docs/open_datasets/ego-exo4d/ego-exo4d_data_format#3d-static-camera-calibration","content":" ","version":"Next","tagName":"h2"},{"title":"Data Format​","type":1,"pageTitle":"Ego-Exo4D Data Format and Loader","url":"/projectaria_tools/docs/open_datasets/ego-exo4d/ego-exo4d_data_format#data-format","content":" Column\tType\tDescriptioncam_uid\tstring\tUnique identifier of camera graph_uid\tstring\tUnique identifier of the world coordinate frame {tx,ty,tz,qx,qy,qz,qw}_world_cam\tfloat\tPose of the camera coordinate frame in world frame T_world_cam, translation (tx, ty, tz) in meters and rotation quaternion (qx, qy, qz, qw) image_width\tint\tImage size in pixels image_height\tint\tImage size in pixels intrinsics_type\tstring\tCamera intrinsics calibration type. Currently support types: KANNALABRANDTK3: KB3 model intrinsics_{0-7}\tfloat\tCamera intrinsics parameters start_frame_idx\tint\tUsed to determine if start frame number of the video is stationary, and if stationary camera pose and intrinsic calibration results can be applied. start_frame_idx and end_frame_idx will both be -1 if the stationary pose and intrinsic calibration can be applied to the whole video end_frame_idx\tint\tUsed to determine if the end frame number of the video is stationary and if stationary camera pose and intrinsic calibration results can be applied. start_frame_idx and end_frame_idx will both be -1 if the stationary pose and intrinsic calibration can be applied to the whole video  ","version":"Next","tagName":"h3"},{"title":"Load 3D Static Calibration data​","type":1,"pageTitle":"Ego-Exo4D Data Format and Loader","url":"/projectaria_tools/docs/open_datasets/ego-exo4d/ego-exo4d_data_format#load-3d-static-calibration-data","content":" Data loaders for MPS outputs are provided as part of Project Aria Tools (projectaria_tools/main/core/mps). As part of this, the loaders put the outputs into data structures that are easier for other tools to consume.  Ego-Exo4d data is unique in that it has recordings from static cameras as well as moving Project Aria glasses. The following commands enable you to load poses and intrinsic calibration of set stationary cameras (GoPros) alongside Project Aria data.  PythonC++ import projectaria_tools.core.mps as mps static_cameras_path = &quot;/path/to/mps/output/trajectory/static_cam_calibs.csv&quot; static_cameras = mps.read_static_camera_calibrations(static_cameras_path)  ","version":"Next","tagName":"h3"},{"title":"HOT3D Dataset","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/hot3d","content":"","keywords":"","version":"Next"},{"title":"Getting Started​","type":1,"pageTitle":"HOT3D Dataset","url":"/projectaria_tools/docs/open_datasets/hot3d#getting-started","content":" https://www.projectaria.com/datasets/hot3D/ - find out more about the dataset and get access to it.Introducing HOT3D: An Egocentric Dataset for 3D Hand and Object Tracking - research paper.HOT3D GitHub repository - install HOT3D Python tooling that will enable you to download and visualize HOT3D data. Use the HOT3D Jupyter notebook tutorial to get to know the downloader and visualizers.  ","version":"Next","tagName":"h2"},{"title":"HOT3D Research Challenges​","type":1,"pageTitle":"HOT3D Dataset","url":"/projectaria_tools/docs/open_datasets/hot3d#hot3d-research-challenges","content":" HOT3D data is used in the following research challenges:  Multiview Egocentric Hand Tracking ChallengeBOP: Benchmark for 6D Object Pose Estimation ","version":"Next","tagName":"h2"},{"title":"Reading in the Wild dataset","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/ritw","content":"","keywords":"","version":"Next"},{"title":"Subsets​","type":1,"pageTitle":"Reading in the Wild dataset","url":"/projectaria_tools/docs/open_datasets/ritw#subsets","content":" The Reading in the Wild dataset can be further divided into two subsets that were collected independently of each other: the Seattle dataset and the Columbus dataset.  Seattle - This subset was collected for training, validation, and testing purposes. It focuses on reading and non-reading activities in diverse scenarios, meaning it covers a wide variety of participants, reading modes, written materials, and more. It contains a mix of normal negative (no text present) and hard negative (text present but not being read) examples, as well as mixed sequences that alternate between reading and not reading. These data were collected in homes, office spaces, libraries, and the outdoors.Columbus - This subset was collected to find cases where models intended to discern whether participants are reading encounter failure points. It contains examples of hard negatives (where text is present but not being read), searching/browsing (which gives confusing gaze patterns), and the reading of non-English texts (where reading direction differs).    ","version":"Next","tagName":"h2"},{"title":"Download the data​","type":1,"pageTitle":"Reading in the Wild dataset","url":"/projectaria_tools/docs/open_datasets/ritw#download-the-data","content":" All RitW data is available at https://www.projectaria.com/datasets/reading-in-the-wild/, but the process differs depending on whether you want to download the Seattle subset or the Columbus subset.  ","version":"Next","tagName":"h2"},{"title":"Seattle subset​","type":1,"pageTitle":"Reading in the Wild dataset","url":"/projectaria_tools/docs/open_datasets/ritw#seattle-subset","content":" This dataset is owned and distributed by Meta with a CC-by-NC4 license. You can request and download the dataset here.  Alternatively, you can access the dataset through the Dataset Explorer, which provides a convenient way to visualize the data and selectively download specific sequences.  ","version":"Next","tagName":"h3"},{"title":"Columbus subset​","type":1,"pageTitle":"Reading in the Wild dataset","url":"/projectaria_tools/docs/open_datasets/ritw#columbus-subset","content":" This dataset is owned and distributed by The Ohio State University (OSU) with an Apache2 license. Please refer to the official OSU repository for access and licensing information.  ","version":"Next","tagName":"h3"},{"title":"Data Explorer​","type":1,"pageTitle":"Reading in the Wild dataset","url":"/projectaria_tools/docs/open_datasets/ritw#data-explorer","content":" You can explore the Seattle Subset using Data Explorer here.  ","version":"Next","tagName":"h2"},{"title":"GitHub repo​","type":1,"pageTitle":"Reading in the Wild dataset","url":"/projectaria_tools/docs/open_datasets/ritw#github-repo","content":" You can find the code for the Reading Recognition Model here.  ","version":"Next","tagName":"h2"},{"title":"Additional info​","type":1,"pageTitle":"Reading in the Wild dataset","url":"/projectaria_tools/docs/open_datasets/ritw#additional-info","content":" The dataset - Learn more about the dataset and how to get access to it. Reading Recognition in the Wild - Read the original research paper that RitW is based on. Blog post - Read the blog post that announced Reading in the Wild. ","version":"Next","tagName":"h2"},{"title":"Project Aria Open Models","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_models","content":"Project Aria Open Models Project Aria has released several open models, powered by Project Aria data, and with the ability to extend beyond Aria data. EgoBlur - an open source AI model from Meta to preserve privacy by detecting and blurring PII from images. Designed to work with egocentric data (such as Aria data) and non-egocentric data.Egocentric Voxel Lifting (EVL) - an opensource AI model from Meta for 3D object detection and surface reconstruction on ProjectAria recordings.Project Aria Eye Tracking - an open source inference code for estimating eye gaze direction and visualizing the outputs.SceneScript - an AI model and method to understand and describe 3D spaces.Photoreal Scene Reconstruction - a system for reconstructing photorealistic 3D scenes captured from an egocentric device.","keywords":"","version":"Next"},{"title":"DTC Object Explorer","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_datasets/object_explorer","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"DTC Object Explorer","url":"/projectaria_tools/docs/open_datasets/object_explorer#overview","content":" The DTC Object Explorer is an interactive webtool that allows researchers to find, visualize, and download high-quality Digital Twin object models in the common GLB format. Like the Aria Dataset Explorer, the DTC Object Explorer has filtering tools that allow you to filter objects by various metadata attributes, such as texture, category, and license. Interactive visualization tools help you quickly verify you’ve found the models you want before downloading those object models to your local machine.  ","version":"Next","tagName":"h2"},{"title":"Using DTC Object Explorer​","type":1,"pageTitle":"DTC Object Explorer","url":"/projectaria_tools/docs/open_datasets/object_explorer#using-dtc-object-explorer","content":" Visit https://dtc.projectaria.com/ to use the DTC Object Explorer. Once there, you have a number of ways to engage with the explorer:  ","version":"Next","tagName":"h2"},{"title":"Filter the kinds of objects that appear​","type":1,"pageTitle":"DTC Object Explorer","url":"/projectaria_tools/docs/open_datasets/object_explorer#filter-the-kinds-of-objects-that-appear","content":" In the Filter objects by search bar near the top of the site, you can enter a variety of properties, such as object_uid, category, and texture. You can specify what you're looking for regarding each property. The search results will automatically update for each restriction you add, and you can clear all your current filters by selecting the &quot;X&quot; on the search bar. The complete list of properties is as follows:  object_uidrelease - The sequence dataset with which the object is associated.categorylinked_sequence - A linked sequence in Dataset Explorer.licenseavg_metallic - A measure of how metallic the surface of the object is.avg_roughness - A measure of how rough the surface of the object is.texture - Either shiny or matte.num_verticesnum_triangles  ","version":"Next","tagName":"h3"},{"title":"Change which object properties are displayed for the search results​","type":1,"pageTitle":"DTC Object Explorer","url":"/projectaria_tools/docs/open_datasets/object_explorer#change-which-object-properties-are-displayed-for-the-search-results","content":" The search results are displayed as a grid of objects that meet your filtering criteria. Each item in the search results will display its image at a minimum and its object_uid and category by default, but you can specify additional properties to be displayed for each item by clicking in the Displayed properties bar and selecting new ones. You can also remove current properties from being displayed by selecting the &quot;X&quot; next to each.  ","version":"Next","tagName":"h3"},{"title":"Object details​","type":1,"pageTitle":"DTC Object Explorer","url":"/projectaria_tools/docs/open_datasets/object_explorer#object-details","content":" Clicking on an object (as opposed to selecting its checkbox) will take you to a page dedicated specifically to that object. It features an in-browser interactive viewer where you can view the digital object from all angles. It also includes all the object’s metadata, as well as the option to download the object directly.  ","version":"Next","tagName":"h3"},{"title":"Select and bulk download objects​","type":1,"pageTitle":"DTC Object Explorer","url":"/projectaria_tools/docs/open_datasets/object_explorer#select-and-bulk-download-objects","content":" All objects returned by your search are selected by default: you can select/deselect specific objects by clicking their checkboxes. You can also deselect all currently selected objects by clicking Clear Selected or select all objects in the search results by clicking Select All, both of which are near the top right of the site.  Once you're satisfied with your choice of objects, you can download all the objects you've selected by clicking Download selected objects in the top right of the site. You'll be asked to enter and submit your email address in order to access the object files. Once you've done so, you can choose which specific assets you want to download (3d-asset_glb, license, and/or metadata), then select Download selected objects to download the relevant JSON.  As part of downloading the JSON, an Object Documentation pop-up with instructions on how to install the Project Aria Tools Python package (one-time setup) and access object data will appear. ","version":"Next","tagName":"h3"},{"title":"EgoBlur","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_models/egoblur","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"EgoBlur","url":"/projectaria_tools/docs/open_models/egoblur#overview","content":" EgoBlur is an open source AI model from Meta to preserve privacy by detecting and blurring PII from images. We provide two FasterRCNN-based detector models, for face blurring and license plates, and perform consistently across the full range of ‘responsible AI labels’, as defined by the CCV2 dataset.  Our models are on-par with the other publicly available anonymization models on non-egocentric data, while significantly outperforming other models on egocentric data that is collected using Project Aria glasses.  Trained on 23M images, 790M bounding boxes.The EgoBlur face and license plate models are approximately 400 MB each and have ~104 million parameters.Based on the Faster RCNN model with a ResNext backbone. The models are trained using Meta’s publicly available Detectron2 and Detectron2go libraries.Benchmarked against the Aria Pilot Dataset and the CCV2 Dataset.EgoBlur models are only trained to locate the position of faces and license plates of vehicles within color or grayscale images. The models are not used to track or identify individual faces or license plates.EgoBlur tooling is available to apply the models to PNG, JPEG, MP4 and VRS files.  ","version":"Next","tagName":"h2"},{"title":"Further reading​","type":1,"pageTitle":"EgoBlur","url":"/projectaria_tools/docs/open_models/egoblur#further-reading","content":" EgoBlur Research Paper - EgoBlur: Responsible Innovation in AriaEgoBlur FAQ, at the bottom of projectaria.com/tools/egoblur  ","version":"Next","tagName":"h3"},{"title":"Getting Started​","type":1,"pageTitle":"EgoBlur","url":"/projectaria_tools/docs/open_models/egoblur#getting-started","content":" To demonstrate how EgoBlur models can be applied and to support privacy preserving research we provide:  EgoBlur VRS Utilities - C++ tool for working with Aria VRS files Go to the EgoBlur VRS Utilities Readme for detailed instructions EgoBlur Demo - Python3 tool for working with PNG, JPEG or MP4 files Go to the EgoBlur Readme for detailed instructions  For these tools, we focused our testing on Fedora 37 and Ubuntu 22.04/20.04 with Nvidia V100 16GB GPU and CUDA 12.1. ","version":"Next","tagName":"h2"},{"title":"Egocentric Voxel Lifting (EVL)","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_models/evl","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Egocentric Voxel Lifting (EVL)","url":"/projectaria_tools/docs/open_models/evl#overview","content":" EVL is an opensource AI model from Meta for 3D object detection and surface reconstruction on ProjectAria recordings. EVL is trained on simulation data, namely Aria Synthetic Environments. It relies on frozen 2D foundation features to set a competitive performance. EVL leverages all egocentric modalities from Aria including posed and calibrated RGB and greyscale video streams and semidense points.  ","version":"Next","tagName":"h2"},{"title":"Further reading​","type":1,"pageTitle":"Egocentric Voxel Lifting (EVL)","url":"/projectaria_tools/docs/open_models/evl#further-reading","content":" Research Paper - EFM3D: A Benchmark for Measuring Progress Towards 3D Egocentric Foundation ModelsGithub repo github.com/facebookresearch/efm3d  ","version":"Next","tagName":"h3"},{"title":"Getting Started​","type":1,"pageTitle":"Egocentric Voxel Lifting (EVL)","url":"/projectaria_tools/docs/open_models/evl#getting-started","content":" To start using the model, check out the EVL model inference demo in the EFM3D repo. ","version":"Next","tagName":"h2"},{"title":"Project Aria Eye Tracking","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_models/eye_tracking","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Project Aria Eye Tracking","url":"/projectaria_tools/docs/open_models/eye_tracking#overview","content":" Project Aria Eye Tracking provides an open source inference model and tooling that can be used to estimate eye gaze direction based on Aria device Eye Tracking camera images. This Pytorch model, trained on data from over ~1200 individuals and ~2M frames, produces outputs that use the Pre March 2024 Eye Gaze Model data structure.  Use projectaria_eyetracking to generate and visualize these outputs on downloaded or streaming data. These outputs will also be compatible with Project Aria Tools MPS eye gaze output and visualizers.  Project Aria Eye Tracking GitHub repository Download and run projectaria_eyetracking Pre March 2024 Eye Gaze Model How the data is structured Project Aria Tools Eye Gaze Code Snippets Project Aria Tooling is compatible with these Eye Gaze outputs Project Aria Client SDK Code snippets that support streaming live Aria data to downstream applications ","version":"Next","tagName":"h2"},{"title":"Tech Insights","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/tech_insights","content":"Tech Insights Technical deeper dives on domain-specific topics. You don't need to read this section to use Project Aria data or glasses, but you may find it interesting to understand how Aria glasses work.","keywords":"","version":"Next"},{"title":"Photoreal Scene Reconstruction","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_models/psr","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Photoreal Scene Reconstruction","url":"/projectaria_tools/docs/open_models/psr#overview","content":" Photoreal Scene Reconstruction is a system for reconstructing photorealistic 3D scenes captured from an egocentric device. In contrast to off-the-shelf Gaussian Splatting reconstruction pipelines that use videos as input through structure-from-motion, this system has two major innovations that greatly improve the reconstruction quality:  Visual-inertial bundle adjustment (VIBA): Unlike the mainstream approach of treating an RGB camera as a frame-rate camera, VIBA allows us to calibrate the precise timestamps and movements of an RGB camera in a high-frequency trajectory format. This supports the system in precisely modeling the online RGB camera calibrations and pixel movements of a rolling-shutter camera. The VIBA input comes from Project Aria’s Machine Perception Services. Gaussian Splatting model: This physical image formation model, based on the Gaussian Splatting algorithm, effectively addresses sensor characteristics, including the rolling-shutter effect of RGB cameras and the dynamic ranges measured by the sensors. This formulation can apply to other rasterization-based techniques.  Included in the system are comprehensive guidelines for using data recorded by Aria Gen 1 devices, including how to preprocess the recordings and reconstruct them using different variations of the Gaussian Splatting algorithm. Provided examples include the reconstruction of scenes using RGB sensors, SLAM cameras, and all cameras combined.  For more details on this method, check out the GitHub repo.  ","version":"Next","tagName":"h2"},{"title":"References​","type":1,"pageTitle":"Photoreal Scene Reconstruction","url":"/projectaria_tools/docs/open_models/psr#references","content":" Research paper - Photoreal Scene Reconstruction from an Egocentric Device ","version":"Next","tagName":"h2"},{"title":"Project Aria Support","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/support","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Project Aria Support","url":"/projectaria_tools/docs/support#overview","content":" This page provides links to various troubleshooting pages, as well as how to request support or report an issue.  ","version":"Next","tagName":"h2"},{"title":"Troubleshooting pages​","type":1,"pageTitle":"Project Aria Support","url":"/projectaria_tools/docs/support#troubleshooting-pages","content":" Data Utilities TroubleshootingAria Glasses Quickstart GuideAria Research Kit Troubleshooting Client SDK and CLI TroubleshootingMachine Perception Services (MPS) Troubleshooting  ","version":"Next","tagName":"h2"},{"title":"How do I get support/report issues?​","type":1,"pageTitle":"Project Aria Support","url":"/projectaria_tools/docs/support#how-do-i-get-supportreport-issues","content":" We encourage you to post issues, feedback and feature requests to GitHub, so that they can be tracked.  Project Aria Tools Issues - recommended for: Project Aria Tools issuesOSI and ARK issues whenever they don't have a standalone GitHub repository (such as most open datasets, the Client SDK or MPS CLI) EgoBlur Issues - recommended for issues about EgoBlur open modelsHot3D Issues - recommended for issues about the HOT3D open dataset  If you are a Research Partner with access to the Aria Research Kit there are several other support options available, although GitHub is ideal for bug reports and feature requests.  Post to Project Aria Discord - best for discussion, feedback or user supportPost to Academic Partners Feedback and Support workplace group - discussion, feedback or user supportEmail AriaOps@meta.com - for feedback or user support ","version":"Next","tagName":"h2"},{"title":"SceneScript","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/open_models/scenescript","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"SceneScript","url":"/projectaria_tools/docs/open_models/scenescript#overview","content":" SceneScript is an AI model and method to understand and describe 3D spaces. It provides a method for representing and inferring scene geometry using an auto-regressive structured language model, and end-to-end learning.  ","version":"Next","tagName":"h2"},{"title":"Further reading​","type":1,"pageTitle":"SceneScript","url":"/projectaria_tools/docs/open_models/scenescript#further-reading","content":" Learn more about SceneScript on projectaria.comSceneScript Research Paper - SceneScript: Reconstructing Scenes With An Autoregressive Structured Language ModelSceneScript GitHub repositoryHuman-in-the-Loop Local Corrections of 3D Scene Layouts via Infilling  ","version":"Next","tagName":"h3"},{"title":"Getting Started​","type":1,"pageTitle":"SceneScript","url":"/projectaria_tools/docs/open_models/scenescript#getting-started","content":" ","version":"Next","tagName":"h2"},{"title":"Installation​","type":1,"pageTitle":"SceneScript","url":"/projectaria_tools/docs/open_models/scenescript#installation","content":" To start using the model, check out the instructions on SceneScript GitHub repository ","version":"Next","tagName":"h3"},{"title":"Camera Photometric and Noise Models for Project Aria Glasses","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/tech_insights/camera_photometric_and_noise_model","content":"","keywords":"","version":"Next"},{"title":"Photometric Models​","type":1,"pageTitle":"Camera Photometric and Noise Models for Project Aria Glasses","url":"/projectaria_tools/docs/tech_insights/camera_photometric_and_noise_model#photometric-models","content":" In their working distance range, Aria camera lenses are well-focused, i.e. their point spread function is at sub-pixel level. Thus, we can establish a simplified photometric model where each camera pixel collects the photon emitted from a tiny surface area around a corresponding world point.  The irradiance of each pixel is attenuated by vignetting. The vignetting of Aria cameras are dominated by two factors (1) cos^4 fall-off (2) mechanical cropping of the lens barrel. Points that falls out of the camera's FOV are not visible, and cannot be applied by the above intrinsic model.  Then each pixel takes the time integral of the irradiance, as the sensors collect the arriving photon over the exposure time. The pixel intensity of a non-linear function of the amount of received photons as the ADC transform is non-linear and saturated.  ","version":"Next","tagName":"h2"},{"title":"Noise Models​","type":1,"pageTitle":"Camera Photometric and Noise Models for Project Aria Glasses","url":"/projectaria_tools/docs/tech_insights/camera_photometric_and_noise_model#noise-models","content":" The two sources of noise dominating Aria camera sensors are:  Shot noise, which accounts for the noise generated due to arrival of photons. Shot noise follows the Poisson distribution.Read noise, which accounts for the noise generated due to ADC conversion, etc. Read noise can be modeled by a zero-mean Gaussian random variable. ","version":"Next","tagName":"h2"},{"title":"How Data From Project Aria Devices is Timestamped","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/tech_insights/device_timestamping","content":"","keywords":"","version":"Next"},{"title":"Aria device hardware​","type":1,"pageTitle":"How Data From Project Aria Devices is Timestamped","url":"/projectaria_tools/docs/tech_insights/device_timestamping#aria-device-hardware","content":" The figure below illustrates the various hardware components in Aria devices and how they are connected electrically. The device consists of a microcontroller unit (MCU) that interfaces with most of the sensors for configuring and controlling them. The MCU is responsible for timestamping the data from these sensors, which enables capturing the multi-modal data with common timestamps across the motion sensors, microphones and camera sensors. The device also has an Application Processor (AP) that runs Android High Level OS.    Figure 1: Project Aria device hardware diagram  The device timestamp is ideally assigned, by the embedded micro-controller (MCU), to the measurement as close as possible to the time the measurement is captured. However, the meaning of the event effectively timestamped and the way the timestamp is obtained differs significantly depending on the sensor.  ","version":"Next","tagName":"h2"},{"title":"Mono Scene/SLAM and Eye Tracking (ET) cameras​","type":1,"pageTitle":"How Data From Project Aria Devices is Timestamped","url":"/projectaria_tools/docs/tech_insights/device_timestamping#mono-sceneslam-and-eye-tracking-et-cameras","content":" The Mono Scene (also called SLAM) and Eye Tracking (ET) cameras have electronic global shutter sensors. They are triggered at regular rate. Their image timestamps mark the center of the exposure window and are derived from the value of a MCU counter. The timestamping error is expected to be upper-bounded by 19us.  ","version":"Next","tagName":"h3"},{"title":"RGB camera​","type":1,"pageTitle":"How Data From Project Aria Devices is Timestamped","url":"/projectaria_tools/docs/tech_insights/device_timestamping#rgb-camera","content":" The RGB camera has an electronic rolling shutter. It is triggered at regular rate, often a divider of the Mono Scene camera rate. The timestamp marks the center of the exposure of the middle row and is obtained similarly to the Mono Scene camera timestamp.  ","version":"Next","tagName":"h3"},{"title":"IMUs, barometer and magnetometer​","type":1,"pageTitle":"How Data From Project Aria Devices is Timestamped","url":"/projectaria_tools/docs/tech_insights/device_timestamping#imus-barometer-and-magnetometer","content":" The two IMUs, the barometer and the magnetometer sensors operate respectively at 800Hz, 1000Hz, 50Hz and 10Hz in free-running mode. We timestamp their data-ready signal on the MCU. Because of on-chip signal processing operations, those timestamps correspond to a time point after the instant for which the measurement is valid. Go to Temporal Alignment of Sensor Data for how to finely align the data with the images.  ","version":"Next","tagName":"h3"},{"title":"Global Navigation Satellite System (GNSS) data​","type":1,"pageTitle":"How Data From Project Aria Devices is Timestamped","url":"/projectaria_tools/docs/tech_insights/device_timestamping#global-navigation-satellite-system-gnss-data","content":" GNSS data is timestamped on the AP at their time of arrival from the receiver of and converted to a device timestamp. Conversion of the timestamp is based on a bidirectional communication between SoC and MCU and is expected to introduce less than 100us of error.  ","version":"Next","tagName":"h3"},{"title":"Spatial Microphones​","type":1,"pageTitle":"How Data From Project Aria Devices is Timestamped","url":"/projectaria_tools/docs/tech_insights/device_timestamping#spatial-microphones","content":" For the audio stream, each samples are individually timestamped with an accuracy expected to be better than one audio sample. This synchronization relies on the MCU periodically injecting an encoded version of the current device timestamp into an unused microphone channel; the AP decodes it on reception.  ","version":"Next","tagName":"h3"},{"title":"Bluetooth and Wi-Fi​","type":1,"pageTitle":"How Data From Project Aria Devices is Timestamped","url":"/projectaria_tools/docs/tech_insights/device_timestamping#bluetooth-and-wi-fi","content":" Bluetooth and Wi-Fi scan data is received and timestamped on the AP using a time estimate of the MCU time. Conversion of timestamp is based on the protocol between SoC and MCU and is expected to introduce less than 100us of error. ","version":"Next","tagName":"h3"},{"title":"Sensor Measurement Models in Project Aria Devices","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/tech_insights/sensor_measurement_model","content":"","keywords":"","version":"Next"},{"title":"IMUs​","type":1,"pageTitle":"Sensor Measurement Models in Project Aria Devices","url":"/projectaria_tools/docs/tech_insights/sensor_measurement_model#imus","content":" For IMUs, we employ an affine model where the value from the readout of accelerometer sas_asa​ or gyroscope sgs_gsg​, is compensated to obtain a &quot;real&quot; acceleration aaa and angular velocity ω\\omegaω by  a=Ma−1(sa−ba)ω=Mg−1(sg−bg)a = M_a^{-1}(s_a - b_a) \\qquad \\omega = M_g^{-1}(s_g - b_g)a=Ma−1​(sa​−ba​)ω=Mg−1​(sg​−bg​)  MaM_aMa​ and MgM_gMg​ are assumed to be upper triangular so that there is no global rotation from the imu body frame to the accelerometer frame.  Inversely, we can simulate the sensor read-out from acceleration or angular velocity by  sa=Maa+basg=Mgω+bgs_a = M_a a + b_a \\qquad s_g = M_g \\omega + b_gsa​=Ma​a+ba​sg​=Mg​ω+bg​  When the read-out signal exceeds a threshold, the signal saturates. Saturation limits are sensor dependent and referenced in the following table for accelerometer and gyrometers.  \taccel-left\taccel-right\tgyro-left\tgyro-rightsaturation\t4g\t8g\t5000\t1000  tip We recommend using Trajectory MPS outputs instead of raw IMU data wherever possible. Go to MPS Code Snippets for how to load open loop or closed loop trajectory data.  ","version":"Next","tagName":"h2"},{"title":"Magnetometer, barometer and audio​","type":1,"pageTitle":"Sensor Measurement Models in Project Aria Devices","url":"/projectaria_tools/docs/tech_insights/sensor_measurement_model#magnetometer-barometer-and-audio","content":" Similar to the IMU rectification model, the sensor readouts for magnetometer, barometer, and audio data are modeled as linear to the real rrr (magnetic field, air pressure and sound intensity).  Audio specifically is bias only. ","version":"Next","tagName":"h2"},{"title":"Project Aria Technical Specifications","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/tech_spec","content":"Project Aria Technical Specifications The Technical Specifications section provides information about Project Aria glasses hardware, the different configurations Aria glasses can use when recording, and how Aria glasses are calibrated.","keywords":"","version":"Next"},{"title":"Temporal Alignment of Aria Sensor Data","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/tech_insights/temporal_alignment_of_sensor_data","content":"","keywords":"","version":"Next"},{"title":"Temporal Alignment​","type":1,"pageTitle":"Temporal Alignment of Aria Sensor Data","url":"/projectaria_tools/docs/tech_insights/temporal_alignment_of_sensor_data#temporal-alignment","content":" Although each sensor data pieces are timestamped in the same time domain, there might be an offset between the instant represented by their timestamps and the actual instant for which the enclosed measurement is valid. For proper sensor fusion where precise temporal alignment of the data is required, we must take this offset into account. We need an estimate of this offset for each sensor.  ","version":"Next","tagName":"h2"},{"title":"Per sensor time offset : dtDeviceSensor\\text{dt}_{\\text{Device}}^{\\text{Sensor}}dtDeviceSensor​​","type":1,"pageTitle":"Temporal Alignment of Aria Sensor Data","url":"/projectaria_tools/docs/tech_insights/temporal_alignment_of_sensor_data#per-sensor-time-offset--textdt_textdevicetextsensor","content":" Since the SLAM (mono scene) camera timestamps are direct measurements of a well defined instant (the center of exposure of our image sensor), we choose the SLAM camera timestamps as the reference timestamp and define dtDeviceSlam=0\\text{dt}_{\\text{Device}}^{\\text{Slam}} = 0dtDeviceSlam​=0. The same goes for the RGB camera : dtDeviceRGB=0\\text{dt}_{\\text{Device}}^{\\text{RGB}} = 0dtDeviceRGB​=0. Note that the next subsection describes in more details the temporal aspect of the image formation model.  For IMUs, the time offset is estimated in the factory by our camera-imu calibration process and is so that the following relation approximately holds (after IMU intrinsics compensation):  a^tDevice=a~(tDevice+dtDeviceAccel+0.5Δt)\\hat{a}_{t_{\\text{Device}}} = \\tilde{a}\\left(t_{\\text{Device}} + \\text{dt}_{\\text{Device}}^{\\text{Accel}} + 0.5\\Delta_t \\right)a^tDevice​​=a~(tDevice​+dtDeviceAccel​+0.5Δt​)  where tDevicet_{\\text{Device}}tDevice​ is a timestamp in the device time domain, a^tDevice\\hat{a}_{t_{\\text{Device}}}a^tDevice​​ is an estimate of the true acceleration at the instant represented by the timestamp tDevicet_{\\text{Device}}tDevice​, dtDeviceAccel\\text{dt}_{\\text{Device}}^{\\text{Accel}}dtDeviceAccel​ is the estimate of the time offset, Δt\\Delta_tΔt​ is the sampling period of the sensor. Finally, the operator t→a~(t)t\\xrightarrow{}\\tilde{a}(t)t​a~(t) is a temporal interpolation of the compensated IMU sample time series (tk,a~k)(t_k, \\tilde{a}_k)(tk​,a~k​). Note the appearance of 0.5Δt0.5\\Delta_t0.5Δt​ in previous equation, which stems from internal implementation choice. The same relation exists for the gyrometer.  For magnetometer, we estimate dtDeviceMag\\text{dt}_{\\text{Device}}^{\\text{Mag}}dtDeviceMag​ to be around +5ms+5ms+5ms with the following relation:  B^tDevice=B~(tDevice+dtDeviceMag)\\hat{B}_{t_{\\text{Device}}} = \\tilde{B}\\left(t_{\\text{Device}} + \\text{dt}_{\\text{Device}}^{\\text{Mag}} \\right)B^tDevice​​=B~(tDevice​+dtDeviceMag​)  Where notation are similar as above with B^\\hat{B}B^ representing the magnetic field vector.  For barometer, audio signal and GPS data, such an offset is undetermined.  ","version":"Next","tagName":"h3"},{"title":"Images formation temporal model: rolling shutter and PLS artifact​","type":1,"pageTitle":"Temporal Alignment of Aria Sensor Data","url":"/projectaria_tools/docs/tech_insights/temporal_alignment_of_sensor_data#images-formation-temporal-model-rolling-shutter-and-pls-artifact","content":" In practice, the images obtained from our environment facing camera are not well described as captured at a unique timestamp for the most demanding applications. First, the regular exposure duration window can range from 19us up to 14ms, second the RGB sensor has a rolling shutter, where each row is being captured at different time per design, finally the SLAM cameras, even if specified as global shutter sensors are impacted by a row-dependent parasitic light sensitivity: a proportion of the photons forming the image are captured outside of the regular exposure window. This proportion depends on the row.  For the RGB sensor, we characterize the rolling shutter behavior through the read-out time Δdtreadout\\Delta \\text{dt}_{\\text{readout}}Δdtreadout​, we define is as the time between the readout of the first row and the readout of the last row. This time depends on the sensor binning configuration, that can differ on a recording basis. The readout time is specified to be 16.26ms16.26ms16.26ms for the full resolution (2880×28802880\\times28802880×2880) and 5ms5ms5ms for the binned/cropped configuration (1408×14081408\\times14081408×1408). We can account for rolling shutter by assigning a center of exposure timestamp to each pixel with the following formulae:  tDevice(p)=tDeviceImage+(row(p)H−0.5)⋅Δdtreadoutt_{\\text{Device}}(\\boldsymbol{p}) = t_{\\text{Device}}^{\\text{Image}} + \\left(\\frac{\\text{row}(\\boldsymbol{p})}{H} - 0.5\\right) \\cdot \\Delta \\text{dt}_{\\text{readout}}tDevice​(p)=tDeviceImage​+(Hrow(p)​−0.5)⋅Δdtreadout​  Where tDevice(p)t_{\\text{Device}}(\\boldsymbol{p})tDevice​(p) is the timestamp of the observation at pixel p\\boldsymbol{p}p, tDeviceImaget_{\\text{Device}}^{\\text{Image}}tDeviceImage​ is the device timestamp of the image assigned by the MCU, p→row(p)\\boldsymbol{p}\\xrightarrow{}\\text{row}(\\boldsymbol{p})p​row(p) is the projection of the pixel coordinate on the image dimension aligned with sensor rows, HHH is the size of the image along this dimension, Δdtreadout\\Delta \\text{dt}_{\\text{readout}}Δdtreadout​ is the readout time value mentioned above.  For even more demanding applications, one might need to compensate timestamp of pixels observation for Slam cameras too. On Aria camera sensors, readout of the image is done row by row. Each row can still accumulate charges after the regular exposure time and until it is fully read out, an effect sometimes called parasitic light sensitivity (PLS). The readout time of the last row Δdtpls\\Delta \\text{dt}_{\\text{pls}}Δdtpls​, i.e. the time a pixel on the last row still accumulates electrons before being discharged is specified by the manufacturer as being 9.12ms. The ratio SplsS_{\\text{pls}}Spls​ of the sensitivity during readout over the sensitivity during regular exposure was estimated to be ~0.01, instead of the ideal 0 value. From this, it results that when dealing with pixel observation time, it might be necessary to take effect into account by assigning to each pixel their effective center of exposure, we suggest the the following formulae:  tDevice(p)=tDeviceImage+12⋅r(p)⋅Spls⋅1+re(p)1+re(p)⋅Splst_{\\text{Device}}(\\boldsymbol{p}) = t_{\\text{Device}}^{\\text{Image}} + \\frac{1}{2} \\cdot r\\left(\\boldsymbol{p}\\right) \\cdot S_{\\text{pls}} \\cdot \\frac{1 + r_e\\left({\\boldsymbol{p}}\\right)}{1 + r_e\\left({\\boldsymbol{p}}\\right) \\cdot S_{\\text{pls}}}tDevice​(p)=tDeviceImage​+21​⋅r(p)⋅Spls​⋅1+re​(p)⋅Spls​1+re​(p)​  Where tDevice(p)t_{\\text{Device}}(\\boldsymbol{p})tDevice​(p) represents the time of the pixel observation (effective center of exposure), r(p)r\\left({\\boldsymbol{p}}\\right)r(p) represents the readout time of the current pixel p\\boldsymbol{p}p and is computed as Δdtplsrow(p)H\\Delta \\text{dt}_{\\text{pls}} \\frac{\\text{row}(\\boldsymbol{p})}{H}Δdtpls​Hrow(p)​, SplsS_{\\text{pls}}Spls​ represents the sensitivity ratio of the readout phase over the exposure phase, re(p)r_e\\left({\\boldsymbol{p}}\\right)re​(p) represents the ratio of the current row readout time over the regular exposure duration. ","version":"Next","tagName":"h3"},{"title":"Project Aria Device Calibration","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/tech_spec/device_calibration","content":"","keywords":"","version":"Next"},{"title":"Further resources​","type":1,"pageTitle":"Project Aria Device Calibration","url":"/projectaria_tools/docs/tech_spec/device_calibration#further-resources","content":" Go to the Project Aria FAQ for more calibration information and resources. ","version":"Next","tagName":"h3"},{"title":"IMU Noise Model for Project Aria Data","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/tech_insights/imu_noise_model","content":"IMU Noise Model for Project Aria Data In our visual-inertial fusion algorithm, we model, as traditionally done, the stochastic part of the IMU error as including three components: Turn-on biasBias random walkWhite noise The noise along any of the axis of the inertial sensor is described as the sum of those contributions: xksampled=xktrue+nk0turn-on bias+nkbias random walk+nkwhite noise x^{\\text{sampled}}_k = x^{\\text{true}}_k + n^{\\text{turn-on bias}}_{k0} + n^{\\text{bias random walk}}_k + n^{\\text{white noise}}_kxksampled​=xktrue​+nk0turn-on bias​+nkbias random walk​+nkwhite noise​ Where: xktruex^{\\text{true}}_kxktrue​ is the real value of the quantity measured projected on the sensor sensitive axis.xksampledx^{\\text{sampled}}_kxksampled​ is the sampled value seen as a random variable.nk0turn-on biasn^{\\text{turn-on bias}}_{k0}nk0turn-on bias​ is a random variable draw from a Gaussian when the device is turned-on (denoted here as step k0k_0k0​).nkwhite noisen^{\\text{white noise}}_{k}nkwhite noise​ is the time-independent noise components and draw from a 0-centered Gaussian at each step kkk. The covariance of the Gaussian is usually characterized by the continuous noise σc\\sigma_cσc​ strength, which needs to be multiplied by the bandwidth (BW)h in Hz to get the distribution of the sample noise: σc2⋅BW\\sigma^2_c \\cdot BWσc2​⋅BW.σc\\sigma_cσc​ is sometimes called Angle Random walk in rad/s/Hzrad/s/\\sqrt{Hz}rad/s/Hz​ or rad/srad/\\sqrt{s}rad/s​ for the gyrometer and the Velocity Random Walk in m/s2/Hzm/s^2/\\sqrt{Hz}m/s2/Hz​ or m/s/(s)m/s/\\sqrt(s)m/s/(​s) for the accelerometer. nkbias random walkn^{\\text{bias random walk}}_knkbias random walk​ is drawn from a random walk process. The parameters describing those noises can be derived from an Allan Variance plot computed from data collected over a period of 24 hours in a temperature stable environment. Table 1: White noise and bias instability parameters of Aria IMU sensors (1 gee = 9.81m/s^2) accel-left\taccel-right\tgyro-left\tgyro-rightwhite noise (σc\\sigma_cσc​)\t0.9×10−4 gees/Hz0.9\\times 10^{-4}\\ \\text{gees}/\\sqrt{Hz}0.9×10−4 gees/Hz​\t0.8×10−4 gees/Hz0.8\\times 10^{-4}\\ \\text{gees}/\\sqrt{Hz}0.8×10−4 gees/Hz​\t5×10−3 dps/Hz5\\times 10^{-3}\\ \\text{dps}/\\sqrt{Hz}5×10−3 dps/Hz​\t1×10−2 dps/Hz1\\times10^{-2}\\ \\text{dps}/\\sqrt{Hz}1×10−2 dps/Hz​ bias instability\t2.8×10−5 gees2.8\\times 10^{-5}\\ \\text{gees}2.8×10−5 gees\t3.5×10−5 gees3.5\\times 10^{-5}\\ \\text{gees}3.5×10−5 gees\t1×10−3 dps1\\times 10^{-3}\\ \\text{dps}1×10−3 dps\t1.3×10−3 dps1.3\\times10^{-3}\\ \\text{dps}1.3×10−3 dps The figure below shows the Allan Variance plot supporting this measurement. Figure 1: Allan Variance plot computed from data collected over 24 hours in a temperature stable environment With our sensor configuration for bandwidth, this leads to the following values for the sample noise: Table 2: Sample noise of Aria IMU sensors accel-left (BW: 343Hz)\taccel-right (BW: 353Hz)\tgyro-left (BW:300Hz)\tgyro-right (BW:116Hz)σkwhite noise\\sigma^{\\text{white noise}}_kσkwhite noise​\t16×10−3 m/s216\\times 10^{-3}\\ m/s^216×10−3 m/s2\t15×10−3 m/s215\\times 10^{-3}\\ m/s^215×10−3 m/s2\t1.5×10−3 rad/s1.5\\times10^{-3}\\ \\text{rad/s}1.5×10−3 rad/s\t1.8×10−3 rad/s1.8\\times10^{-3}\\ \\text{rad/s}1.8×10−3 rad/s The data duration used for Allan Variance was not enough to capture the bias random walk confidently. This is because real MEMS sensors are not that well modelled by this stochastic model. To address this, we tune the parameter of the bias random walk used for sensor-fusion. We start from the bias instability measured on the Allan Variance of the sensor (the floor of the curve) and inflate it by a tuning factor.","keywords":"","version":"Next"},{"title":"Project Aria Hardware Specifications","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/tech_spec/hardware_spec","content":"","keywords":"","version":"Next"},{"title":"Sensor specifications​","type":1,"pageTitle":"Project Aria Hardware Specifications","url":"/projectaria_tools/docs/tech_spec/hardware_spec#sensor-specifications","content":" ","version":"Next","tagName":"h2"},{"title":"Visual sensors​","type":1,"pageTitle":"Project Aria Hardware Specifications","url":"/projectaria_tools/docs/tech_spec/hardware_spec#visual-sensors","content":" The following table summarizes the specs for the five glasses on Aria glasses (two Mono Scene/SLAM, one RGB, and two ET cameras).  Table 1: Aria Glasses Camera Specs  Camera\tHFOV (deg)\tVFOV (deg)\tIFOV (deg/pix)\tMaximum resolution (pix)\tDownsampled resolution (pix)\tMax frame rate (FPS)\tNominal frame rate (FPS)\tShutterMono Scene (x2)\t150\t120\t0.26\t640x480\t-\t30\t10\tglobal RGB (x1)\t110\t110\t0.038\t2880x2880\t1408x1408\t30\t10\trolling ET (x2)\t64\t48\t0.2\t640x480\t320x240\t90\t10\tglobal  note Cameras on Project Aria devices are installed sideways. Project Aria Tools visualizers rotate the images to show a more natural view.  ","version":"Next","tagName":"h3"},{"title":"Non-visual sensors​","type":1,"pageTitle":"Project Aria Hardware Specifications","url":"/projectaria_tools/docs/tech_spec/hardware_spec#non-visual-sensors","content":" The non-visual sensors in Aria glasses are:  Two IMUs operating at 1000Hz and 800Hz respectivelyOne Magnetometer operating at 10HzOne barometer operating at 50HzSeven-channel spatial microphone array with a sampling rate of 48kHz The microphone also has a stereo mode where only two channels record One GPS receiver, Wi-Fi beacon, and Bluetooth beacon.  All cameras, as well as the IMU, magnetometer, barometer and microphone are calibrated and all sensor measurements are timestamped on a common clock at nanosecond resolution. The SLAM and RGB cameras have fisheye lenses to maximize the visible field of view.  ","version":"Next","tagName":"h3"},{"title":"Coordinate systems​","type":1,"pageTitle":"Project Aria Hardware Specifications","url":"/projectaria_tools/docs/tech_spec/hardware_spec#coordinate-systems","content":" Applications like stereo vision and navigation usually handle 2D and 3D points in different spaces, and transformations need to be conducted between them. With Project Aria data, we attach a local R3 coordinate frame to each sensor.    Figure 2: Sensors and Sensor Directions on Project Aria Devices  To find out more about the coordinate systems and access calibration data, go to:  2D Image Coordinate System Conventions3D Coordinate Frame ConventionsCalibration Code Snippets  ","version":"Next","tagName":"h3"},{"title":"Other hardware specifications​","type":1,"pageTitle":"Project Aria Hardware Specifications","url":"/projectaria_tools/docs/tech_spec/hardware_spec#other-hardware-specifications","content":" ","version":"Next","tagName":"h2"},{"title":"Compute​","type":1,"pageTitle":"Project Aria Hardware Specifications","url":"/projectaria_tools/docs/tech_spec/hardware_spec#compute","content":" Qualcomm SD835, 4GB RAM, 128GB storageFlash memory (UFS)Android 7.1SW configurable user button and switch  ","version":"Next","tagName":"h3"},{"title":"Weight & Size​","type":1,"pageTitle":"Project Aria Hardware Specifications","url":"/projectaria_tools/docs/tech_spec/hardware_spec#weight--size","content":" 75g in two sizes 147mm and 152mm frame width, with adjustable nose pads and temple arms (87% fit coverage).  ","version":"Next","tagName":"h3"},{"title":"Visual Correction​","type":1,"pageTitle":"Project Aria Hardware Specifications","url":"/projectaria_tools/docs/tech_spec/hardware_spec#visual-correction","content":" Removable lenses with plano (Non-Rx) or single vision Rx correction [-4.5D to +3.5D].  ","version":"Next","tagName":"h3"},{"title":"Battery Life​","type":1,"pageTitle":"Project Aria Hardware Specifications","url":"/projectaria_tools/docs/tech_spec/hardware_spec#battery-life","content":" Capacity is 2.5Wh. Operating time depends on the recording profile. Battery life is 1.5 hours of continuous recording + 30 hours standby when using profile 0:  10 FPS ET x 210 FPS Mono Scene (SLAM) x 21 FPS RGB 8MPIMU, Wi-Fi + GPS and 7-channel audio on at nominal FPS  Aria glasses connects to USB via a magnetic connector on the right temple arm. ","version":"Next","tagName":"h3"},{"title":"Project Aria Recording Profiles","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/tech_spec/recording_profiles","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Project Aria Recording Profiles","url":"/projectaria_tools/docs/tech_spec/recording_profiles#overview","content":" Project Aria glasses have multiple recording profiles that enable users to choose what sensors to record with and what settings to use. Aria glasses recording profiles can vary by:  Sub-selection of sensor streamsRGB and ET (Eye Tracking) camera resolutionMono Scene (often, but not exclusively used for SLAM), RGB and ET camera frame rate and auto-exposure settingsImage stream formatNumber of audio channels: all (7) v.s. stereo(2)  The table below provides a detailed spec of each profile we currently support. We add new profiles when necessary.  This page covers:  Recording profile specsGeneral guidance if you're making recordingsHow to make custom recordings with the Mobile Companion app    ","version":"Next","tagName":"h2"},{"title":"Recording Profile Specs​","type":1,"pageTitle":"Project Aria Recording Profiles","url":"/projectaria_tools/docs/tech_spec/recording_profiles#recording-profile-specs","content":" The following table provides a detailed spec of each profile we currently support. Keep in mind the following requirements when selecting a recording profile:  If Wi-Fi data is being recorded (wifiScanModeActive), you won't be able to create TICSync recordings with that profileTo request SLAM (or Multi-SLAM) MPS, you'll need Monoscene/SLAM cameras + IMU enabledTo request Eye Gaze MPS, you'll need Eye Tracking cameras enabledTo request Hand Tracking MPS, you'll need Monoscene/SLAM camers enabled   Microphones ET Cameras RGB Cameras SLAM (Mono Scene Cameras) GPS IMU 1 IMU 2 Magnetometer Barometer Wi-Fi Bluetooth Profile\tChannels Sample Rate (kHz)\tResolution FPS\tImage Format\tResolution FPS\tAuto Exposure\tImage Format\tResolution FPS\tAuto Exposure\tImage Format\tData Rate (Hz)\tData Rate (Hz)\tData Rate (Hz)\tData Rate (Hz)\tData Rate (Hz)\tScan Duration(s)\tScan Duration(s)0\t7 48 320x240 10 JPEG 2880x2880 1 ON JPEG 640x480 10 ON JPEG 1 1000 800 10 50 10 10 2\t- - - - - 1408x1408 20 ON JPEG 640x480 20 ON JPEG 1 1000 800 10 50 10 10 4\t7 48 - - - 1408x1408 10 ON JPEG - - - - 1 1000 800 10 50 - - 5\t- - 640x480 20 JPEG 1408x1408 20 ON JPEG - - - - - 1000 800 - - - - 7\t7 48 - - - 1408x1408 10 ON RAW - - - - 1 1000 800 10 50 - - 8\t7 48 320x240 30 JPEG 1408x1408 5 ON JPEG 640x480 15 ON JPEG - 1000 800 10 50 - - 9\t7 48 320x240 10 JPEG 1408x1408 20 ON JPEG 640x480 10 ON JPEG - 1000 800 10 50 - - 10\t7 48 320x240 10 JPEG 1408x1408 10 ON JPEG 640x480 10 ON JPEG 1 1000 800 10 50 10 10 12\t- - 320x240 10 JPEG 1408x1408 10 ON JPEG 640x480 10 ON JPEG - 1000 800 10 50 - - 14\t- - 320x240 10 JPEG 1408x1408 1 ON JPEG 640x480 30 ON JPEG 1 1000 800 10 50 - - 15\t7 48 320x240 10 JPEG 1408x1408 30 ON JPEG 640x480 30 ON JPEG - 1000 800 10 50 - -    Microphones ET Cameras RGB Cameras SLAM (Mono Scene Cameras) GPS IMU 1 IMU 2 Magnetometer Barometer Wi-Fi Bluetooth Profile\tChannels Sample Rate (kHz)\tResolution FPS\tImage Format\tResolution FPS\tAuto Exposure\tImage Format\tResolution FPS\tAuto Exposure\tImage Format\tData Rate (Hz)\tData Rate (Hz)\tData Rate (Hz)\tData Rate (Hz)\tData Rate (Hz)\tScan Duration(s)\tScan Duration(s)16\t2 48 640x480 90 JPEG 1408x1408 10 ON JPEG - - - - - 1000 800 10 50 - - 18\t7 48 320x240 10 JPEG 1408x1408 10 ON JPEG 640x480 10 ON JPEG - 1000 800 10 50 - - 19\t- - - - - 1408x1408 10 ON JPEG 640x480 10 ON JPEG 1 1000 800 10 50 10 10 20\t2 48 - - - - - - - - - - - - 1000 800 - - - - 21\t7 48 320x240 30 JPEG 1408x1408 15 ON JPEG 640x480 15 ON JPEG - 1000 800 10 50 - - 22\t7 48 320x240 10 JPEG 1408x1408 30 ON JPEG 640x480 10 ON JPEG - 1000 800 10 50 - - 23\t7 48 320x240 10 JPEG 1408x1408 30 ON JPEG 640x480 10 ON JPEG - 1000 800 10 50 - - 24\t- - - - - 2880x2880 10 ON JPEG 640x480 10 ON JPEG - 1000 800 10 50 - - 25\t- - - - - 1408x1408 10 ON JPEG 640x480 20 ON JPEG 1 1000 800 - - 10 10 26\t2 48 - - - 2880x2880 1 ON JPEG - - - - 1 - - - - - - 27\t7 48 320x240 10 JPEG 1408x1408 10 ON JPEG 640x480 10 ON JPEG - 1000 800 10 50 10 - 28\t7 48 320x240 60 JPEG 1408x1408 30 ON JPEG 640x480 30 ON JPEG - 1000 800 10 50 - - 29\t2 48 - - - 2880x2880 1/0.1 ON JPEG with decimation - - - - 1 - - - - - -   The Decimated JPEG outputs for profile29 means that 9 out 10 JPEG frames are skipped, creating an equivalent frame rate of 0.1FPS. The aim is to create a recording profile that can support 15+ recording hours.  ","version":"Next","tagName":"h2"},{"title":"Viewing recording profile details on device​","type":1,"pageTitle":"Project Aria Recording Profiles","url":"/projectaria_tools/docs/tech_spec/recording_profiles#viewing-recording-profile-details-on-device","content":" There are several ways you can view recording profile information if you have access to Project Aria glasses  Mobile Companion app​  Open the Mobile Companion App and select New Recording SessionTap Recording ProfileSelect More Info in the top right corner The More Info page will show all recording profiles as well as the current settings for the Custom Profile    Project Aria Client SDK​  If you've set up your glasses with the Client SDK you can export recording profiles to a JSON file.  ","version":"Next","tagName":"h3"},{"title":"General guidance if you're making recordings​","type":1,"pageTitle":"Project Aria Recording Profiles","url":"/projectaria_tools/docs/tech_spec/recording_profiles#general-guidance-if-youre-making-recordings","content":" These are some sensor profiles researchers have found useful for particular kinds of research. Commonly used recording profiles are Profiles 0, 2, 5, 9, 10, 15 and 23.  ","version":"Next","tagName":"h2"},{"title":"If you’re not sure what you want​","type":1,"pageTitle":"Project Aria Recording Profiles","url":"/projectaria_tools/docs/tech_spec/recording_profiles#if-youre-not-sure-what-you-want","content":" Profile10 is interesting to explore, it gathers data with all sensors and the RGB Camera records at 10 fps. All sensor data is useful for exploring multimodal ML models.  If you need high RGB Resolution (2880x2880 rather than 1408x1408), and 1FPS is sufficient shutter speed, use Profile0.  ","version":"Next","tagName":"h3"},{"title":"If you're streaming data​","type":1,"pageTitle":"Project Aria Recording Profiles","url":"/projectaria_tools/docs/tech_spec/recording_profiles#if-youre-streaming-data","content":" While you can use any recording profile when streaming, we recommend only using Profiles 12 and 18, which are optimized for streaming.  ","version":"Next","tagName":"h3"},{"title":"If you need a high frame rate​","type":1,"pageTitle":"Project Aria Recording Profiles","url":"/projectaria_tools/docs/tech_spec/recording_profiles#if-you-need-a-high-frame-rate","content":" Use Profiles 2, 9 or 15, depending on whether you want EyeTracking or GPS. Profile2 does not have ET, Profile15 does not have GPS.  ","version":"Next","tagName":"h3"},{"title":"If your research focuses on audio​","type":1,"pageTitle":"Project Aria Recording Profiles","url":"/projectaria_tools/docs/tech_spec/recording_profiles#if-your-research-focuses-on-audio","content":" Try Profiles 4, 7 (no SLAM) or 10.  ","version":"Next","tagName":"h3"},{"title":"To avoid image pre-processing​","type":1,"pageTitle":"Project Aria Recording Profiles","url":"/projectaria_tools/docs/tech_spec/recording_profiles#to-avoid-image-pre-processing","content":" In situations where you want to use RAW videos and skip the Image Sensor Processor (ISP) as much as possible, Profile7 is helpful.  Please note, because Profile7 delivers RAW image files, not JPEGs the data is 8x more costly to store. This profile also uses more energy while recording and may heat up faster than others.  ","version":"Next","tagName":"h3"},{"title":"Long duration recordings​","type":1,"pageTitle":"Project Aria Recording Profiles","url":"/projectaria_tools/docs/tech_spec/recording_profiles#long-duration-recordings","content":"   Profiles 20 and 26 are optimized for 5+ recording hours. Profile 29 is optimized for recording times over 15 hours and uses JPEG decimation to provide 0.1FPS RGB data.  ","version":"Next","tagName":"h3"},{"title":"How to make custom recordings with the Mobile Companion app​","type":1,"pageTitle":"Project Aria Recording Profiles","url":"/projectaria_tools/docs/tech_spec/recording_profiles#how-to-make-custom-recordings-with-the-mobile-companion-app","content":" The custom profile feature enables users to alter the most commonly used sensor configurations. Further customizations could be added, such as auto-exposure or whether to record images as JPEG or RAW, if there is sufficient demand. Please contact us using one of our support channels if you have any feature requests.  Some custom configurations are restricted, to protect the device or quality of recording:  FPS across cameras must be the same, or multiples of each other for the glasses to record.[Warning] RGB Cameras at 2880 x 2880 at 15-20 fps will cause elevated thermal loads and reduced battery lifeTo ensure device stability, recordings at 2880x2880 at 30fps are not allowed  To set a Custom Recording Profile:  In the Mobile Companion app Dashboard, select New Recording SessionSelect Recording ProfileThe first recording profile in the list will be Custom Profile You may need to scroll up to see this option Select the Custom ProfileSelect Edit parametersAdjust sensor settings Tap More info on the Recording Profiles Tab to see more details, such as auto exposure ","version":"Next","tagName":"h2"},{"title":"Camera Intrinsic Models for Project Aria devices","type":0,"sectionRef":"#","url":"/projectaria_tools/docs/tech_insights/camera_intrinsic_models","content":"","keywords":"","version":"Next"},{"title":"The linear camera model​","type":1,"pageTitle":"Camera Intrinsic Models for Project Aria devices","url":"/projectaria_tools/docs/tech_insights/camera_intrinsic_models#the-linear-camera-model","content":" The linear camera model (a.k.a pinhole model) is parametrized by 4 coefficients : f_x, f_y, c_x, c_y.  (fx,fy)(f_x, f_y)(fx​,fy​) are the focal lengths, and cx,cyc_x, c_ycx​,cy​ are the coordinate of the projection of the optical axis. It maps from world point (x,y,z)(x,y,z)(x,y,z) to 2D camera pixel p=(u,v)\\mathbf{p}=(u, v)p=(u,v) with the following formulae.  u=fxx/z+cxv=fyy/z+cy u = f_x x/z + c_x \\\\ v = f_y y/z + c_yu=fx​x/z+cx​v=fy​y/z+cy​  Or, in polar coordinates:  u=fxtan(θ)cos⁡(φ)+cx,v=fytan(θ)sin⁡(φ)+cy. u = f_x tan(\\theta) \\cos(\\varphi) + c_x, \\\\ v = f_y tan(\\theta) \\sin(\\varphi) + c_y.u=fx​tan(θ)cos(φ)+cx​,v=fy​tan(θ)sin(φ)+cy​.  Inversely, we can unproject from 2D camera pixel p=(u,v)\\mathbf{p}=(u, v)p=(u,v) to the homogeneous coordinate of the world point by  x/z=(u−cx)/fx,y/z=(v−cy)/fy.x/z=(u-c_x)/f_x, \\\\ y/z=(v-c_y)/f_y.x/z=(u−cx​)/fx​,y/z=(v−cy​)/fy​.  The linear camera model preserves linearity in 3D space, thus straight lines in the real world are supposed to look straight under the linear camera model.  ","version":"Next","tagName":"h2"},{"title":"The spherical camera model​","type":1,"pageTitle":"Camera Intrinsic Models for Project Aria devices","url":"/projectaria_tools/docs/tech_insights/camera_intrinsic_models#the-spherical-camera-model","content":" The spherical camera model is, similarly from the linear camera model parametrized by 4 coefficients : f_x, f_y, c_x, c_y. The pixel coordinates are linear to solid angles rather than the homography coordinate system. The projection function can be written in polar coordinates  u=fxθcos⁡(φ)+cx,v=fyθsin⁡(φ)+cy. u = f_x \\theta \\cos(\\varphi) + c_x, \\\\ v = f_y \\theta \\sin(\\varphi) + c_y.u=fx​θcos(φ)+cx​,v=fy​θsin(φ)+cy​.  Note the difference from the linear camera model — under spherical projection, 3D straight lines look curved in images.  Inversely, we can unproject from 2D camera pixel p=(u,v)\\mathbf{p}=(u, v)p=(u,v) to the homogeneous coordinate of the world point by  θ=(u−cx)2/fx2+(v−cy)2/fy2,φ=arctan⁡((u−cx)/fx,(v−cy)/fy). \\theta = \\sqrt{(u - c_x)^2/f_x^2 + (v - c_y)^2/f_y^2}, \\\\ \\varphi = \\arctan((u - c_x)/f_x, (v - c_y)/f_y).θ=(u−cx​)2/fx2​+(v−cy​)2/fy2​​,φ=arctan((u−cx​)/fx​,(v−cy​)/fy​).  ","version":"Next","tagName":"h2"},{"title":"The KannalaBrandtK3 (KB3) model​","type":1,"pageTitle":"Camera Intrinsic Models for Project Aria devices","url":"/projectaria_tools/docs/tech_insights/camera_intrinsic_models#the-kannalabrandtk3-kb3-model","content":" The KannalaBrandtK3 model adds radial distortion to the linear model  u=fxr(θ)cos⁡(φ)+cx,v=fyr(θ)sin⁡(φ)+cy. u = f_x r(\\theta) \\cos(\\varphi) + c_x, \\quad v = f_y r(\\theta) \\sin(\\varphi) + c_y.u=fx​r(θ)cos(φ)+cx​,v=fy​r(θ)sin(φ)+cy​.  where  r(θ)=θ+k0θ3+k1θ5+k2θ7+k3θ9+... r(\\theta) = \\theta + k_0 \\theta^3 + k_1 \\theta^5 + k_2 \\theta^7 + k_3 \\theta^9 + ...r(θ)=θ+k0​θ3+k1​θ5+k2​θ7+k3​θ9+...  In KannalaBrandtK3 model we use a 9-th order polynomial with four radial distortion parameters k0,...k3k_0, ... k_3k0​,...k3​.  To unproject from camera pixel (u,v)(u, v)(u,v) to the world point (θ,φ)(\\theta, \\varphi)(θ,φ), we first compute  φ=arctan⁡((u−cx)/fx,(v−cy)/fy)r(θ)=(u−cx)2/fx2+(v−cy)2/fy2 \\varphi = \\arctan((u - c_x)/f_x, (v - c_y)/f_y) \\\\ r(\\theta) = \\sqrt{(u - c_x)^2/f_x^2 + (v - c_y)^2/f_y^2}φ=arctan((u−cx​)/fx​,(v−cy​)/fy​)r(θ)=(u−cx​)2/fx2​+(v−cy​)2/fy2​​  Then we use Newton method to inverse the function r(θ)r(\\theta)r(θ) to compute θ\\thetaθ. See the code here.  ","version":"Next","tagName":"h2"},{"title":"The Fisheye62 model​","type":1,"pageTitle":"Camera Intrinsic Models for Project Aria devices","url":"/projectaria_tools/docs/tech_insights/camera_intrinsic_models#the-fisheye62-model","content":" The Fisheye62 model adds tangential distortion on top of the KB3 model parametrized by two new coefficients: p_0 p_1.  u=fx.(ur+tx(ur,vr))+cx,v=fy.(vr+ty(ur,vr))+cy. u = f_x . (u_r + t_x(u_r, v_r)) + c_x, \\\\ v = f_y . (v_r + t_y(u_r, v_r)) + c_y.u=fx​.(ur​+tx​(ur​,vr​))+cx​,v=fy​.(vr​+ty​(ur​,vr​))+cy​.  where  ur=r(θ)cos⁡(φ),vr=r(θ)sin⁡(φ). u_r = r(\\theta) \\cos(\\varphi), \\\\ v_r = r(\\theta) \\sin(\\varphi).ur​=r(θ)cos(φ),vr​=r(θ)sin(φ).  and  tx(ur,vr)=p0(2ur2+r(θ)2)+2p1urvr,ty(ur,vr)=p1(2vr2+r(θ)2)+2p0urvr. t_x(u_r, v_r) = p_0(2 u_r^2 + r(\\theta)^2) + 2p_1u_rv_r, \\\\ t_y(u_r, v_r) = p_1(2 v_r^2 + r(\\theta)^2) + 2p_0u_rv_r.tx​(ur​,vr​)=p0​(2ur2​+r(θ)2)+2p1​ur​vr​,ty​(ur​,vr​)=p1​(2vr2​+r(θ)2)+2p0​ur​vr​.  To unproject from camera pixel (u,v)(u, v)(u,v) to the world point (θ,φ)(\\theta, \\varphi)(θ,φ), we first use Newton method to compute uru_rur​ and vrv_rvr​ from (u−cx)/fx(u - c_x)/f_x(u−cx​)/fx​ and (v−cy)/fy(v - cy)/f_y(v−cy)/fy​, and then compute (θ,φ)(\\theta, \\varphi)(θ,φ) using the above KB3 unproject method.  ","version":"Next","tagName":"h2"},{"title":"The FisheyeRadTanThinPrism (Fisheye624) model​","type":1,"pageTitle":"Camera Intrinsic Models for Project Aria devices","url":"/projectaria_tools/docs/tech_insights/camera_intrinsic_models#the-fisheyeradtanthinprism-fisheye624-model","content":" The FisheyeRadTanThinPrism (also called Fisheye624 in file and codebase) models thin-prism distortion (noted tptptp) on top of the Fisheye62 model above. Its parametrization contains 4 additional coefficients: s_0 s_1 s_2 s_3. The projection function writes:  u=fx⋅(ur+tx(ur,vr)+tpx(ur,vr))+cx,v=fy⋅(vr+ty(ur,vr)+tpy(ur,vr))+cy. u = f_x \\cdot (u_r + t_x(u_r, v_r) + tp_x(u_r, v_r)) + c_x, \\\\ v = f_y \\cdot (v_r + t_y(u_r, v_r) + tp_y(u_r, v_r)) + c_y.u=fx​⋅(ur​+tx​(ur​,vr​)+tpx​(ur​,vr​))+cx​,v=fy​⋅(vr​+ty​(ur​,vr​)+tpy​(ur​,vr​))+cy​.  u_r, v_r, t_x, t_y are defined as in the Fisheye62 model, while tpxtp_xtpx​ and tpytp_ytpy​ are defined as:  tpx(ur,vr)=s0r(θ)2+s1r(θ)4,tpy(ur,vr)=s2r(θ)2+s3r(θ)4. tp_x(u_r, v_r) = s_0 r(\\theta)^2 + s_1 r(\\theta)^4, \\\\ tp_y(u_r, v_r) = s_2 r(\\theta)^2 + s_3 r(\\theta)^4.tpx​(ur​,vr​)=s0​r(θ)2+s1​r(θ)4,tpy​(ur​,vr​)=s2​r(θ)2+s3​r(θ)4.  To unproject from camera pixel (u,v)(u, v)(u,v) to the world point (θ,φ)(\\theta, \\varphi)(θ,φ), we first use Newton method to compute uru_rur​ and vrv_rvr​ from (u−cx)/fx(u - c_x)/f_x(u−cx​)/fx​ and (v−cy)/fy(v - cy)/f_y(v−cy)/fy​, and then compute (θ,φ)(\\theta, \\varphi)(θ,φ) using the above KB3 unproject method.  Note that in practice, in our codebase and calibration file we assume fxf_xfx​ and fyf_yfy​ are equal. ","version":"Next","tagName":"h2"}]