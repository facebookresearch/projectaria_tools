{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0347389",
   "metadata": {},
   "source": [
    "# MPS Tutorial\n",
    "This sample will show you how to use the Aria MPS data via the MPS apis.\n",
    "Please refer to the MPS wiki for more information about data formats and schemas\n",
    "\n",
    "### Notebook stuck?\n",
    "Note that because of Jupyter and Plotly issues, sometimes the code may stuck at visualization. We recommend **restart the kernels** and try again to see if the issue is resolved.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "163e63fa",
   "metadata": {},
   "source": [
    "## Download the MPS sample dataset locally\n",
    "> The sample dataset will get downloaded to a **tmp** folder by default. Please modify the path if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa38162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from urllib.request import urlretrieve\n",
    "from zipfile import ZipFile\n",
    "\n",
    "google_colab_env = 'google.colab' in str(get_ipython())\n",
    "if google_colab_env:\n",
    "    print(\"Running from Google Colab, installing projectaria_tools and getting sample data\")\n",
    "    !pip install projectaria-tools\n",
    "    mps_sample_path = \"./mps_sample_data/\"\n",
    "else:\n",
    "    mps_sample_path = \"/tmp/mps_sample_data/\"\n",
    "\n",
    "base_url = \"https://www.projectaria.com/async/sample/download/?bucket=mps&filename=\"\n",
    "os.makedirs(mps_sample_path, exist_ok=True)\n",
    "\n",
    "filenames = [\n",
    "    \"sample.vrs\",\n",
    "    \"trajectory.zip\",\n",
    "    \"eye_gaze_v3.zip\",\n",
    "    \"hand_tracking.zip\"]\n",
    "\n",
    "print(\"Downloading sample data\")\n",
    "for filename in tqdm(filenames):\n",
    "    print(f\"Processing: {filename}\")\n",
    "    full_path: str = os.path.join(mps_sample_path, filename)\n",
    "    urlretrieve(f\"{base_url}{filename}\", full_path)\n",
    "    if filename.endswith(\".zip\"):\n",
    "        with ZipFile(full_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(path=mps_sample_path)\n",
    "            if \"eye_gaze\" in filename:\n",
    "                eye_gaze_path = os.path.join(mps_sample_path, \"eye_gaze\")\n",
    "                os.makedirs(eye_gaze_path, exist_ok=True)\n",
    "                os.rename(os.path.join(mps_sample_path, \"general_eye_gaze.csv\"), os.path.join(eye_gaze_path, \"general_eye_gaze.csv\"))\n",
    "                os.rename(os.path.join(mps_sample_path, \"personalized_eye_gaze.csv\"), os.path.join(eye_gaze_path, \"personalized_eye_gaze.csv\"))\n",
    "                "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44984456",
   "metadata": {},
   "source": [
    "## Load the trajectory, point cloud and eye gaze using the MPS apis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c7c09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from projectaria_tools.core import data_provider, mps\n",
    "from projectaria_tools.core.mps.utils import (\n",
    "    filter_points_from_confidence,\n",
    "    get_gaze_vector_reprojection,\n",
    "    get_nearest_eye_gaze,\n",
    "    get_nearest_pose,\n",
    ")\n",
    "from projectaria_tools.core.stream_id import StreamId\n",
    "import numpy as np\n",
    "\n",
    "# Load the VRS file\n",
    "vrsfile = os.path.join(mps_sample_path, \"sample.vrs\")\n",
    "\n",
    "# Trajectory and global points\n",
    "closed_loop_trajectory = os.path.join(\n",
    "    mps_sample_path, \"trajectory\", \"closed_loop_trajectory.csv\"\n",
    ")\n",
    "global_points = os.path.join(mps_sample_path, \"trajectory\", \"global_points.csv.gz\")\n",
    "\n",
    "# Eye gaze\n",
    "generalized_eye_gaze_path = os.path.join(\n",
    "    mps_sample_path, \"eye_gaze\", \"general_eye_gaze.csv\"\n",
    ")\n",
    "calibrated_eye_gaze_path = os.path.join(\n",
    "    mps_sample_path, \"eye_gaze\", \"personalized_eye_gaze.csv\"\n",
    ")\n",
    "\n",
    "# Hand tracking\n",
    "wrist_and_palm_poses_path = os.path.join(\n",
    "    mps_sample_path, \"hand_tracking\", \"wrist_and_palm_poses.csv\"\n",
    ")\n",
    "\n",
    "# Create data provider and get T_device_rgb\n",
    "provider = data_provider.create_vrs_data_provider(vrsfile)\n",
    "# Since we want to display the position of the RGB camera, we are querying its relative location\n",
    "# from the device and will apply it to the device trajectory.\n",
    "T_device_RGB = provider.get_device_calibration().get_transform_device_sensor(\n",
    "    \"camera-rgb\"\n",
    ")\n",
    "\n",
    "## Load trajectory and global points\n",
    "mps_trajectory = mps.read_closed_loop_trajectory(closed_loop_trajectory)\n",
    "points = mps.read_global_point_cloud(global_points)\n",
    "\n",
    "## Load eyegaze\n",
    "generalized_eye_gazes = mps.read_eyegaze(generalized_eye_gaze_path)\n",
    "calibrated_eye_gazes = mps.read_eyegaze(calibrated_eye_gaze_path)\n",
    "\n",
    "## Load hand tracking\n",
    "wrist_and_palm_poses = mps.hand_tracking.read_wrist_and_palm_poses(\n",
    "    wrist_and_palm_poses_path\n",
    ")\n",
    "\n",
    "# Loaded data must be not empty\n",
    "assert(\n",
    "    len(mps_trajectory) != 0 and\n",
    "    len(points) != 0 and\n",
    "    len(generalized_eye_gazes) != 0 and\n",
    "    len(calibrated_eye_gazes) != 0 and\n",
    "    len(wrist_and_palm_poses) != 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f2d6248",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fc8fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Helper function to build the frustum\n",
    "def build_cam_frustum(transform_world_device):\n",
    "    points = (\n",
    "        np.array(\n",
    "            [[0, 0, 0], [0.5, 0.5, 1], [-0.5, 0.5, 1], [-0.5, -0.5, 1], [0.5, -0.5, 1]]\n",
    "        )\n",
    "        * 0.6\n",
    "    )\n",
    "    transform_world_rgb = transform_world_device @ T_device_RGB\n",
    "    points_transformed = transform_world_rgb @ points.transpose()\n",
    "    return go.Mesh3d(\n",
    "        x=points_transformed[0, :],\n",
    "        y=points_transformed[1, :],\n",
    "        z=points_transformed[2, :],\n",
    "        i=[0, 0, 0, 0, 1, 1],\n",
    "        j=[1, 2, 3, 4, 2, 3],\n",
    "        k=[2, 3, 4, 1, 3, 4],\n",
    "        showscale=False,\n",
    "        visible=False,\n",
    "        colorscale=\"jet\",\n",
    "        intensity=points[:, 2],\n",
    "        opacity=1.0,\n",
    "        hoverinfo=\"none\",\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e125bbdc",
   "metadata": {},
   "source": [
    "## Visualize the trajectory and point cloud in a 3D interactive plot\n",
    "* Load trajectory\n",
    "* Load global point cloud\n",
    "* Render dense trajectory (1Khz) as points.\n",
    "* Render subsampled 6DOF poses via camera frustum. Use calibration to transform RGB camera pose to world frame\n",
    "* Render subsampled point cloud\n",
    "\n",
    "_Please wait a minute for all the data to load. Zoom in to the point cloud and adjust your view. Then use the time slider to move the camera_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f26ead5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all world positions from the trajectory\n",
    "traj = np.empty([len(mps_trajectory), 3])\n",
    "for i in range(len(mps_trajectory)):\n",
    "    traj[i, :] = mps_trajectory[i].transform_world_device.translation()\n",
    "\n",
    "# Subsample trajectory for quick display\n",
    "skip = 1000\n",
    "mps_trajectory_subset = mps_trajectory[::skip]\n",
    "steps = [None]*len(mps_trajectory_subset)\n",
    "\n",
    "# Load each pose as a camera frustum trace\n",
    "cam_frustums = [None]*len(mps_trajectory_subset)\n",
    "\n",
    "for i in range(len(mps_trajectory_subset)):\n",
    "    pose = mps_trajectory_subset[i]\n",
    "    cam_frustums[i] = build_cam_frustum(pose.transform_world_device)\n",
    "    timestamp = pose.tracking_timestamp.total_seconds()\n",
    "    step = dict(method=\"update\", args=[{\"visible\": [False] * len(cam_frustums) + [True] * 2}, {\"title\": \"Trajectory and Point Cloud\"},], label=timestamp,)\n",
    "    step[\"args\"][0][\"visible\"][i] = True  # Toggle i'th trace to \"visible\"\n",
    "    steps[i] = step\n",
    "cam_frustums[0].visible = True\n",
    "    \n",
    "# Filter the point cloud by inv depth and depth and load\n",
    "points = filter_points_from_confidence(points)\n",
    "# Retrieve point position\n",
    "point_cloud = np.stack([it.position_world for it in points])\n",
    "\n",
    "# Create slider to allow scrubbing and set the layout\n",
    "sliders = [dict(currentvalue={\"suffix\": \" s\", \"prefix\": \"Time :\"}, pad={\"t\": 5}, steps=steps,)]\n",
    "layout = go.Layout(sliders=sliders, scene=dict(bgcolor='lightgray', dragmode='orbit', aspectmode='data', xaxis_visible=False, yaxis_visible=False,zaxis_visible=False))\n",
    "\n",
    "# Plot trajectory and point cloud\n",
    "# We color the points by their z coordinate\n",
    "trajectory = go.Scatter3d(x=traj[:, 0], y=traj[:, 1], z=traj[:, 2], mode=\"markers\", marker={\"size\": 2, \"opacity\": 0.8, \"color\": \"red\"}, name=\"Trajectory\", hoverinfo='none')\n",
    "global_points = go.Scatter3d(x=point_cloud[:, 0], y=point_cloud[:, 1], z=point_cloud[:, 2], mode=\"markers\",\n",
    "    marker={\"size\" : 1.5, \"color\": point_cloud[:, 2], \"cmin\": -1.5, \"cmax\": 2, \"colorscale\": \"viridis\",},\n",
    "    name=\"Global Points\", hoverinfo='none')\n",
    "\n",
    "# draw\n",
    "plot_figure = go.Figure(data=cam_frustums + [trajectory, global_points], layout=layout)\n",
    "plot_figure.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f37caf18",
   "metadata": {},
   "source": [
    "## Visualize generalized and calibrated eye gaze projection on an rgb image.\n",
    "* Load Eyegaze MPS output\n",
    "* Select a random RGB frame\n",
    "* Find the closest eye gaze data for the RGB frame\n",
    "* Project the eye gaze for the RGB frame by **using a fixed depth of 1m** or existing depth if available.\n",
    "* Show the gaze cross on the RGB image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d31979e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_stream_id = StreamId(\"214-1\")\n",
    "rgb_stream_label = provider.get_label_from_stream_id(rgb_stream_id)\n",
    "num_rgb_frames = provider.get_num_data(rgb_stream_id)\n",
    "rgb_frame = provider.get_image_data_by_index(rgb_stream_id, (int)(num_rgb_frames-5))\n",
    "assert rgb_frame[0] is not None, \"no rgb frame\"\n",
    "\n",
    "image = rgb_frame[0].to_numpy_array()\n",
    "capture_timestamp_ns = rgb_frame[1].capture_timestamp_ns\n",
    "generalized_eye_gaze = get_nearest_eye_gaze(generalized_eye_gazes, capture_timestamp_ns)\n",
    "calibrated_eye_gaze = get_nearest_eye_gaze(calibrated_eye_gazes, capture_timestamp_ns)\n",
    "# get projection function\n",
    "device_calibration = provider.get_device_calibration()\n",
    "cam_calibration = device_calibration.get_camera_calib(rgb_stream_label)\n",
    "assert cam_calibration is not None, \"no camera calibration\"\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 10))\n",
    "\n",
    "# Draw a cross at the projected gaze center location on the RGB image at available depth or if unavailable a 1m proxy\n",
    "depth_m = generalized_eye_gaze.depth or 1.0\n",
    "generalized_gaze_center_in_pixels = get_gaze_vector_reprojection(generalized_eye_gaze, rgb_stream_label, device_calibration, cam_calibration, depth_m)\n",
    "if generalized_gaze_center_in_pixels is not None:\n",
    "    ax1.imshow(image)\n",
    "    ax1.plot(generalized_gaze_center_in_pixels[0], generalized_gaze_center_in_pixels[1], '+', c=\"red\", mew=1, ms=20)\n",
    "    ax1.grid(False)\n",
    "    ax1.axis(False)\n",
    "    ax1.set_title(\"Generalized Eye Gaze\")\n",
    "else:\n",
    "    print(f\"Eye gaze center projected to {generalized_gaze_center_in_pixels}, which is out of camera sensor plane.\")\n",
    "    \n",
    "depth_m = calibrated_eye_gaze.depth or 1.0\n",
    "calibrated_gaze_center_in_pixels = get_gaze_vector_reprojection(calibrated_eye_gaze, rgb_stream_label, device_calibration, cam_calibration, depth_m = 1.0)\n",
    "if calibrated_gaze_center_in_pixels is not None:\n",
    "    ax2.imshow(image)\n",
    "    ax2.plot(calibrated_gaze_center_in_pixels[0], calibrated_gaze_center_in_pixels[1], '+', c=\"red\", mew=1, ms=20)\n",
    "    ax2.grid(False)\n",
    "    ax2.axis(False)\n",
    "    ax2.set_title(\"Calibrated Eye Gaze\")\n",
    "else:\n",
    "    print(f\"Eye gaze center projected to {calibrated_gaze_center_in_pixels}, which is out of camera sensor plane.\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab51e157",
   "metadata": {},
   "source": [
    "## Visualize wrist and palm pose projection on RGB and SLAM images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57249d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional\n",
    "\n",
    "from projectaria_tools.core.calibration import CameraCalibration, DeviceCalibration\n",
    "from projectaria_tools.core.sensor_data import TimeDomain, TimeQueryOptions\n",
    "\n",
    "time_domain: TimeDomain = TimeDomain.DEVICE_TIME\n",
    "time_query_closest: TimeQueryOptions = TimeQueryOptions.CLOSEST\n",
    "\n",
    "# Get stream ids, stream labels, stream timestamps, and camera calibrations for RGB and SLAM cameras\n",
    "stream_ids: Dict[str, StreamId] = {\n",
    "    \"rgb\": StreamId(\"214-1\"),\n",
    "    \"slam-left\": StreamId(\"1201-1\"),\n",
    "    \"slam-right\": StreamId(\"1201-2\"),\n",
    "}\n",
    "stream_labels: Dict[str, str] = {\n",
    "    key: provider.get_label_from_stream_id(stream_id)\n",
    "    for key, stream_id in stream_ids.items()\n",
    "}\n",
    "stream_timestamps_ns: Dict[str, List[int]] = {\n",
    "    key: provider.get_timestamps_ns(stream_id, time_domain)\n",
    "    for key, stream_id in stream_ids.items()\n",
    "}\n",
    "cam_calibrations = {\n",
    "    key: device_calibration.get_camera_calib(stream_label)\n",
    "    for key, stream_label in stream_labels.items()\n",
    "}\n",
    "for key, cam_calibration in cam_calibrations.items():\n",
    "    assert cam_calibration is not None, f\"no camera calibration for {key}\"\n",
    "\n",
    "# Get device calibration and transform from device to sensor\n",
    "device_calibration = provider.get_device_calibration()\n",
    "\n",
    "\n",
    "def get_T_device_sensor(key: str):\n",
    "    return device_calibration.get_transform_device_sensor(stream_labels[key])\n",
    "\n",
    "\n",
    "# Get a sample frame for each of the RGB, SLAM left, and SLAM right streams\n",
    "sample_timestamp_ns: int = stream_timestamps_ns[\"rgb\"][120]\n",
    "sample_frames = {\n",
    "    key: provider.get_image_data_by_time_ns(\n",
    "        stream_id, sample_timestamp_ns, time_domain, time_query_closest\n",
    "    )[0]\n",
    "    for key, stream_id in stream_ids.items()\n",
    "}\n",
    "\n",
    "# Get the wrist and palm pose\n",
    "mps_data_paths_provider = mps.MpsDataPathsProvider(mps_sample_path)\n",
    "mps_data_paths = mps_data_paths_provider.get_data_paths()\n",
    "mps_data_provider = mps.MpsDataProvider(mps_data_paths)\n",
    "wrist_and_palm_pose = mps_data_provider.get_wrist_and_palm_pose(\n",
    "    sample_timestamp_ns, time_query_closest\n",
    ")\n",
    "\n",
    "# Helper functions for reprojection and plotting\n",
    "def get_point_reprojection(\n",
    "    point_position_device: np.array, key: str\n",
    ") -> Optional[np.array]:\n",
    "    point_position_camera = get_T_device_sensor(key).inverse() @ point_position_device\n",
    "    point_position_pixel = cam_calibrations[key].project(point_position_camera)\n",
    "    return point_position_pixel\n",
    "\n",
    "\n",
    "def get_wrist_and_palm_pixels(key: str) -> np.array:\n",
    "    left_wrist = get_point_reprojection(\n",
    "        wrist_and_palm_pose.left_hand.wrist_position_device, key\n",
    "    )\n",
    "    left_palm = get_point_reprojection(\n",
    "        wrist_and_palm_pose.left_hand.palm_position_device, key\n",
    "    )\n",
    "    right_wrist = get_point_reprojection(\n",
    "        wrist_and_palm_pose.right_hand.wrist_position_device, key\n",
    "    )\n",
    "    right_palm = get_point_reprojection(\n",
    "        wrist_and_palm_pose.right_hand.palm_position_device, key\n",
    "    )\n",
    "    return left_wrist, left_palm, right_wrist, right_palm\n",
    "\n",
    "\n",
    "def plot_wrists_and_palms(plt, left_wrist, left_palm, right_wrist, right_palm):\n",
    "    def plot_point(point, color):\n",
    "        plt.plot(*point, \".\", c=color, mew=1, ms=20)\n",
    "\n",
    "    if left_wrist is not None:\n",
    "        plot_point(left_wrist, \"blue\")\n",
    "    if left_palm is not None:\n",
    "        plot_point(left_palm, \"blue\")\n",
    "    if right_wrist is not None:\n",
    "        plot_point(right_wrist, \"red\")\n",
    "    if right_palm is not None:\n",
    "        plot_point(right_palm, \"red\")\n",
    "\n",
    "\n",
    "# Display wrist and palm positions on RGB, SLAM left, and SLAM right images\n",
    "plt.figure()\n",
    "rgb_image = sample_frames[\"rgb\"].to_numpy_array()\n",
    "plt.grid(False)\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(rgb_image)\n",
    "left_wrist, left_palm, right_wrist, right_palm = get_wrist_and_palm_pixels(\"rgb\")\n",
    "plot_wrists_and_palms(plt, left_wrist, left_palm, right_wrist, right_palm)\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(1, 2, 1)\n",
    "slam_left_image = sample_frames[\"slam-left\"].to_numpy_array()\n",
    "plt.grid(False)\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(slam_left_image, cmap=\"gray\", vmin=0, vmax=255)\n",
    "left_wrist, left_palm, right_wrist, right_palm = get_wrist_and_palm_pixels(\"slam-left\")\n",
    "plot_wrists_and_palms(plt, left_wrist, left_palm, right_wrist, right_palm)\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "slam_right_image = sample_frames[\"slam-right\"].to_numpy_array()\n",
    "plt.grid(False)\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(slam_right_image, interpolation=\"nearest\", cmap=\"gray\")\n",
    "left_wrist, left_palm, right_wrist, right_palm = get_wrist_and_palm_pixels(\"slam-right\")\n",
    "plot_wrists_and_palms(plt, left_wrist, left_palm, right_wrist, right_palm)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
